{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Remove all rows with duplicate fleet_number values.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"", "csv_path": "infiagent/csv/2.csv", "instruction": "Remove all rows with duplicate fleet_number values.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the dataset from csv\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# delete data rows with duplicate 'fleet_number'\ndf = df.drop_duplicates(subset=['fleet_number'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Remove all rows with duplicate fleet_number values\ndata = data.drop_duplicates(subset='fleet_number')\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Eliminate rows containing incomplete data.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Eliminate rows containing incomplete data.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Remove rows with any missing values\ndf = df.dropna()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove rows with any missing values\nclean_data = data.dropna()\n\nprint(clean_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Identify and delete duplicate rows from the table, if any.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Identify and delete duplicate rows from the table, if any.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Identify duplicate rows in the dataframe\nduplicates = df.duplicated()\n\n# Delete the duplicate rows\ndf = df[~duplicates]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Identify and delete duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Delete any duplicate rows in the table.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Delete any duplicate rows in the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv('infiagent/csv/4.csv')\n\n# Delete duplicate values\ndata = data.drop_duplicates()\n\nprint(data)", "assistant_code": "# Importing necessary library\nimport pandas as pd\n\n# Loading the data from csv\ndf = pd.read_csv('data.csv')\n\n# Removing duplicates\ndf = df.drop_duplicates()\n\n# Printing the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Eliminate rows where 'Target' is null or contains no value.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Eliminate rows where 'Target' is null or contains no value.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Filter out rows where 'Target' is null or empty\ndf = df[pd.notnull(df['Target'])]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Eliminate rows where 'Target' is null or contains no value\ndf = df.dropna(subset=['Target'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: If there are any duplicates in the source and target columns, keep only the first occurrence and discard the rest.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "If there are any duplicates in the source and target columns, keep only the first occurrence and discard the rest.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Remove duplicates from 'Source' and 'Target' columns\ndf.drop_duplicates(subset=['Source','Target'],keep='first',inplace=True)\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicates in the 'Source' and 'Target' columns\ndf = df.drop_duplicates(subset=['Source', 'Target'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Filter the table to exclude rows where the value in the 'Weight' column is greater than 1.5 times the Interquartile Range of the column.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Filter the table to exclude rows where the value in the 'Weight' column is greater than 1.5 times the Interquartile Range of the column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/5.csv')\n\n# Calculate IQR for 'Weight' column\nQ1 = data['Weight'].quantile(0.25)\nQ3 = data['Weight'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define bounds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Remove outliers\ndata = data[(data['Weight'] >= lower_bound) & (data['Weight'] <= upper_bound)]\n\n# Print the result\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the Interquartile Range of the 'Weight' column\nQ1 = df['Weight'].quantile(0.25)\nQ3 = df['Weight'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Filter the rows where the 'Weight' is greater than 1.5 * IQR\ndf_filtered = df[df['Weight'] <= 1.5 * IQR]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Identify and eliminate duplicate rows from the table.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Identify and eliminate duplicate rows from the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Identify and eliminate duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Delete all rows with missing values in the SCOREMARGIN column.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Delete all rows with missing values in the SCOREMARGIN column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Filter out the rows where SCOREMARGIN is null\ndf = df[df['SCOREMARGIN'].notna()]\n\nprint(df)", "assistant_code": "# Import the necessary libraries\nimport pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Delete all rows with missing values in the 'SCOREMARGIN' column\ndf = df.dropna(subset=['SCOREMARGIN'])\n\n# Print the processed DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Eliminate rows where the 'VISITORDESCRIPTION' field is left blank.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Eliminate rows where the 'VISITORDESCRIPTION' field is left blank.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Delete rows where 'VISITORDESCRIPTION' is empty\ndf = df.dropna(subset=['VISITORDESCRIPTION'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove rows where the 'VISITORDESCRIPTION' field is blank\ndf = df[df['VISITORDESCRIPTION'].notna()]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Remove any duplicate rows from the dataset.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Remove any duplicate rows from the dataset.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/7.csv')\n\n# Remove duplicate rows\ndata = data.drop_duplicates()\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Eliminate and remove the redundant records by identifying them.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Eliminate and remove the redundant records by identifying them.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# Importing the required libraries\nimport pandas as pd\n\n# Loading the data from the csv file\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Identifying and Deleting the duplicate records\ndata = data.drop_duplicates()\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Identify and remove entries where 'Height', 'Length' or 'Diameter' is not provided.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Identify and remove entries where 'Height', 'Length' or 'Diameter' is not provided.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Identify records where 'Height','Length' or 'Diameter' is missing\nmissing_records = df[['Height','Length','Diameter']].isnull().any(axis=1)\n\n# Delete records where 'Height','Length' or 'Diameter' is missing\ndf = df[~missing_records]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check for entries where 'Height', 'Length' or 'Diameter' is not provided \nmissing_entries = df.isnull(subset=['Height', 'Length', 'Diameter'])\n\n# Remove these entries\ndf = df[~missing_entries.any(axis=1)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Check and delete any duplicate recordings based on 'index' and 'timestamp' columns.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Check and delete any duplicate recordings based on 'index' and 'timestamp' columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file from the path given\ndata = pd.read_csv('infiagent/csv/9.csv')\n\n# Checking and deleting duplicate values based on 'index' and 'timestamp'\ndata.drop_duplicates(subset=['index','timestamp'],keep=False,inplace=True)\n\n# Output the resulting dataframe\nprint(data)", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check and delete duplicates based on 'index' and 'timestamp'\ndf.drop_duplicates(subset=['index', 'timestamp'], keep='first', inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Delete any rows that have missing values in any of the emotion columns.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Delete any rows that have missing values in any of the emotion columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Delete rows with missing values in emotion columns\nemotion_columns = ['anger','contempt','disgust','fear','happiness','neutral','sadness','surprise']\ndf = df.dropna(subset=emotion_columns)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Define the emotion columns\nemotion_columns = ['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise', 'blur']\n\n# Drop the rows with missing values in any of the emotion columns\ndata = data.dropna(subset=emotion_columns)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Remove all duplicate rows from the table based on the 'name' column.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Remove all duplicate rows from the table based on the 'name' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Remove duplicate rows based on the 'name' column\ndf = df.drop_duplicates(subset=['name'])\n\n# Print the resulting DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicates based on 'name' column\ndf = df.drop_duplicates(subset='name')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Eliminate rows with matching 'Category' and 'Offense-Type' combinations from the database.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Eliminate rows with matching 'Category' and 'Offense-Type' combinations from the database.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load csv file into DataFrame\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Remove duplicate rows based on 'Category' and 'Offense_Type' columns\ndf = df.drop_duplicates(subset=['Category','Offense_Type'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Identify and remove rows with matching 'Category' and 'Offense_Type' combinations\ndf = df.drop_duplicates(subset=['Category', 'Offense_Type'])\n\n# Print the processed dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Remove all rows where the 'Age' column is missing.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Remove all rows where the 'Age' column is missing.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Delete rows where the 'Age' column is missing\ndf = df.dropna(subset=['Age'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove all rows where the 'Age' column is missing\ndf = df.dropna(subset=['Age'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Eliminate duplicate rows in the data set based on the 'weight' column.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Eliminate duplicate rows in the data set based on the 'weight' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# delete duplicate rows based on 'weight'\ndf = df.drop_duplicates(subset='weight')\n\n# print the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Eliminate duplicate rows based on the 'weight' column\ndf = df.drop_duplicates(subset='weight')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Identify and delete any duplicate entries from the table based on the 'NAME' and 'JOBTITLE' columns.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Identify and delete any duplicate entries from the table based on the 'NAME' and 'JOBTITLE' columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Pre-process the data - drop duplicates based on 'NAME' and 'JOBTITLE' columns\ndf = df.drop_duplicates(subset=['NAME','JOBTITLE'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Identify and delete duplicate entries based on 'NAME' and 'JOBTITLE'\ndf = df.drop_duplicates(subset=['NAME', 'JOBTITLE'])\n\n# Print the processed dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Exclude rows where 'Gross' data is missing.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Exclude rows where 'Gross' data is missing.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Drop rows where 'Gross' data is missing\ndf = df.dropna(subset=['Gross'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('data.csv')\n\n# Exclude rows where 'Gross' data is missing\ndata = data.dropna(subset=['Gross'])\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Remove any duplicates in the dataset.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Remove any duplicates in the dataset.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load csv file\ndf = pd.read_csv('infiagent/csv/13.csv')\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Loading the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Removing duplicates from the dataset\ndf = df.drop_duplicates()\n\n# Printing the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Remove all rows where 'AT' (Air Temperature) is missing.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Remove all rows where 'AT' (Air Temperature) is missing.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Remove rows where 'AT' is missing\ncleaned_data = data.dropna(subset=['AT'])\n\nprint(cleaned_data)", "assistant_code": "# import pandas library\nimport pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Remove all rows where 'AT' (Air Temperature) is missing.\ndf = df.dropna(subset=['AT'])\n\n# Print the resulting dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Delete rows where the batting average is NaN or 0.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Delete rows where the batting average is NaN or 0.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Remove rows where batting_average is NaN or 0\ndf = df.dropna(subset=['batting_average'])\ndf = df[df['batting_average'] != 0]\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where the batting average is NaN or 0\ndf = df[(df['batting_average'].notna()) & (df['batting_average'] != 0)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Delete any duplicate rows in the table based on all columns.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Delete any duplicate rows in the table based on all columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Removing duplicate entries\ndf = df.drop_duplicates()\n\n# Print the processed dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndata = data.drop_duplicates()\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Eliminate any repetitive records.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Eliminate any repetitive records.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Remove duplicates rows\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove duplicate rows from the data\ndata = data.drop_duplicates()\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Eliminate any duplicate rows within the table that share the same values in the 'wage' column.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Eliminate any duplicate rows within the table that share the same values in the 'wage' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/15.csv\")\n\n# Delete duplicate values based on 'wage' column\ndf = df.drop_duplicates(subset='wage')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Eliminate duplicate rows based on 'wage' column\ndf = df.drop_duplicates(subset='wage')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Remove any duplicate rows based on the 'Date' column.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Remove any duplicate rows based on the 'Date' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove duplicate rows based on 'Date' column\ndata.drop_duplicates(subset='Date',inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows based on 'Date' column\ndf = df.drop_duplicates(subset='Date')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Eliminate any repetitive rows, if present.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Eliminate any repetitive rows, if present.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove repetitive rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Delete rows that contain missing values for all columns in the table.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Delete rows that contain missing values for all columns in the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/20.csv')\n\n# Remove rows with any missing Value\nclean_data = data.dropna()\n\n# print the cleaned data\nprint(clean_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows that contain missing values for all columns\ndf.dropna(how='all', inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Remove duplicate rows from the data.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Remove duplicate rows from the data.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\n# Output the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Remove all rows where date is null.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Remove all rows where date is null.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Remove all rows where date is null\ndf = df[df['date'].notna()]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove all rows where date is null\ndata = data.dropna(subset=['date'])\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Remove all rows that have duplicate information in the table.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Remove all rows that have duplicate information in the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Delete any duplicate rows in the table\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Eliminate records with null 'number_of_dependents' to enhance data analysis precision.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Eliminate records with null 'number_of_dependents' to enhance data analysis precision.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Removing all records with 'number_of_dependents' as null\ndf = df[df['number_of_dependents'].notna()]\n\nprint(df)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Eliminate records with null 'number_of_dependents'\ndf = df.dropna(subset=['number_of_dependents'])\n\n# Print the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Eliminate any redundant records by virtue of 'revolving_utilization_of_unsecured_lines' and 'number_of_open_credit_lines_and_loans' being equal.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Eliminate any redundant records by virtue of 'revolving_utilization_of_unsecured_lines' and 'number_of_open_credit_lines_and_loans' being equal.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/23.csv')\n\n# Remove any duplicate records based on 'revolving_utilization_of_unsecured_lines' and 'number_of_open_credit_lines_and_loans'.\ndata = data.drop_duplicates(subset=['revolving_utilization_of_unsecured_lines','number_of_open_credit_lines_and_loans'])\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Drop the duplicates based on 'revolving_utilization_of_unsecured_lines' and 'number_of_open_credit_lines_and_loans'\ndf = df.drop_duplicates(subset=['revolving_utilization_of_unsecured_lines', 'number_of_open_credit_lines_and_loans'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Delete records where both 'monthly_income' and 'number_of_open_credit_lines_and_loans' are NULL to clean the data.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Delete records where both 'monthly_income' and 'number_of_open_credit_lines_and_loans' are NULL to clean the data.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Loading the CSV data\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Deleting rows where both 'monthly_income' and 'number_of_open_credit_lines_and_loans' are null\ndf = df.dropna(subset=['monthly_income','number_of_open_credit_lines_and_loans'],how='all')\n\n# Print the cleaned data\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete records where both 'monthly_income' and 'number_of_open_credit_lines_and_loans' are NULL \ndf = df.dropna(subset=['monthly_income', 'number_of_open_credit_lines_and_loans'], how='all')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Remove duplicate rows with identical values in the 'Income' column and keep the original table structure.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Remove duplicate rows with identical values in the 'Income' column and keep the original table structure.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Drop duplicate rows based on 'Income'\ndf = df.drop_duplicates(subset='Income')\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows based on 'Income' column\ndf = df.drop_duplicates(subset=['Income'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Delete all rows where 'Ethnicity' field contains white spaces at the start or end.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Delete all rows where 'Ethnicity' field contains white spaces at the start or end.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/24.csv')\n\n# Remove leading and trailing whitespaces from 'Ethnicity' field\ndata = data[data['Ethnicity'].str.strip() == data['Ethnicity']]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Strip white spaces from 'Ethnicity' field\ndf['Ethnicity'] = df['Ethnicity'].str.strip()\n\n# Delete rows where 'Ethnicity' field is null after stripping white spaces\ndf = df[df['Ethnicity'].notna()]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Remove duplicate rows from the entire dataset.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Remove duplicate rows from the entire dataset.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the dataset from the csv file\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Perform duplicate removal\ndf = df.drop_duplicates()\n\n# print resulted dataframe \nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Delete all rows where the value for 'WEIGHTING' is not available.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Delete all rows where the value for 'WEIGHTING' is not available.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Remove rows where 'WEIGHTING' is missing\ncleaned_data = data.dropna(subset=['WEIGHTING'])\n\nprint(cleaned_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete all rows where the value for 'WEIGHTING' is not available\ndf = df.dropna(subset=['WEIGHTING'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Eliminate all rows where the Calendar Date (TDB) exhibits duplicate values.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Eliminate all rows where the Calendar Date (TDB) exhibits duplicate values.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Remove duplicates based on Calendar Date (TDB)\ndf = df.drop_duplicates(subset='Calendar Date (TDB)')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows based on 'Calendar Date (TDB)'\ndf = df.drop_duplicates(subset=['Calendar Date (TDB)'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Remove the duplicate records based on \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Remove the duplicate records based on \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\".", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Remove duplicate records based on specified columns\ndf = df.drop_duplicates(subset=[\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicates\ndf = df.drop_duplicates(subset=[\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\"])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Remove rows with empty 'Volume' values.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Remove rows with empty 'Volume' values.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# delete rows where 'Volume' has null values\ndf = df[df['Volume'].notna()]\n\nprint(df)", "assistant_code": "# necessary library for data manipulation\nimport pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Remove rows with empty 'Volume' values\ndata = data.dropna(subset=['Volume'])\n\n# Print the result\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Remove any repeated values in the 'Date' column.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Remove any repeated values in the 'Date' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv(\"infiagent/csv/29.csv\")\n\n# Drop duplicate entries in the 'Date' column\ndf = df.drop_duplicates(subset='Date')\n\n# prepares the final result\nresult = df\n\nprint(result)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# remove repeated values in the 'Date' column\ndf = df.drop_duplicates(subset='Date')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Identify and remove any duplicate rows based on all columns.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Identify and remove any duplicate rows based on all columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Identify and remove any duplicate rows based on all columns.\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Identify and remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Eliminate the rows possessing identical values in the 'EQ3' column.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Eliminate the rows possessing identical values in the 'EQ3' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Remove rows with duplicate values in the 'EQ3' column\ndf = df.drop('EQ3',axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows with duplicate values in the 'EQ3' column\ndf = df.drop_duplicates(subset=['EQ3'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Eliminate duplicate columns in the table.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Eliminate duplicate columns in the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Remove duplicate rows in the DataFrame\ndf.drop_duplicates(inplace=True)\n\n# Print the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate columns\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Filter out the rows where the value of cases_min is not available.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Filter out the rows where the value of cases_min is not available.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Remove rows where the number of cases_min is missing\ndf = df[df['No. of cases_min'].notnull()]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter out the rows where the value of cases_min is not available\ndata = data.dropna(subset=['No. of cases_min'])\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Delete all records where the values for both 'No. of cases' and 'No. of deaths' are 0.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Delete all records where the values for both 'No. of cases' and 'No. of deaths' are 0.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Delete records where both 'No. of cases' and 'No. of deaths' are 0\ndf = df[~((df['No. of cases'] == '0') & (df['No. of deaths'] == '0'))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('data.csv')\n\n# Delete rows where 'No. of cases' and 'No. of deaths' are both 0\ndata = data[(data['No. of cases'] != 0) | (data['No. of deaths'] != 0)]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Remove any duplicate rows present in the data.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Remove any duplicate rows present in the data.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Remove any duplicate rows present in the data\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Exclude records with 'abc-news' in the 'url' field and 'source' not equal to 'abc news'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Exclude records with 'abc-news' in the 'url' field and 'source' not equal to 'abc news'.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# Filter out the unwanted records and keep only the valid records\nfiltered_data = data[~((data['url'].str.contains('abc-news')) & (data['source'] != 'abc-news'))]\n\n# Output the canonical data\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_data = data[(data['url'].str.contains('abc-news')) & (data['source'] != 'abc news')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Eliminate duplicate entries based on 'url' and 'publishedAt' columns.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Eliminate duplicate entries based on 'url' and 'publishedAt' columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# Identify and remove duplicate records based on 'url' and 'publishedAt'\ndata = data.drop_duplicates(subset=['url','publishedAt'])\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicates based on 'url' and 'publishedAt' columns\ndf = df.drop_duplicates(subset=['url', 'publishedAt'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Filter out rows with NULL values in 'positive_diffsel' and 'negative_diffsel' columns.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Filter out rows with NULL values in 'positive_diffsel' and 'negative_diffsel' columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load in the data csv\ndata = pd.read_csv('infiagent/csv/34.csv')\n\n# Remove any rows with missing values in the 'positive_diffsel' and 'negative_diffsel' columns\ndata = data.dropna(subset=['positive_diffsel','negative_diffsel'])\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter out rows with NULL values in 'positive_diffsel' and 'negative_diffsel' columns.\ndata = data.dropna(subset=['positive_diffsel', 'negative_diffsel'])\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Eliminate duplicate rows in the dataset considering all columns.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Eliminate duplicate rows in the dataset considering all columns.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# importing necessary library\nimport pandas as pd\n\n# loading the csv data\ndata = pd.read_csv('infiagent/csv/35.csv')\n\n# preprocessing the data by removing duplicates\nclean_data = data.drop_duplicates()\n\n# print the cleaned data\nprint(clean_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndata = data.drop_duplicates()\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Eliminate any duplicate entries that may exist, based on the country name.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Eliminate any duplicate entries that may exist, based on the country name.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# necessary libraries\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# remove duplicates\ndf = df.drop_duplicates(subset='country')\n\n# print the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove duplicate rows based on the 'country' column\ndata.drop_duplicates(subset='country', keep='first', inplace=True)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Filter out any outliers in the data, consider any GDP per capita above $50,000 or below $300 for the year 2007 as outliers and remove them from the dataset.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Filter out any outliers in the data, consider any GDP per capita above $50,000 or below $300 for the year 2007 as outliers and remove them from the dataset.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# import the necessary libraries\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Filter out the outliers by keeping only the rows where gdpPercap_2007 is within the specified range\ndf = df[(df['gdpPercap_2007'] >= 300) & (df['gdpPercap_2007'] <= 50000)]\n\n# Output the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data, remove rows where gdpPercap_2007 is above 50000 or below 300\ndf = df[(df['gdpPercap_2007'] >= 300) & (df['gdpPercap_2007'] <= 50000)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Delete the records that contain duplicate 'transcript_id'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Delete the records that contain duplicate 'transcript_id'.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Remove the records that contain duplicate 'transcript_id'\ndf = df.drop_duplicates(subset=['transcript_id'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Delete the records that contain duplicate 'transcript_id'\ndf = df.drop_duplicates(subset='transcript_id')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Remove any rows that have missing data.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Remove any rows that have missing data.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the data from csv\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# remove any rows that have missing data\ndf = df.dropna()\n\nprint(df)", "assistant_code": "# import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# read the csv file\ndf = pd.read_csv('data.csv')\n\n# remove rows with missing data\ndf = df.dropna()\n\n# print the processed dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Remove rows where 'Weight' or 'Height' fields have -99.0 as these might represent missing or incorrect data.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Remove rows where 'Weight' or 'Height' fields have -99.0 as these might represent missing or incorrect data.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Read the csv file\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Remove rows where 'Weight' or 'Height' fields have -99.0\ndf = df[(df['Weight'] != -99.0) & (df['Height'] != -99.0)]\n\n# Print the processed data\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where 'Weight' or 'Height' fields have -99.0\ndf = df[(df['Weight'] != -99.0) & (df['Height'] != -99.0)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Delete the duplicate rows in the table if any.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Delete the duplicate rows in the table if any.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load data from csv\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Delete duplicate rows\ndf.drop_duplicates(inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# remove duplicates\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Retain only the rows where the 'standard_indentification_level_1' column is not empty.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Retain only the rows where the 'standard_indentification_level_1' column is not empty.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load data from CSV file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Remove rows where 'standard_indentification_level_1' column is empty\ndf = df.dropna(subset=['standard_indentification_level_1'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe where 'standard_indentification_level_1' column is not empty\ndf = df[df['standard_indentification_level_1'].notna()]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Find any duplicate rows based on all columns, keep only one copy of such rows and delete the rest.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Find any duplicate rows based on all columns, keep only one copy of such rows and delete the rest.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from CSV file\ndata = pd.read_csv('infiagent/csv/43.csv')\n\n# Remove duplicate rows\ndata_no_duplicates = data.drop_duplicates()\n\nprint(data_no_duplicates)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Delete any rows in the table that have duplicate values on the \"Date\" column.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Delete any rows in the table that have duplicate values on the \"Date\" column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Remove duplicated rows based on \"Date\" column\ndf = df.drop_duplicates(subset='Date')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate dates\ndf = df.drop_duplicates(subset='Date')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Remove duplicate entries (same values in all columns) from the table.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Remove duplicate entries (same values in all columns) from the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Remove duplicate entries\ndf = df.drop_duplicates()\n\n# Output the cleaned data\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Eliminate repetitive rows from the table by verifying each record.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Eliminate repetitive rows from the table by verifying each record.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Remove duplicate rows\ndf.drop_duplicates(inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Exclude all records with exceptional 'Value' column values.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Exclude all records with exceptional 'Value' column values.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Compute IQR\nQ1 = df['Value'].quantile(0.25)\nQ3 = df['Value'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Suppress outliers\ndf = df[~((df['Value'] < (Q1 - 1.5 * IQR)) | (df['Value'] > (Q3 + 1.5 * IQR)))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Identifying exceptional values in the 'Value' column\nexceptional_values = df['Value'].quantile([0.025, 0.975])  # 2.5% and 97.5% quantiles\n\n# Exclude the rows with exceptional 'Value' column values\ndf_excluded = df[(df['Value'] >= exceptional_values[0]) & (df['Value'] <= exceptional_values[1])]\n\nprint(df_excluded)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Delete all rows with 'FREQUENCY' and 'A' where the 'Flag Codes' column is NULL.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Delete all rows with 'FREQUENCY' and 'A' where the 'Flag Codes' column is NULL.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Identify rows with Frequency A and Null values in Flag Codes column\n\ndf = df.loc[~((df['FREQUENCY'] == 'A') & (df['Flag Codes'].isnull())),:]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Delete all rows with 'FREQUENCY' and 'A' where the 'Flag Codes' column is NULL\ndata = data[~((data['FREQUENCY'] == 'A') & (data['Flag Codes'].isnull()))]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Remove any duplicate entries from the table, if present.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Remove any duplicate entries from the table, if present.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Remove duplicate entries\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate entries\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Remove any potential duplicate rows from the table.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Remove any potential duplicate rows from the table.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Remove any potential duplicate rows from the table\ndf = df.drop_duplicates()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate rows\ndf = df.drop_duplicates()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Delete rows where there are missing values in any column.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Delete rows where there are missing values in any column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "# Importing necessary library\nimport pandas as pd\n\n# Loading the dataset\ndf = pd.read_csv(\"infiagent/csv/49.csv\")\n\n# Removing rows with missing or null values\ndf = df.dropna()\n\n# Printing the processed DataFrame\nprint(df)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# delete rows where there are missing values in any column\ndf = df.dropna()\n\n# print the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Eliminate any duplicate values in the 'city' column.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Eliminate any duplicate values in the 'city' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load data from csv\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Remove duplicate values in the 'city' column\ndf.drop_duplicates(subset=['city'],keep='first',inplace=True)\n\nprint(df)", "assistant_code": "# Required Libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Eliminate duplicates in 'city' column\ndf.drop_duplicates(subset='city', keep='first', inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Eliminate duplicate entries in the 'temp' column.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Eliminate duplicate entries in the 'temp' column.", "instruction_type": "Delete-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Remove duplicate values in the 'temp' column\ndf = df.drop_duplicates(subset='temp')\n\n# Output the processed dataframe\nprint(df)", "assistant_code": "# Required Libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove duplicate values in 'temp' column\ndf = df.drop_duplicates(subset=['temp'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Delete the column 'Viscera weight'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Delete the column 'Viscera weight'.", "instruction_type": "Delete-Data Processing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Drop the 'Viscera weight' column\ndata = data.drop('Viscera weight',axis=1)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete the 'Viscera weight' column\ndf = df.drop(['Viscera weight'], axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Exclude records with significant outliers in the 'cnt' column from the dataset.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Exclude records with significant outliers in the 'cnt' column from the dataset.", "instruction_type": "Delete-Data Processing", "reference_code": "import pandas as pd\n\n# load the dataset\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Calculate the IQR of cnt\nQ1 = df['cnt'].quantile(0.25)\nQ3 = df['cnt'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define the limits for outliers\nlower_limit = Q1 - 1.5 * IQR\nupper_limit = Q3 + 1.5 * IQR\n\n# Remove outliers from the data\ndf_filtered = df[(df['cnt'] >= lower_limit) & (df['cnt'] <= upper_limit)]\n\nprint(df_filtered)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Identify the outliers in the 'cnt' column\nQ1 = df['cnt'].quantile(0.25)\nQ3 = df['cnt'].quantile(0.75)\nIQR = Q3 - Q1\nfilter = (df['cnt'] >= Q1 - 1.5 * IQR) & (df['cnt'] <= Q3 + 1.5 *IQR)\n\n# Apply the filter to exclude the outliers\ndf = df.loc[filter] \n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Remove rows where the length of gloss is below 10.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Remove rows where the length of gloss is below 10.", "instruction_type": "Delete-Data Processing", "reference_code": "import pandas as pd\n\n# Read the data from the csv file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Remove the rows where length of the gloss is less than 10\ndf = df[df[\"gloss\"].str.len() >= 10]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove rows where the length of gloss is below 10\ndata = data[data['gloss'].str.len() >= 10]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Delete the row with the highest ID value.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Delete the row with the highest ID value.", "instruction_type": "Delete-Data Processing", "reference_code": "# import the necessary libraries\nimport pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Find the row with the maximum ID number\nmax_id = df['ID'].idxmax()\n\n# Delete the row with the maximum ID\ndf = df.drop(max_id)\n\n# Display the final dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Find the row with the highest ID value\nmax_id_row = df[df['ID'] == df['ID'].max()]\n\n# Delete the row with the highest ID value\ndf = df.drop(max_id_row.index)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Group the data by 'Ethnicity' and eliminate any Groups with less than 5 rows.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Group the data by 'Ethnicity' and eliminate any Groups with less than 5 rows.", "instruction_type": "Delete-Grouping", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# group by 'Ethnicity'\ngrouped = df.groupby('Ethnicity')\n\n# filter groups with less than 5 rows\nfiltered_df = grouped.filter(lambda x: len(x) > 5)\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'Ethnicity' and filter groups with less than 5 rows\ndf = df.groupby('Ethnicity').filter(lambda x: len(x) >= 5)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Aggregate the data based on 'Calendar Date (TDB)' and remove groups with less than 5 rows.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Aggregate the data based on 'Calendar Date (TDB)' and remove groups with less than 5 rows.", "instruction_type": "Delete-Grouping", "reference_code": "import pandas as pd\n\n# loading the data\ndata = pd.read_csv('infiagent/csv/27.csv')\n\n# grouping the data by 'Calendar Date (TDB)' and filtering our groups containing less than 5 rows\ngrouped = data.groupby('Calendar Date (TDB)').filter(lambda x: len(x) >= 5)\n\nprint(grouped)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Aggregate the data based on 'Calendar Date (TDB)' and remove groups with less than 5 rows\ndf = df.groupby('Calendar Date (TDB)').filter(lambda x: len(x) >= 5)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Filter the table by rows where 'Total Annual Memberships Sold' is greater than or equal to 125000.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Filter the table by rows where 'Total Annual Memberships Sold' is greater than or equal to 125000.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Remove the rows where 'Total Annual Memberships Sold' is less than 125000\ndf = df[df['Total Annual Memberships Sold']>=125000]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Filter the DataFrame to include only rows where 'Total Annual Memberships Sold' is greater than or equal to 125000\nfiltered_df = df[df['Total Annual Memberships Sold'] >= 125000]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Clear the table except for rows with station type 'Passenger'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Clear the table except for rows with station type 'Passenger'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Remove all data where station_type is not 'Passenger'\ndf = df[df['station_type'] == 'Passenger']\n\n# Print the final DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Keep only the rows where station_type is 'Passenger'\ndf = df[df['station_type'] == 'Passenger']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Delete all rows from the table where the 'start_time' is later than the 'end_time'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Delete all rows from the table where the 'start_time' is later than the 'end_time'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Conversion of 'start_time' and 'end_time' to datetime\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\n\n# Deleting rows where 'start_time' is later than 'end_time'\ndf = df[df['start_time'] <= df['end_time']]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Convert the 'start_time' and 'end_time' columns to datetime\ndata['start_time'] = pd.to_datetime(data['start_time'])\ndata['end_time'] = pd.to_datetime(data['end_time'])\n\n# Delete rows where 'start_time' is later than 'end_time'\ndata = data[data['start_time'] <= data['end_time']]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Eliminate the rows from the table that have a 'Happiness Score' lower than 7.0.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Eliminate the rows from the table that have a 'Happiness Score' lower than 7.0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Remove rows where 'Happiness Score' is less than 7.0\ndf = df[df['Happiness Score'] >= 7.0]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Eliminate the rows with 'Happiness Score' less than 7.0\ndf = df[df['Happiness Score'] >= 7.0]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Delete records from the table where 'Economy (GDP per Capita)' is less than the average 'Economy (GDP per Capita)' for all countries.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Delete records from the table where 'Economy (GDP per Capita)' is less than the average 'Economy (GDP per Capita)' for all countries.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Calculate the average of 'Economy (GDP per Capita)'\naverage_economy = df['Economy (GDP per Capita)'].mean()\n\n# Delete records from the table where 'Economy (GDP per Capita)' is lower than average_economy\ndf = df[df['Economy (GDP per Capita)'] >= average_economy]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'Economy (GDP per Capita)'\naverage_gdp = df['Economy (GDP per Capita)'].mean()\n\n# Delete records where 'Economy (GDP per Capita)' is less than the average\ndf = df[df['Economy (GDP per Capita)'] >= average_gdp]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Delete all rows where \"num. calls transferred\" is zero.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Delete all rows where \"num. calls transferred\" is zero.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(\"infiagent/csv/4.csv\")\n\n# Remove all records where 'num. calls transferred' is zero\ndf = df[df[\"num. calls transferred\"] != 0]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'num. calls transferred' is zero\ndf = df[df['num. calls transferred'] != 0]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Exclude rows with 'num. busy overflows' exceeding 10.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Exclude rows with 'num. busy overflows' exceeding 10.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\nfile_path = 'infiagent/csv/4.csv'\ndata = pd.read_csv(file_path)\n\n# Delete outliers where 'num. busy overflows' is greater than 10\ndata = data[data['num. busy overflows'] <= 10]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Exclude rows with 'num. busy overflows' exceeding 10\ndf = df[df['num. busy overflows'] <= 10]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Delete rows where \"num. calls abandoned\" is more than 'num. calls answered'.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Delete rows where \"num. calls abandoned\" is more than 'num. calls answered'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# delete rows where 'num. calls abandoned' > 'num. calls answered'\ndf = df[df['num. calls abandoned'] <= df['num. calls answered']]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where \"num. calls abandoned\" is more than 'num. calls answered'\ndf = df[df['num. calls abandoned'] <= df['num. calls answered']]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Delete the rows where 'Type' is not 'Directed'.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Delete the rows where 'Type' is not 'Directed'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Filter the data,keeping only rows where 'Type' is 'Directed'\ndf = df[df['Type'] == 'Directed']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete the rows where 'Type' is not 'Directed'\ndf = df[df['Type'] == 'Directed']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Remove all rows where 'Weight' is less than 5.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Remove all rows where 'Weight' is less than 5.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Filter and remove all rows where 'Weight' is less than 5\ndf = df[df['Weight'] >= 5]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove rows where 'Weight' is less than 5\ndf = df[df['Weight'] >= 5]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Filter the rows where 'Source' is not equal to 0 and 'Weight' is greater than or equal to 10.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Filter the rows where 'Source' is not equal to 0 and 'Weight' is greater than or equal to 10.", "instruction_type": "Delete-Screening", "reference_code": "# import necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Filter the data\ndf = df[~((df['Source'] == 0) & (df['Weight'] < 10))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the rows where 'Source' is not equal to 0 and 'Weight' is greater than or equal to 10\nfiltered_df = df[(df['Source'] != 0) & (df['Weight'] >= 10)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Delete the rows where the 'MEANGAM' is less than 21.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Delete the rows where the 'MEANGAM' is less than 21.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Filter out the rows where 'MEANGAM' is less than 21\ndf = df[df['MEANGAM'] >= 21]\n\n# Print the resultant dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'MEANGAM' is less than 21\ndf = df[df['MEANGAM'] >= 21]\n\n# Print the resulting dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Delete rows where 'TRUE_TIME' year is not 2014.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Delete rows where 'TRUE_TIME' year is not 2014.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/6.csv')\n\n# Convert TRUE_TIME to datetime\ndata['TRUE_TIME'] = pd.to_datetime(data['TRUE_TIME'],format='%Y.%m.%d_%H:%M:%S_TAI')\n\n# Filter data for year 2014\ndata = data[data['TRUE_TIME'].dt.year == 2014]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'TRUE_TIME' to datetime format\ndf['TRUE_TIME'] = pd.to_datetime(df['TRUE_TIME'])\n\n# Filter the data where 'TRUE_TIME' year is 2014\ndf = df[df['TRUE_TIME'].dt.year == 2014]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Clear all table records where the visitor description includes the term 'turnover'.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Clear all table records where the visitor description includes the term 'turnover'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/7.csv')\n\n# Filter out records where the visitor description contains 'turnover'\ndata = data[~data['VISITORDESCRIPTION'].str.contains('turnover',na=False)]\n\n# Print the data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove the rows where the visitor description includes the term 'turnover'\ndf = df[~VISITORDESCRIPTION.str.contains('turnover', na=False)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Remove any rows from the dataset where the GAME_ID appears less than 5 times.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Remove any rows from the dataset where the GAME_ID appears less than 5 times.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Count the occurrence of each GAME_ID\ngame_id_counts = df['GAME_ID'].value_counts()\n\n# Identify GAME_IDs that occur less than 5 times\nless_than_5 = game_id_counts[game_id_counts < 5].index\n\n# Remove rows where GAME_ID occurs less than 5 times\ndf = df[~df['GAME_ID'].isin(less_than_5)]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of times each GAME_ID appears\ngame_id_counts = df['GAME_ID'].value_counts()\n\n# Filter out the GAME_IDs that appear less than 5 times\ndf = df[df['GAME_ID'].isin(game_id_counts[game_id_counts >= 5].index)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Remove the entries where the gender is 'I'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Remove the entries where the gender is 'I'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Filter out the rows where Sex is 'I'\ndf = df[df['Sex'] != 'I']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove the entries where the gender is 'I'\ndf = df[df['Sex'] != 'I']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Delete records where the 'Height' is more than the 95th percentile of the height distribution.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Delete records where the 'Height' is more than the 95th percentile of the height distribution.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Calculate the 95th percentile for 'Height'\nheight_95th_percentile = df['Height'].quantile(0.95)\n\n# Filter out records where 'Height' is more than the 95th percentile\ndf = df[df['Height'] <= height_95th_percentile]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the 95th percentile of the 'Height' distribution\nheight_95_percentile = df['Height'].quantile(0.95)\n\n# Filter the dataframe to keep only records where 'Height' is less than or equal to the 95th percentile\ndf_filtered = df[df['Height'] <= height_95_percentile]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Eliminate rows with blur values exceeding 0.5 to enhance overall data precision.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Eliminate rows with blur values exceeding 0.5 to enhance overall data precision.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data from csv file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# delete rows where blur values are more than 0.5\ndf = df[df['blur'] <= 0.5]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Delete rows where the 'blur' value is greater than 0.5\ndata = data[data['blur'] <= 0.5]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Filter the table to exclude rows where 'Expungible' is False and 'Offense' contains 'Assault'", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Filter the table to exclude rows where 'Expungible' is False and 'Offense' contains 'Assault'", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided path\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Remove all entries where 'Expungible' is False and 'Offense' contains 'Assault'\ndf = df[~((df['Expungible'] == False) & (df['Offense'].str.contains('Assault')))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\ndf_filtered = df[(df['Expungible'] == True) & (df['Offense'].str.contains('Assault'))]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Delete all rows from the vehicles table where the value of the 'cylinders' column is less than 5.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Delete all rows from the vehicles table where the value of the 'cylinders' column is less than 5.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Delete records where 'cylinders' are less than 5\ndf = df[df['cylinders'] >= 5]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'cylinders' is less than 5\ndf = df[df['cylinders'] >= 5]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Delete all records where 'modelyear' is less than 1975 and 'origin' is 3.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Delete all records where 'modelyear' is less than 1975 and 'origin' is 3.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Filter the data where 'modelyear' is before 75 and 'origin' is 3\ndf = df[~((df['modelyear'] < 75) & (df['origin'] == 3))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Deleting all records where 'modelyear' is less than 1975 and 'origin' is 3\ndf = df[~((df['modelyear'] < 1975) & (df['origin'] == 3))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Eliminate rows where 'ANNUAL_RT' is less than 'Gross'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Eliminate rows where 'ANNUAL_RT' is less than 'Gross'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Check if 'ANNUAL_RT' and 'Gross' are not numeric,if so convert them\nif df['ANNUAL_RT'].dtype != 'float':\n    df['ANNUAL_RT'] = df['ANNUAL_RT'].str.replace(',','').astype(float)\n\nif df['Gross'].dtype != 'float':\n    df['Gross'] = df['Gross'].str.replace(',','').astype(float)\n\n# Remove entries where 'ANNUAL_RT' is less than 'Gross'\ndf = df[df['ANNUAL_RT'] >= df['Gross']]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'ANNUAL_RT' is less than 'Gross'\ndf = df[df['ANNUAL_RT'] >= df['Gross']]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Remove the rows where 'GUSTS' is greater than 10 and the 'DIR' is greater than 200.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Remove the rows where 'GUSTS' is greater than 10 and the 'DIR' is greater than 200.", "instruction_type": "Delete-Screening", "reference_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data from a CSV file\ndf = pd.read_csv('infiagent/csv/13.csv')\n\n# Remove trailing and leading whitespaces from column names\ndf.columns = df.columns.str.strip()\n\n# Delete rows where 'GUSTS' is above 10 and 'DIR' is above 200\ndf = df[~((df['GUSTS'] > 10) & (df['DIR'] > 200))]\n\n# Print the resulting DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove the rows where 'GUSTS' is greater than 10 and the 'DIR' is greater than 200\ndf = df[(df['GUSTS'] <= 10) & (df['DIR'] <= 200)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Eliminate the rows where both the indicator of free agency eligibility and the indicator of arbitration eligibility are equal to 0.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Eliminate the rows where both the indicator of free agency eligibility and the indicator of arbitration eligibility are equal to 0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Delete the rows where indicator of free agency eligibility and indicator of arbitration eligibility both are 0\ndf = df[~((df['indicator_of_free_agency_eligibility'] == 0.0) & (df['indicator_of_arbitration_eligibility'] == 0.0))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Eliminate the rows where both the indicator of free agency eligibility and the indicator of arbitration eligibility are equal to 0\ndf = df[~((df['indicator_of_free_agency_eligibility'] == 0) & (df['indicator_of_arbitration_eligibility'] == 0))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Remove all entries with a 'looks' rating less than 3.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Remove all entries with a 'looks' rating less than 3.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Delete all records where the 'looks' rating is below 3\ndf = df[df['looks'] >= 3]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove entries with 'looks' rating less than 3\ndf = df[df['looks'] >= 3]\n\n# Output the resulting dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Delete all records where the 'union' is 1 and 'goodhlth' is 0.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Delete all records where the 'union' is 1 and 'goodhlth' is 0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Delete the records where 'union' is 1 and 'goodhlth' is 0\ndf = df[~((df['union'] == 1) & (df['goodhlth'] == 0))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete all records where the 'union' is 1 and 'goodhlth' is 0\ndf = df[~((df['union'] == 1) & (df['goodhlth'] == 0))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Delete rows from the data table where the month is 5 and the temperature is less than 0.2.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Delete rows from the data table where the month is 5 and the temperature is less than 0.2.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas DataFrame.\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Remove all rows from data wherein month is 5 and the temperature is below 0.2.\ndf = df[~((df['mnth'] == 5) & (df['temp'] < 0.2))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Screen the data: remove rows where month is 5 and temperature is less than 0.2\ndf = df[~((df['mnth'] == 5) & (df['temp'] < 0.2))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Delete records with humidity greater than 80% on working days.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Delete records with humidity greater than 80% on working days.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Delete the records where the humidity is greater than 80% on working days.\ndf = df[~((df['hum'] > 0.8) & (df['workingday'] == 1))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete records with humidity greater than 80% on working days\ndf = df[~((df['hum'] > 80) & (df['workingday'] == 1))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Remove all entries in the table that have a 'Market Cap' below 750,000,000.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Remove all entries in the table that have a 'Market Cap' below 750,000,000.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove the commas in the 'Market Cap' column for conversion to int\ndata['Market Cap'] = data['Market Cap'].str.replace(',','')\n\n# Convert the 'Market Cap' column to int\ndata['Market Cap'] = pd.to_numeric(data['Market Cap'])\n\n# Filter out the rows where 'Market Cap' is less than 750,000,000\nfiltered_data = data[data['Market Cap'] >= 750000000]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Clean the 'Market Cap' column and convert it to numeric\ndf['Market Cap'] = df['Market Cap'].str.replace(',', '').astype(int)\n\n# Filter the dataframe\ndf = df[df['Market Cap'] >= 750,000,000]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Filter the table to show only rows where the value in the 'Close' column is above or equal to 100.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Filter the table to show only rows where the value in the 'Close' column is above or equal to 100.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove the rows where 'Close' is below 100\ndf = df[df['Close'] >= 100]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data to only include rows where the 'Close' column value is above or equal to 100\nfiltered_data = data[data['Close'] >= 100]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Exclude all records with occupation 'Handlers-cleaners'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Exclude all records with occupation 'Handlers-cleaners'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Remove all rows where occupation is 'Handlers-cleaners'\ndf = df[df['occupation'] != ' Handlers-cleaners']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Exclude records with occupation 'Handlers-cleaners'\ndf = df[df['occupation'] != 'Handlers-cleaners']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Remove rows where 'capital gain' minus 'capital loss' is less than zero.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Remove rows where 'capital gain' minus 'capital loss' is less than zero.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Compute the difference between 'capital-gain' and 'capital-loos'\ndf['capital_diff'] = df['capital-gain'] - df['capital-loos']\n\n# Delete rows where 'capital gain' - 'capital loss' is less than 0\ndf = df[df['capital_diff'] >= 0]\n\n# Drop the 'capital_diff' column as it's no longer needed\ndf.drop('capital_diff',axis=1,inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the difference between 'capital gain' and 'capital loss'\ndf['capital_gain_loss_diff'] = df['capital-gain'] - df['capital-loos']\n\n# Filter rows where the difference is less than zero\ndf = df[df['capital_gain_loss_diff'] >= 0]\n\n# Drop the auxiliary column\ndf = df.drop('capital_gain_loss_diff', axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Eliminate any 'education' groups with fewer than 10 individuals.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Eliminate any 'education' groups with fewer than 10 individuals.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Remove 'education' groups that have less than 10 individuals.\ncounts = df['education'].value_counts()\ndf = df[df['education'].isin(counts[counts >= 10].index)]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the number of individuals in each 'education' group\neducation_counts = df['education'].value_counts()\n\n# Get the 'education' groups with fewer than 10 individuals\nsmall_education_groups = education_counts[education_counts < 10].index\n\n# Remove these groups from the dataframe\ndf = df[~df['education'].isin(small_education_groups)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Delete all data belonging to the 'Community Services' department group.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Delete all data belonging to the 'Community Services' department group.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load CSV file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Filter out records related to 'Community Services' dept_group\ndf = df[df['dept_group'] != 'Community Services']\n\n# Output the revised DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter out rows where 'dept_group' column is not 'Community Services'\ndf = df[df['dept_group'] != 'Community Services']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Remove rows where 'budget_year_end' is earlier than 'budget_year_start'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Remove rows where 'budget_year_end' is earlier than 'budget_year_start'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/19.csv')\n\n# Convert 'budget_year_start' and 'budget_year_end' to datetime format\ndata['budget_year_start'] = pd.to_datetime(data['budget_year_start'])\ndata['budget_year_end'] = pd.to_datetime(data['budget_year_end'])\n\n# Remove rows where 'budget_year_end' is earlier than 'budget_year_start'\ndata = data[data['budget_year_end'] >= data['budget_year_start']]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'budget_year_start' and 'budget_year_end' to datetime\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\ndf['budget_year_end'] = pd.to_datetime(df['budget_year_end'])\n\n# Remove rows where 'budget_year_end' is earlier than 'budget_year_start'\ndf = df[df['budget_year_end'] >= df['budget_year_start']]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Delete the rows where the 'coa_dept_id' is even.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Delete the rows where the 'coa_dept_id' is even.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Pre-processing the dataframe to handle possible non-standard data\ndf = df[pd.to_numeric(df['coa_dept_id'],errors='coerce').notnull()]\ndf['coa_dept_id'] = df['coa_dept_id'].astype(int)\n\n# Delete the rows where the 'coa_dept_id' is even\ndf = df[df['coa_dept_id'] % 2 != 0]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where the 'coa_dept_id' is even\ndf = df[df['coa_dept_id'] % 2 != 0]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Remove rows from the table where the 'Number of Discharges' is less than 50.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Remove rows from the table where the 'Number of Discharges' is less than 50.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Convert 'Number of Discharges' column to numeric,errors coerce will replace non-numeric values with NaNs\ndf['Number of Discharges'] = pd.to_numeric(df['Number of Discharges'],errors='coerce')\n\n# Dropping rows from the dataframe where 'Number of Discharges' is less than 50\ndf = df[df['Number of Discharges'] >= 50]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where 'Number of Discharges' is less than 50\ndf = df[df['Number of Discharges'] >= 50]\n\n# Output the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Filter the table to show only rows where 'Measure Name' is equal to 'READM-30-HIP-KNEE-HRRP'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Filter the table to show only rows where 'Measure Name' is equal to 'READM-30-HIP-KNEE-HRRP'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/20.csv')\n\n# Deleting rows where 'Measure Name' does not equal to 'READM-30-HIP-KNEE-HRRP'\ndata = data[data['Measure Name'] == 'READM-30-HIP-KNEE-HRRP']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_data = data[data['Measure Name'] == 'READM-30-HIP-KNEE-HRRP']\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Eliminate all storm records with 'max_storm_cat' below 3.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Eliminate all storm records with 'max_storm_cat' below 3.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Filter out rows where 'max_storm_cat' is less than 3\ndf = df[df['max_storm_cat'] >= 3]\n\n# Output the processed DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'max_storm_cat' is below 3\ndf = df[df['max_storm_cat'] >= 3]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Eliminate all rows where the value for 'deaths' is 0.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Eliminate all rows where the value for 'deaths' is 0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV file into a dataframe\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Remove all entries that have 'deaths' equal to 0\ndf = df[df['deaths'] != 0]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Eliminate rows where the value for 'deaths' is 0\ndata = data[data['deaths'] != 0]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Eliminate all rows where the 'damage_USD' is less than 1000000.0.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Eliminate all rows where the 'damage_USD' is less than 1000000.0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/21.csv')\n\n# Remove all rows where the 'damage_USD' is less than 1000000.0\ndata = data[data['damage_USD'] >= 1000000.0]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'damage_USD' is less than 1000000.0\ndf = df[df['damage_USD'] >= 1000000.0]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Filter out all rows where the 'vaccines' field reads 'Pfizer/BioNTech'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Filter out all rows where the 'vaccines' field reads 'Pfizer/BioNTech'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/22.csv')\n\n# Filter the rows where 'vaccines' column is not 'Pfizer/BioNTech'\nfiltered_data = data[data['vaccines'] != 'Pfizer/BioNTech']\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\ndf_filtered = df[df['vaccines'] == 'Pfizer/BioNTech']\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Remove all rows where the value of 'Age' is within the range of 30 to 50.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Remove all rows where the value of 'Age' is within the range of 30 to 50.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Loading the data from csv\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Deleting rows where 'Age' is between 30 to 50\ndf = df[(df['Age'] < 30) | (df['Age'] > 50)]\n\n# Printing the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Removing all rows where the 'Age' is between 30 and 50\ndf = df[(df['Age'] < 30) | (df['Age'] > 50)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Filter out all rows with 'DebtRatio' exceeding 1000.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Filter out all rows with 'DebtRatio' exceeding 1000.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data from the file\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# remove all rows where 'DebtRatio' is greater than 1000\ndf = df[df['DebtRatio'] <= 1000]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter out the rows with 'DebtRatio' exceeding 1000\ndf_filtered = df[df['DebtRatio'] <= 1000]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Remove all rows where the value in the 'age' column is below 18.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Remove all rows where the value in the 'age' column is below 18.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Delete all rows where 'age' column value is less than 18\ndf = df[df['age'] >= 18]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove all rows where the value in the 'age' column is below 18\ndf = df[df['age'] >= 18]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Remove all records where 'NumberOfOpenCreditLinesAndLoans' is more than three standard deviations away from the mean.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Remove all records where 'NumberOfOpenCreditLinesAndLoans' is more than three standard deviations away from the mean.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Calculate the mean and standard deviation of 'NumberOfOpenCreditLinesAndLoans'\nmean = data['NumberOfOpenCreditLinesAndLoans'].mean()\nstd_dev = data['NumberOfOpenCreditLinesAndLoans'].std()\n\n# Define the upper and lower limits for outliers\nupper_limit = mean + 3 * std_dev\nlower_limit = mean - 3 * std_dev\n\n# Remove outliers\ndata = data[(data['NumberOfOpenCreditLinesAndLoans'] >= lower_limit) & (data['NumberOfOpenCreditLinesAndLoans'] <= upper_limit)]\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate mean and standard deviation of 'NumberOfOpenCreditLinesAndLoans'\nmean = df['NumberOfOpenCreditLinesAndLoans'].mean()\nstd = df['NumberOfOpenCreditLinesAndLoans'].std()\n\n# Remove records where 'NumberOfOpenCreditLinesAndLoans' is more than three standard deviations away from the mean\ndf = df[np.abs(df['NumberOfOpenCreditLinesAndLoans'] - mean) <= 3*std]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Remove records where 'Wins' is less than the median 'Wins' value in the dataset.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Remove records where 'Wins' is less than the median 'Wins' value in the dataset.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Delete records where 'Wins' is less than the median\nmedian_wins = data['Wins'].median()\ndata = data[data['Wins'] >= median_wins]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of the 'Wins' column\nmedian_wins = df['Wins'].median()\n\n# Filter the DataFrame to only include rows where 'Wins' is greater than or equal to the median\ndf = df[df['Wins'] >= median_wins]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Filter rows with 'X' values below -2.9E+08.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Filter rows with 'X' values below -2.9E+08.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Remove rows where the 'X' value is less than -2.9E+08\ndf = df[df['X'] >= -2.9E+08]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter rows with 'X' values below -2.9E+08\ndf_filtered = df[df['X'] < -2.9E+08]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Retain rows where the 'Calendar Date (TDB)' is in the 'A.D.' era.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Retain rows where the 'Calendar Date (TDB)' is in the 'A.D.' era.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/27.csv')\n\n# Delete rows where the 'Calendar Date (TDB)' is not in the 'A.D.' era\ndata = data[data['Calendar Date (TDB)'].str.contains('A.D.')]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data to retain only rows where 'Calendar Date (TDB)' is in the 'A.D.' era\ndata = data[data['Calendar Date (TDB)'].str.contains('A.D.')]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Remove all rows that have a 'Calendar Date (TDB)' of 'A.D. 1999-Feb-10 00:58:29.0000'.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Remove all rows that have a 'Calendar Date (TDB)' of 'A.D. 1999-Feb-10 00:58:29.0000'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/27.csv')\n\n# Delete rows with 'Calendar Date (TDB)' of 'A.D. 1999-Feb-10 00:58:29.0000'\ndata = data[data['Calendar Date (TDB)'] != 'A.D. 1999-Feb-10 00:58:29.0000']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove the rows where 'Calendar Date (TDB)' is 'A.D. 1999-Feb-10 00:58:29.0000'\ndf = df[df['Calendar Date (TDB)'] != 'A.D. 1999-Feb-10 00:58:29.0000']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Delete all records where the \"cut\" is \"Fair\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Delete all records where the \"cut\" is \"Fair\".", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/28.csv')\n\n# Delete all records where \"cut\" is \"Fair\" \ndata = data[data['cut'] != 'Fair']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete the records where the 'cut' is 'Fair'\ndf = df[df['cut'] != 'Fair']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Remove all rows where the value of \"depth\" is within the range of 50 to 55.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Remove all rows where the value of \"depth\" is within the range of 50 to 55.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Delete the records where 'depth' is between 50 and 55\ndf = df[(df['depth'] < 50) | (df['depth'] > 55)]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where depth is between 50 and 55\ndf = df[(df['depth'] < 50) | (df['depth'] > 55)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Remove records with \"clarity\" containing \"I\" (Imperfections).", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Remove records with \"clarity\" containing \"I\" (Imperfections).", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# filter the records where \"clarity\" contains \"I\" and drop them\ndf = df[~df['clarity'].str.contains(\"I\")]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove records with \"clarity\" containing \"I\"\ndf = df[~df['clarity'].str.contains('I')]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Remove any rows where the 'price' is less than 500.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Remove any rows where the 'price' is less than 500.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(\"infiagent/csv/28.csv\")\n\n# Remove rows where price is less than 500\ndf = df[df['price'] >= 500]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove any rows where the 'price' is less than 500\ndf = df[df['price'] >= 500]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Remove the records from the data where 'Low' is less than 75.5.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Remove the records from the data where 'Low' is less than 75.5.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/29.csv')\n\n# Delete the entries where 'Low' is less than 75.5\ndata = data[data['Low'] >= 75.5]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove the records where 'Low' is less than 75.5\ndata = data[data['Low'] >= 75.5]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Remove entries with 'total_votes' value less than 50,000.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Remove entries with 'total_votes' value less than 50,000.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Clean the data to keep rows where 'total_votes' are greater or equal to 50,000\ndf = df[df['total_votes'] >= 50000]\n\n# Output the cleaned data to check the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove entries with 'total_votes' value less than 50000\ndf = df[df['total_votes'] >= 50000]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Eliminate rows where 'votes_dem' is smaller than 'votes_gop' and 'state_abbr' is \"AK\".", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Eliminate rows where 'votes_dem' is smaller than 'votes_gop' and 'state_abbr' is \"AK\".", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load data from csv\ndata = pd.read_csv('infiagent/csv/30.csv')\n\n# Filter out entries where votes_dem is less than votes_gop and state_abbr is \"AK\"\nfiltered_data = data.loc[~((data['votes_dem'] < data['votes_gop']) & (data['state_abbr'] == 'AK'))]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter out rows where 'votes_dem' is smaller than 'votes_gop' and 'state_abbr' is \"AK\"\ndf = df[~((df['votes_dem'] < df['votes_gop']) & (df['state_abbr'] == 'AK'))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Delete all rows where 'EQ3' is less than -0.08.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Delete all rows where 'EQ3' is less than -0.08.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Apply selection criteria and delete rows where 'EQ3' is less than -0.08\ndf = df[df['EQ3'] >= -0.08]\n\n# Print the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'EQ3' is less than -0.08\ndf = df[df['EQ3'] >= -0.08]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Retain the rows where the length of the 'EQ6' value is equal to or greater than 0.05.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Retain the rows where the length of the 'EQ6' value is equal to or greater than 0.05.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/31.csv\")\n\n# Remove rows where the length of 'EQ6' is less than 0.05\ndf = df[df['EQ6'].apply(lambda x: len(str(x))) >= 0.05]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the rows where the length of the 'EQ6' value is equal to or greater than 0.05\ndf_filtered = df[df['EQ6'].apply(lambda x: len(str(x)) >= 0.05)]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Delete rows containing 'No. of cases' equal to 0.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Delete rows containing 'No. of cases' equal to 0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Remove all rows where 'No. of cases' is 0\ndf = df[df['No. of cases'] != '0']\n\n# Print the updated data\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'No. of cases' is equal to 0\ndf = df[df['No. of cases'] != '0']\n\n# Print the resulting dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: [rewritten instruction]: Delete all entries from the table where 'publishedAt' is earlier than '2018-08-28'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "[rewritten instruction]: Delete all entries from the table where 'publishedAt' is earlier than '2018-08-28'.", "instruction_type": "Delete-Screening", "reference_code": "# Required Library\nimport pandas as pd\nfrom datetime import datetime\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Convert 'publishedAt' to datetime format\ndf['publishedAt'] = pd.to_datetime(df['publishedAt'])\n\n# Filter out rows where 'publishedAt' is before '2018-08-28'\ndf = df[df['publishedAt'] >= datetime.strptime('2018-08-28','%Y-%m-%d')]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Convert 'publishedAt' column to datetime\ndata['publishedAt'] = pd.to_datetime(data['publishedAt'])\n\n# Filter the data to keep only entries from '2018-08-28' and later\ndata = data[data['publishedAt'] >= '2018-08-28']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Filter out all rows with a 'positive_diffsel' column value below 4.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Filter out all rows with a 'positive_diffsel' column value below 4.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv(\"infiagent/csv/34.csv\")\n\n# Remove rows where 'positive_diffsel' value is less than 4\ndata = data[data['positive_diffsel'] >= 4]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter out all rows with a 'positive_diffsel' column value below 4\ndf = df[df['positive_diffsel'] >= 4]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Remove rows where 'max_diffsel' value is below the median of 'max_diffsel' values.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Remove rows where 'max_diffsel' value is below the median of 'max_diffsel' values.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/34.csv')\n\n# Compute the median max_diffsel\nmedian_max_diffsel = data['max_diffsel'].median()\n\n# Delete all rows where max_diffsel < median_max_diffsel\nfiltered_data = data[data['max_diffsel'] >= median_max_diffsel]\n\n# Output the first 10 rows of the filtered data\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of 'max_diffsel' column\nmedian_max_diffsel = df['max_diffsel'].median()\n\n# Remove rows where 'max_diffsel' is below the median\ndf = df[df['max_diffsel'] >= median_max_diffsel]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Delete records where 'site' field contains brackets ().", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Delete records where 'site' field contains brackets ().", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# filter out records where 'site' field contains brackets\ndf = df[~df['site'].str.contains(\"\\(|\\)\")]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove the records where 'site' field contains brackets ()\ndf = df[~df['site'].str.contains('\\(') & df['site'].str.contains('\\)')]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Eliminate all rows where the population is less than 1 million.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Eliminate all rows where the population is less than 1 million.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data \ndata = pd.read_csv('infiagent/csv/35.csv')\n\n# Remove entries where the population is less than 1 million\ndata = data[data['pop'] >= 1000000]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter out rows where the population is less than 1 million\ndata = data[data['pop'] >= 1000000]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Remove any records where the 'lifeexp' is less than 30.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Remove any records where the 'lifeexp' is less than 30.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Remove any records where the 'lifeexp' is less than 30\ndf = df[df['lifeexp'] >= 30]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove rows where 'lifeexp' is less than 30\ndata = data[data['lifeexp'] >= 30]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Delete all records for years before 1992.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Delete all records for years before 1992.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Delete all records for any year before 1992.\ndf = df[df['year'] >= 1992]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data to only include records from 1992 and onwards\ndf = df[df['year'] >= 1992]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Remove all entries where the GDP per capita in 1952 is less than $500.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Remove all entries where the GDP per capita in 1952 is less than $500.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Read the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Remove entries where gdpPercap_1952 is less than 500\ndf = df[df['gdpPercap_1952'] >= 500]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove entries where the GDP per capita in 1952 is less than 500\ndf = df[df['gdpPercap_1952'] >= 500]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Delete all rows where the GDP per capita in 2007 is less than the average GDP per capita in 1952.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Delete all rows where the GDP per capita in 2007 is less than the average GDP per capita in 1952.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the dataset from the provided CSV file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Calculate the average GDP per capita in 1952\navg_gdp_1952 = df['gdpPercap_1952'].mean()\n\n# Delete rows where the GDP per capita in 2007 is less than the average GDP per capita in 1952\ndf = df[df['gdpPercap_2007'] >= avg_gdp_1952]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average GDP per capita in 1952\navg_gdp_1952 = df['gdpPercap_1952'].mean()\n\n# Delete rows where the GDP per capita in 2007 is less than the average GDP per capita in 1952\ndf = df[df['gdpPercap_2007'] >= avg_gdp_1952]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Delete all rows where the 'country' contains the keyword 'China'.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Delete all rows where the 'country' contains the keyword 'China'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Delete rows where 'country' contains keyword 'China'\ndf = df[~df['country'].str.contains('China')]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where the 'country' contains the keyword 'China'\ndf = df[~df['country'].str.contains('China')]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Eliminate records where the 'Age' is less than 10.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Eliminate records where the 'Age' is less than 10.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Remove records where the 'Age' is less than 10\ndf = df[df['Age'] >= 10]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data where 'Age' is greater than or equal to 10\ndf = df[df['Age'] >= 10]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Remove all records that have average 'Age' in their 'transcript_id' smaller than 5.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Remove all records that have average 'Age' in their 'transcript_id' smaller than 5.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Calculate the average age for each 'transcript_id'\naverage_age = df.groupby('transcript_id')['Age'].mean()\n\n# Find the 'transcript_id' whose average age is smaller than 5\ntranscript_id_to_remove = average_age[average_age < 5].index\n\n# Remove the records\ndf = df[~df['transcript_id'].isin(transcript_id_to_remove)]\n\n# Print the final dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average age for each transcript\navg_age_transcripts = df.groupby('transcript_id')['Age'].mean()\n\n# Get the transcript ids with average age less than 5\ntranscript_ids_to_remove = avg_age_transcripts[avg_age_transcripts < 5].index\n\n# Remove the records with the transcript ids\ndf = df[~df['transcript_id'].isin(transcript_ids_to_remove)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Remove the rows where 'Average Price' is less than 550.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Remove the rows where 'Average Price' is less than 550.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Remove the rows where 'Average Price' is less than 550\ndata = data[data['Average Price'] > 550]\n\nprint(data)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data\ndata = pd.read_csv('data.csv')\n\n# removing the rows where 'Average Price' is less than 550\nfiltered_data = data[data['Average Price'] >= 550]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Remove rows where '% Dly Qt to Traded Qty' is less than 20%.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Remove rows where '% Dly Qt to Traded Qty' is less than 20%.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the CSV file\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# remove whitespaces from columns\ndf.columns = df.columns.str.strip()\n\n# remove rows where '% Dly Qt to Traded Qty' is less than 20%\ndf = df[df['% Dly Qt to Traded Qty'] >= 20]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where '% Dly Qt to Traded Qty' is less than 20%\ndf = df[df['% Dly Qt to Traded Qty'] >= 20]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Delete records of superheroes whose 'Skin color' is not provided (represented by '-').", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Delete records of superheroes whose 'Skin color' is not provided (represented by '-').", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndata = pd.read_csv('infiagent/csv/39.csv')\n\n# delete the records where 'Skin color' is not provided (represented by '-')\ndata = data[data['Skin color'] != '-']\n\n# print the resulting data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete records where 'Skin color' is '-'\ndf = df[df['Skin color'] != '-']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Eliminate the rows from the table where the star rating is less than 3.0.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Eliminate the rows from the table where the star rating is less than 3.0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Filter out rows where star_rating is less than 3.0\ndf = df[df['star_rating'] >= 3.0]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete the rows where the star_rating is less than 3.0\ndf = df[df['star_rating'] >= 3.0]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Delete all rows where the 'hotel_id' matches the 'parent_brand_name' value.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Delete all rows where the 'hotel_id' matches the 'parent_brand_name' value.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/40.csv')\n\n# Delete all rows where the 'hotel_id' matches the 'parent_brand_name' value\ndata = data[data['hotel_id'] != data['parent_brand_name']]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete all rows where the 'hotel_id' matches the 'parent_brand_name' value\ndf = df[df['hotel_id'] != df['parent_brand_name']]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Remove entries where 'hotel_name' includes the term \"Court\".", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Remove entries where 'hotel_name' includes the term \"Court\".", "instruction_type": "Delete-Screening", "reference_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/40.csv')\n\n# Deleting details where 'hotel_name' contains the word 'Court'\ndata = data[~data['hotel_name'].str.contains('Court')]\n\n# Output the result\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove entries where 'hotel_name' includes the term \"Court\"\ndf = df[~df['hotel_name'].str.contains('Court')]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Remove the row when the basement square footage (sqft_basement) is equal to zero.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Remove the row when the basement square footage (sqft_basement) is equal to zero.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# delete the rows where sqft_basement equals to 0\ndf = df[df['sqft_basement'] != 0]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove rows where sqft_basement is zero\ndata = data[data['sqft_basement'] != 0]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Remove rows with 'waterfront' value equal to '0'.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Remove rows with 'waterfront' value equal to '0'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# delete records where 'waterfront' is '0'\ndf = df[df['waterfront'] != 0]\n\n# print the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('data.csv')\n\n# Remove rows where 'waterfront' is 0\ndata = data[data['waterfront'] != 0]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Filter the table to exclude rows where the 'zipcode' column matches any value in a subquery that retrieves the distinct 'zipcode' values from another table.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Filter the table to exclude rows where the 'zipcode' column matches any value in a subquery that retrieves the distinct 'zipcode' values from another table.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/41.csv')\n\n# Subquery to retrieve distinct zipcodes from another table\nsubquery = ['zipcode1','zipcode2','zipcode3']\n\n# Remove rows where the 'zipcode' value matches the subquery\ndata = data[~data['zipcode'].isin(subquery)]\n\n# Print the updated data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Create a subquery that retrieves the distinct 'zipcode' values from another table\nsubquery = pd.read_csv('subquery.csv')['zipcode'].unique()\n\n# Filter the data to exclude rows where the 'zipcode' column matches any value in the subquery\nfiltered_data = data[~data['zipcode'].isin(subquery)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Delete records where the 'long' value is less than -125 or greater than -120.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Delete records where the 'long' value is less than -125 or greater than -120.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Delete records where 'long' value is not within the range of -125 to -120\ndf = df[(df['long'] >= -125) & (df['long'] <= -120)]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete records where the 'long' value is less than -125 or greater than -120\ndf = df[(df['long'] >= -125) & (df['long'] <= -120)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Delete all records that have an 'importance.score' less than 0.03.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Delete all records that have an 'importance.score' less than 0.03.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Filter out records where 'importance.score' is less than 0.03\ndf = df[df['importance.score'] >= 0.03]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete all records that have an 'importance.score' less than 0.03\ndf = df[df['importance.score'] >= 0.03]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Delete all rows where 'row m/z' is less than the 10th percentile or greater than the 90th percentile - removing outliers.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Delete all rows where 'row m/z' is less than the 10th percentile or greater than the 90th percentile - removing outliers.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Calculate the 10th and 90th percentile\nlower_percentile = df['row m/z'].quantile(0.10)\nupper_percentile = df['row m/z'].quantile(0.90)\n\n# Remove rows where 'row m/z' is less than 10th percentile or greater than 90th percentile\ndf = df[(df['row m/z'] >= lower_percentile) & (df['row m/z'] <= upper_percentile)]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the 10th and 90th percentiles of 'row m/z'\nperc_10 = df['row m/z'].quantile(0.10)\nperc_90 = df['row m/z'].quantile(0.90)\n\n# Remove rows where 'row m/z' is less than the 10th percentile or greater than the 90th percentile\ndf = df[(df['row m/z'] >= perc_10) & (df['row m/z'] <= perc_90)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Delete the entries that have 'southeast' in the region column and 'female' in the sex column.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Delete the entries that have 'southeast' in the region column and 'female' in the sex column.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/43.csv')\n\n# Filter out the required records \ndata = data[~((data['region']=='southeast') & (data['sex']=='female'))]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data by removing entries where sex is 'female' and region is 'southeast'\ndf = df[~((df['sex'] == 'female') & (df['region'] == 'southeast'))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Delete rows where the 'charges' are below the average 'charges'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Delete rows where the 'charges' are below the average 'charges'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Calculate the average of 'charges'\naverage_charges = df['charges'].mean()\n\n# Filter out rows where 'charges' is below the average\ndf = df[df['charges'] >= average_charges]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'charges'\naverage_charges = df['charges'].mean()\n\n# Delete rows where the 'charges' are below the average 'charges'\ndf = df[df['charges'] >= average_charges]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Remove all records where 'age' is above 60 and 'smoker' is 'yes'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Remove all records where 'age' is above 60 and 'smoker' is 'yes'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Filter out the records where 'age' is above 60 and 'smoker' is 'yes'\ndf = df[~((df['age'] > 60) & (df['smoker'] == 'yes'))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Remove all records where 'age' is above 60 and 'smoker' is 'yes'\ndf = df[~((df['age'] > 60) & (df['smoker'] == 'yes'))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Eliminate any row where the \"Close\" value is less than the \"Open\" value.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Eliminate any row where the \"Close\" value is less than the \"Open\" value.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Remove rows where \"Close\" value is less than \"Open\" value\ndf = df[df['Close'] >= df['Open']]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where 'Close' is less than 'Open'\ndf = df[df['Close'] >= df['Open']]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Delete rows where the \"Range\" value is less than 1.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Delete rows where the \"Range\" value is less than 1.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# loading the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# creating a new column 'Range' which is the difference between 'High' and 'Low' per day\ndf['Range'] = df['High'] - df['Low']\n\n# deleting rows where 'Range' is less than 1\ndf = df[df['Range'] >= 1]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the Range and add it as a new column\ndf['Range'] = df['High'] - df['Low']\n\n# Delete rows where the Range is less than 1\ndf = df[df['Range'] >= 1]\n\n# Drop the 'Range' column as it is no longer needed\ndf = df.drop(columns=['Range'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Remove rows from the table where MedianHouseValue column falls below 1.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Remove rows from the table where MedianHouseValue column falls below 1.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Filtering the rows where 'MedianHouseValue' is less than 1\ndf = df[df['MedianHouseValue'] >= 1]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where MedianHouseValue is less than 1\ndf = df[df['MedianHouseValue'] >= 1]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Filter rows based on the \"Population\" being greater than or equal to the 10th percentile of all populations in the dataset.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Filter rows based on the \"Population\" being greater than or equal to the 10th percentile of all populations in the dataset.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/45.csv')\n\n# Compute the 10th percentile of population values\npop_threshold = data['Population'].quantile(0.10)\n\n# Keep only the rows where population is greater than or equal to the 10th percentile value\ndata = data[data['Population'] >= pop_threshold]\n\n# Print the resulting DataFrame\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Calculate the 10th percentile of the 'Population' column\npop_10th_percentile = df['Population'].quantile(0.1)\n\n# Filter rows where 'Population' is greater than or equal to the 10th percentile\nfiltered_df = df[df['Population'] >= pop_10th_percentile]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Delete all records belonging to frequency \"A\".", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Delete all records belonging to frequency \"A\".", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data\ndata = pd.read_csv('infiagent/csv/46.csv')\n\n# Remove rows where FREQUENCY is 'A'\ndata = data[data['FREQUENCY'] != 'A']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter out the records where frequency is 'A'\ndf = df[df['FREQUENCY'] != 'A']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Remove rows where the 'Agriculture' value is less than 10.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Remove rows where the 'Agriculture' value is less than 10.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Remove rows where 'Agriculture' is less than 10\ndf = df[df['Agriculture'] >= 10]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove rows where the 'Agriculture' value is less than 10\ndf = df[df['Agriculture'] >= 10]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Remove rows with 'English' value less than 50 or 'Foreign Languages' value less than 70.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Remove rows with 'English' value less than 50 or 'Foreign Languages' value less than 70.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/47.csv')\n\n# Delete rows where the 'English' value is less than 50 or 'Foreign Languages' is less than 70\ndata = data[(data['English'] >= 50) & (data['Foreign Languages'] >= 70)]\n\n# print the cleaned data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows with 'English' value less than 50 or 'Foreign Languages' value less than 70\ndf = df[(df['English'] >= 50) & (df['Foreign Languages'] >= 70)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Delete all the rows in which 'class1' and 'class2' are both 0.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Delete all the rows in which 'class1' and 'class2' are both 0.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Delete all the rows in which 'class1' and 'class2' are both 0.\ndf = df[~((df['class1'] == 0) & (df['class2'] == 0))]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'class1' and 'class2' are both 0\ndf = df[~((df['class1'] == 0.0) & (df['class2'] == 0.0))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Delete all rows where the 'class1' value is either 1 or the 'class3' value is less than 0.2.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Delete all rows where the 'class1' value is either 1 or the 'class3' value is less than 0.2.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/48.csv')\n\n# Delete rows where 'class1' value is equal to 1 or 'class3' value is less than 0.2\ndata = data[(data['class1'] != 1) & (data['class3'] >= 0.2)]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Delete rows where 'class1' is 1 or 'class3' is less than 0.2\ndf = df[~((df['class1'] == 1) | (df['class3'] < 0.2))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Clear all rows with all column values equal to 0.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Clear all rows with all column values equal to 0.", "instruction_type": "Delete-Screening", "reference_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data from the provided csv file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# drop the rows where all the classes are 0\ndf = df[(df.iloc[:,1:] != 0).any(axis=1)]\n\n# print the resulting dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Drop the rows where all column values are 0\ndf = df.drop(df[df.iloc == 0].index)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Delete all records where the \"Volume\" is less than 50,000.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Delete all records where the \"Volume\" is less than 50,000.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Delete all records where the \"Volume\" is less than 50,000\ndf = df[df['Volume'] >= 50000]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Delete rows where the 'Volume' is less than 50000\ndf = df[df['Volume'] >= 50000]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Remove instances where the wind speed is below 2.5 and the day is '2015-07-24 16:39:38'.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Remove instances where the wind speed is below 2.5 and the day is '2015-07-24 16:39:38'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# remove instances where the wind speed is below 2.5 and the day is '2015-07-24 16:39:38'\ndf_filtered = df[~((df['wind_speed'] < 2.5) & (df['day'] == '2015-07-24 16:39:38'))]\n\nprint(df_filtered)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove instances where the wind speed is below 2.5 and the day is '2015-07-24 16:39:38'\ndf = df[~((df['wind_speed'] < 2.5) & (df['day'] == '2015-07-24 16:39:38'))]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Eliminate rows with pressure values falling outside the normal atmospheric pressure range of 980 to 1030 hPa.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Eliminate rows with pressure values falling outside the normal atmospheric pressure range of 980 to 1030 hPa.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/50.csv')\n\n# Remove rows where the pressure is outside the typical atmospheric pressure range\ndata = data[(data['pressure'] >= 980) & (data['pressure'] <= 1030)]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Delete rows with pressure values outside the normal atmospheric pressure range of 980 to 1030 hPa\ndf = df[(df['pressure'] >= 980) & (df['pressure'] <= 1030)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Filter out all rows where the description contains the word 'rain'.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Filter out all rows where the description contains the word 'rain'.", "instruction_type": "Delete-Screening", "reference_code": "import pandas as pd\n\n# loading the csv file\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# removing rows where description contains 'rain'\ndf = df[~df['description'].str.contains('rain')]\n\nprint(df)", "assistant_code": "# Importing necessary library\nimport pandas as pd\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Filtering the data\ndf_rain = df[df['description'].str.contains('rain')]\n\nprint(df_rain)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Filter rows where the 'Date' is in the list of dates where the 'Miles traveled today' was less than or equal to 50000.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Filter rows where the 'Date' is in the list of dates where the 'Miles traveled today' was less than or equal to 50000.", "instruction_type": "Delete-Subquery", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Create a list of dates where 'Miles traveled today' was more than 50000\ndates = df[df['Miles traveled today (midnight to 11:59 pm)'] > 50000]['Date'].tolist()\n\n# Delete rows where the 'Date' is not in the list of dates\ndf = df[df['Date'].isin(dates)]\n\nprint(df)", "assistant_code": "# import the necessary libraries\nimport pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data where 'Miles traveled today' is less than or equal to 50000\nfiltered_df = df[df['Miles traveled today (midnight to 11:59 pm)'] <= 50000]\n\n# Get the dates from the filtered data\ndates = filtered_df['Date'].tolist()\n\n# Filter the original data where 'Date' is in the list of dates\nresult_df = df[df['Date'].isin(dates)]\n\nprint(result_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Eliminate all records where the 'station_code' corresponds to any 'station_code' in a subquery that includes the 'station' column with the keyword 'Believe'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Eliminate all records where the 'station_code' corresponds to any 'station_code' in a subquery that includes the 'station' column with the keyword 'Believe'.", "instruction_type": "Delete-Subquery", "reference_code": "import pandas as pd\n\n# Step 1: Load data\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Step 2: Delete rows where station contains the keyword 'Believe'\nbelieve_stations = df[df['station'].str.contains('Believe')]['station_code'].to_list()\ndf = df[~df['station_code'].isin(believe_stations)]\n\n# Step 3: Output the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a subquery that includes the 'station' column with the keyword 'Believe'\nsubquery = df[df['station'].str.contains('Believe')]['station_code']\n\n# Eliminate all records where the 'station_code' corresponds to any 'station_code' in the subquery\ndf = df[~df['station_code'].isin(subquery)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Eliminate all rows where 'Offense' appears in the top 5 most frequently occurring 'Offenses'.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Eliminate all rows where 'Offense' appears in the top 5 most frequently occurring 'Offenses'.", "instruction_type": "Delete-Subquery", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Get the top 5 most frequent 'Offenses'\ntop_offenses = df['Offense'].value_counts().index[:5]\n\n# Delete all rows where 'Offense' is listed in the top 5 most frequent 'Offenses'\ndf = df[~df['Offense'].isin(top_offenses)]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Find the top 5 most frequently occurring 'Offenses'\ntop_5_offenses = df['Offense'].value_counts().nlargest(5).index.tolist()\n\n# Remove rows where 'Offense' appears in the top 5 most frequently occurring 'Offenses'\ndf = df[~df['Offense'].isin(top_5_offenses)]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Create a pie chart that represents the percentage of 'Annual Member Sign-Ups', '24-Hour Passes Purchased', and '7-Day Passes Purchased' for the final date in the table.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Create a pie chart that represents the percentage of 'Annual Member Sign-Ups', '24-Hour Passes Purchased', and '7-Day Passes Purchased' for the final date in the table.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# get the last row of the table\nlast_row = df.iloc[-1]\n\n# get the data for the pie chart\nsignups = last_row['Annual Member Sign-Ups (midnight to 11:59 pm)']\nhour_passes = last_row['24-Hour Passes Purchased (midnight to 11:59 pm)']\nday_passes = last_row['7-Day Passes Purchased (midnight to 11:59 pm)']\n\n# create the pie chart\nplt.pie([signups,hour_passes,day_passes],labels=['Annual Member Sign-Ups','24-Hour Passes Purchased','7-Day Passes Purchased'],autopct='%1.1f%%')\nplt.title('Proportion of types of membership for the last date')\n\n# display the pie chart\nplt.show()", "assistant_code": "# import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Select the last date's data\nlast_date_data = df.iloc[-1]\n\n# Prepare data for the pie chart\nlabels = ['Annual Member Sign-Ups', '24-Hour Passes Purchased', '7-Day Passes Purchased']\nsizes = [last_date_data['Total Annual Memberships Sold'], last_date_data['24-Hour Passes Purchased (midnight to 11:59 pm)'], last_date_data['7-Day Passes Purchased (midnight to 11:59 pm)']]\n\n# Create a pie chart\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n\n# Draw a circle at the center to turn it into a donut chart\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Create a bar chart illustrating the frequency of trips based on the 'route_code'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Create a bar chart illustrating the frequency of trips based on the 'route_code'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading data from csv\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Counting number of trips per 'route_code'\nroute_counts = df['route_code'].value_counts()\n\n# Plotting bar graph\nroute_counts.plot(kind='bar')\nplt.title(\"Trips per Route Code\")\nplt.xlabel(\"Route Code\")\nplt.ylabel(\"Count of Trips\")\n\nplt.show()\n\n# There will be no print statement as visual graph is the output\n# Done", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'route_code' and count the frequency of each route\nroute_counts = df['route_code'].value_counts()\n\n# Create a bar chart of the route frequencies\nplt.figure(figsize=(10, 6))\nroute_counts.plot(kind='bar', color='navy')\nplt.title('Frequency of Trips per Route')\nplt.xlabel('Route Code')\nplt.ylabel('Frequency of Trips')\nplt.show()\n\n# Print the frequencies\nprint(route_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Create a pie chart showing the percentage of each 'origin_station_code' compared to the entire dataset.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Create a pie chart showing the percentage of each 'origin_station_code' compared to the entire dataset.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data from the csv\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Count the occurrence of each 'origin_station_code'\nresult = df['origin_station_code'].value_counts()\n\n# Plot pie chart\nresult.plot(kind = 'pie',autopct='%1.1f%%')\nplt.title('Proportions of each origin station code')\nplt.ylabel('')  # Empty the y-axis label for tidiness\n\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the count of each origin_station_code\norigin_counts = df['origin_station_code'].value_counts()\n\n# Create a pie chart\nplt.figure(figsize=(10,6))\nplt.pie(origin_counts, labels=origin_counts.index, autopct='%1.1f%%')\n\n# Display the plot\nplt.title('Percentage of each Origin Station Code')\nplt.show()\n\n# Print the result\nprint('The pie chart has been created.')"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Develop a bar chart depicting the 'Happiness Score' for each country in 'Western Europe'.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Develop a bar chart depicting the 'Happiness Score' for each country in 'Western Europe'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Filter the data for 'Western Europe'\nwestern_europe_df = df[df['Region'] == 'Western Europe']\n\n# Bar chart of 'Happiness Score' for each country in 'Western Europe'\nplt.figure(figsize=(10,6))\nplt.bar(western_europe_df['Country'],western_europe_df['Happiness Score'])\nplt.xlabel('Country')\nplt.ylabel('Happiness Score')\nplt.title('Happiness Score for each country in Western Europe')\nplt.xticks(rotation=90)\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data for 'Western Europe' region\ndf_western_europe = df[df['Region'] == 'Western Europe']\n\n# Create the bar chart\nplt.figure(figsize=(10,6))\nplt.bar(df_western_europe['Country'], df_western_europe['Happiness Score'])\nplt.xlabel('Country')\nplt.ylabel('Happiness Score')\nplt.title('Happiness Score for each country in Western Europe')\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Create a pie chart illustrating the distribution of 'Dystopia Residual' among nations in the 'Western Europe' region.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Create a pie chart illustrating the distribution of 'Dystopia Residual' among nations in the 'Western Europe' region.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# filter countries within 'Western Europe'\nwe_df = df[df['Region'] == 'Western Europe']\n\n# prepare data for the pie chart\nlabels = we_df['Country']\nsizes = we_df['Dystopia Residual']\n\n# draw the pie chart\nplt.pie(sizes,labels=labels,autopct='%1.1f%%',startangle=140)\n\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter data for 'Western Europe' region\nwestern_europe_df = df[df['Region'] == 'Western Europe']\n\n# Extract the 'Dystopia Residual' column\ndystopia_residuals = western_europe_df['Dystopia Residual']\n\n# Create a pie chart\nplt.pie(dystopia_residuals, labels=westeren_europe_df['Country'], autopct='%1.1f%%')\n\n# Display the chart\nplt.title('Distribution of Dystopia Residual in Western Europe')\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Draw a bar chart showing the count of 'Source' for each unique 'Weight' value.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Draw a bar chart showing the count of 'Source' for each unique 'Weight' value.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/5.csv')\n\n# Group by 'Weight' and count the 'Source'\ngrouped_data = data.groupby('Weight')['Source'].count()\n\n# Draw bar chart\nplt.bar(grouped_data.index,grouped_data.values)\nplt.xlabel('Weight')\nplt.ylabel('Count of Source')\nplt.title('Count of Source for each unique Weight value')\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the number of 'Source' for each unique 'Weight'\nweight_counts = df.groupby('Weight')['Source'].count()\n\n# Create the bar chart\nplt.bar(weight_counts.index, weight_counts.values)\nplt.xlabel('Weight')\nplt.ylabel('Count of Source')\nplt.title('Count of Source for each unique Weight')\nplt.xticks(weight_counts.index)\n\n# Show the plot\nplt.show()\n\n# Print the result\nprint(weight_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Create a bar graph displaying the mean 'MEANJZD' figure for each 'TIME' category.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Create a bar graph displaying the mean 'MEANJZD' figure for each 'TIME' category.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# compute average 'MEANJZD' for each 'TIME'\naverage_MEANJZD = df.groupby('TIME')['MEANJZD'].mean()\n\n# plot bar chart\nplt.figure(figsize=(10,6))\naverage_MEANJZD.plot(kind='bar')\nplt.title('Average MEANJZD value for each TIME')\nplt.xlabel('TIME')\nplt.ylabel('Average MEANJZD')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\nplt.show()\n\nprint(average_MEANJZD)", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean 'MEANJZD' for each 'TIME' category\nmean_jzd = df.groupby('TIME')['MEANJZD'].mean()\n\n# Create the bar graph\nplt.bar(mean_jzd.index, mean_jzd.values)\nplt.xlabel('TIME')\nplt.ylabel('Mean MEANJZD')\nplt.title('Mean MEANJZD for each TIME category')\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\nprint(mean_jzd)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Plot a line graph to show the trend of 'MEANSHR' over time.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Plot a line graph to show the trend of 'MEANSHR' over time.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load data from CSV file\ndf = pd.read_csv(\"infiagent/csv/6.csv\")\n\n# Convert the TRUE_TIME column into datetime format\ndf[\"TRUE_TIME\"] = pd.to_datetime(df[\"TRUE_TIME\"],format=\"%Y.%m.%d_%H:%M:%S_TAI\")\n\n# set TRUE_TIME as the index\ndf.set_index(\"TRUE_TIME\",inplace=True)\n\n# Create a line plot showing the trend of 'MEANSHR' over time\nplt.plot(df.index,df['MEANSHR'])\nplt.xlabel('Date')\nplt.ylabel('MEANSHR')\nplt.title('Trend of MEANSHR over time')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'TRUE_TIME' column to datetime\ndf['TRUE_TIME'] = pd.to_datetime(df['TRUE_TIME'])\n\n# Set 'TRUE_TIME' as the index\ndf.set_index('TRUE_TIME', inplace=True)\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(df.index, df['MEANSHR'])\nplt.xlabel('Time')\nplt.ylabel('MEANSHR')\nplt.title('Trend of MEANSHR over time')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Create a pie chart displaying the percentage of each distinct NEUTRALDESCRIPTION in the table.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Create a pie chart displaying the percentage of each distinct NEUTRALDESCRIPTION in the table.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV data into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Count the occurrences of each unique value in the \"NEUTRALDESCRIPTION\" column\nneutral_desc_counts = df['NEUTRALDESCRIPTION'].value_counts()\n\n# Generate a pie chart representing the proportion of each unique \"NEUTRALDESCRIPTION\"\nplt.figure(figsize=(10,10))\nplt.pie(neutral_desc_counts,labels=neutral_desc_counts.index,autopct='%1.1f%%')\nplt.title('Proportion of each unique NEUTRALDESCRIPTION')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# clean the data\ndf = df.dropna(subset=['NEUTRALDESCRIPTION'])\n\n# count the frequency of each unique value in the 'NEUTRALDESCRIPTION' column\nneutral_desc_freq = df['NEUTRALDESCRIPTION'].value_counts()\n\n# create a pie chart\nplt.figure(figsize=(10,6))\nplt.pie(neutral_desc_freq, labels=neutral_desc_freq.index, autopct='%1.1f%%')\nplt.title('Percentage of Each Distinct Neutral Description')\nplt.show()\n\n# print the results\nprint('The pie chart has been created successfully.')"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Draw a histogram of the 'Rings' column to illustrate the distribution of age.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Draw a histogram of the 'Rings' column to illustrate the distribution of age.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Validate that 'Rings' column exists\nif 'Rings' in data.columns:\n\n    # Plotting histogram for 'Rings' column\n    plt.hist(data['Rings'],bins=30,color ='blue',edgecolor='black')\n    plt.title('Distribution of Age')\n    plt.xlabel('Rings')\n    plt.ylabel('Count')\n    plt.show()\n    \nelse:\n    print(\"Column 'Rings' does not exist in data.\")\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data from csv file\ndf = pd.read_csv('data.csv')\n\n# Draw a histogram of the 'Rings' column\nplt.hist(df['Rings'], bins=30, edgecolor='black')\n\nplt.title('Histogram of Rings')\nplt.xlabel('Rings')\nplt.ylabel('Frequency')\n\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Draw a pie chart to illustrate the proportion of different emotions recorded for 'marina' based on average values.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Draw a pie chart to illustrate the proportion of different emotions recorded for 'marina' based on average values.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# filter rows by name\ndf_marina = df[df['name'] == 'marina']\n\n# calculate average values for each emotion\navg_emotions = df_marina[['anger','contempt','disgust','fear','happiness','neutral','sadness','surprise']].mean()\n\n# draw pie chart\nfig,ax = plt.subplots()\nax.pie(avg_emotions,labels=avg_emotions.index,autopct='%1.1f%%')\nplt.title('Emotions Proportions for Marina')\n\nprint('The pie chart is displayed.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Selecting the rows for 'marina'\nmarina_df = df[df['name'] == 'marina']\n\n# Calculating the average values for each emotion\navg_emotions = marina_df.mean()[2:]\n\n# Plotting the pie chart\nplt.figure(figsize=(10, 7))\nplt.pie(avg_emotions, labels=avg_emotions.index, autopct='%1.1f%%')\nplt.title('Proportion of Different Emotions for Marina')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Create a pie chart to display the proportion of various offenses.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Create a pie chart to display the proportion of various offenses.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data from the CSV file\ndata = pd.read_csv('infiagent/csv/10.csv')\n\n# Group the data by the 'Offense' column and count the number of each offense\noffense_counts = data['Offense'].value_counts()\n\n# Create the pie chart\nplt.pie(offense_counts,labels = offense_counts.index)\n\n# Add title\nplt.title('Proportion of Various Offenses')\n\n# Show the pie chart\nplt.show()\n\n# To print the output\nprint('Pie chart displaying the proportion of various offenses has been created.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Generate the data for the pie chart\noffense_counts = df['Offense'].value_counts()\n\n# Create the pie chart\nplt.pie(offense_counts, labels=offense_counts.index, autopct='%1.1f%%')\n\n# Display the chart\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Create a bar graph displaying the count of 'JOBTITLE' within each 'DEPTID' division from the dataset.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Create a bar graph displaying the count of 'JOBTITLE' within each 'DEPTID' division from the dataset.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv(\"infiagent/csv/12.csv\")\n\n# Group by 'DEPTID' and count the number of 'JOBTITLE'\ngrouped_data = data.groupby('DEPTID')['JOBTITLE'].count()\n\n# Plot a bar chart\nplt.figure(figsize=(10,8))\ngrouped_data.plot(kind='bar')\nplt.xlabel('DEPTID')\nplt.ylabel('Number of JOBTITLE')\nplt.title('Number of JOBTITLE in each DEPTID')\nplt.show()\n\n# remember to print the output at the end\nprint('The bar chart was successfully created.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'DEPTID' and 'JOBTITLE', and count the number of employees in each group\ngrouped_df = df.groupby(['DEPTID', 'JOBTITLE']).size().reset_index(name='Counts')\n\n# Pivot the table so we can plot the count of each 'JOBTITLE' within each 'DEPTID'\npivot_df = grouped_df.pivot(index='DEPTID', columns='JOBTITLE', values='Counts').fillna(0)\n\n# Plot the data\npivot_df.plot(kind='bar', stacked=True)\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Create a bar chart illustrating the count of players according to their respective salary ranges (0-1000, 1000-2000, 2000-3000, and 3000+).", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Create a bar chart illustrating the count of players according to their respective salary ranges (0-1000, 1000-2000, 2000-3000, and 3000+).", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndata = pd.read_csv('infiagent/csv/14.csv')\n\n# Define salary ranges and labels\nbins = [0,1000,2000,3000,max(data['salary_in_thousands_of_dollars'])]\nlabels = ['0-1000','1000-2000','2000-3000','3000+']\n\n# Add a new column 'salary_range' to the dataset\ndata['salary_range'] = pd.cut(data['salary_in_thousands_of_dollars'],bins,labels=labels)\n\n# Count the number of players in each salary range\nsalary_range_counts = data['salary_range'].value_counts()\n\n# Create a bar plot\nplt.bar(salary_range_counts.index,salary_range_counts.values)\nplt.xlabel('Salary Range')\nplt.ylabel('Number of Players')\nplt.title('Number of Players in Each Salary Range')\n\nprint(plt.show())", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Bin the salary into different ranges and count the number of players in each range\nsalary_counts = pd.cut(data['salary_in_thousands_of_dollars'], bins=[0, 1000, 2000, 3000, float('inf')], include_lowest=True).value_counts()\n\n# Create a bar chart\nplt.bar(salary_counts.index, salary_counts.values)\nplt.xlabel('Salary Range')\nplt.ylabel('Count of Players')\nplt.title('Count of Players According to Their Salary Ranges')\nplt.show()\n\n# Print the salary distribution\nprint(salary_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Plot a histogram of the number_of_strike_outs to understand its distribution.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Plot a histogram of the number_of_strike_outs to understand its distribution.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# take the column 'number_of_strike_outs' and plot its histogram\nplt.hist(df['number_of_strike_outs'],bins='auto',color='#0504aa',alpha=0.7,rwidth=0.85)\n\n# set title and labels\nplt.title('Histogram of Number of Strike Outs')\nplt.xlabel('Number of Strike Outs')\nplt.ylabel('Frequency')\n\n# show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Plot the histogram\nplt.hist(df['number_of_strike_outs'], bins=20, alpha=0.5, color='g', edgecolor='black')\nplt.title('Distribution of Number of Strike Outs')\nplt.xlabel('Number of Strike Outs')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Draw a bar chart representing the number of individuals in each 'looks' category.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Draw a bar chart representing the number of individuals in each 'looks' category.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Group by 'looks' and count the number of unique individuals in each category\nlook_counts = df['looks'].value_counts()\n\n# Plot the frequency of each 'looks' category\nlook_counts.plot(kind='bar',figsize=(8,5))\n\n# Set the title and labels\nplt.title('Number of individuals in each looks category')\nplt.xlabel('Looks')\nplt.ylabel('Number of individuals')\n\n# Show the plot\nplt.show()\n\n# Print the results\nprint(\"The bar chart has been successfully plotted.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the number of individuals in each 'looks' category\nlooks_counts = df['looks'].value_counts()\n\n# Create the bar chart\nplt.bar(looks_counts.index, looks_counts.values)\nplt.xlabel('Looks')\nplt.ylabel('Number of individuals')\nplt.title('Number of individuals in each \"looks\" category')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Create a line graph that displays the average 'wage' for each 'educ' level.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Create a line graph that displays the average 'wage' for each 'educ' level.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the DataFrame\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Group by 'educ' and calculate the average wage\navg_wage_by_educ = df.groupby('educ')['wage'].mean()\n\n# Create the plot\nplt.plot(avg_wage_by_educ.index,avg_wage_by_educ.values)\n\n# Set the title and labels\nplt.title('Average Wage by Education Level')\nplt.xlabel('Education Level')\nplt.ylabel('Average Wage')\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'educ' and calculate the mean 'wage'\navg_wage_by_educ = df.groupby('educ')['wage'].mean()\n\n# Create the line plot\nplt.figure(figsize=(6, 6)\nplt.plot(avg_wage_by_educ)\nplt.xlabel('Educational Level')\nplt.ylabel('Average Wage')\nplt.title('Average Wage by Educational Level')\nplt.grid(True)\nplt.show()\n\nprint(avg_wage_by_educ)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Create a histogram by dividing the total number of users ('cnt') into 20 equal intervals to visualize its distribution.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Create a histogram by dividing the total number of users ('cnt') into 20 equal intervals to visualize its distribution.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Histogram of total users\nplt.hist(df['cnt'],bins=20,edgecolor='black')\nplt.title('Distribution of Total Users')\nplt.xlabel('Total Users')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a histogram of the 'cnt' column\nplt.hist(df['cnt'], bins=20, edgecolor='black')\n\n# Set the title and labels\nplt.title('Distribution of Total Users')\nplt.xlabel('Count of Users')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Display a pie chart representation of 'Volume' traded per day, using 'Date' for grouping.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Display a pie chart representation of 'Volume' traded per day, using 'Date' for grouping.", "instruction_type": "Draw-Chart", "reference_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the .csv file\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Convert 'Volume' column to int without commas\ndf['Volume'] = df['Volume'].str.replace(',','').astype(int)\n\n# Group the data by 'Date' and sum 'Volume'\ngrouped_df = df.groupby('Date').Volume.sum()\n\n# Create the plot\nplt.figure(figsize=(10,10)) # adjust as per your requirement\nplt.pie(grouped_df,labels=grouped_df.index,autopct='%1.1f%%',startangle=90)\n\n# Display the pie chart\nplt.axis('equal') # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Volume Traded per Day')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove comma from 'Volume' and convert it to float for plotting\ndf['Volume'] = df['Volume'].str.replace(',', '').astype(float)\n\n# Group by 'Date' and calculate sum of 'Volume' for each day\ngrouped_volume = df.groupby('Date')['Volume'].sum()\n\n# Plot pie chart\nplt.figure(figsize=(10,6))\nplt.pie(grouped_volume, labels = grouped_volume.index, autopct='%1.1f%%')\nplt.title('Volume Traded per Day')\nplt.show()\n\n# No specific output is expected\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Create a bar graph displaying the 'Market Cap' values for each date.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Create a bar graph displaying the 'Market Cap' values for each date.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Convert the 'Market Cap' column to numeric data,removing commas\ndf['Market Cap'] = df['Market Cap'].str.replace(',','').astype(int)\n\n# Plot a bar chart with 'Date' on the x-axis and 'Market Cap' on the y-axis\nplt.bar(df['Date'],df['Market Cap'])\nplt.xlabel('Date')\nplt.ylabel('Market Cap')\nplt.title('Market Cap per Date')\nplt.xticks(rotation=90)\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Remove commas from 'Market Cap' column and convert it to numeric\ndata['Market Cap'] = pd.to_numeric(data['Market Cap'].str.replace(',', ''))\n\n# Plot the 'Market Cap' values over the dates\nplt.figure(figsize=(10, 6))\nplt.bar(data['Date'], data['Market Cap'])\nplt.xlabel('Date')\nplt.ylabel('Market Cap')\nplt.title('Market Cap values over dates')\nplt.xticks(rotation=45)\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Plot a bar chart indicating the number of individuals from each 'native-country'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Plot a bar chart indicating the number of individuals from each 'native-country'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Analyze the 'native-country' column\ncountry_count = data['native-country'].value_counts()\n\n# Plot\nplt.figure(figsize=(10,5))\ncountry_count.plot(kind='bar')\nplt.xlabel('Native Country')\nplt.ylabel('Number of individuals')\nplt.title('Number of individuals from each native country')\n\n# Print out the chart\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of individuals from each country\ncountry_counts = df['native-country'].value_counts()\n\n# Plot the bar chart\nplt.bar(country_counts.index, country_counts.values)\nplt.xlabel('Country')\nplt.ylabel('Number of Individuals')\nplt.title('Number of Individuals from Each Country')\nplt.xticks(rotation=90)\n\n# Show the plot\nplt.show()\n\n# Print the number of individuals from each country\nfor i in range(len(country_counts)):\n    print(f\"{country_counts.index[i]}: {country_counts.values[i]}\")"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Create a histogram showing the distribution of the 'age' variable.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Create a histogram showing the distribution of the 'age' variable.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Create a histogram of the 'age' column\nplt.hist(df['age'],bins=30,edgecolor='black')\n\n# Set the title and labels\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\n# Display the plot\nplt.show()\n\nprint(\"Histogram has been successfully created.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create the histogram\nplt.hist(df['age'], bins=20, color='blue', edgecolor='black')\n\n# Set the title and labels\nplt.title('Distribution of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Create a pie chart illustrating the proportion of departments within each 'dept_group'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Create a pie chart illustrating the proportion of departments within each 'dept_group'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# preparing data for pie chart\ngrouped_data = df['Department Name'].groupby(df['dept_group']).count()\n\n# Plotting pie chart\nplt.figure(figsize=(10,7))\nplt.pie(grouped_data,labels=grouped_data.index,autopct='%1.1f%%')\nplt.title('Distribution of departments in each \"dept_group\"')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'dept_group' and count the number of departments in each group\ndept_group_counts = df['dept_group'].value_counts()\n\n# Create the pie chart\nplt.pie(dept_group_counts, labels=dept_group_counts.index, autopct='%1.1f%%')\n\n# Display the chart\nplt.title('Proportion of Departments within Each Dept Group')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Draw a histogram to compare the frequency of 'dept_group' across the department.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Draw a histogram to compare the frequency of 'dept_group' across the department.", "instruction_type": "Draw-Chart", "reference_code": "# Required Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the Data\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Count the frequency\ndept_group_freq = df['dept_group'].value_counts()\n\n# Plot Histogram\nplt.figure(figsize=(10,6))\nplt.bar(dept_group_freq.index,dept_group_freq.values,color='blue')\nplt.xlabel('Department Group')\nplt.ylabel('Frequency')\nplt.title('Frequency of Department Group')\nplt.show()\n\n# There is no actual output as the objective is to plot a chart\nprint(\"The histogram has been plotted.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the frequency of each 'dept_group'\ndept_group_counts = df['dept_group'].value_counts()\n\n# Plot the histogram\nplt.figure(figsize=(10,6))\nplt.bar(dept_group_counts.index, dept_group_counts.values)\nplt.xlabel('Department Group')\nplt.ylabel('Frequency')\nplt.title('Frequency of Department Group')\nplt.show()\n\n# Print the results\nprint(dept_group_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Create a histogram to analyze the distribution of the 'Excess Readmission Ratio'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Create a histogram to analyze the distribution of the 'Excess Readmission Ratio'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Clean the data by removing rows with NaN values in 'Excess Readmission Ratio' column\ndf = df.dropna(subset=['Excess Readmission Ratio'])\n\n# Plot histogram\nplt.hist(df['Excess Readmission Ratio'],bins=50,alpha=.5,edgecolor='black')\nplt.title('Histogram of Excess Readmission Ratio')\nplt.xlabel('Excess Readmission Ratio')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\nprint('Histogram plotted successfully.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Create histogram for 'Excess Readmission Ratio'\nplt.hist(data['Excess Readmission Ratio'], bins=50, edgecolor='black')\n\n# Setting the title and labels\nplt.title('Distribution of Excess Readmission Ratio')\nplt.xlabel('Excess Readmission Ratio')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Create a pie chart displaying the relative proportion of each 'max_storm_cat' in terms of its contribution to the overall 'damage_USD' amount.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Create a pie chart displaying the relative proportion of each 'max_storm_cat' in terms of its contribution to the overall 'damage_USD' amount.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Calculate the total damage caused by each storm category\ndamage_by_storm = df.groupby('max_storm_cat')['damage_USD'].sum()\n\n# Plot a pie chart\nplt.pie(damage_by_storm,labels = damage_by_storm.index,autopct='%1.1f%%')\nplt.title('Percentage Contribution of Each Storm Category to Total Damage')\nplt.show()\n\n# Note: The actual output will depend on the data. The print statement is not required in this context as the output is a graph.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# drop the rows where 'damage_USD' is NaN\ndf = df.dropna(subset=['damage_USD'])\n\n# group the data by 'max_storm_cat' and sum the 'damage_USD' for each category\ndf_grouped = df.groupby('max_storm_cat')['damage_USD'].sum()\n\n# plot the pie chart\nplt.figure(figsize=(10,7))\nplt.pie(df_grouped, labels = df_grouped.index, autopct='%1.1f%%')\nplt.title('Relative proportion of each max_storm_cat to the overall damage_USD')\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Prepare a bar chart for 'daily_vaccinations' for the country 'Albania'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Prepare a bar chart for 'daily_vaccinations' for the country 'Albania'.", "instruction_type": "Draw-Chart", "reference_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Preprocess the data\n# Replacing the NaN values with 0\ndf = df.fillna(0)\n\n# Filter Data for 'Albania'\ndf_Albania = df[df['country'] == 'Albania']\n\n# Generate the Bar chart\nplt.figure(figsize=(10,6))\nplt.bar(df_Albania['date'],df_Albania['daily_vaccinations'])\nplt.xlabel('Date')\nplt.ylabel('Daily Vaccinations')\nplt.title('Daily Vaccinations in Albania')\nplt.xticks(rotation = 45)\nplt.tight_layout()\n\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter data for country 'Albania'\nalbania_data = data[data['country'] == 'Albania']\n\n# Plot the bar chart\nplt.figure(figsize=(10, 5))\nplt.bar(albania_data['date'], albania_data['daily_vaccinations'])\nplt.xlabel('Date')\nplt.ylabel('Daily Vaccinations')\nplt.title('Daily Vaccinations in Albania')\nplt.grid(True)\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Draw a pie chart showing the proportion of users with a 'low', 'medium', and 'high' 'income_level' to visualize income distribution.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Draw a pie chart showing the proportion of users with a 'low', 'medium', and 'high' 'income_level' to visualize income distribution.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_csv('infiagent/csv/23.csv')\n\n# Defining income levels\ndef income_level(income):\n    if income < 3000:\n        return 'low'\n    elif 3000 <= income < 6000:\n        return 'medium'\n    else:\n        return 'high'\n\n# Apply the income_level function to monthly_income column to create income_level column\ndata['income_level'] = data['monthly_income'].apply(income_level)\n\n# Obtain the count of each income level category\nincome_distribution = data['income_level'].value_counts()\n\n# Plot the pie chart\nplt.figure(figsize=(10,6))\nplt.pie(income_distribution,labels = income_distribution.index,autopct='%1.1f%%')\nplt.title('Income Distribution')\nplt.axis('equal')\n\n# To display output\nprint('plt.show()')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize income into 'low', 'medium', and 'high'\ndef income_category(income):\n    if income <= 3000:\n        return 'low'\n    elif 3000 < income <= 6000:\n        return 'medium'\n    else:\n        return 'high'\n\n# Apply the function to the 'monthly_income' column to create a new 'income_level' column\ndf['income_level'] = df['monthly_income'].apply(income_category)\n\n# Count the number of people in each income category\nincome_counts = df['income_level'].value_counts()\n\n# Plot the pie chart\nplt.pie(income_counts, labels = income_counts.index, autopct='%1.1f%%')\nplt.title('Income Distribution')\nplt.show()\n\n# The result will be a pie chart showing the proportion of users in each income category\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Construct a pie chart illustrating the relative percentage of 'Married' and 'Unmarried' customers in the given dataset.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Construct a pie chart illustrating the relative percentage of 'Married' and 'Unmarried' customers in the given dataset.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/24.csv')\n\n# Count the 'Married' and 'Unmarried' customers\nmarital_status_counts = data['Married'].value_counts()\n\n# Make the pie chart\nplt.figure(figsize=(10,6))\nplt.pie(marital_status_counts,labels=marital_status_counts.index,autopct='%1.1f%%')\n\n# Add title and legend\nplt.title('Proportion of \\'Married\\' vs \\'Unmarried\\' customers')\nplt.legend()\n\n# Show the chart\nplt.show()\n\n# Print result\nprint('Pie chart generated successfully.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Prepare the data for the pie chart\nmarried_count = df['Married'].value_counts()['Yes']\nunmarried_count = df['Married'].value_counts()['No']\n\n# Create the pie chart\nplt.pie([married_count, unmarried_count], labels=['Married', 'Unmarried'], autopct='%1.1f%%')\n\n# Display the chart\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Create a pie chart showing the proportions of the number of dependents in the dataset.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Create a pie chart showing the proportions of the number of dependents in the dataset.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Handle non-standard data. In this case,replace NA with 0\ndf = df.fillna(0)\n\n# Count the number of each value in the 'NumberOfDependents' column\ndep_counts = df['NumberOfDependents'].value_counts()\n\n# Plot a pie chart\nplt.figure(figsize=(10,10))\nplt.pie(dep_counts,labels=dep_counts.index,autopct='%1.1f%%')\nplt.title('Proportions of the number of dependents')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Count the number of dependents\ndependent_counts = df['NumberOfDependents'].value_counts()\n\n# Create a pie chart\nplt.pie(dependent_counts, labels=dependent_counts.index, autopct='%1.1f%%')\nplt.title('Proportions of Number of Dependents')\nplt.show()\n\n# Print the number of dependents\nprint(dependent_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Generate a histogram to display the distribution of 'Wins' values across all teams.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Generate a histogram to display the distribution of 'Wins' values across all teams.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Create a histogram of 'Wins' distribution\nplt.hist(data['Wins'],bins=range(data['Wins'].min(),data['Wins'].max() + 1))\nplt.xlabel('Wins')\nplt.ylabel('Number of Teams')\nplt.title('Distribution of Wins Across Teams')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Plot the histogram\nplt.hist(df['Wins'], bins=20, color='blue', edgecolor='black')\nplt.title('Distribution of Wins')\nplt.xlabel('Number of Wins')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\n# Since function doesn't return any value, hence printing none/null.\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Create a bar graph to display the number of diamonds grouped by \"cut\" category.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Create a bar graph to display the number of diamonds grouped by \"cut\" category.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV data\ndata = pd.read_csv('infiagent/csv/28.csv')\n\n# Count the number of diamonds for each cut type\ncut_counts = data['cut'].value_counts()\n\n# Draw a bar chart \nplt.figure(figsize=(8,5))\ncut_counts.plot(kind='bar',color='blue',alpha=0.7)\nplt.title('Count of Diamonds for Each Cut Type')\nplt.xlabel('Cut Type')\nplt.ylabel('Count')\nplt.show()\n\n# There is no result to print as the output is a chart", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of diamonds in each \"cut\" category\ncut_counts = df['cut'].value_counts()\n\n# Create a bar plot of the counts\nplt.bar(cut_counts.index, cut_counts.values)\nplt.xlabel('Cut Quality')\nplt.ylabel('Number of Diamonds')\nplt.title('Number of Diamonds Grouped by Cut Quality')\n\n# Show the plot\nplt.show()\n\n# Print the number of diamonds in each \"cut\" category\nfor i in range(len(cut_counts)):\n    print(f\"{cut_counts.index[i]}: {cut_counts.values[i]}\")"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Draw a histogram showing the frequency distribution of \"price\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Draw a histogram showing the frequency distribution of \"price\".", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Plot a histogram for the 'price' column\nplt.hist(df['price'],bins=50,color='blue',edgecolor='black')\nplt.title('Frequency Distribution of Price')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\nprint(\"Histogram is displayed.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Draw the histogram\nplt.hist(df['price'], bins=10)\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.title('Frequency Distribution of Price')\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Draw a line chart showing price (Y-axis) against dates (X-axis), where price lines include 'Open', 'High', 'Low', 'Close'.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Draw a line chart showing price (Y-axis) against dates (X-axis), where price lines include 'Open', 'High', 'Low', 'Close'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Convert 'Date' to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Plotting the data\nplt.figure(figsize=(10,5))\nplt.plot(df['Date'],df['Open'],label='Open')\nplt.plot(df['Date'],df['High'],label='High')\nplt.plot(df['Date'],df['Low'],label='Low')\nplt.plot(df['Date'],df['Close'],label='Close')\nplt.title('Price vs Time')\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.legend()\nplt.show()\n\n# Print statement to output the results\nprint(\"The plot has been successfully generated.\")", "assistant_code": "# necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# create a new figure\nplt.figure()\n\n# set the size of the figure and create a set of subplots\nplt.figure(figsize=(14, 7))\n\n# plot the 'Open' price line\nplt.plot(df['Date'], df['Open'], label='Open')\n\n# plot the 'High' price line\nplt.plot(df['Date'], df['High'], label='High')\n\n# plot the 'Low' price line\nplt.plot(df['Date'], df['Low'], label='Low')\n\n# plot the 'Close' price line\nplt.plot(df['Date'], df['Close'], label='Close')\n\n# set the title of the plot\nplt.title('Price Lines')\n\n# set the label of x and y axis\nplt.xlabel('Date')\nplt.ylabel('Price')\n\n# add a legend to the plot\nplt.legend()\n\n# show the plot\nplt.show()\n\n# print result\nprint(\"The line chart has been drawn successfully.\")"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Draw a pie chart representing the distribution of 'votes_dem' and 'votes_gop' for 'state_abbr' \"AK\".", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Draw a pie chart representing the distribution of 'votes_dem' and 'votes_gop' for 'state_abbr' \"AK\".", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Filter data for 'state_abbr' \"AK\"\ndf_ak = df[df['state_abbr'] == 'AK']\n\n# Calculate total 'votes_dem' and 'votes_gop' for \"AK\"\nvotes_dem_total = df_ak['votes_dem'].sum()\nvotes_gop_total = df_ak['votes_gop'].sum()\n\n# Plot\nlabels = ['votes_dem','votes_gop']\nsizes = [votes_dem_total,votes_gop_total]\nplt.pie(sizes,labels=labels,autopct='%1.1f%%')\nplt.axis('equal')  \nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data for the state \"AK\"\ndf_AK = df[df['state_abbr'] == 'AK']\n\n# Plot a pie chart\nplt.figure(figsize=(10,7))\nplt.pie(df_AK[['votes_dem', 'votes_gop']].sum(), labels = ['Democratic Votes', 'Republican Votes'], autopct='%1.1f%%')\nplt.title('Distribution of Votes for State AK')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Create a line chart displaying the fluctuations in 'votes_dem' and 'votes_gop' for each respective 'state_abbr'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Create a line chart displaying the fluctuations in 'votes_dem' and 'votes_gop' for each respective 'state_abbr'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Group by 'state_abbr' and get the sum of 'votes_dem' and 'votes_gop'\ngrouped = df.groupby('state_abbr')[['votes_dem','votes_gop']].sum()\n\n# Reset the index for plotting\ngrouped.reset_index(inplace=True)\n\n# Plotting\nplt.figure(figsize=(15,8))\nfor column in ['votes_dem','votes_gop']:\n    plt.plot(grouped['state_abbr'],grouped[column],marker='',linewidth=2,label=column)\n\nplt.legend()\nplt.title('Trend of votes_dem and votes_gop for each state_abbr')\nplt.xlabel('State_abbr')\nplt.ylabel('Votes')\nplt.grid()\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'state_abbr' and calculate the mean of 'votes_dem' and 'votes_gop'\ngrouped_df = df.groupby('state_abbr').agg({'votes_dem': 'mean', 'votes_gop': 'mean'})\n\n# Create the line chart\nplt.figure(figsize=(10, 6))\nfor state in grouped_df.index:\n    plt.plot(grouped_df.loc[state, 'votes_dem'], label=state + '_dem')\n    plt.plot(grouped_df.loc[state, 'votes_gop'], label=state + '_gop')\nplt.xlabel('State Abbreviation')\nplt.ylabel('Mean Votes')\nplt.title('Fluctuations in Votes for Each State')\nplt.legend()\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Plot a line chart to show the trend of the average value of 'EQ1' to 'EQ8'.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Plot a line chart to show the trend of the average value of 'EQ1' to 'EQ8'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/31.csv')\n\n# Select the columns 'EQ1' to 'EQ8'\neq_cols = data.loc[:,'EQ1':'EQ8']\n\n# Calculate the average value of each column\neq_cols_avg = eq_cols.mean()\n\n# Plot the average values\nplt.figure(figsize=(10,6))\nplt.plot(eq_cols_avg.index,eq_cols_avg.values,marker='o',linestyle='-')\nplt.xlabel('EQ Number')\nplt.ylabel('Average Value')\nplt.title('Trend of the Average Value of EQ1 to EQ8')\nplt.grid(True)\n\nprint('The line chart has been drawn and displayed.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average value of 'EQ1' to 'EQ8' for each line\ndf['average'] = df.loc[:, 'EQ1':'EX8'].mean(axis=1)\n\n# Plot the line chart\nplt.figure(figsize=(10, 6))\nplt.plot(df['average'])\nplt.xlabel('Index')\nplt.ylabel('Average Value')\nplt.title('Trend of the average value of EQ1 to EX8')\nplt.grid(True)\nplt.show()\n\n# Print the result\nprint('The line chart has been plotted successfully.')"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Create a histogram of the 'EQ4' column to visually represent its distribution.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Create a histogram of the 'EQ4' column to visually represent its distribution.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Draw histogram for the 'EQ4' column\nplt.hist(df['EQ4'],bins=50,color='blue',edgecolor='black')\n\n# Set the title and labels\nplt.title('Histogram of EQ4')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\nprint(\"The histogram is successfully plotted.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create histogram for 'EQ4' column\nplt.hist(df['EQ4'], bins=50, alpha=0.5, color='g', edgecolor='black')\n\n# Set the title and labels\nplt.title('Histogram of EQ4')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Plot a line chart reflecting the trend of publishing frequency over time.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Plot a line chart reflecting the trend of publishing frequency over time.", "instruction_type": "Draw-Chart", "reference_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Convert 'publishedAt' column to datetime format\ndf['publishedAt'] = pd.to_datetime(df['publishedAt'])\n\n# Group the data by date and count the number of publications each day\npublication_trend = df.groupby(df['publishedAt'].dt.date).size()\n\n# Plot a line chart reflecting the trend of publishing frequency over time\nplt.figure(figsize=(10,6))\nplt.plot(publication_trend,label='Publication Frequency')\nplt.xlabel('Date')\nplt.ylabel('Frequency')\nplt.title('Publishing Frequency Over Time')\nplt.legend()\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# convert the 'publishedAt' column to datetime\ndf['publishedAt'] = pd.to_datetime(df['publishedAt'])\n\n# extract the date only from 'publishedAt'\ndf['date'] = df['publishedAt'].dt.date\n\n# count the number of articles published each day\ndf_grouped = df.groupby('date').size().reset_index(name='frequency')\n\n# sort the dataframe by date\ndf_grouped = df_grouped.sort_values('date')\n\n# plot the line chart\nplt.figure(figsize=(10,5))\nplt.plot(df_grouped['date'], df_grouped['frequency'], marker='o', linestyle='-')\nplt.xlabel('Date')\nplt.ylabel('Publishing Frequency')\nplt.title('Trend of Publishing Frequency Over Time')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Create a bar graph to display the count of each 'site'.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Create a bar graph to display the count of each 'site'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Count values for each 'site'\nsite_counts = df['site'].value_counts()\n\n# Plot chart\nplt.figure(figsize=(10,6))\nplt.bar(site_counts.index,site_counts.values)\nplt.xlabel('Site')\nplt.ylabel('Count')\nplt.title('Count of each Site')\nplt.show()\n\nprint(\"The bar chart has been displayed.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Count the number of occurrences of each 'site'\nsite_counts = data['site'].value_counts()\n\n# Create a bar graph of the counts\nplt.bar(site_counts.index, site_counts.values)\nplt.xlabel('Site')\nplt.ylabel('Count')\nplt.title('Count of Each Site')\nplt.xticks(rotation=90)\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Draw a pie chart depicting the proportion of each 'site' based on count in the data set.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Draw a pie chart depicting the proportion of each 'site' based on count in the data set.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the CSV data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Counting the occurrences of each site\nsite_counts = df['site'].value_counts()\n\n# Plotting the pie chart\nplt.figure(figsize=(10,10))\nplt.pie(site_counts,labels = site_counts.index,autopct='%1.1f%%')\nplt.title(\"Proportion of each 'site' based on count\")\nplt.show()\n\n# No specific result to print out as the instruction is to draw a plot\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Count the number of occurrences of each 'site'\nsite_counts = data['site'].value_counts()\n\n# Create a pie chart depicting the proportion of each 'site'\nplt.figure(figsize=(10, 8))\nplt.pie(site_counts, labels=site_counts.index, autopct='%1.1f%%')\nplt.title('Proportion of each site')\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Plot a pie chart of the distribution of 'Gender'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Plot a pie chart of the distribution of 'Gender'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# count the values for 'Gender'\ngender_counts = df['Gender'].value_counts()\n\n# plot the pie chart\nplt.figure(figsize=(10,6))\nplt.pie(gender_counts,labels = gender_counts.index,autopct='%1.1f%%')\nplt.title(\"Distribution of 'Gender'\")\nplt.show()\n\n# print the distribution\nprint(gender_counts)", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Calculate the number of male and female\nnum_male = df[df['Gender'] == 1].shape[0]\nnum_female = df[df['Gender'] == 0].shape[0]\n\n# Plot a pie chart\nplt.pie([num_male, num_female], labels=['Male', 'Female'], autopct='%1.1f%%')\nplt.title('Distribution of Gender')\nplt.show()\n\n# The following code is to make sure the output is not blocked\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Calculate the number of male and female\nnum_male = df[df['Gender'] == 1].shape[0]\nnum_female = df[df['Gender'] == 0].shape[0]\n\n# Plot a pie chart\nplt.pie([num_male, num_female], labels=['Male', 'Female'], autopct='%1.1f%%')\nplt.title('Distribution of Gender')\nplt.show()\n\nprint(\"The pie chart has been plotted.\")"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Create a bar chart with 'Age' on the x-axis and the count of different 'part_of_speech' on the y-axis.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Create a bar chart with 'Age' on the x-axis and the count of different 'part_of_speech' on the y-axis.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset from csv\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Group by 'Age' and count the number of different 'part_of_speech'\nage_speech_count = df.groupby('Age')['part_of_speech'].nunique()\n\n# Create a new DataFrame from the series\ndf_age_speech_count = age_speech_count.reset_index()\n\n# Plot a bar chart\nplt.bar(df_age_speech_count['Age'],df_age_speech_count['part_of_speech'])\nplt.xlabel('Age')\nplt.ylabel('Number of different \"part_of_speech\"')\nplt.title('Number of different \"part_of_speech\" for each \"Age\"')\n\n# Print the DataFrame\nprint(df_age_speech_count)", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv data\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'Age' and 'part_of_speech' and count the values\ngrouped_df = df.groupby(['Age', 'part_of_speech']).size().reset_index(name='counts')\n\n# Pivot the dataframe to have 'Age' on x-axis and 'part_of_speech' on y-axis\npivot_df = grouped_df.pivot(index='Age', columns='part_of_speech', values='counts').fillna(0)\n\n# Plot the bar chart\npivot_df.plot(kind='bar', stacked=True)\n\n# Set the title and labels\nplt.title('Bar chart of Age vs part_of_speech')\nplt.xlabel('Age')\nplt.ylabel('Count of part_of_speech')\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Plot the trend of the average 'Impaired' level for each 'Age'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Plot the trend of the average 'Impaired' level for each 'Age'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Group by 'Age' and calculate average 'Impaired' \navg_impairment_by_age = df.groupby('Age')['Impaired'].mean()\n\n# Plot the trend\nplt.plot(avg_impairment_by_age.index,avg_impairment_by_age.values)\nplt.xlabel('Age')\nplt.ylabel('Average Impairment Level')\nplt.title('Trend of Average Impairment Level by Age')\nplt.grid(True)\nplt.show()\n\nprint(\"The trend of the average 'Impaired' level for each 'Age' has been plotted.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'Age' and calculate the mean 'Impaired'\naverage_impaired_by_age = df.groupby('Age')['Impaired'].mean()\n\n# Plot the trend of the average 'Impaired' level for each 'Age'\nplt.figure(figsize=(10, 6))\nplt.plot(average_impaired_by_age.index, average_impaired_by_age.values)\nplt.xlabel('Age')\nplt.ylabel('Average Impaired Level')\nplt.title('Trend of Average Impaired Level for Each Age')\nplt.grid(True)\nplt.show()\n\n# Print the result\nprint(average_impaired_by_age)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Plot a line graph of 'Close Price' over 'Date' for 'GODREJIND' in 'Series' 'EQ'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Plot a line graph of 'Close Price' over 'Date' for 'GODREJIND' in 'Series' 'EQ'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# loading the data from the csv file\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'],format='%d-%b-%Y')\n\n# filter rows for 'GODREJIND' and 'EQ'\nfiltered_df = df[(df['Symbol'] == 'GODREJIND') & (df['Series'] == 'EQ')]\n\n# plot 'Close Price' over 'Date'\nplt.figure(figsize=(10,6))\nplt.plot_date(filtered_df['Date'],filtered_df['Close Price'],'b-',xdate=True)\nplt.title('Close Price Over Time for GODREJIND in EQ Series')\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data\ndata = pd.read_csv('data.csv')\n\n# Selecting the data for the symbol 'GODREJIND' and Series 'EQ'\nselected_data = data[(data['Symbol'] == 'GODREJIND') & (data['Series'] == 'EQ')]\n\n# Plotting the Close Price over Date\nplt.figure(figsize=(10,5))\nplt.plot(selected_data['Date'], selected_data['Close Price'])\nplt.xlabel('Date')\nplt.ylabel('Close Price')\nplt.title('Close Price over Date for GODREJIND in EQ Series')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Plot a histogram of 'Height' data to view distribution.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Plot a histogram of 'Height' data to view distribution.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/39.csv')\n\n# Clean the data,replace any non-numeric value in 'Height' with NaN\ndata['Height'] = pd.to_numeric(data['Height'],errors='coerce')\n\n# Drop the rows where 'Height' is NaN\ndata = data.dropna(subset=['Height'])\n\n# Plotting the histogram\nplt.hist(data['Height'],bins=10,edgecolor='black')\n\n# Labeling the axes and the title\nplt.xlabel('Height')\nplt.ylabel('Frequency')\nplt.title('Histogram of Heights')\n\n# Display the plot\nplt.show()\n\nprint('Histogram is displayed.')", "assistant_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Plot the histogram of 'Height'\nplt.hist(data['Height'], bins=20, color='blue', edgecolor='black')\nplt.title('Height Distribution')\nplt.xlabel('Height')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\n# Since function doesn't return any value, hence printing none/null.\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Draw a pie chart to represent the number of superheroes in each 'Race' category.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Draw a pie chart to represent the number of superheroes in each 'Race' category.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Get the count of each race\nrace_counts = df['Race'].value_counts()\n\n# Plot the pie chart\nplt.figure(figsize=(10,6))\nplt.pie(race_counts,labels=race_counts.index,autopct='%1.1f%%')\nplt.title('Superheroes in Each Race Category')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data from csv\ndf = pd.read_csv('data.csv')\n\n# Count the number of superheroes in each 'Race' category\nrace_counts = df['Race'].value_counts()\n\n# Draw a pie chart\nplt.figure(figsize=(10, 6))\nplt.pie(race_counts, labels=race_counts.index, autopct='%1.1f%%')\nplt.title('Number of Superheroes in Each Race Category')\nplt.show()\n\n# This code will not output any result as a print statement but it will display a plot."}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Create a bar graph to show the number of hotels under each parent brand name.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Create a bar graph to show the number of hotels under each parent brand name.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# count hotels for each parent brand name\nhotel_counts = df['parent_brand_name'].value_counts()\n\n# draw a bar chart\nplt.figure(figsize=(10,6))\nhotel_counts.plot(kind='bar')\nplt.xlabel('Parent Brand Name')\nplt.ylabel('Count of Hotels')\nplt.title('Count of Hotels for Each Parent Brand Name')\nplt.show()\n\nprint('Plot created successfully')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'parent_brand_name' and count the number of hotels in each group\nbrand_counts = df.groupby('parent_brand_name')['hotel_id'].count()\n\n# Create the bar graph\nplt.figure(figsize=(10, 6))\nbrand_counts.plot(kind='bar', color='blue')\nplt.xlabel('Parent Brand Name')\nplt.ylabel('Number of Hotels')\nplt.title('Number of Hotels under Each Parent Brand Name')\nplt.show()\n\n# Print the answer\nprint(brand_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Draw a pie chart denoting the proportion of hotels of different star_ratings in the New York City.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Draw a pie chart denoting the proportion of hotels of different star_ratings in the New York City.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Filter out data for New York City\ndf_nyc = df[df['city_name'] == 'New York City']\n\n# Group the data by star_rating and compute the proportions\nstar_rating_counts = df_nyc['star_rating'].value_counts(normalize=True)\n\n# Plot the pie chart\nplt.figure(figsize=(10,6))\nplt.pie(star_rating_counts,labels = star_rating_counts.index,autopct='%1.1f%%')\nplt.title('Proportion of hotels of different star ratings in New York City')\nplt.show()\n\n# Print the result\nprint(\"The pie chart has been successfully generated!\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe to only include rows where the city is New York City\ndf_nyc = df[df['city_name'] == 'New York City']\n\n# Group the dataframe by star rating and count the number of hotels in each rating\nhotel_counts = df_nyc['star_rating'].value_counts()\n\n# Create a pie chart of the counts\nplt.pie(hotel_counts, labels=hotel_counts.index, autopct='%1.1f%%')\nplt.title('Proportion of Hotels by Star Rating in New York City')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Draw a chart to compare the frequency of properties based on their 'condition' rating.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Draw a chart to compare the frequency of properties based on their 'condition' rating.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Calculate the frequency of each condition rating\ncondition_frequencies = df['condition'].value_counts()\n\n# Plot the frequencies\nplt.figure(figsize=(10,6))\nplt.bar(condition_frequencies.index,condition_frequencies.values,color='blue',alpha=0.7)\nplt.title('The frequency of properties based on their condition rating')\nplt.xlabel('Condition Rating')\nplt.ylabel('Frequency')\nplt.grid()\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the frequency of each condition\ncondition_counts = df['condition'].value_counts()\n\n# Plot a bar chart\nplt.bar(condition_counts.index, condition_counts.values)\nplt.xlabel('Condition Rating')\nplt.ylabel('Frequency')\nplt.title('Frequency of Properties Based on Condition Rating')\nplt.xticks(range(len(condition_counts)))\n\n# Show the plot\nplt.show()\n\n# Print the frequency of each condition\nfor i in range(len(condition_counts)):\n    print(f'Condition {i+1}: {condition_counts.index[i]} - Frequency: {condition_counts.values[i]}')"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Create a bar chart displaying the distribution of 'importance.score'.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Create a bar chart displaying the distribution of 'importance.score'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Set the plot size\nplt.figure(figsize=(10,5))\n\n# Plot the data\ndf['importance.score'].value_counts().plot(kind='bar')\n\nplt.title('Distribution of importance.score')\nplt.xlabel('Score')\nplt.ylabel('Frequency')\n\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Plot the distribution of 'importance.score'\nplt.hist(data['importance.score'], bins=50, alpha=0.5, color='g', edgecolor='black')\nplt.xlabel('importance.score')\nplt.ylabel('Frequency')\nplt.title('Distribution of importance.score')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Draw a line chart to illustrate the trend of 'importance.score' for different 'LibraryID'.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Draw a line chart to illustrate the trend of 'importance.score' for different 'LibraryID'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_csv('infiagent/csv/42.csv')\n\n# Group the dataset by 'LibraryID' and calculate average 'importance.score'\ngrouped_data = data.groupby('LibraryID')['importance.score'].mean().reset_index()\n\n# Sort the data by 'LibraryID'\ngrouped_data.sort_values(by='LibraryID',inplace=True)\n\n# Plotting\nplt.figure(figsize=(10,5))\nplt.plot(grouped_data['LibraryID'],grouped_data['importance.score'])\nplt.title('Trend of importance.score for different LibraryID')\nplt.xlabel('LibraryID')\nplt.ylabel('Average Importance Score')\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# To show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where 'LibraryID' is NaN\ndf = df.dropna(subset=['LibraryID'])\n\n# Group by 'LibraryID' and calculate the mean of 'importance.score'\nmean_importance_score = df.groupby('LibraryID')['importance.score'].mean()\n\n# Plot the line chart\nplt.figure(figsize=(10, 6))\nplt.plot(mean_importance_score.index, mean_importance_score.values, marker='o', linestyle='-')\nplt.xlabel('LibraryID')\nplt.ylabel('Mean Importance Score')\nplt.title('Trend of Importance Score for Different LibraryID')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Draw a pie chart showing the percentage of records for each 'bmi_category'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Draw a pie chart showing the percentage of records for each 'bmi_category'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Categorize the bmi data into categories\ndf['bmi_category'] = pd.cut(df['bmi'],bins=[0,18.5,25,30,35,100],labels=['Underweight','Healthy Weight','Overweight','Obese','Severely Obese'])\n\n# Calculate the percentage of records for each 'bmi_category'\nbmi_category_percents = df['bmi_category'].value_counts(normalize=True) * 100\n\n# Create a pie chart\nplt.figure(figsize=(10,7))\nplt.pie(bmi_category_percents,labels = bmi_category_percents.index,autopct='%1.1f%%',startangle=140)\n\nplt.axis('equal')  # Equal aspect ratio ensures pie chart is circular\nplt.title('Percentage of records for each BMI category')\n\nprint(\"plt.show()\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# categorize 'bmi' into 'underweight', 'normal', 'overweight', 'obese'\ndf['bmi_category'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, float('inf')], labels=['underweight', 'normal', 'overweight', 'obese'])\n\n# calculate the number of records for each 'bmi_category'\nbmi_counts = df['bmi_category'].value_counts()\n\n# plot the pie chart\nplt.pie(bmi_counts, labels=bmi_counts.index, autopct='%1.1f%%')\nplt.title('Percentage of Records for Each BMI Category')\nplt.show()\n\n# No print statement is needed as the output would be a plot"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Draw a line chart to show the trend of the average 'charges' with 'age' for 'smoker' and 'non-smoker'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Draw a line chart to show the trend of the average 'charges' with 'age' for 'smoker' and 'non-smoker'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndata = pd.read_csv('infiagent/csv/43.csv')\n\n# Group the data by age and smoker status and calculate the average charges\navg_charges = data.groupby(['age','smoker'])['charges'].mean().reset_index()\n\n# Separate the data for smokers and non-smokers\nsmoker_data = avg_charges[avg_charges['smoker'] == 'yes']\nnon_smoker_data = avg_charges[avg_charges['smoker'] == 'no']\n\n# Generate the line plot\nplt.figure(figsize=(10,6))\nplt.plot(smoker_data['age'],smoker_data['charges'],label='Smokers')\nplt.plot(non_smoker_data['age'],non_smoker_data['charges'],label='Non-smokers')\n\n# Adding title and labels\nplt.title('Average charges with age for Smokers Vs Non-smokers')\nplt.xlabel('Age')\nplt.ylabel('Average Charges')\n\n# Adding legend\nplt.legend()\n\n# Print statement to describe the output\nprint(\"\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Clean the data\ndf = df[df['smoker'].notna()]\n\n# Separate the data into smoker and non-smoker\ndf_smoker = df[df['smoker'] == 'yes']\ndf_non_smoker = df[df['smoker'] == 'no']\n\n# Calculate the average charges for each age group for both smoker and non-smoker\naverage_charges_smoker = df_smoker.groupby('age')['charges'].mean()\naverage_charges_non_smoker = df_non_smoker.groupby('age')['charges'].mean()\n\n# Plot the line chart\nplt.figure(figsize=(10, 5))\nplt.plot(average_charges_smoker.index, average_charges_smoker.values, label='Smoker')\nplt.plot(average_charges_non_smoker.index, average_charges_non_smoker.values, label='Non-Smoker')\nplt.xlabel('Age')\nplt.ylabel('Average Charges')\nplt.title('Trend of Average Charges with Age for Smoker and Non-Smoker')\nplt.legend()\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Plot a line graph displaying the trend of \"High\" values over the \"Date\" time.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Plot a line graph displaying the trend of \"High\" values over the \"Date\" time.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the dataset\ndf = pd.read_csv(\"infiagent/csv/44.csv\")\n\n# convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'],format='%d-%b-%y')\n\n# sort data frame by date\ndf = df.sort_values(by='Date')\n\n# plot the trend of high values over date\nplt.figure(figsize=(10,6))\nplt.plot(df['Date'],df['High'],label='High')\nplt.xlabel('Date')\nplt.ylabel('High Value')\nplt.title('Trend of High Values Over Date')\nplt.legend()\nplt.show()", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the CSV data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Plotting the graph\nplt.figure(figsize=(10, 5)\nplt.plot(df['Date'], df['High'])\nplt.xlabel('Date')\nplt.ylabel('High')\nplt.title('Trend of High values over time')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Create a bar chart indicating the number of records that fall into each \"Volume\" category ('Low', 'Medium', 'High').", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Create a bar chart indicating the number of records that fall into each \"Volume\" category ('Low', 'Medium', 'High').", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Create \"Volume\" categories based on their values\ndf['Volume Category'] = pd.cut(df['Volume'],bins=[0,df['Volume'].quantile(0.33),df['Volume'].quantile(0.67),df['Volume'].max()],labels=['Low','Medium','High'])\n\n# Count the number of records in each category\nvolume_count = df['Volume Category'].value_counts()\n\n# Plot the bar chart\nvolume_count.plot(kind='bar')\n\n# Add chart labels and title\nplt.xlabel('Volume Category')\nplt.ylabel('Count')\nplt.title('Number of records in each \"Volume\" category')\nplt.show()\n\n# Print the count\nprint(volume_count)", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reading the csv file\ndf = pd.read_csv('data.csv')\n\n# Creating a function to categorize the Volume\ndef categorize_volume(volume):\n    if volume < 20000000:\n        return 'Low'\n    elif volume < 40000000:\n        return 'Medium'\n    else:\n        return 'High'\n\n# Applying the function to the 'Volume' column\ndf['Volume_Category'] = df['Volume'].apply(categorize_volume)\n\n# Counting the number of records in each Volume category\nvolume_counts = df['Volume_Category'].value_counts()\n\n# Creating a bar chart\nplt.bar(volume_counts.index, volume_counts)\nplt.xlabel('Volume Category')\nplt.ylabel('Number of Records')\nplt.title('Number of Records in Each Volume Category')\nplt.show()\n\n# Print the result\nprint(\"The bar chart has been created.\")"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Draw a pie chart that shows the distribution of houses across different income categories (divide MedInc into categories: low, medium, high).", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Draw a pie chart that shows the distribution of houses across different income categories (divide MedInc into categories: low, medium, high).", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Define the bins for different income categories\nbins = [0,3,6,df['MedInc'].max()]\nlabels = ['low','medium','high']\n\n# Create a new column 'IncomeCategory' for categorizing the incomes\ndf['IncomeCategory'] = pd.cut(df['MedInc'],bins=bins,labels=labels)\n\n# Count the number of houses in each income category\nincome_counts = df['IncomeCategory'].value_counts()\n\n# Plot a pie chart for the distribution of houses in different income categories\nplt.figure(figsize=(10,7))\nplt.pie(income_counts,labels=income_counts.index,startangle=140,autopct='%1.1f%%')\nplt.title('Distribution of Houses Across Different Income Categories')\nplt.axis('equal')   # Equal aspect ratio ensures that the pie chart is circular\n\nprint(plt.show())", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define the income categories and their boundaries\nincome_categories = {'low': (0, df['MedInc'].quantile(0.33)), \n                    'medium': (df['MedInc'].quantile(0.33), df['MedInc'].quantile(0.67)), \n                    'high': (df['MedInc'].quantile(0.67), df['MedInc'].max())}\n\n# Categorize each house into an income category\ndf['IncomeCategory'] = pd.cut(df['MedInc'], bins=[income_categories['low'][0], income_categories['low'][1], \n                                              income_categories['medium'][0], income_categories['medium'][1], \n                                              income_categories['high'][0], income_categories['high'][1]])\n\n# Count the number of houses in each income category\ncategory_counts = df['IncomeCategory'].value_counts()\n\n# Draw the pie chart\nplt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%')\nplt.title('Distribution of houses across income categories')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Draw a line graph showing the trend of MedianHouseValue over different longitudes.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Draw a line graph showing the trend of MedianHouseValue over different longitudes.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/45.csv\")\n\n# Group data by Longitude and calculate mean of MedianHouseValue for each group\ngrouped_df = df.groupby('Longitude')['MedianHouseValue'].mean().reset_index()\n\n# Sorting the dataframe\ngrouped_df = grouped_df.sort_values(by='Longitude')\n\n# Plotting\nplt.figure(figsize=(10,5))\nplt.plot(grouped_df['Longitude'],grouped_df['MedianHouseValue'])\nplt.title('Trend of MedianHouseValue over different Longitudes')\nplt.xlabel('Longitude')\nplt.ylabel('MedianHouseValue')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by Longitude and calculate the mean of MedianHouseValue for each group\ndf_grouped = df.groupby('Longitude')['MedianHouseValue'].mean()\n\n# Plot the trend of MedianHouseValue over different longitudes\nplt.figure()\ndf_grouped.plot()\nplt.xlabel('Longitude')\nplt.ylabel('MedianHouseValue')\nplt.title('Trend of MedianHouseValue over different longitudes')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Construct a histogram to display the distribution of house ages ('HouseAge').", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Construct a histogram to display the distribution of house ages ('HouseAge').", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load CSV data into a DataFrame\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Draw a histogram to show the distribution of house ages\nplt.hist(df['HouseAge'],bins=20,color='lightblue',edgecolor='black')\nplt.xlabel('House Age')\nplt.ylabel('Frequency')\nplt.title('Distribution of House Ages')\nplt.grid(True)\nplt.show()\n\n# No result to be printed as output is a plot\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a histogram of the 'HouseAge'\nplt.hist(df['HouseAge'], bins=30, edgecolor='black')\n\n# Set the title and labels\nplt.title('Distribution of House Ages')\nplt.xlabel('House Age (years)')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Create a pie chart to display the proportion of 'Value' possessed by each 'LOCATION' for every record.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Create a pie chart to display the proportion of 'Value' possessed by each 'LOCATION' for every record.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/46.csv')\n\n# Group data by 'LOCATION' and calculate the sum of 'Value' for each location\nlocation_value = data.groupby('LOCATION')['Value'].sum()\n\n# Create a pie chart\nplt.pie(location_value,labels = location_value.index,autopct='%1.1f%%')\n\n# Display the pie chart\nplt.show()\n\n# Print the result\nprint(\"The pie chart has been successfully created and displayed.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'LOCATION' and calculate the sum of 'Value' for each location\ndf_grouped = df.groupby('LOCATION')['Value'].sum().reset_index()\n\n# Create a pie chart\nplt.figure(figsize=(10,6))\nplt.pie(df_grouped['Value'], labels=df_grouped['LOCATION'], autopct='%1.1f%%')\nplt.title('Proportion of Value by Location')\nplt.show()\n\n# Note: This code will not output any result directly but it will display a plot."}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Plot a bar chart with 'Year' on x-axis and 'Business' on y-axis to depict the trend over years.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Plot a bar chart with 'Year' on x-axis and 'Business' on y-axis to depict the trend over years.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv(\"infiagent/csv/47.csv\")\n\n# Draw the bar chart\nplt.figure(figsize=(10,6))\nplt.bar(data['Year'],data['Business'])\n\n# Set the labels and title\nplt.xlabel(\"Year\")\nplt.ylabel(\"Business\")\nplt.title(\"Trend of 'Business' over the years\")\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create the bar chart\nplt.figure(figsize=(10, 5))\nplt.bar(df['Year'], df['Business'])\nplt.xlabel('Year')\nplt.ylabel('Business')\nplt.title('Trend of Business over years')\nplt.show()\n\n# This code will not output any result as a print statement but it will display a plot."}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Create a line graph that displays the trend of 'Foreign Languages' over time.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Create a line graph that displays the trend of 'Foreign Languages' over time.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load data\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# set the Year as index\ndf.set_index('Year',inplace=True)\n\n# plot the data\nplt.figure(figsize=(10,6))\nplt.plot(df.index,df['Foreign Languages'],marker='o',linestyle='-')\nplt.title('Trend of Foreign Languages over the years')\nplt.xlabel('Year')\nplt.ylabel('Foreign Languages')\nplt.grid(True)\nplt.show()\n\n# note: the plot will show on execution,no need to print.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a line graph for 'Foreign Languages'\nplt.figure(figsize=(10, 5))\nplt.plot(df['Year'], df['Foreign Languages'])\nplt.xlabel('Year')\nplt.ylabel('Foreign Languages')\nplt.title('Trend of Foreign Languages over time')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Plot a pie-chart to show the proportions of 'Agriculture', 'Biology', and 'Engineering' in the year 1976.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Plot a pie-chart to show the proportions of 'Agriculture', 'Biology', and 'Engineering' in the year 1976.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data from csv file\ndata = pd.read_csv('infiagent/csv/47.csv')\n\n# Select specific rows for the year 1976\ndata_1976 = data[data['Year'] == 1976][['Agriculture','Biology','Engineering']]\n\n# Create a pie chart\nplt.pie(data_1976.values[0],labels=data_1976.columns,autopct='%1.1f%%')\nplt.title('Proportions of Agriculture,Biology,and Engineering in 1976')\nplt.show()\n\n# Output result\nprint(\"Pie chart generated successfully.\")", "assistant_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select data for the year 1976\ndf_1976 = df[df['Year'] == 1976]\n\n# Select the relevant columns\ndf_selected = df_1976[['Agriculture', 'Biology', 'Engineering']]\n\n# Plot a pie chart\nplt.pie(df_selected, labels = df_selected.columns, autopct='%1.1f%%')\nplt.title('Proportions in 1976')\nplt.show()\n\n# Print the proportions\nprint(df_selected)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Create a bar chart displaying the relationship between 'ID' and 'class7', and assign colors to the bars according to the values in 'class8'.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Create a bar chart displaying the relationship between 'ID' and 'class7', and assign colors to the bars according to the values in 'class8'.", "instruction_type": "Draw-Chart", "reference_code": "# import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# create a bar plot\nplt.figure(figsize=(10,6))\nplt.bar(df['ID'],df['class7'],color=df['class8'].map({0: 'b',1: 'g'}))\n\n# add titles and labels\nplt.title('Bar graph of ID versus class7 colored by class8')\nplt.xlabel('ID')\nplt.ylabel('class7')\n\n# show the plot\nplt.show()  \n\nprint(\"The plot has been successfully created.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a list of colors based on the values in 'class8'\ncolors = df['class8'].apply(lambda x: 'red' if x == 1.0 else 'blue')\n\n# Plot the bar chart\nplt.bar(df['ID'], df['class7'], color=colors)\n\n# Set the title and labels\nplt.title('Relationship between ID and class7')\nplt.xlabel('ID')\nplt.ylabel('class7')\n\n# Show the plot\nplt.show()\n\n# Print the colors used in the plot\nprint(colors)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Generate a histogram displaying the distribution of 'class4' separated by 'class2'.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Generate a histogram displaying the distribution of 'class4' separated by 'class2'.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt \n\n# Read the csv file\ndata = pd.read_csv('infiagent/csv/48.csv')\n\n# Create a histogram of the distribution of 'class4' with 'class2'\nplt.hist([data['class4'],data['class2']],label=['class4','class2'])\n\n# Add labels to the axes and title to the chart\nplt.xlabel('Classes')\nplt.ylabel('Distribution')\nplt.title('Histogram of class4 and class2')\n\n# Show the legend\nplt.legend()\n\n# Display the plot\nplt.show()\n\n# Print required output\nprint('Histogram has been successfully created for class4 and class2')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a pivot table to group 'class4' by 'class2'\npivot_table = df.pivot_table(index='class2', columns='class4', values='ID', aggfunc='count')\n\n# Plot the histogram\npivot_table.plot(kind='bar', stacked=True)\n\n# Set the title and labels\nplt.title('Distribution of class4 separated by class2')\nplt.xlabel('class2')\nplt.ylabel('Count of class4')\n\n# Show the plot\nplt.show()\n\n# Print the pivot table for better understanding\nprint(pivot_table)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Create a pie chart to display the percentage of each category with non-zero entries.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Create a pie chart to display the percentage of each category with non-zero entries.", "instruction_type": "Draw-Chart", "reference_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Determine the number of non-zero values in each category\nclass_counts = df.drop('ID',axis=1).sum()\n\n# Draw a pie chart to display the proportion of non-zero values in each class\nplt.pie(class_counts,labels = class_counts.index,autopct='%1.1f%%')\nplt.title('Proportion of non-zero values in each class')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Calculate the sum of each column\ncolumn_sums = df.iloc[:, 1:].sum(axis=1)\n\n# Create a new DataFrame for the sums\nsums_df = df['ID'].reset_index().rename(columns={'ID': 'category', '1': 'sum'})\n\n# Plot a pie chart\nplt.figure(figsize=(10, 7))\nplt.pie(sums_df['sum'], labels=sums_df['category'], autopct='%1.1f%%')\nplt.title('Percentage of each category with non-zero entries')\nplt.show()\n\n# Print the DataFrame with the sums\nprint(sums_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Draw a line graph tracking the change in \"Close\" price over time.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Draw a line graph tracking the change in \"Close\" price over time.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Convert 'Time' column to datetime format\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Set 'Time' column as index\ndf.set_index('Time',inplace=True)\n\n# Plot 'Close' column\nplt.plot(df['Close'])\nplt.ylabel('Close Price')\nplt.xlabel('Time')\nplt.title('Change in Close Price Over Time')\nplt.show()\n\n# There is no specific print statement in this task as the result is a plot.", "assistant_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Plot the 'Close' price over time\nplt.figure(figsize=(12, 6))\nplt.plot(df['Time'], df['Close'])\nplt.xlabel('Time')\nplt.ylabel('Close Price')\nplt.title('Close Price Over Time')\nplt.grid(True)\nplt.show()\n\n# No specific output is expected\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Construct a pie chart illustrating the proportion of each 'description' within the data set.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Construct a pie chart illustrating the proportion of each 'description' within the data set.", "instruction_type": "Draw-Chart", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Create a new dataframe based on the count of each description\ndescription_counts = df['description'].value_counts()\n\n# Create the pie chart\nplt.pie(description_counts,labels = description_counts.index,autopct='%1.1f%%')\n\n# Add a title\nplt.title('Share of Each Description')\n\n# Show the pie chart\nplt.show()\n\n# Note - there is no need for a print statement as plt.show() will display the pie chart.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Prepare data for the pie chart\ndata = df['description'].value_counts()\n\n# Plotting\nplt.figure(figsize=(10,7))\nplt.pie(data, labels=data.index, autopct='%1.1f%%')\nplt.title('Proportion of each description')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Create a bar chart with 'Date' on the x-axis and 'Miles traveled today (midnight to 11:59 pm)' on the y-axis.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Create a bar chart with 'Date' on the x-axis and 'Miles traveled today (midnight to 11:59 pm)' on the y-axis.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Parse 'Date' column to datetime format\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Sort data by 'Date'\ndata = data.sort_values('Date')\n\n# Set 'Date' as index of the dataset\ndata.set_index('Date',inplace=True)\n\n# Plotting the bar graph for 'Date' against 'Miles traveled today (midnight to 11:59 pm)'\ndata['Miles traveled today (midnight to 11:59 pm)'].plot(kind='bar')\n\n# Display the plot\nplt.show()\n\n# Since we only generate code and do not execute it,we use placeholder text to explain the expected result.\nprint(\"A bar graph will be shown where the X-axis represents the date and the Y-axis represents the miles traveled today (midnight to 11:59 pm).\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create the bar chart\nplt.figure(figsize=(12,6))\nplt.bar(df['Date'], df['Miles traveled today (midnight to 11:59 pm)'])\nplt.xlabel('Date')\nplt.ylabel('Miles traveled today (midnight to 11:59 pm)')\nplt.title('Miles traveled today over the past 24 hours')\nplt.grid(True)\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Create a line chart displaying the progression of 'Miles traveled to date:' as it changes over time.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Create a line chart displaying the progression of 'Miles traveled to date:' as it changes over time.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Convert 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Selecting required columns and setting 'Date' as index\ndf = df[['Date','Miles traveled to date:']].set_index('Date')\n\n# Plotting the line graph\nplt.figure(figsize=(10,6))\nplt.plot(df['Miles traveled to date:'],marker='o')\nplt.title('Trend of Miles traveled to date over time')\nplt.xlabel('Date')\nplt.ylabel('Miles traveled to date')\nplt.grid(True)\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Create a line chart\nplt.figure(figsize=(10, 5))\nplt.plot(df['Date'], df['Miles traveled to date:'])\nplt.title('Progression of Miles Traveled to Date')\nplt.xlabel('Date')\nplt.ylabel('Miles Traveled to Date')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Create a line graph that displays the relationship between 'origin_station' and 'destination_station', using 'start_time' as the x-axis and 'end_time' as the y-axis.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Create a line graph that displays the relationship between 'origin_station' and 'destination_station', using 'start_time' as the x-axis and 'end_time' as the y-axis.", "instruction_type": "Draw-Graph", "reference_code": "# import the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# Convert start_time and end_time to datetime\ndata['start_time'] = pd.to_datetime(data['start_time'])\ndata['end_time'] = pd.to_datetime(data['end_time'])\n\n# Sort the data by start_time\ndata = data.sort_values('start_time')\n\n# Create a group by object for origin_station and destination_station\ngrouped = data.groupby(['origin_station','destination_station'])\n\n# Iterate over each group and plot a line graph for start_time vs end_time\nfor name,group in grouped:\n  plt.figure(figsize=(10,6))\n  plt.plot(group['start_time'],group['end_time'])\n  plt.title(f'Origin: {name[0]} Destination: {name[1]}')\n  plt.xlabel('Start Time')\n  plt.ylabel('End Time')\n\nplt.show()", "assistant_code": "# import the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# read the csv file\ndf = pd.read_csv('data.csv')\n\n# convert 'start_time' and 'end_time' to datetime format\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\n\n# create a plot figure\nplt.figure(figsize=(10, 8))\n\n# plot the 'start_time' vs 'end_time' for each 'origin_station' and 'destination_station'\nfor origin, destination in zip(df['origin_station'], df['destination_station']):\n    plt.plot(df[df['origin_station'] == origin]['start_time'], df[df['origin_station'] == origin]['end_time'], label=destination)\n\n# set the title and labels\nplt.title('Relationship between origin station and destination station')\nplt.xlabel('Start Time')\nplt.ylabel('End Time')\n\n# add a legend\nplt.legend(title='Destination Station')\n\n# show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Create a line graph representing the 'Freedom' in relation to 'Generosity' for each country.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Create a line graph representing the 'Freedom' in relation to 'Generosity' for each country.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/3.csv')\n\n# Sort the data frame by 'Freedom'\ndata = data.sort_values('Freedom')\n\n# Plotting Freedom against Generosity\nplt.figure(figsize=(10,6))\nplt.plot(data['Freedom'],data['Generosity'])\nplt.xlabel('Freedom')\nplt.ylabel('Generosity')\nplt.title('Freedom vs Generosity for each Country')\nplt.show()\n\n# No explicit result to print with this instruction\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create the line plot\nplt.figure(figsize=(10,6))\nplt.plot(df['Generosity'], df['Freedom'])\nplt.xlabel('Generosity')\nplt.ylabel('Freedom')\nplt.title('Freedom in relation to Generosity')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Create a bar chart showing the frequency of EVENTMSGTYPE for improved data visualization.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Create a bar chart showing the frequency of EVENTMSGTYPE for improved data visualization.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load csv file\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Calculate frequency of each EVENTMSGTYPE\nevent_freq = df['EVENTMSGTYPE'].value_counts()\n\n# Plot\nplt.figure(figsize=(10,5))\nplt.bar(event_freq.index,event_freq.values)\nplt.xlabel('EVENTMSGTYPE')\nplt.ylabel('Frequency')\nplt.title('Bar graph of EVENTMSGTYPE against its frequency')\nplt.show()\n\n# Print frequency of each EVENTMSGTYPE\nprint(event_freq)", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the frequency of each unique EVENTMSGTYPE\nevent_msg_type_freq = df['EVENTMSGTYPE'].value_counts()\n\n# Create a bar chart\nplt.figure(figsize=(10,6))\nevent_msg_type_freq.plot(kind='bar', color='skyblue')\nplt.title('Frequency of EVENTMSGTYPE')\nplt.xlabel('EVENTMSGTYPE')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\n# Print the frequency of each unique EVENTMSGTYPE\nprint(event_msg_type_freq)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Create a line chart displaying the SCOREMARGIN values for GAME_ID '0020200722' across various PERIODS.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Create a line chart displaying the SCOREMARGIN values for GAME_ID '0020200722' across various PERIODS.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/7.csv')\n\n# Select the data for the specified game_id\ndata = data[data['GAME_ID'] == 20200722]\n\n# Convert SCOREMARGIN to numeric,handling non-numeric values\ndata['SCOREMARGIN'] = pd.to_numeric(data['SCOREMARGIN'],errors='coerce')\n\n# Group by periods and calculate the mean of score margin\ngrouped_data = data.groupby('PERIOD')['SCOREMARGIN'].mean()\n\n# Plot the data\nplt.figure(figsize=(10,6))\nplt.plot(grouped_data.index,grouped_data.values)\nplt.xlabel('Period')\nplt.ylabel('Score Margin')\nplt.title('Score margin over different periods for GAME_ID 0020200722')\nplt.grid(True)\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe for the game_id '0020200722'\ndf_filtered = df[df['GAME_ID'] == '0020200722']\n\n# Plotting the SCOREMARGIN values for each period\nplt.figure(figsize=(10,5))\nplt.plot(df_filtered['PERIOD'], df_filtered['SCOREMARGIN'])\nplt.xlabel('PERIOD')\nplt.ylabel('SCOREMARGIN')\nplt.title('SCOREMARGIN for GAME_ID 0020200722 across various PERIODS')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Plot the 'sadness' emotion values over the 'timestamp', filtered for person named 'ciro'. Represent each point as a dot on the graph.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Plot the 'sadness' emotion values over the 'timestamp', filtered for person named 'ciro'. Represent each point as a dot on the graph.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Filter the rows for person named 'ciro'\nciro_data = df[df['name'] == 'ciro']\n\n# Convert the timestamp to datetime format for better plotting\nciro_data['timestamp'] = pd.to_datetime(ciro_data['timestamp'],format='%Hh-%Mm-%Ss')\n\n# Plot the 'sadness' emotion values over the 'timestamp'\nplt.plot(ciro_data['timestamp'],ciro_data['sadness'],'o')\n\n# Set the title and labels of the plot\nplt.title('Sadness emotion values over time for ciro')\nplt.xlabel('Timestamp')\nplt.ylabel('Sadness')\n\n# Show the plot\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Filtering the data for person 'ciro'\ndf_ciro = df[df['name'] == 'ciro']\n\n# Extracting only 'sadness' and 'timestamp' columns\ndf_ciro_filtered = df_ciro[['timestamp', 'sadness']]\n\n# Creating the plot\nplt.figure(figsize=(10, 6))\nplt.plot(df_ciro_filtered['timestamp'], df_ciro_filtered['sadness'], '.')\nplt.xlabel('Timestamp')\nplt.ylabel('Sadness')\nplt.title('Sadness emotion values over the Timestamp for person Ciro')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Draw a line graph tracking the sum of 'Count' for each year.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Draw a line graph tracking the sum of 'Count' for each year.", "instruction_type": "Draw-Graph", "reference_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Group by 'Date' and compute the sum of 'Count' for each year\ngrouped_df = df.groupby('Date').sum()\n\n# Plot the data\nplt.plot(grouped_df.index,grouped_df['Count'])\n\n# Define title and labels\nplt.title('Sum of Count Per Year')\nplt.xlabel('Year')\nplt.ylabel('Sum of Count')\n\n# Show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'], format='%Y')\n\n# Extract year from date\ndf['Year'] = df['Date'].dt.year\n\n# Sum 'Count' for each year\ncount_per_year = df.groupby('Year')['Count'].sum()\n\n# Create line graph\nplt.figure(figsize=(10, 5))\nplt.plot(count_per_year.index, count_per_year.values, marker='o', linestyle='-')\nplt.xlabel('Year')\nplt.ylabel('Sum of Count')\nplt.title('Sum of Count for each year')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Create a box plot displaying the 'mpg' values for each 'modelyear' category.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Create a box plot displaying the 'mpg' values for each 'modelyear' category.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Plotting the values\nplt.figure(figsize=(10,5))\ndf.boxplot(column='mpg',by='modelyear',grid=False)\n\nplt.title('Boxplot of mpg by modelyear')\nplt.xlabel('modelyear')\nplt.ylabel('mpg')\nplt.show()\n\nprint('Boxplot Created Successfully.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a box plot\nbox_plot = plt.boxplot(df['mpg'].groupby(df['modelyear']))\n\n# Show the plot\nplt.show()\n\n# Print the result\nprint(box_plot)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Draw a line graph of the trend of 'mpg' over 'modelyear' for each 'cylinders' group.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Draw a line graph of the trend of 'mpg' over 'modelyear' for each 'cylinders' group.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Preprocessing the data\ndf = df[['mpg','modelyear','cylinders']]\n\n# Group the data by 'modelyear' and 'cylinders',then take the mean of 'mpg'\ngrouped_df = df.groupby(['modelyear','cylinders']).mean().reset_index()\n\n# Generate a list of unique cylinder numbers\ncylinders = grouped_df['cylinders'].unique()\n\n# Create a subplot for each cylinder group\nfor cylinder in cylinders:\n    temp_df = grouped_df[grouped_df['cylinders'] == cylinder]\n    plt.plot(temp_df['modelyear'],temp_df['mpg'],label=f'Cylinder {cylinder}')\n\n# Setting title and labels\nplt.title('Trend of mpg over modelyear for different cylinders')\nplt.xlabel('Model Year')\nplt.ylabel('Miles Per Gallon')\n\n# Show legend and plot\nplt.legend()\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'cylinders' and 'modelyear', then calculate the mean of 'mpg'\ngrouped_df = df.groupby(['cylinders', 'modelyear'])['mpg'].mean().reset_index()\n\n# Pivot the dataframe to have 'modelyear' as columns and 'mpg' as values\npivot_df = grouped_df.pivot(index='cylinders', columns='modelyear', values='mpg').fillna(0)\n\n# Plot the line graph\npivot_df.plot(kind='line')\n\n# Set the title and labels\nplt.title('Trend of mpg over modelyear for each cylinders group')\nplt.xlabel('cylinders')\nplt.ylabel('mpg')\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Create a pie chart displaying the distribution of players between those who are free agent eligible and those who are not.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Create a pie chart displaying the distribution of players between those who are free agent eligible and those who are not.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Counts of free agency eligibility\nfree_agent_counts = df['indicator_of_free_agency_eligibility'].value_counts()\n\n# Create figure and axis\nfig,ax = plt.subplots()\n\n# Draw pie chart\nax.pie(free_agent_counts,labels=['Not Eligible','Eligible'],autopct='%1.1f%%')\n\n# Set title\nax.set_title('Proportion of Players Eligible for Free Agency')\n\n# Display plot\nplt.show()\n\n# To check the output.\nprint('Successfully plot the pie chart.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the number of players who are free agent eligible and those who are not\nfree_agent_eligible = df['indicator_of_free_agency_eligibility'].sum()\nnot_free_agent_eligible = len(df) - free_agent_eligible\n\n# Plot the pie chart\nsizes = [free_agent_eligible, not_free_agent_eligible]\nlabels = ['Free Agent Eligible', 'Not Free Agent Eligible']\nexplode = (0.1, 0)  # only \"explode\" the 1st slice (i.e. 'Free Agent Eligible')\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Create a pie chart that displays the percentage of 'weathersit' occurrences throughout the entire dataset.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Create a pie chart that displays the percentage of 'weathersit' occurrences throughout the entire dataset.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Count the occurrences of each value in 'weathersit'\nweathersit_counts = df['weathersit'].value_counts()\n\n# Plot a pie chart of the counts\nplt.figure(figsize=(8,6))\nplt.pie(weathersit_counts,labels=weathersit_counts.index,autopct='%1.1f%%')\nplt.title('Distribution of Weathersit')\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the occurrences of each 'weathersit' value\nweathersit_counts = df['weathersit'].value_counts()\n\n# Create a pie chart\nplt.pie(weathersit_counts, labels=weathersit_counts.index, autopct='%1.1f%%')\n\n# Display the plot\nplt.show()\n\n# Print the percentage of 'weathersit' occurrences\nprint(weathersit_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Create a line graph that shows the trend of department numbers for each 'dept_group' between the years 2016 and 2017.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Create a line graph that shows the trend of department numbers for each 'dept_group' between the years 2016 and 2017.", "instruction_type": "Draw-Graph", "reference_code": "# import required libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Convert budget_year_start and budget_year_end to datetime\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\ndf['budget_year_end'] = pd.to_datetime(df['budget_year_end'])\n\n# Filter data for years 2016 and 2017\ndf = df[(df['budget_year_start'].dt.year == 2016) & (df['budget_year_end'].dt.year == 2017)]\n\n# Count number of departments for each dept_group\ngroup_counts = df['dept_group'].value_counts()\n\n# Draw line graph\nplt.figure(figsize=(10,6))\nplt.plot(group_counts.index,group_counts.values,marker='o')\nplt.xlabel('Dept Group')\nplt.ylabel('Number of Departments')\nplt.title('Number of Departments for Each Dept Group (2016-2017)')\nplt.grid()\n\n# Print the graph\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'budget_year_start' and 'budget_year_end' columns to datetime\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\ndf['budget_year_end'] = pd.to_datetime(df['budget_year_end'])\n\n# Filter the data for the years 2016 and 2017\ndf = df[(df['budget_year_start'].dt.year >= 2016) & (df['budget_year_end'].dt.year <= 2017)]\n\n# Group the data by 'dept_group' and count the number of departments in each group\ndept_count = df.groupby('dept_group')['Department Name'].count()\n\n# Create the line graph\nplt.figure(figsize=(10,6))\nplt.plot(dept_count.index, dept_count.values, marker='o', linestyle='-')\nplt.xlabel('Dept Group')\nplt.ylabel('Number of Departments')\nplt.title('Trend of Department Numbers for each Dept Group between 2016 and 2017')\nplt.grid(True)\nplt.show()\n\n# Print the result\nprint(dept_count)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Draw a line graph showing changes in 'Number of Discharges' over time (based on 'Start Date' and 'End Date') for the 'FROEDTERT MEMORIAL LUTHERAN HOSPITAL'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Draw a line graph showing changes in 'Number of Discharges' over time (based on 'Start Date' and 'End Date') for the 'FROEDTERT MEMORIAL LUTHERAN HOSPITAL'.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Convert Start Date and End Date to datetime object\ndf['Start Date'] = pd.to_datetime(df['Start Date'])\ndf['End Date'] = pd.to_datetime(df['End Date'])\n\n# Select data for the given hospital\nselected_hospital_df = df[df['Hospital Name'] == 'FROEDTERT MEMORIAL LUTHERAN HOSPITAL']\n\n# Sort the dates in ascending order\nselected_hospital_df = selected_hospital_df.sort_values(by=['Start Date'])\n\n# Draw a line graph\nplt.plot(selected_hospital_df['Start Date'],selected_hospital_df['Number of Discharges'],label='Start Date')\nplt.plot(selected_hospital_df['End Date'],selected_hospital_df['Number of Discharges'],label='End Date')\nplt.xlabel('Date')\nplt.ylabel('Number of Discharges')\nplt.title('Changes in Number of Discharges over Time for FROEDTERT MEMORIAL LUTHERAN HOSPITAL')\nplt.legend()\nplt.show()\n\nprint('The graph has been drawn.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data for FROEDTERT MEMORIAL LUTHERAN HOSPITAL\ndata = pd.read_csv('data.csv')\nfiltered_data = data[data['Hospital Name'] == 'FROEDTERT MEMORIAL LUTHERAN HOSPITAL']\n\n# Convert the 'Start Date' and 'End Date' columns to datetime\nfiltered_data['Start Date'] = pd.to_datetime(filtered_data['Start Date'])\nfiltered_data['End Date'] = pd.to_datetime(filtered_data['End Date'])\n\n# Calculate the discharge counts for each month\ndischarge_counts = filtered_data.groupby(filtered_data['Start Date'].dt.month)['Number of Discharges'].sum()\n\n# Create a line plot of the discharge counts\nplt.figure(figsize=(10, 5))\nplt.plot(discharge_counts.index, discharge_counts.values, marker='o', linestyle='-')\nplt.xlabel('Month')\nplt.ylabel('Number of Discharges')\nplt.title('Changes in Number of Discharges over Time for FROEDTERT MEMORIAL LUTHERAN HOSPITAL')\nplt.grid(True)\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Create a bar graph illustrating the distribution of 'max_storm_cat'.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Create a bar graph illustrating the distribution of 'max_storm_cat'.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndata = pd.read_csv('infiagent/csv/21.csv')\n\n# group the data by 'max_storm_cat' and count the number of occurrences\ngrouped_data = data['max_storm_cat'].value_counts()\n\n# plot the bar graph\nplt.figure(figsize=(10,6))\nplt.bar(grouped_data.index,grouped_data.values)\nplt.xlabel('Max Storm Category')\nplt.ylabel('Number of Storms')\nplt.title('Distribution of Max Storm Category')\nplt.show()\n\n# No specific result to print as a graph will be displayed on executing the code.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a bar graph of the distribution of 'max_storm_cat'\ndf['max_storm_cat'].value_counts().sort_index().plot(kind='bar')\n\n# Set the title and labels\nplt.title('Distribution of Max Storm Category')\nplt.xlabel('Storm Category')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\n# Print the result\nprint(\"The bar graph has been successfully created and displayed.\")"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Plot a bar graph to show the distribution of 'number_of_open_credit_lines_and_loans' to examine credit lines or loans behavior.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Plot a bar graph to show the distribution of 'number_of_open_credit_lines_and_loans' to examine credit lines or loans behavior.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Take a look at the 'number_of_open_credit_lines_and_loans' column\ncredit_lines_and_loans = df['number_of_open_credit_lines_and_loans']\n\n# Plot a bar graph\nplt.figure(figsize=(12,6))\ncredit_lines_and_loans.value_counts().sort_index().plot(kind='bar')\nplt.title('Distribution of Number of Open Credit Lines and Loans')\nplt.xlabel('Number of Open Credit Lines and Loans')\nplt.ylabel('Frequency')\n\n#printing the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Plot the distribution of 'number_of_open_credit_lines_and_loans'\ndf['number_of_open_credit_lines_and_loans'].plot(kind='bar', edgecolor='black')\n\n# Set the title and labels\nplt.title('Distribution of Number of Open Credit Lines and Loans')\nplt.xlabel('Category')\nplt.ylabel('Number of Open Credit Lines and Loans')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Construct a graph illustrating the progression of reported cases in the 'Eastern Mediterranean' WHO region across different years.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Construct a graph illustrating the progression of reported cases in the 'Eastern Mediterranean' WHO region across different years.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Filter the data for the 'Eastern Mediterranean' WHO region\ndf_region = df[df['WHO Region'] == 'Eastern Mediterranean']\n\n# Group the data by year and calculate the sum of cases for each year\ndf_region_cases = df_region.groupby('Year')['No. of cases_median'].sum()\n\n# Generate the graph\nplt.figure(figsize=(10,6))\nplt.plot(df_region_cases)\nplt.title('Trend of cases reported in the \"Eastern Mediterranean\" WHO region over the years')\nplt.xlabel('Year')\nplt.ylabel('No. of cases')\nplt.show()\n\n# Print the graph name \nprint('Graph: Trend of cases reported in the \"Eastern Mediterranean\" WHO region over the years')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data for the 'Eastern Mediterranean' region\ndf = pd.read_csv('data.csv')\n\n# Filter data for 'Eastern Mediterranean' region and extract the years and corresponding cases\ndf_filtered = df[df['WHO Region'] == 'Eastern Mediterranean'][['Year', 'No. of cases']]\n\n# Convert the 'Year' column to integer and 'No. of cases' to float\ndf_filtered[['Year', 'No. of cases']] = df_filtered[['Year', 'No. of cases']].astype(data_type)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(df_filtered['Year'], df_filtered['No. of cases'], marker='', linestyle='-')\nplt.title('Progression of reported cases in the Eastern Mediterranean region across different years')\nplt.xlabel('Year')\nplt.ylabel('Number of cases')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Plot a line graph representing 'positive_diffsel' values over 'negative_diffsel'.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Plot a line graph representing 'positive_diffsel' values over 'negative_diffsel'.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/34.csv')\n\n# Plot the data\nplt.figure(figsize=(10,6))\nplt.plot(data['negative_diffsel'],data['positive_diffsel'],label='Positive Selection Diff over Negative Selection Diff')\nplt.xlabel('Negative Selection Diff')\nplt.ylabel('Positive Selection Diff')\nplt.legend(loc=\"upper right\")\nplt.title('Positive Selection Diff over Negative Selection Diff')\n\n# Showing the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Set the aspect ratio of the plot to ensure it is a line\nplt.figure(figsize=(10, 1))\n\n# Plot 'positive_diffsel' vs 'negative_diffsel'\nplt.plot(data['negative_diffsel'], data['positive_diffsel'])\n\n# Set the title and labels\nplt.title('Positive Difference Selection vs Negative Difference Selection')\nplt.xlabel('Negative Difference Selection')\nplt.ylabel('Positive Difference Selection')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Draw a bar graph depicting the increase in life expectancy over the year for the country 'afghanistan'.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Draw a bar graph depicting the increase in life expectancy over the year for the country 'afghanistan'.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Filter data for the country Afghanistan\nafghanistan_data = df[df['country'] == 'afghanistan']\n\n# Set the year as the index\nafghanistan_data.set_index('year',inplace=True)\n\n# Plot a bar chart for life expectancy\nplt.bar(afghanistan_data.index,afghanistan_data['lifeexp'])\nplt.xlabel('Year')\nplt.ylabel('Life Expectancy')\nplt.title('Life Expectancy over the years in Afghanistan')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\ndf = df[df['country'] == 'afghanistan']\n\n# Sort the data by year\ndf = df.sort_values('year')\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(df['year'], df['lifeexp'], marker='o', linestyle='-')\nplt.title('Increase in Life Expectancy over the Year for Afghanistan')\nplt.xlabel('Year')\nplt.ylabel('Life Expectancy')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Plot a line graph showcasing GDP per capita trend with year for each continent.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Plot a line graph showcasing GDP per capita trend with year for each continent.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load dataset from csv\ndf = pd.read_csv(\"infiagent/csv/35.csv\")\n\n# group by continent and year,and calculate average GDP per Capita\ngrouped_df = df.groupby(['continent','year'])['gdppercap'].mean().reset_index()\n\n# create subplots\nfig,ax = plt.subplots()\n\n# loop through each continent\nfor continent in grouped_df['continent'].unique():\n    # get data for specific continent\n    continent_data = grouped_df[grouped_df['continent'] == continent]\n    # plot line graph\n    ax.plot(continent_data['year'],continent_data['gdppercap'],label=continent)\n\n# set graph title,x-axis and y-axis labels\nplt.title('GDP per capita trend by Continent')\nplt.xlabel('Year')\nplt.ylabel('GDP per capita')\n\n# create legend\nplt.legend()\n\n# show the graph\nplt.show()\n\n# there is no data to print,hence pass in print\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'continent' and 'year', then get the mean of 'gdppercap'\ndf_grouped = df.groupby(['continent', 'year'])['gdppercap'].mean().reset_index()\n\n# List of unique continents\ncontinents = df_grouped['continent'].unique()\n\n# Plotting\nfor continent in continents:\n    df_temp = df_grouped[df_grouped['continent'] == continent]\n    plt.plot(df_temp['year'], df_temp['gdppercap'], label=continent)\n\nplt.xlabel('Year')\nplt.ylabel('GDP per capita')\nplt.title('GDP per capita trend with year for each continent')\nplt.legend()\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Plot the correlation between gdpPercap_1952 and gdpPercap_2007.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Plot the correlation between gdpPercap_1952 and gdpPercap_2007.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Create a scatter plot\nplt.figure(figsize=(10,8))\nplt.scatter(df['gdpPercap_1952'],df['gdpPercap_2007'])\nplt.xlabel('GDP Per Capita in 1952')\nplt.ylabel('GDP Per Capita in 2007')\nplt.title('Relationship between GDP Per Capita in 1952 and 2007')\n\n# Show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the correlation\ncorrelation = df['gdpPercap_1952'].corr(df['gdpPercap_2007'])\n\n# Plot the data\nplt.scatter(df['gdpPercap_1952'], df['gdpPercap_2007'])\nplt.xlabel('gdpPercap_1952')\nplt.ylabel('gdpPercap_2007')\nplt.title('Correlation between gdpPercap_1952 and gdpPercap_2007')\nplt.show()\n\nprint(correlation)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Create a bar chart to compare the count of good and bad superheroes in each publishing company.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Create a bar chart to compare the count of good and bad superheroes in each publishing company.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Clean the data,removing rows where 'Alignment' or 'Publisher' is not defined\ndf = df[df['Alignment'] != '-']\ndf = df[df['Publisher'] != '-']\n\n# Group by 'Publisher' and 'Alignment',and count the number in each category\ngrouped_df = df.groupby(['Publisher','Alignment']).size().unstack(fill_value=0)\n\n# Plot the data as a bar graph\ngrouped_df.plot(kind='bar',stacked=True)\n\n# Displaying the plot\nplt.title('Number of Good and Bad Superheroes in Each Publisher')\nplt.xlabel('Publisher')\nplt.ylabel('Number of Superheroes')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data based on the Alignment column to get only the heroes with 'good' and 'bad' alignment\ndf = df[df['Alignment'].isin(['good', 'bad'])]\n\n# Count the number of heroes with 'good' and 'bad' alignment for each publishing company\ndf_grouped = df.groupby(['Publisher', 'Alignment']).size().unstack(fill_value=0)\n\n# Plot the bar chart\ndf_grouped.plot(kind='bar', stacked=False)\n\n# Set the title and labels for the axes\nplt.title('Good and Bad Superheroes per Publishing Company')\nplt.xlabel('Publisher')\nplt.ylabel('Count')\n\n# Show the plot\nplt.show()\n\n# Print the dataframe\nprint(df_grouped)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Create a line graph to show how the number of reviews changes for each different bubble score.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Create a line graph to show how the number of reviews changes for each different bubble score.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load data from the csv file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Group the dataset by 'bubble_score' and compute the sum of 'review_count' for each unique bubble score\ngrouped_df = df.groupby('bubble_score')['review_count'].sum()\n\n# Draw the graph\nplt.figure(figsize=(10,6))\nplt.plot(grouped_df.index,grouped_df.values)\n\n# Setting labels and title\nplt.xlabel('Bubble Score')\nplt.ylabel('Total Review Count')\nplt.title('Review count\\'s variation against each unique bubble score')\n\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by bubble_score and review_count\ngrouped_data = df.groupby('bubble_score')['review_count'].plot(kind='line')\n\n# Setting the title and labels\nplt.title('Number of Reviews by Bubble Score')\nplt.xlabel('Bubble Score')\nplt.ylabel('Number of Reviews')\n\n# Show the plot\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Draw a bar graph listing the average prices of properties, grouped by the number of bedrooms.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Draw a bar graph listing the average prices of properties, grouped by the number of bedrooms.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data into a pandas DataFrame\ndata = pd.read_csv(\"infiagent/csv/41.csv\")\n\n# Group by 'bedrooms' and calculate the mean of 'price'\naverage_price_per_bedroom = data.groupby('bedrooms')['price'].mean()\n\n# Plot a bar graph\naverage_price_per_bedroom.plot(kind='bar',figsize=(10,8))\nplt.title('Average Property Prices Grouped by Number of Bedrooms')\nplt.xlabel('Number of Bedrooms')\nplt.ylabel('Average Price')\nplt.grid(True)\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by the number of bedrooms and calculate the average price\naverage_prices = df.groupby('bedrooms')['price'].mean()\n\n# Create a bar graph\nplt.bar(average_prices.index, average_prices.values)\nplt.xlabel('Number of Bedrooms')\nplt.ylabel('Average Price')\nplt.title('Average Price by Number of Bedrooms')\n\n# Show the plot\nplt.show()\n\n# Print the average prices\nprint(average_prices)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Represent the correlation between 'importance.score' and 'row retention time' through a graphical depiction.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Represent the correlation between 'importance.score' and 'row retention time' through a graphical depiction.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Plotting the relationship between 'importance.score' and 'row retention time'\nplt.figure(figsize=(10,6))\nplt.scatter(df['row retention time'],df['importance.score'])\nplt.xlabel('row retention time')\nplt.ylabel('importance.score')\nplt.title('Relationship between \"importance.score\" and \"row retention time\"')\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Drop the rows with missing values\ndata = data.dropna(subset=['importance.score', 'row retention time'])\n\n# Create a correlation matrix\ncorrelation_matrix = data[['importance.score', 'row retention time']].corr()\n\n# Extract the correlation coefficient\ncorrelation_coefficient = correlation_matrix['importance.score']['row retention time'].value\n\n# Create a scatter plot\nplt.scatter(data['importance.score'], data['row retention time'])\nplt.title('Correlation between Importance Score and Row Retention Time')\nplt.xlabel('Importance Score')\nplt.ylabel('Row Retention Time')\nplt.grid(True)\nplt.show()\n\n# Print the correlation coefficient\nprint(\"The correlation coefficient is: \", correlation_coefficient)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Draw a graph showing the distribution of 'charges' in each 'region'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Draw a graph showing the distribution of 'charges' in each 'region'.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from CSV file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Group the data by region and calculate total charges for each\ngrouped = df.groupby('region')['charges'].sum().reset_index()\n\n# Plot the data\nplt.figure(figsize=(10,5))\nplt.bar(grouped['region'],grouped['charges'],color='b')\nplt.title('Distribution of charges in each region')\nplt.xlabel('Region')\nplt.ylabel('Charges')\nplt.show()\n\n# There is no result to be printed out as this instruction requests to draw a graph.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'region' and calculate the mean of 'charges' in each region\nregion_charges = df.groupby('region')['charges'].mean()\n\n# Plot the distribution of 'charges' in each 'region'\nplt.figure(figsize=(10, 6))\nplt.bar(region_charges.index, region_charges.values)\nplt.xlabel('Region')\nplt.ylabel('Mean Charges')\nplt.title('Distribution of Charges in Each Region')\nplt.show()\n\n# Print the mean charges in each region\nprint(region_charges)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Develop a line graph to demonstrate the trend of 'Value' over 'TIME' for 'MEASURE' as \"PC_GDP\".", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Develop a line graph to demonstrate the trend of 'Value' over 'TIME' for 'MEASURE' as \"PC_GDP\".", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the csv data\ndata = pd.read_csv('infiagent/csv/46.csv')\n\n# filter the data where MEASURE is 'PC_GDP'\ndata_filtered = data[data['MEASURE'] == 'PC_GDP']\n\n# sort the data by TIME\ndata_filtered = data_filtered.sort_values('TIME')\n\n# plot the line graph\nplt.figure(figsize=(10,6))\nplt.plot(data_filtered['TIME'],data_filtered['Value'])\nplt.xlabel('TIME')\nplt.ylabel('Value')\nplt.title('Trend of Value over TIME for MEASURE \"PC_GDP\"')\nplt.grid(True)\n\n# Show the plot\nplt.show()", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filtering the data for the specific MEASURE\ndf_filtered = df[df['MEASURE'] == 'PC_GDP']\n\n# Convert TIME column to numeric for plotting\ndf_filtered['TIME'] = pd.to_numeric(df_filtered['TIME'])\n\n# Creating the line plot\nplt.figure(figsize=(10,6))\nplt.plot(df_filtered['TIME'], df_filtered['Value'], marker='o', linestyle='-')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.title('Trend of Value over Time for MEASURE as PC_GDP')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Draw a bar graph showing the total \"Volume\" of each hour.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Draw a bar graph showing the total \"Volume\" of each hour.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/49.csv')\n\n# Convert the 'Time' column to datetime format\ndata['Time'] = pd.to_datetime(data['Time'])\n# Extract the hour from the 'Time' column\ndata['Hour'] = data['Time'].dt.hour\n\n# Group the data by the hour and sum the 'Volume' column\nhourly_volume = data.groupby('Hour')['Volume'].sum()\n\n# Plot the summed 'Volume' for each hour\nplt.bar(hourly_volume.index,hourly_volume.values)\nplt.xlabel('Hour')\nplt.ylabel('Total Volume')\nplt.title('Total Volume for Each Hour')\nplt.show()\n\n# Print the graph\nprint(hourly_volume)", "assistant_code": "# import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# convert Time column to datetime and extract the hour\ndf['Time'] = pd.to_datetime(df['Time']).dt.hour\n\n# group by hour and sum the Volume\ndf_grouped = df.groupby('Time')['Volume'].sum()\n\n# plot the result\nplt.bar(df_grouped.index, df_grouped.values)\nplt.xlabel('Hour')\nplt.ylabel('Total Volume')\nplt.title('Total Volume per Hour')\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Plot a line graph to show the trend of humidity over time on '2015-07-24'.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Plot a line graph to show the trend of humidity over time on '2015-07-24'.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Preprocessing. Convert the 'day' column to datetime format\ndf['day'] = pd.to_datetime(df['day'])\n\n# Set 'day' column as the index of the dataframe\ndf.set_index('day',inplace=True)\n\n# Filter records for '2015-07-24'\ndf_day = df.loc['2015-07-24']\n\n# Plot the trend of humidity over the day\nplt.figure(figsize=(10,5))\nplt.plot(df_day.index,df_day['humidity'])\n\n# Setting plot title and labels\nplt.title('Humidity variation on 2015-07-24')\nplt.xlabel('Time')\nplt.ylabel('Humidity')\n\n# Display the plot\nplt.show()\n\nprint()", "assistant_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'dt' column to datetime\ndf['dt'] = pd.to_datetime(df['dt'])\n\n# Select the data of '2015-07-24'\ndf_filtered = df[df['dt'].dt.date.astype(str) == '2015-07-24']\n\n# Plot the graph\nplt.figure(figsize=(12, 6))\nplt.plot(df_filtered['dt'], df_filtered['humidity'])\nplt.xlabel('Time')\nplt.ylabel('Humidity')\nplt.title('Humidity over time on 2015-07-24')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Create a bar graph with the x-axis representing the distinct 'day' values and the y-axis displaying the mean 'temp' values.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Create a bar graph with the x-axis representing the distinct 'day' values and the y-axis displaying the mean 'temp' values.", "instruction_type": "Draw-Graph", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Convert 'day' column to datetime\ndf['day'] = pd.to_datetime(df['day'])\n\n# Group the data by 'day' and calculate average 'temp' for each day\ngrouped_df = df.groupby(df['day'].dt.date).agg({'temp':'mean'})\n\n# Draw bar chart\nplt.figure(figsize=(10,8))\nplt.bar(grouped_df.index,grouped_df['temp'])\nplt.xlabel('Day')\nplt.ylabel('Average Temperature')\nplt.title('Bar chart of Average Temperature per Day')\nplt.show()\n\n# Note: This code will not print anything,it will draw a bar chart as per the instruction.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'temp' column to float data type\ndf['temp'] = df['temp'].astype(float)\n\n# Group by 'day' and calculate the mean of 'temp'\ngrouped_df = df.groupby('day')['temp'].mean()\n\n# Create the bar graph\nplt.figure(figsize=(10,5))\nplt.bar(grouped_df.index, grouped_df.values)\nplt.xlabel('Day')\nplt.ylabel('Mean Temperature')\nplt.title('Mean Temperature by Day')\nplt.xticks(rotation = 90)\n\n# Print the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Create a scatter plot depicting the relationship between 'Annual Member Sign-Ups' and 'Date'.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Create a scatter plot depicting the relationship between 'Annual Member Sign-Ups' and 'Date'.", "instruction_type": "Draw-Plot", "reference_code": "# Import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\nfile_path = 'infiagent/csv/1.csv'\ndf = pd.read_csv(file_path)\n\n# Convert 'Date' column to datetime type\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Plot\nplt.figure(figsize=(10,7))\nplt.scatter(df['Date'],df['Annual Member Sign-Ups (midnight to 11:59 pm)'])\nplt.title('Scatter plot of Annual Member Sign-Ups against Date')\nplt.xlabel('Date')\nplt.ylabel('Annual Member Sign-Ups (midnight to 11:59 pm)')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the CSV file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Extract the month and year from the 'Date' column\ndf['Month'] = df['Date'].dt.month\ndf['Year'] = df['Date'].dt.year\n\n# Create the scatter plot\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(df['Year'], df['Month'], c=df['Annual Member Sign-Ups (midnight to 11:59 pm']))\n\n# Create a colorbar\nplt.colorbar(scatter)\n\n# Set the title and labels\nplt.title('Relationship between Annual Member Sign-Ups and Date')\nplt.xlabel('Year')\nplt.ylabel('Month')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Generate a scatter plot illustrating the relationship between 'Happiness Score' and 'Economy (GDP per Capita)' for every country.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Generate a scatter plot illustrating the relationship between 'Happiness Score' and 'Economy (GDP per Capita)' for every country.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Create the scatter plot\nplt.figure(figsize=(10,6))\nplt.scatter(df['Economy (GDP per Capita)'],df['Happiness Score'],alpha=0.5)\nplt.title('Happiness Score vs Economy (GDP per Capita)')\nplt.xlabel('Economy (GDP per Capita)')\nplt.ylabel('Happiness Score')\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Generate the scatter plot\nplt.scatter(df['Economy (GDP per Capita)'], df['Happiness Score'])\nplt.title('Happiness Score vs Economy (GDP per Capita)')\nplt.xlabel('Economy (GDP per Capita)')\nplt.ylabel('Happiness Score')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Create a scatter plot to display the connection between 'lng_org' and 'lng_dest'.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Create a scatter plot to display the connection between 'lng_org' and 'lng_dest'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/5.csv')\n\n# Check for missing values in 'lng_org' and 'lng_dest'\ndata = data[data['lng_org'].notna() & data['lng_dest'].notna()]\n\n# Draw the scatter plot\nplt.scatter(data['lng_org'],data['lng_dest'])\nplt.xlabel('Longitude of Origin')\nplt.ylabel('Longitude of Destination')\nplt.title(\"Scatter Plot of 'lng_org' and 'lng_dest'\")\nplt.show()\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df.lng_org, df.lng_dest)\n\n# Set the title and labels\nplt.title('Scatter plot of lng_org vs lng_dest')\nplt.xlabel('lng_org')\nplt.ylabel('lng_dest')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Plot a scatter graph with 'USFLUX' on the horizontal axis and 'MEANSHR' on the vertical axis.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Plot a scatter graph with 'USFLUX' on the horizontal axis and 'MEANSHR' on the vertical axis.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Create the plot\nplt.scatter(df['USFLUX'],df['MEANSHR'])\n\n# Set the title and labels\nplt.title('Scatter plot of USFLUX against MEANSHR')\nplt.xlabel('USFLUX')\nplt.ylabel('MEANSHR')\n\n# Show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df['USFLUX'], df['MEANSHR'])\n\n# Set labels and title\nplt.xlabel('USFLUX')\nplt.ylabel('MEANSHR')\nplt.title('Scatter plot of USFLUX vs MEANSHR')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Create a scatter diagram to illustrate the correlation between 'Shell weight' and 'Whole weight'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Create a scatter diagram to illustrate the correlation between 'Shell weight' and 'Whole weight'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Create a scatter plot for 'Shell weight' vs 'Whole weight'\nplt.scatter(data['Shell weight'],data['Whole weight'])\n\nplt.title('Scatter plot: Shell weight vs Whole weight')\nplt.xlabel('Shell weight')\nplt.ylabel('Whole weight')\n\n# Show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Scatter plot of Shell weight vs Whole weight\nplt.scatter(df['Shell weight'], df['Whole weight'])\nplt.xlabel('Shell weight')\nplt.ylabel('Whole weight')\nplt.title('Scatter plot of Shell weight vs Whole weight')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Create a histogram displaying the distribution of ages in the data.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Create a histogram displaying the distribution of ages in the data.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Create a histogram for 'Age'\nplt.hist(df['Age'],bins=range(min(df['Age']),max(df['Age']) + 1,1),color = 'blue',edgecolor = 'black')\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Create the histogram\nplt.hist(data['Age'], bins=20, color='blue', edgecolor='black')\n\n# Set the title and labels\nplt.title('Distribution of Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\n# Since function doesn't return any value, hence printing none/null.\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Create a scatter plot with 'weight' plotted on the x-axis and 'mpg' plotted on the y-axis, where the color of each point represents the corresponding value of 'modelyear'.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Create a scatter plot with 'weight' plotted on the x-axis and 'mpg' plotted on the y-axis, where the color of each point represents the corresponding value of 'modelyear'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/11.csv')\n\n# Set the columns for the x and y axes\nx = data['weight']\ny = data['mpg']\n\n# Set the colors based on 'modelyear'\ncolors = data['modelyear']\n\n# Create the scatter plot\nplt.scatter(x,y,c=colors)\n\n# Add the labels to the x and y axes\nplt.xlabel('Weight')\nplt.ylabel('MPG')\n\n# Add the title to the plot\nplt.title('Scatter plot: Weight vs MPG')\n\n# Show the plot\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df['weight'], df['mpg'], c=df['modelyear'])\n\n# Set the labels for the axes\nplt.xlabel('Weight')\nplt.ylabel('MPG')\n\n# Create a colorbar\nplt.colorbar(label='Model Year')\n\n# Display the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Plot a Scatter plot between 'ANNUAL_RT' and 'Gross' for visual correlation.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Plot a Scatter plot between 'ANNUAL_RT' and 'Gross' for visual correlation.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/12.csv')\n\n# Clean/Preprocess the data\n# Replace missing values with 0\ndata['ANNUAL_RT'] = data['ANNUAL_RT'].fillna(0)\ndata['Gross'] = data['Gross'].fillna(0)\n\n# Convert 'ANNUAL_RT' and 'Gross' columns to numeric values\ndata['ANNUAL_RT'] = pd.to_numeric(data['ANNUAL_RT'])\ndata['Gross'] = pd.to_numeric(data['Gross'])\n\n# Plotting\nplt.scatter(data['ANNUAL_RT'],data['Gross'])\nplt.title('Correlation between ANNUAL_RT and Gross')\nplt.xlabel('ANNUAL_RT')\nplt.ylabel('Gross')\nplt.show()\n\nprint('Scatter plot generated successfully')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove any null values from the columns 'ANNUAL_RT' and 'Gross'\ndf = df.dropna(subset=['ANNUAL_RT', 'Gross'])\n\n# Scatter plot between 'ANNUAL_RT' and 'Gross'\nplt.scatter(df['ANNUAL_RT'], df['Gross'])\nplt.title('Scatter plot between ANNUAL_RT and Gross')\nplt.xlabel('ANNUAL_RT')\nplt.ylabel('Gross')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Generate a line plot showing 'BARO' trends over the given time period.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Generate a line plot showing 'BARO' trends over the given time period.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Generate a line plot for 'BARO' trends\nplt.plot(data['DATE TIME'],data['BARO'])\nplt.xlabel('Date Time')\nplt.ylabel('Barometric Pressure (hPa)')\nplt.title('BARO Trends Over Time')\nplt.xticks(rotation=45)\nplt.show()", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Reading the csv file\ndf = pd.read_csv('data.csv')\n\n# Plotting 'BARO' trends over the given time period\nplt.figure(figsize=(12,6))\nplt.plot(df['TIME'], df['BARO'])\nplt.xlabel('Time')\nplt.ylabel('BARO')\nplt.title('Trend of BARO over the given time period')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Create a scatter plot of salary vs. number_of_home_runs; one for players with free agency eligibility and one for those without.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Create a scatter plot of salary vs. number_of_home_runs; one for players with free agency eligibility and one for those without.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# read the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# segregate data based on the 'indicator_of_free_agency_eligibility' status\nfa_eligible = df[df['indicator_of_free_agency_eligibility'] == 1.0]\nfa_not_eligible = df[df['indicator_of_free_agency_eligibility'] == 0.0]\n\n# create a scatter plot for those with free agency eligibility\nplt.scatter(fa_eligible['number_of_home_runs'],fa_eligible['salary_in_thousands_of_dollars'],color = 'red')\nplt.title('Scatter plot for players with free agency eligibility')\nplt.xlabel('Number of Home Runs')\nplt.ylabel('Salary in thousands of dollars')\nplt.show()\n\n# create a scatter plot for those without free agency eligibility\nplt.scatter(fa_not_eligible['number_of_home_runs'],fa_not_eligible['salary_in_thousands_of_dollars'],color = 'blue')\nplt.title('Scatter plot for players without free agency eligibility')\nplt.xlabel('Number of Home Runs')\nplt.ylabel('Salary in thousands of dollars')\nplt.show()\n\nprint(\"Plot Successfully Created\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter data based on free agency eligibility\ndf_free_agency = df[df['indicator_of_free_agency_eligibility'] == 1]\n\n# Filter data without free agency eligibility\ndf_no_free_agency = df[df['indicator_of_free_agency_eligibility'] == 0]\n\n# Create scatter plot for players with free agency\nplt.scatter(df_free_agency['salary_in_thousands_of_dollars'], df_free_agency['number_of_home_runs'], color='b', label='Free Agency Eligible')\n\n# Create scatter plot for players without free agency\nplt.scatter(df_no_free_agency['salary_in_thousands_of_dollars'], df_no_free_agency['number_of_home_runs'], color='r', label='Not Free Agency Eligible')\n\n# Setting the title and labels\nplt.title('Salary vs Number of Home Runs')\nplt.xlabel('Salary in thousands of dollars')\nplt.ylabel('Number of Home Runs')\n\n# Adding the legend\nplt.legend()\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Create a scatter plot with 'exper' on the x-axis and 'wage' on the y-axis.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Create a scatter plot with 'exper' on the x-axis and 'wage' on the y-axis.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Draw a scatter plot using 'exper' as the x-axis and 'wage' as the y-axis\nplt.scatter(df['exper'],df['wage'])\nplt.title('Scatter plot of exper and wage') \nplt.xlabel('exper') \nplt.ylabel('wage') \n\n# Save the figure\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df['exper'], df['wage'])\n\n# Set labels and title\nplt.xlabel('Experience')\nplt.ylabel('Wage')\nplt.title('Scatter plot of Wage vs Experience')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Create a line plot that shows the trend of 'atemp' for the first 10 instances.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Create a line plot that shows the trend of 'atemp' for the first 10 instances.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Select the first 10 rows of 'atemp'\natemp_data = df['atemp'].head(10)\n\n# Generate the plot\nplt.figure(figsize=(10,6))\nplt.plot(atemp_data,marker='o',linestyle='-',color='b')\nplt.xlabel('Instances')\nplt.ylabel('atemp')\nplt.title('Change in atemp Over First 10 Instances')\nplt.grid(True)\nplt.show()\n\nprint(\"Plot generated successfully.\")", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select the first 10 instances\ndf_first_10 = df.head(10)\n\n# Plot the trend of 'atemp' for the first 10 instances\nplt.figure(figsize=(10, 5))\nplt.plot(df_first_10['atemp'])\nplt.xlabel('Instance')\nplt.ylabel('atemp')\nplt.title('Trend of atemp for the first 10 instances')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Draw a time series plot for 'total_vaccinations' for all dates for the country 'Albania'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Draw a time series plot for 'total_vaccinations' for all dates for the country 'Albania'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Filter the data for the country 'Albania'\nalbania_df = df[df['country'] == 'Albania']\n\n# Convert the 'date' column to datetime\nalbania_df['date'] = pd.to_datetime(albania_df['date'])\n\n# Draw the time series plot\nplt.figure(figsize=(10,6))\nplt.plot(albania_df['date'],albania_df['total_vaccinations'])\nplt.title(\"Time series plot of 'total_vaccinations' for Albania\")\nplt.xlabel('Date')\nplt.ylabel('Total Vaccinations')\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter data for country 'Albania'\nalbania_data = data[data['country'] == 'Albania']\n\n# Plotting\nplt.figure(figsize=(12,6))\nplt.plot(albania_data['date'], albania_data['total_vaccinations'])\nplt.xlabel('Date')\nplt.ylabel('Total Vaccinations')\nplt.title('Time series of Total Vaccinations in Albania')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,AL,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Draw a scatter plot for 'daily_vaccinations' and 'total_vaccinations' displaying the correlation between them for 'Albania'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,AL,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Draw a scatter plot for 'daily_vaccinations' and 'total_vaccinations' displaying the correlation between them for 'Albania'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# filter out rows with the specific country 'Albania'\ndf_albania = df[df['country'] == 'Albania']\n\n# draw a scatter plot to display the correlation between 'daily_vaccinations' and 'total_vaccinations'\nplt.scatter(df_albania['daily_vaccinations'],df_albania['total_vaccinations'])\nplt.title('Correlation between Daily Vaccinations and Total Vaccinations in Albania')\nplt.xlabel('Daily Vaccinations')\nplt.ylabel('Total Vaccinations')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Select data for 'Albania'\nalbania_data = data[data['country'] == 'Albania']\n\n# Create the scatter plot\nplt.scatter(albania_data['daily_vaccinations'], albania_data['total_vaccinations'])\nplt.title('Correlation between Daily and Total Vaccinations in Albania')\nplt.xlabel('Daily Vaccinations')\nplt.ylabel('Total Vaccinations')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Construct a scatter plot to show the correlation between 'age' and 'debt_ratio', with 'age' on the x-axis and 'debt_ratio' on the y-axis, to visualize if age impacts debt ratio.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Construct a scatter plot to show the correlation between 'age' and 'debt_ratio', with 'age' on the x-axis and 'debt_ratio' on the y-axis, to visualize if age impacts debt ratio.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Create a scatter plot between 'age' and 'debt_ratio'\nplt.figure(figsize=(8,6))\nplt.scatter(df['age'],df['debt_ratio'],alpha=0.5)\nplt.title('Correlation between Age and Debt Ratio')\nplt.xlabel('age')\nplt.ylabel('debt_ratio')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check for any missing values in the 'age' and 'debt_ratio' columns\ndf = df.dropna(subset=['age', 'debt_ratio'])\n\n# Construct the scatter plot\nplt.scatter(df['age'], df['debt_ratio'])\n\n# Set the labels for the axes\nplt.xlabel('Age')\nplt.ylabel('Debt Ratio')\n\n# Display the plot\nplt.show()\n\n# As per the instruction, there is no specific necessary data to print out.\n# However, we print the dataframe df to ensure that the data has been loaded correctly\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Create a histogram displaying the distribution of 'Income' data separated by 'Gender', including accurate labels.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Create a histogram displaying the distribution of 'Income' data separated by 'Gender', including accurate labels.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Preprocess the data\ndf['Gender'] = df['Gender'].str.strip()  # remove any whitespace around the gender data\n\n# Define the income data for each gender\nmale_income = df[df['Gender'] == 'Male']['Income']\nfemale_income = df[df['Gender'] == 'Female']['Income']\n\n# Generate the histogram\nplt.hist([male_income,female_income],label=['Male','Female'],bins=10)\n\n# Set appropriate labels\nplt.xlabel('Income')\nplt.ylabel('Count')\nplt.title('Income by Gender')\nplt.legend()\n\n# print the graph\nprint(plt.show())", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Separate data by gender\ndf_male = df[df['Gender'] == 'Male']\ndf_female = df[df['Gender'] == 'Female']\n\n# Create histogram\nplt.hist(df_male['Income'], color='blue', alpha=0.5, label='Male')\nplt.hist(df_female['Income'], color='red', alpha=0.5, label='Female')\n\nplt.xlabel('Income')\nplt.ylabel('Frequency')\nplt.title('Income Distribution by Gender')\nplt.legend()\n\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Draw a scatter plot between 'Income' and 'Limit' with distinct colors for 'Male' and 'Female'.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Draw a scatter plot between 'Income' and 'Limit' with distinct colors for 'Male' and 'Female'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/24.csv')\n\n# Assigning distinct color for Male and Female\ncolors = {'Male':'red','Female':'blue'}\n\n# Create a scatter plot of 'Income' vs 'Limit'\nplt.scatter(data['Income'],data['Limit'],c=data['Gender'].map(colors))\n\nplt.title('Scatter plot between Income and Limit')\nplt.xlabel('Income')\nplt.ylabel('Limit')\n\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Separate the data by Gender\ndf_male = df[df['Gender'] == 'Male']\ndf_female = df[df['Gender'] == 'Female']\n\n# Create a scatter plot\nplt.scatter(df_male['Income'], df_male['Limit'], color='blue', label='Male')\nplt.scatter(df_female['Income'], df_female['Limit'], color='red', label='Female')\nplt.xlabel('Income')\nplt.ylabel('Limit')\nplt.legend()\nplt.show()\n\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Create a histogram based on the 'age' column data.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Create a histogram based on the 'age' column data.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Prepare the data for the histogram\nage_data = data['age']\n\n# Create a histogram using matplotlib\nplt.hist(age_data,bins='auto')\n\n# Provide labels for x and y axis and a title\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Distribution of Age')\n\n# Finally,show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a histogram of the 'age' column\nplt.hist(df['age'], bins=10, edgecolor='black')\n\n# Set the title and labels\nplt.title('Histogram of Age')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\n# Show the plot\nplt.show()\n\n# Since function doesn't return any value, hence printing none/null.\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Generate a bar plot showing the total number of wins per school.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Generate a bar plot showing the total number of wins per school.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Generate a bar plot for total wins per school\ntotal_wins_per_school = data.groupby('School')['Wins'].sum()\ntotal_wins_per_school.plot(kind='bar')\nplt.xlabel('School')\nplt.ylabel('Total Wins')\nplt.title('Total Wins per School')\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Cleaning the data\ndf = df.dropna(subset=['School', 'Wins'])\n\n# Grouping the data by school and summing the wins\nwins_per_school = df.groupby('School')['Wins'].sum()\n\n# Creating the bar plot\nplt.figure(figsize=(10,6))\nwins_per_school.plot(kind='bar')\nplt.title('Total Wins per School')\nplt.xlabel('School')\nplt.ylabel('Total Wins')\n\n# Saving the plot to a file\nplt.savefig('total_wins_per_school.png')\n\nprint('The bar plot has been saved to total_wins_per_school.png.')"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Create a scatter plot displaying \"carat\" values on the x-axis and \"price\" values on the y-axis to illustrate the connection between \"carat\" and \"price\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Create a scatter plot displaying \"carat\" values on the x-axis and \"price\" values on the y-axis to illustrate the connection between \"carat\" and \"price\".", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/28.csv')\n\n# Extract the 'carat' and 'price' columns for the scatter plot\nx_data = data['carat']\ny_data = data['price']\n\n# Create the scatter plot\nplt.scatter(x_data,y_data)\n\n# Set the titles for x and y axis\nplt.xlabel('Carat')\nplt.ylabel('Price')\n\n# Set the title for the plot\nplt.title('Scatter plot of Price vs Carat')\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df['carat'], df['price'])\n\n# Set the labels for the axes\nplt.xlabel('carat')\nplt.ylabel('price')\n\n# Display the plot\nplt.show()\n\n# Since function doesn't return any value, hence printing none/null.\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Create a scatter diagram displaying the correlation between 'Volume' (Y-axis) and 'Open' price (X-axis) for the entire dataset.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Create a scatter diagram displaying the correlation between 'Volume' (Y-axis) and 'Open' price (X-axis) for the entire dataset.", "instruction_type": "Draw-Plot", "reference_code": "# import necessary libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndata = pd.read_csv('infiagent/csv/29.csv')\n\n# plot the scatter plot\nplt.scatter(data['Open'],data['Volume'])\nplt.title('Scatter plot: Open Price vs Volume')\nplt.xlabel('Open Price')\nplt.ylabel('Volume')\n\n# show the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create scatter plot\nplt.figure(figsize=(10, 6))\nplt.scatter(df['Open'], df['Volume'])\nplt.xlabel('Open')\nplt.ylabel('Volume')\nplt.title('Correlation between Open price and Volume')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Draw a scatter plot of 'votes_dem' vs 'votes_gop' color-coded based on 'state_abbr'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Draw a scatter plot of 'votes_dem' vs 'votes_gop' color-coded based on 'state_abbr'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# remove comma from 'diff' and convert to int type\ndf['diff'] = df['diff'].str.replace(',','').astype(int)\n\n# Convert percentage to float in 'per_point_diff'\ndf['per_point_diff'] = df['per_point_diff'].str.rstrip('%').astype('float') / 100.0\n\n# Create scatter plot \nplt.figure(figsize=(10,8))\nfor state_abbr in df['state_abbr'].unique():\n    plt.scatter(df[df['state_abbr']==state_abbr]['votes_dem'],df[df['state_abbr']==state_abbr]['votes_gop'],label=state_abbr)\nplt.xlabel('votes_dem')\nplt.ylabel('votes_gop')\nplt.legend(loc='best')\nplt.title('Scatter plot of \"votes_dem\" vs \"votes_gop\" color-coded based on \"state_abbr\"')\n\nprint('Generated graph successfully.')", "assistant_code": "# import necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# remove the commas from the 'state_abbr' column and convert it to integer type\ndf['state_abbr'] = df['state_abbr'].str.replace(',', '').astype(int)\n\n# scatter plot of 'votes_dem' vs 'votes_gop' color-coded based on 'state_abbr'\nplt.figure(figsize=(10, 6))\nfor abbreviation in df['state_abbr'].unique():\n    df_filtered = df[df['state_abbr'] == abbreviation]\n    plt.scatter(df_filtered['votes_dem'], df_filtered['votes_gop'], label=abbreviation)\n\nplt.title('Scatter plot of votes_dem vs votes_gop')\nplt.xlabel('votes_dem')\nplt.ylabel('votes_gop')\nplt.legend()\nplt.show()\nprint()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Create a line graph to display the changing pattern of 'No. of Cases' for the country Angola across different years.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Create a line graph to display the changing pattern of 'No. of Cases' for the country Angola across different years.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read the .csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Filter the data for 'Angola'\nangola_data = df[df['Country']=='Angola']\n\n# Plot a line graph\nplt.plot(angola_data['Year'], angola_data['No. of cases'])\n\n# Set the labels and title\nplt.xlabel('Year')\nplt.ylabel('No. of Cases')\nplt.title('Trend of No. of Cases over the years for Angola')\n\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter data for Angola\nangola_data = df[df['Country'] == 'Angola']\n\n# Extract the 'Year' and 'No. of cases' columns\nangola_data = angola_data[['Year', 'No. of cases']].dropna()\n\n# Convert 'Year' column to integer\nangola_data['Year'] = angola_data['Year'].astype(int)\n\n# Create line graph\nplt.figure(figsize=(10, 5))\nplt.plot(angola_data['Year'], angola_data['No. of cases'], marker='o', linestyle='-')\nplt.title('Changing pattern of \"No. of Cases\" for Angola across different years')\nplt.xlabel('Year')\nplt.ylabel('No. of Cases')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Create a scatter plot with 'site' on the horizontal axis and 'abs_diffsel' on the vertical axis.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Create a scatter plot with 'site' on the horizontal axis and 'abs_diffsel' on the vertical axis.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from CSV\ndata = pd.read_csv('infiagent/csv/34.csv')\n\n# Convert 'site' column to string type\ndata['site'] = data['site'].astype(str)\n\n# Create the Scatter Plot\nplt.figure(figsize=(10,6))\nplt.scatter(data['site'],data['abs_diffsel'],alpha=0.5)\n\n# Adding additional details to the plot\nplt.title('Scatter plot of \"site\" vs \"abs_diffsel\"')\nplt.xlabel('site')\nplt.ylabel('abs_diffsel')\n\n# Display the plot\nplt.show()\n\nprint('Chart drawn successfully.')", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Create scatter plot\nplt.scatter(df['site'], df['abs_diffsel'])\nplt.xlabel('site')\nplt.ylabel('abs_diffsel')\nplt.title('Scatter plot of site vs abs_diffsel')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Draw a scatter plot showcasing the correlation between life expectancy and GDP per capita for all continents.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Draw a scatter plot showcasing the correlation between life expectancy and GDP per capita for all continents.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/35.csv')\n\n# Group the data by continent\ngrouped = data.groupby('continent')\n\n# Scatter plot for each continent\nfor name,group in grouped:\n    plt.scatter(group['lifeexp'],group['gdppercap'],label=name)\n\nplt.xlabel(\"Life Expectancy\")\nplt.ylabel(\"GDP Per Capita\")\nplt.legend()\nplt.title(\"Life Expectancy vs GDP per Capita for all continents\")\nplt.show()\n\n#There is no print statement in the code as it is meant to generate a scatter plot. The result will be a scatter plot.", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by continent and calculate the mean of life expectancy and GDP per capita\ngrouped_df = df.groupby('continent').agg({'lifeexp': 'mean', 'gdppercap': 'mean'}).reset_index()\n\n# Create the scatter plot\nplt.figure(figsize=(10,6))\nfor i in grouped_df.index:\n    plt.scatter(grouped_df.loc[i, 'lifeexp'], grouped_df.loc[i, 'gdppercap'], label=grouped_df.loc[i, 'continent'])\n\nplt.xlabel('Life Expectancy')\nplt.ylabel('GDP per Capita')\nplt.title('Correlation between Life Expectancy and GDP per Capita by Continent')\nplt.legend()\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Create a line chart to display the evolution of GDP per capita from 1952 to 2007 for India.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Create a line chart to display the evolution of GDP per capita from 1952 to 2007 for India.", "instruction_type": "Draw-Plot", "reference_code": "# Import the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Select the data for India\nindia_gdp = df[df['country'] == 'India']\n\n# Removing the country column as we do not need it for plotting\nindia_gdp = india_gdp.drop(['country'],axis=1)\n\n# Transpose the data to have years as rows and gdp as columns\nindia_gdp = india_gdp.transpose()\n\n# Reset the index \nindia_gdp = india_gdp.reset_index()\n\n# Rename the columns\nindia_gdp.columns = ['Year','GDP']\n\n# Convert 'Year' column to datetime type and extract the year\nindia_gdp['Year'] = pd.to_datetime(india_gdp['Year'].str.strip('gdpPercap_'))\nindia_gdp['Year'] = india_gdp['Year'].dt.year\n\n# Plot the data\nplt.figure(figsize=(12,6))\nplt.plot(india_gdp['Year'],india_gdp['GDP'],marker='o')\nplt.title(\"GDP per capita growth from 1952 to 2007 for India\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"GDP per Capita\")\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\nindia_data = df[df['country'] == 'India']\n\n# Select the GDP per capita values for India\ngdp_india = india_data.iloc[:, 1:]\n\n# Set the year as the index\nindia_data.index = india_data['country']\nindia_data = india_data.loc['India', gdp_india]\n\n# Plot the line chart\nplt.figure(figsize=(10, 6))\nplt.plot(gdp_india.index, gdp_india.values, marker='o', linestyle='-')\nplt.title('Evolution of GDP per capita from 1952 to 2007 for India')\nplt.xlabel('Year')\nplt.ylabel('GDP per capita')\nplt.grid(True)\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Draw a scatter plot of 'price' against 'sqft_living' to examine the relationship between them.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Draw a scatter plot of 'price' against 'sqft_living' to examine the relationship between them.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Set the size of the figure\nplt.figure(figsize=(10,6))\n\n# Set title and labels for the figure\nplt.title(\"Scatter plot of Price against Square Foot Living\")\nplt.xlabel(\"Square Foot Living\")\nplt.ylabel(\"Price\")\n\n# Plot the scatter graph\nplt.scatter(df['sqft_living'],df['price'])\n\n# Display the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Scatter plot of 'price' against 'sqft_living'\nplt.scatter(df['sqft_living'], df['price'])\nplt.title('Price vs Square Foot Living')\nplt.xlabel('Square Foot Living')\nplt.ylabel('Price')\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Draw a bar plot of 'region' vs average 'charges' for 'smoker'='yes' and 'smoker'='no'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Draw a bar plot of 'region' vs average 'charges' for 'smoker'='yes' and 'smoker'='no'.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# load data\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# calculate average charges grouped by region and smoker status\ngrouped_df = df.groupby(['region','smoker'])['charges'].mean().unstack()\n\n# create the plot\ngrouped_df.plot(kind='bar')\n\n# set labels and title\nplt.xlabel('Region')\nplt.ylabel('Average Charges')\nplt.title('Average Charges by Region and Smoker Status')\n\n# display the legend\nplt.legend(title='Smoker')\n\n# print the plot\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter data for 'smoker' = 'yes' and 'smoker' = 'no'\ndf_yes = df[df['smoker'] == 'yes']\ndf_no = df[df['smoker'] == 'no']\n\n# Calculate the average 'charges' for each 'region'\navg_charges_yes = df_yes.groupby('region')['charges'].mean()\navg_charges_no = df_no.groupby('region')['charges'].mean()\n\n# Create a bar plot\nplt.figure(figsize=(10, 5))\nplt.bar(avg_charges_yes.index, avg_charges_yes.values, color='b', label='Smoker Yes')\nplt.bar(avg_charges_no.index, avg_charges_no.values, color='r', label='Smoker No')\nplt.xlabel('Region')\nplt.ylabel('Average Charges')\nplt.title('Average Charges by Region for Smokers and Non-Smokers')\nplt.legend()\nplt.show()\n\n# Print the average charges for each region for smokers and non-smokers\nprint('Average charges for smokers by region:')\nprint(avg_charges_yes)\nprint('\\nAverage charges for non-smokers by region:')\nprint(avg_charges_no)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Create a scatter plot where \"High\" variables are plotted along the x-axis and \"Low\" variables along the y-axis.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Create a scatter plot where \"High\" variables are plotted along the x-axis and \"Low\" variables along the y-axis.", "instruction_type": "Draw-Plot", "reference_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/44.csv')\n\n# Extract the necessary columns\nhigh_values = data['High']\nlow_values = data['Low']\n\n# Create the scatter plot\nplt.scatter(high_values,low_values)\n\n# Add labels and title\nplt.xlabel('High Values')\nplt.ylabel('Low Values')\nplt.title('Scatter plot of High values vs Low values')\n\n# Show the plot\nplt.show()\n\nprint()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df['High'], df['Low'])\n\n# Label the axes\nplt.xlabel('High')\nplt.ylabel('Low')\n\n# Title the plot\nplt.title('Scatter plot of High vs Low')\n\n# Show the plot\nplt.show()"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Create a scatter plot illustrating the correlation between House Age and the Median House Value.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Create a scatter plot illustrating the correlation between House Age and the Median House Value.", "instruction_type": "Draw-Plot", "reference_code": "import matplotlib.pyplot as plt\nimport pandas as pd\n\n# loading the dataset\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# plotting the graph\nplt.figure(figsize=(10,6))\nplt.scatter(df['HouseAge'],df['MedianHouseValue'])\nplt.title('House Age vs Median House Value')\nplt.xlabel('House Age')\nplt.ylabel('Median House Value')\nplt.grid(True)\nplt.show()", "assistant_code": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create the scatter plot\nplt.scatter(df['HouseAge'], df['MedianHouseValue'])\nplt.xlabel('House Age')\nplt.ylabel('Median House Value')\nplt.title('Correlation between House Age and Median House Value')\n\n# Show the plot\nplt.show()\n\n# No requirement for printing any output, the output is the plot itself."}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Add a new column that shows whether the value of cases_median falls within the range of 2000 to 50,000.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Add a new column that shows whether the value of cases_median falls within the range of 2000 to 50,000.", "instruction_type": "Insert - Data Processing", "reference_code": "import pandas as pd\n\n# Read the csv\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Create new column\ndf['No. of cases_median_Range'] = df['No. of cases_median'].apply(lambda x: 'Yes' if 2000 <= x <= 50000 else 'No')\n\n# Output the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Check if the 'No. of cases_median' falls within the range of 2000 to 50,000 and add the results to a new column\ndf['within_range'] = df['No. of cases_median'].between(2000, 50000)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Add a new column to display the cumulative total of 'Miles traveled to date:'.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Add a new column to display the cumulative total of 'Miles traveled to date:'.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file data into a DataFrame.\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Create a new column to keep the running total of 'Miles traveled to date:'.\ndf['Running Total of Miles traveled to date:'] = df['Miles traveled to date:'].cumsum()\n\n# Print the updated DataFrame.\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column to display the cumulative total of 'Miles traveled to date:'\ndf['Cumulative Miles Traveled to Date'] = df['Miles traveled to date:'].cumsum()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Create a new column called 'Weekly Total Passes' that calculates the sum of '7-Day Passes Purchased' for each week.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Create a new column called 'Weekly Total Passes' that calculates the sum of '7-Day Passes Purchased' for each week.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Set 'Date' as the index of the dataframe\ndf = df.set_index('Date')\n\n# Resample data on a weekly basis and calculate sum of '7-Day Passes Purchased (midnight to 11:59 pm)' for each week\ndf['Weekly Total Passes'] = df['7-Day Passes Purchased (midnight to 11:59 pm)'].resample('W').sum()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Weekly Total Passes' that sums up the '7-Day Passes Purchased' for each week\ndf['Weekly Total Passes'] = df['7-Day Passes Purchased (midnight to 11:59 pm)'].sum()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Create a new column called 'Total_emotion_score' that calculates the sum of all emotion values for each row.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Create a new column called 'Total_emotion_score' that calculates the sum of all emotion values for each row.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/9.csv')\n\n# Add a new column 'Total_emotion_score' that sums up all emotion values per row\ndata['Total_emotion_score'] = data[['anger','contempt','disgust','fear','happiness','neutral','sadness','surprise']].sum(axis=1)\n\n# Output the result\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Total_emotion_score' that is the sum of all emotion columns\ndf['Total_emotion_score'] = df['anger'] + df['contempt'] + df['disgust'] + df['fear'] + df['happiness'] + df['neutral'] + df['sadness'] + df['surprise'] + df['blur']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Create a new column named 'emotion_dominant' that indicates the emotion with the highest score in each row.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Create a new column named 'emotion_dominant' that indicates the emotion with the highest score in each row.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# List of all emotions\nemotions = ['anger','contempt','disgust','fear','happiness','neutral','sadness','surprise']\n\n# Add a new column 'emotion_dominant' that shows which emotion had the highest score for each row.\ndf['emotion_dominant'] = df[emotions].idxmax(axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Find the index of the maximum value in each row of the anger, contempt, disgust, fear, happiness, neutral, sadness and surprise columns\ndf['emotion_dominant'] = df[['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']].idxmax(axis=1)\n\n# Convert the column to a more readable format\ndf['emotion_dominant'] = df['emotion_dominant'].replace(['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise'], ['Anger', 'Contempt', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Create a new column named 'Mean_Balance' and calculate the average balance for each 'Gender'.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Create a new column named 'Mean_Balance' and calculate the average balance for each 'Gender'.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Calculate the mean balance for each 'Gender'\nmean_balance = df.groupby('Gender')['Balance'].transform('mean')\n\n# Insert the new column 'Mean_Balance'\ndf['Mean_Balance'] = mean_balance\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean balance for each gender\nmean_balance = df.groupby('Gender')['Balance'].mean()\n\n# Create a new dataframe with the mean balance for each gender\ndf_mean_balance = pd.DataFrame(mean_balance)\n\n# Rename the column to 'Mean_Balance'\ndf_mean_balance.rename(columns={'Balance': 'Mean_Balance'}, inplace=True)\n\n# Merge the new dataframe with the original dataframe\ndf = pd.merge(df, df_mean_balance, how='left', on='Gender')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Add a column named 'Sum' to the table that calculates the sum of the values in columns X, Y, and Z for each row.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Add a column named 'Sum' to the table that calculates the sum of the values in columns X, Y, and Z for each row.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the given CSV\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Add a column named 'Sum' that calculates the sum of the values in columns X,Y,and Z for each row\ndf['Sum'] = df[['X','Y','Z']].sum(axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Compute the sum of the values in columns 'X', 'Y', and 'Z' for each row and add the result to a new column 'Sum'\ndf['Sum'] = df['X'] + df['Y'] + df['Z']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Add a new column 'EQ_Sum' that is the sum of 'EQ1' to 'EQ8' for each row.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Add a new column 'EQ_Sum' that is the sum of 'EQ1' to 'EQ8' for each row.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Compute the sum of 'EQ1' to 'EQ8' for each row and generate a new column 'EQ_Sum'\ndf['EQ_Sum'] = df[['EQ1','EQ2','EQ3','EQ4','EQ5','EQ6','EQ7','EQ8']].sum(axis=1)\n\n# Print the first 10 rows of the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Generate a new column 'EQ_Sum' that is the sum of columns 'EQ1' to 'EQ8'\ndf['EQ_Sum'] = df['EQ1'] + df['EQ2'] + df['EQ3'] + df['EQ4'] + df['EQ5'] + df['EQ6'] + df['EQ7'] + df['EQ8']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Calculate the average of 'positive_diffsel' column using a sub-query and insert the result into the 'positive_diffsel' column.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Calculate the average of 'positive_diffsel' column using a sub-query and insert the result into the 'positive_diffsel' column.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Calculate the average of 'positive_diffsel'\naverage_positive_diffsel = df['positive_diffsel'].mean()\n\n# Insert into 'positive_diffsel' the output of the sub-query \ndf['positive_diffsel'] = average_positive_diffsel\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'positive_diffsel' column\naverage_positive_diffsel = df['positive_diffsel'].mean()\n\n# Update the 'positive_diffsel' column with the average value\ndf['positive_diffsel'] = average_positive_diffsel\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Aggregate data by 'LibraryID', calculate the mean, and count the number of 'importance.score' for each group.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Aggregate data by 'LibraryID', calculate the mean, and count the number of 'importance.score' for each group.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Aggregate data by 'LibraryID' and calculate mean and count of 'importance.score'\nagg_df = df.groupby('LibraryID').agg(mean_importance_score=('importance.score','mean'),count_importance_score=('importance.score','count'))\n\nprint(agg_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'LibraryID' and calculate the mean and count of 'importance.score'\ngrouped_df = df.groupby('LibraryID')['importance.score'].agg(['mean', 'count'])\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Add a row at the end of the table showing the total number of 'children' in the dataset.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Add a row at the end of the table showing the total number of 'children' in the dataset.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Loading the CSV file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Compute the total number of 'children'\ntotal_children = df['children'].sum()\n\n# Construct a new DataFrame with the total children calculated and append it to the existing DataFrame\ntotal_row = pd.DataFrame({'children': [total_children]},index=['Total'])\ndf = pd.concat([df,total_row])\n\n# Print the DataFrame with the total row\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# calculate the total number of 'children'\ntotal_children = df['children'].sum()\n\n# add a new row at the end of the dataframe with the total number of 'children'\ndf = pd.concat([df, pd.DataFrame({'children': [total_children]})])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Add a new field 'smoker_count' which is the count of 'smoker'='yes' for each 'region'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Add a new field 'smoker_count' which is the count of 'smoker'='yes' for each 'region'.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# loading the data from csv file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Grouping by 'region' and counting 'smoker'='yes' \ndf_smoker = df[df['smoker'] == 'yes'].groupby('region')['smoker'].count().reset_index()\n\n# Renaming the 'smoker' column to 'smoker_count'\ndf_smoker.rename(columns={'smoker': 'smoker_count'},inplace=True)\n\n# Merging the smoker_count with the initial dataframe\ndf_final = pd.merge(df,df_smoker,on='region',how='left')\n\n# replace NaN values in 'smoker_count' with 0\ndf_final['smoker_count'] = df_final['smoker_count'].fillna(0)\n\nprint(df_final)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the number of smokers in each region\nsmoker_count = df[df['smoker'] == 'yes'].groupby('region')['smoker'].count()\n\n# Add the smoker count to the original dataframe\ndf = df.merge(smoker_count.reset_index(), on='region', how='left')\ndf = df.rename(columns={'smoker_y': 'smoker_count'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Add a new column 'Sciences' which is an average of 'Biology', 'Physical Sciences' and 'Math and Statistics' for each row.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Add a new column 'Sciences' which is an average of 'Biology', 'Physical Sciences' and 'Math and Statistics' for each row.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# calculate the average of 'Biology','Physical Sciences' and 'Math and Statistics' for each row\ndf['Sciences'] = df[['Biology','Physical Sciences','Math and Statistics']].mean(axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the average of 'Biology', 'Physical Sciences' and 'Math and Statistics' for each row and store the value in a new column 'Sciences'\ndf['Sciences'] = df[['Biology', 'Physical Sciences', 'Math and Statistics']].mean(axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Introduce a new column 'Total' into the table that calculates the sum of all class values for each corresponding row.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Introduce a new column 'Total' into the table that calculates the sum of all class values for each corresponding row.", "instruction_type": "Insert-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Add a new column 'Total' which is the sum of all classes for each row\ndf['Total'] = df.iloc[:,1:].sum(axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Total' that is the sum of all the class columns\ndf['Total'] = df.iloc[:, 1:].sum(axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Compute a new column 'Average Daily Mileage' by dividing 'Miles traveled today' by 'Trips over the past 24-hours'.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78", "csv_path": "infiagent/csv/1.csv", "instruction": "Compute a new column 'Average Daily Mileage' by dividing 'Miles traveled today' by 'Trips over the past 24-hours'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Create a new column \"Average Daily Mileage\"\ndf['Average Daily Mileage'] = df['Miles traveled today (midnight to 11:59 pm)'] / df['Trips over the past 24-hours (midnight to 11:59pm)']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute 'Average Daily Mileage'\ndf['Average Daily Mileage'] = df['Miles traveled today (midnight to 11:59 pm)'] / df['Trips over the past 24-hours (midnight to 11:59pm)']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Create a new column 'travel_time' that represents the difference between 'act_dep_time' and 'act_arr_time' for each station.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Create a new column 'travel_time' that represents the difference between 'act_dep_time' and 'act_arr_time' for each station.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# convert 'act_dep_time' and 'act_arr_time' into datetime\ndf['act_dep_time'] = pd.to_datetime(df['act_dep_time'])\ndf['act_arr_time'] = pd.to_datetime(df['act_arr_time'])\n\n# calculate the difference between 'act_dep_time' and 'act_arr_time'\ndf['travel_time'] = df['act_dep_time'] - df['act_arr_time']\n\nprint(df)", "assistant_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'act_dep_time' and 'act_arr_time' to datetime format\ndf['act_dep_time'] = pd.to_datetime(df['act_dep_time'])\ndf['act_arr_time'] = pd.to_datetime(df['act_arr_time'])\n\n# Calculate the difference and create a new column 'travel_time'\ndf['travel_time'] = (df['act_arr_time'] - df['act_dep_time']).dt.total_seconds()/3600\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Create a new column which shows the ratio between 'Health (Life Expectancy)' and 'Economy (GDP per Capita)' for each country.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Create a new column which shows the ratio between 'Health (Life Expectancy)' and 'Economy (GDP per Capita)' for each country.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load csv data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Create new column to calculate ratio between 'Health (Life Expectancy)' and 'Economy (GDP per Capita)'\ndf['Ratio_Health_Economy'] = df['Health (Life Expectancy)'] / df['Economy (GDP per Capita)']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column that is the ratio of 'Health (Life Expectancy)' and 'Economy (GDP per Capita)'\ndf['Health_to_Economy_Ratio'] = df['Health (Life Expectancy)'] / df['Economy (GDP per Capita)']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Create a new column by multiplying 'Health (Life Expectancy)' and 'Economy (GDP per Capita)' for each country.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Create a new column by multiplying 'Health (Life Expectancy)' and 'Economy (GDP per Capita)' for each country.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Generate the new column by multiplying 'Health (Life Expectancy)' and 'Economy (GDP per Capita)'\ndf['Health_and_Economy'] = df['Health (Life Expectancy)'] * df['Economy (GDP per Capita)']\n\n# Print the DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column by multiplying 'Health (Life Expectancy)' and 'Economy (GDP per Capita)'\ndf['Health_Economy_Product'] = df['Health (Life Expectancy)'] * df['Economy (GDP per Capita)']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Add a new column to the data denoting \"Total calls activity\", which is the sum of \"num. calls answered\", \"num. calls abandoned\", \"num. calls transferred\", \"num. calls timed out\".", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Add a new column to the data denoting \"Total calls activity\", which is the sum of \"num. calls answered\", \"num. calls abandoned\", \"num. calls transferred\", \"num. calls timed out\".", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Add a new column 'Total calls activity' \ndf['Total calls activity'] = df['num. calls answered'] + df['num. calls abandoned'] + df['num. calls transferred'] + df['num. calls timed out']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Add a new column \"Total calls activity\" which is the sum of \"num. calls answered\", \"num. calls abandoned\", \"num. calls transferred\", \"num. calls timed out\"\ndata['Total calls activity'] = data['num. calls answered'] + data['num. calls abandoned'] + data['num. calls transferred'] + data['num. calls timed out']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Add a column displaying the call answer rate, which is obtained by dividing \"num. calls answered\" by the total of \"num. calls answered\" and \"num. calls abandoned\".", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Add a column displaying the call answer rate, which is obtained by dividing \"num. calls answered\" by the total of \"num. calls answered\" and \"num. calls abandoned\".", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load data from the provided path\ndata = pd.read_csv('infiagent/csv/4.csv')\n\n# Calculate the call answer rate and add it as a new column to the dataframe\n# Ensure to handle division by zero by adding a small value to the denominator\ndata['call answer rate'] = data['num. calls answered'] / (data['num. calls answered'] + data['num. calls abandoned'] + 1e-9)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the total number of calls handled\ndf['total calls handled'] = df['num. calls answered'] + df['num. calls abandoned']\n\n# Calculate the call answer rate\ndf['call answer rate'] = df['num. calls answered'] / df['total calls handled']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Create a new column labeled \"weight_Increased_by_10\" in the table, and populate it with the values obtained by multiplying the \"weight\" column by 10.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Create a new column labeled \"weight_Increased_by_10\" in the table, and populate it with the values obtained by multiplying the \"weight\" column by 10.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load data from csv\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Add a new column 'Weight_Increased_by_10' with 'Weight' multiplied by 10\ndf['Weight_Increased_by_10'] = df['Weight'] * 10\n\n# Display the modified dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column \"weight_Increased_by_10\" \ndf['weight_Increased_by_10'] = df['Weight'] * 10\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Introduce a new column named 'Lng_Change' that displays the difference between 'lng_dest' and 'lng_org' in the table.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Introduce a new column named 'Lng_Change' that displays the difference between 'lng_dest' and 'lng_org' in the table.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Calculate the difference between 'lng_dest' and 'lng_org'\ndf['Lng_Change'] = df['lng_dest'] - df['lng_org']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the difference between 'lng_dest' and 'lng_org'\ndf['Lng_Change'] = df['lng_dest'] - df['lng_org']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Add a column 'USFLUX_Billion', where values equal to 'USFLUX' divided by 1e9.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Add a column 'USFLUX_Billion', where values equal to 'USFLUX' divided by 1e9.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Add a new column 'USFLUX_Billion' by dividing 'USFLUX' by 1e9\ndf['USFLUX_Billion'] = df['USFLUX'] / 1e9\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column 'USFLUX_Billion' by dividing 'USFLUX' by 1e9\ndf['USFLUX_Billion'] = df['USFLUX'] / 1e9\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Create a new column representing the time difference between 'WCTIMESTRING' and 'PCTIMESTRING'.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Create a new column representing the time difference between 'WCTIMESTRING' and 'PCTIMESTRING'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/7.csv')\n\n# Create a new column 'TIME_DIFF' indicating the time difference between 'WCTIMESTRING' and 'PCTIMESTRING'\ndata['TIME_DIFF'] = pd.to_datetime(data['WCTIMESTRING']) - pd.to_datetime(data['PCTIMESTRING'])\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'WCTIMESTRING' and 'PCTIMESTRING' to datetime format\ndf['WCTIMESTRING'] = pd.to_datetime(df['WCTIMESTRING'])\ndf['PCTIMESTRING'] = pd.to_datetime(df['PCTIMESTRING'])\n\n# Calculate the time difference and create a new column\ndf['TIME_DIFF'] = df['WCTIMESTRING'] - df['PCTIMESTRING']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Insert a new column \"Density\" to table, calculated as Whole weight divided by the product of Length, Diameter and Height.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Insert a new column \"Density\" to table, calculated as Whole weight divided by the product of Length, Diameter and Height.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Read the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Calculate Density and insert this as a new column in DataFrame\ndf['Density'] = df['Whole weight'] / (df['Length'] * df['Diameter'] * df['Height'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the density and insert it as a new column\ndf['Density'] = df['Whole weight'] / (df['Length'] * df['Diameter'] * df['Height'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Create a new column 'Weight Loss' that represents the difference between 'Whole weight' and the sum of 'Shucked weight', 'Viscera weight' and 'Shell weight'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Create a new column 'Weight Loss' that represents the difference between 'Whole weight' and the sum of 'Shucked weight', 'Viscera weight' and 'Shell weight'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Creating the new column 'Weight Loss'\ndf['Weight Loss'] = df['Whole weight'] - (df['Shucked weight'] + df['Viscera weight'] + df['Shell weight'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Create a new column 'Weight Loss' that represents the difference between 'Whole weight' and the sum of 'Shucked weight', 'Viscera weight' and 'Shell weight'\ndata['Weight Loss'] = data['Whole weight'] - (data['Shucked weight'] + data['Viscera weight'] + data['Shell weight'])\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Create a new column 'Count_Score' in the table, where its value is the result of dividing 'Count' by 'Age' for each row.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Create a new column 'Count_Score' in the table, where its value is the result of dividing 'Count' by 'Age' for each row.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/10.csv')\n\n# Add 'Count_Score' column to the table calculated as Count divided by Age\ndata['Count_Score'] = data['Count'] / data['Age']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Perform the calculation and create a new column 'Count_Score'\ndf['Count_Score'] = df['Count'] / df['Age']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Create a new column named 'efficiency' that calculates the miles per gallon per horsepower.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Create a new column named 'efficiency' that calculates the miles per gallon per horsepower.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/11.csv')\n\n# Add 'efficiency' column\ndata['efficiency'] = data['mpg'] / data['horsepower']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the efficiency and create a new column\ndf['efficiency'] = df['mpg'] / df['horsepower']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Add a column for the ratio of 'displacement' to the vehicle's average 'displacement' within its 'cylinders' group.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Add a column for the ratio of 'displacement' to the vehicle's average 'displacement' within its 'cylinders' group.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Calculate the average displacement for each 'cylinders' group\naverage_displacement = df.groupby('cylinders')['displacement'].transform('mean')\n\n# Add a new column for the ratio of 'displacement' to its average in its 'cylinders' group\ndf['displacement_ratio'] = df['displacement'] / average_displacement\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average displacement for each cylinders group\navg_displacement = df.groupby('cylinders')['displacement'].transform('mean')\n\n# Add a new column with the ratio of displacement to the average displacement within its cylinders group\ndf['displacement_ratio'] = df['displacement'] / avg_displacement\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Calculate the monthly rate from the given 'ANNUAL_RT' and add this as a new column 'MONTHLY_RT'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Calculate the monthly rate from the given 'ANNUAL_RT' and add this as a new column 'MONTHLY_RT'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the given csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Calculate the monthly rate from the annual rate and add it as a new column\ndf['MONTHLY_RT'] = df['ANNUAL_RT'] / 12\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the monthly rate from the annual rate\ndf['MONTHLY_RT'] = df['ANNUAL_RT'] / 12\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Calculate the number of years from the hire date to the current date and add the result to a new column called 'YEARS_OF_SERVICE'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Calculate the number of years from the hire date to the current date and add the result to a new column called 'YEARS_OF_SERVICE'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\nfrom datetime import datetime as dt\n\n# read the csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# handle non-standard data\ndf = df.fillna('')\n\n# convert HIRE_DT to datetime type\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'])\n\n# calculate the number of years of service\ndf['YEARS_OF_SERVICE'] = (dt.now() - df['HIRE_DT']).dt.days // 365\n\nprint(df)", "assistant_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'HIRE_DT' column to datetime format\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'])\n\n# Calculate the number of years from the hire date to the current date\ndf['YEARS_OF_SERVICE'] = (datetime.now() - df['HIRE_DT']).dt.days / 365.25\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Add a new column 'TEMP_FAHRENHEIT' by converting 'AT' from Celsius to Fahrenheit.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Add a new column 'TEMP_FAHRENHEIT' by converting 'AT' from Celsius to Fahrenheit.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Add a new column with temperature in Fahrenheit\ndata['TEMP_FAHRENHEIT'] = data['AT'] * 9/5 + 32\n\nprint(data)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Handle non-standard data\ndf['AT'].fillna(0, inplace=True)\n\n# Convert 'AT' from Celsius to Fahrenheit and add it as a new column 'TEMP_FAHRENHEIT'\ndf['TEMP_FAHRENHEIT'] = df['AT'].apply(lambda x: (x * 9/5) + 32)\n\n# Check the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Insert a column 'batting_efficiency' that is calculated as the number_of_hits divided by number_of_runs.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Insert a column 'batting_efficiency' that is calculated as the number_of_hits divided by number_of_runs.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Calculate batting_efficiency and insert it as a new column\ndf['batting_efficiency'] = df['number_of_hits'] / df['number_of_runs']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate batting_efficiency and add it as a new column\ndf['batting_efficiency'] = df['number_of_hits'] / df['number_of_runs']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Create a new column named 'feelslike_temp' calculated as (0.70 * atemp) + (0.30 * hum).", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Create a new column named 'feelslike_temp' calculated as (0.70 * atemp) + (0.30 * hum).", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Add 'feelslike_temp' column computed as (0.70 * atemp) + (0.30 * hum)\ndf['feelslike_temp'] = (0.70 * df['atemp']) + (0.30 * df['hum'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'feelslike_temp'\ndf['feelslike_temp'] = (0.70 * df['atemp']) + (0.30 * df['hum'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Add a new column that shows the difference between the number of 'registered' and 'casual' bike rentals.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Add a new column that shows the difference between the number of 'registered' and 'casual' bike rentals.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/16.csv')\n\n# Create a new column 'diff' calculating the difference between 'registered' and 'casual' bike rentals\ndata['diff'] = data['registered'] - data['casual']\n\n# Print the updated data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Add a new column that shows the difference between the number of 'registered' and 'casual' bike rentals\ndf['difference'] = df['registered'] - df['casual']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Create a 'temperature_difference' column that calculates the difference between temp and atemp.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Create a 'temperature_difference' column that calculates the difference between temp and atemp.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Compute the temperature difference and create a new column\ndf['temperature_difference'] = df['temp'] - df['atemp']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a 'temperature_difference' column\ndf['temperature_difference'] = df['temp'] - df['atemp']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Create a new column indicating the difference between the 'High' and 'Low' values.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Create a new column indicating the difference between the 'High' and 'Low' values.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load data\ndata = pd.read_csv('infiagent/csv/17.csv')\n\n# [Insert-Data Preprocessing] Create a new column indicating the difference between the 'High' and 'Low' values.\ndata['Price Difference'] = data['High'] - data['Low']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Difference' that is the difference between 'High' and 'Low'\ndf['Difference'] = df['High'] - df['Low']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Create a new column labeled 'Days of Service' to determine the total days of service for each record by subtracting the Start Date from the End Date.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Create a new column labeled 'Days of Service' to determine the total days of service for each record by subtracting the Start Date from the End Date.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/20.csv',parse_dates=['Start Date','End Date'])\n\n# Calculate 'Days of Service' and add as new column\ndata['Days of Service'] = (data['End Date'] - data['Start Date']).dt.days\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Start Date' and 'End Date' columns to datetime\ndf['Start Date'] = pd.to_datetime(df['Start Date'])\ndf['End Date'] = pd.to_datetime(df['End Date'])\n\n# Calculate the total days of service and create a new column 'Days of Service'\ndf['Days of Service'] = (df['End Date'] - df['Start Date']).dt.days\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Add a column 'max_wind_per_cat' calculating the wind speed per storm category by dividing 'max_sust_wind' by 'max_storm_cat'.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Add a column 'max_wind_per_cat' calculating the wind speed per storm category by dividing 'max_sust_wind' by 'max_storm_cat'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Add a column 'max_wind_per_cat' calculating the wind speed per storm category \ndf['max_wind_per_cat'] = df['max_sust_wind'] / df['max_storm_cat']\n\n# Replace inf and NaN values with 0\ndf['max_wind_per_cat'] = df['max_wind_per_cat'].replace([float('inf'),float('-inf')],0).fillna(0)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column 'max_wind_per_cat' calculated as 'max_sust_wind' divided by 'max_storm_cat'\ndf['max_wind_per_cat'] = df['max_sust_wind'] / df['max_storm_cat']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Add a column 'damage_in_billion_USD' converting 'damage_USD' into billions.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Add a column 'damage_in_billion_USD' converting 'damage_USD' into billions.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv data into a DataFrame\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Insert a new column 'damage_in_billion_USD' and convert 'damage_USD' into billions\ndf['damage_in_billion_USD'] = df['damage_USD'] / 1e9\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'damage_USD' to damage_in_billion_USD\ndf['damage_in_billion_USD'] = df['damage_USD'] / 1e9\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Calculate the difference between 'people_vaccinated' and 'people_fully_vaccinated' for each entry.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Calculate the difference between 'people_vaccinated' and 'people_fully_vaccinated' for each entry.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Calculate the differences\ndf['vaccination_difference'] = df['people_vaccinated'] - df['people_fully_vaccinated']\n\n# Display the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Calculate the difference between 'people_vaccinated' and 'people_fully_vaccinated' for each entry\ndata['vaccination_difference'] = data['people_vaccinated'] - data['people_fully_vaccinated']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Create a new column called 'total_due_days' that calculates the sum of 'number_of_time30-59_days_past_due_not_worse', 'number_of_times90_days_late', and 'number_of_times_late' to represent the overall lateness of the records.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Create a new column called 'total_due_days' that calculates the sum of 'number_of_time30-59_days_past_due_not_worse', 'number_of_times90_days_late', and 'number_of_times_late' to represent the overall lateness of the records.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/23.csv')\n\n# Create a new column 'total_due_days'\ndata['total_due_days'] = data['number_of_time30-59_days_past_due_not_worse'] + data['number_of_times90_days_late'] + data['number_of_time60-89_days_past_due_not_worse']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Fill the missing values with 0\ndf.fillna(0, inplace=True)\n\n# Create a new column 'total_due_days' that is the sum of 'number_of_time30-59_days_past_due_not_worse', \n# 'number_of_times90_days_late', and 'number_of_times_late'\ndf['total_due_days'] = df['number_of_time30-59_days_past_due_not_worse'] + df['number_of_times90_days_late'] + df['number_of_times_late']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Change the column names to lowercase and add a new column 'income_to_limit_ratio' which shows the ratio of 'Income' to 'Limit' for each person.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Change the column names to lowercase and add a new column 'income_to_limit_ratio' which shows the ratio of 'Income' to 'Limit' for each person.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Change column names to lowercase\ndf.columns = map(str.lower,df.columns)\n\n# Add 'income_to_limit_ratio' column\ndf['income_to_limit_ratio'] = df['income'] / df['limit']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Change the column names to lowercase\ndata.columns = data.columns.str.lower()\n\n# Add a new column 'income_to_limit_ratio'\ndata['income_to_limit_ratio'] = data['income']/data['limit']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Calculate 'DebtPerDependent' by dividing 'DebtRatio' by 'NumberOfDependents' only if 'NumberOfDependents' is not zero. If 'NumberOfDependents' is zero, set 'DebtPerDependent' equal to 'DebtRatio'.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Calculate 'DebtPerDependent' by dividing 'DebtRatio' by 'NumberOfDependents' only if 'NumberOfDependents' is not zero. If 'NumberOfDependents' is zero, set 'DebtPerDependent' equal to 'DebtRatio'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the dataset \ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Replace NA/missing values with 0 in 'NumberOfDependents' column\ndf['NumberOfDependents'].replace(np.nan,0,inplace=True)\n\n# Add a new column 'DebtPerDependent' by dividing 'DebtRatio' by 'NumberOfDependents'\n# If 'NumberOfDependents' is 0,then replace the value with 'DebtRatio'\ndf['DebtPerDependent'] = df.apply(lambda row: row.DebtRatio/row.NumberOfDependents if row.NumberOfDependents != 0 else row.DebtRatio,axis = 1)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Fill NA values with 0\ndf = df.fillna(0)\n\n# Create a new column 'DebtPerDependent' \n# if 'NumberOfDependents' is not zero, divide 'DebtRatio' by 'NumberOfDependents'\n# else, set 'DebtPerDependent' equal to 'DebtRatio'\ndf['DebtPerDependent'] = np.where(df['NumberOfDependents'] != 0, df['DebtRatio'] / df['NumberOfDependents'], df['DebtRatio'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Add a column called 'TotalLatePayments' which combines 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTimes90DaysLate' and 'NumberOfTime60-89DaysPastDueNotWorse'.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Add a column called 'TotalLatePayments' which combines 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTimes90DaysLate' and 'NumberOfTime60-89DaysPastDueNotWorse'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Replacing NA values with 0\ndata = data.replace('NA',0)\n\n# Convert the relevant columns to numeric type for computation\ndata['NumberOfTime30-59DaysPastDueNotWorse'] = pd.to_numeric(data['NumberOfTime30-59DaysPastDueNotWorse'])\ndata['NumberOfTimes90DaysLate'] = pd.to_numeric(data['NumberOfTimes90DaysLate'])\ndata['NumberOfTime60-89DaysPastDueNotWorse'] = pd.to_numeric(data['NumberOfTime60-89DaysPastDueNotWorse'])\n\n# Add the new column 'TotalLatePayments'\ndata['TotalLatePayments'] = data['NumberOfTime30-59DaysPastDueNotWorse'] + data['NumberOfTimes90DaysLate'] + data['NumberOfTime60-89DaysPastDueNotWorse']\n\n# Print the result\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill NA values with 0\ndf.fillna(0, inplace=True)\n\n# Add a new column 'TotalLatePayments' which is the sum of 'NumberOfTime30-59DaysPastDueNotWorse', \n# 'NumberOfTimes90DaysLate' and 'NumberOfTime60-89DaysPastDueNotWorse'\ndf['TotalLatePayments'] = df['NumberOfTime30-59DaysPastDueNotWorse'] + df['NumberOfTimes90DaysLate'] + df['NumberOfTime60-89DaysPastDueNotWorse']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Add a new column 'WIN_SCORE' calculated as 'Wins' multiplied by 'WEIGHTING'.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Add a new column 'WIN_SCORE' calculated as 'Wins' multiplied by 'WEIGHTING'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Add a new column 'WIN_SCORE'\ndata['WIN_SCORE'] = data['Wins'] * data['WEIGHTING']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Loading the data\ndata = pd.read_csv('data.csv')\n\n# Handling missing values\ndata.fillna(0, inplace=True)\n\n# Adding a new column 'WIN_SCORE'\ndata['WIN_SCORE'] = data['Wins'] * data['WEIGHTING']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Create a new column 'TEAM_ID' and assign distinct identifiers to every team.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Create a new column 'TEAM_ID' and assign distinct identifiers to every team.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Add a new column 'TEAM_ID' with unique identifiers\ndata['TEAM_ID'] = data.index + 1\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Creating a new column 'TEAM_ID' and assign distinct identifiers to every team\ndf['TEAM_ID'] = range(1, 1 + len(df) - df.isnull().sum().sum())\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Add a new column named \"Total\" which is the sum of columns X, Y, and Z.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Add a new column named \"Total\" which is the sum of columns X, Y, and Z.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# create a new column 'Total' which is the sum of 'X','Y','Z'\ndf['Total'] = df['X'] + df['Y'] + df['Z']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Perform the addition of columns X, Y and Z and store the result in a new column named \"Total\"\ndf['Total'] = df['X'] + df['Y'] + df['Z']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Calculate the sum of 'X', 'Y', 'Z' for each data and present the results with the corresponding date.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Calculate the sum of 'X', 'Y', 'Z' for each data and present the results with the corresponding date.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Compute the sum of 'X','Y','Z' for each row and store the result in a new column 'Sum'\ndf['Sum'] = df['X'] + df['Y'] + df['Z']\n\n# Create a new DataFrame with the 'Calendar Date (TDB)' and 'Sum' columns\nresult = df[['Calendar Date (TDB)','Sum']]\n\n# Print the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the columns X, Y, Z from string to float type\ndf[['X', 'Y', 'Z']] = df[['X', 'Y', 'Z']].apply(pd.to_numeric, errors='coerce')\n\n# Calculate the sum of 'X', 'Y', 'Z' for each date\nresult = df[['Calendar Date (TDB)', 'X', 'Y', 'Z']].groupby('Calendar Date (TDB)').sum()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Insert a new column \"volume\" calculated as x*y*z for each diamond, representing the approximate volume of each diamond.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Insert a new column \"volume\" calculated as x*y*z for each diamond, representing the approximate volume of each diamond.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Create a new column \"volume\" which is the product of \"x\",\"y\",and \"z\"\ndf['volume'] = df['x'] * df['y'] * df['z']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the volume of each diamond\ndf['volume'] = df['x'] * df['y'] * df['z']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Calculate the \"price_per_carat\" column for each diamond by dividing the \"price\" column by the \"carat\" column and insert the new column into the table.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Calculate the \"price_per_carat\" column for each diamond by dividing the \"price\" column by the \"carat\" column and insert the new column into the table.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Calculate price_per_carat by dividing \"price\" by \"carat\"\ndf['price_per_carat'] = df['price'] / df['carat']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the price per carat\ndf['price_per_carat'] = df['price'] / df['carat']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Insert a new column 'Fluctuation' to the table which represent the daily price fluctuation using the formula (High-Low).", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Insert a new column 'Fluctuation' to the table which represent the daily price fluctuation using the formula (High-Low).", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Insert the 'Fluctuation' column to the dataframe\ndf['Fluctuation'] = df['High'] - df['Low']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Fluctuation' using the formula (High-Low)\ndf['Fluctuation'] = df['High'] - df['Low']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Create a new column containing the product of 'Open' and 'Close' for each row.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Create a new column containing the product of 'Open' and 'Close' for each row.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Compute the new column\ndf['Product'] = df['Open'] * df['Close']\n\n# Print the resulting dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column that is the product of 'Open' and 'Close'\ndf['OpenCloseProduct'] = df['Open'] * df['Close']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Add a new column named 'votes_ind', representing the votes for independent candidates, which can be calculated by subtracting 'votes_gop' and 'votes_dem' from 'total_votes'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Add a new column named 'votes_ind', representing the votes for independent candidates, which can be calculated by subtracting 'votes_gop' and 'votes_dem' from 'total_votes'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Replace non-standard data in 'diff' column \ndf['diff'] = df['diff'].str.replace(',','').astype(float)\n\n# Add a new column 'votes_ind'\ndf['votes_ind'] = df['total_votes'] - df['votes_dem'] - df['votes_gop']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the votes for independent candidates\ndf['votes_ind'] = df['total_votes'] - df['votes_dem'] - df['votes_gop']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Create a new column 'votes_total' by adding 'votes_dem' and 'votes_gop' columns together.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Create a new column 'votes_total' by adding 'votes_dem' and 'votes_gop' columns together.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\nfile_path = 'infiagent/csv/30.csv'\ndata = pd.read_csv(file_path)\n\n# Add a new column 'votes_total'\ndata['votes_total'] = data['votes_dem'] + data['votes_gop']\n\n# Print the updated dataframe\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'votes_total' by adding 'votes_dem' and 'votes_gop' columns together\ndf['votes_total'] = df['votes_dem'] + df['votes_gop']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Insert a new column 'total_per' that combines the 'per_dem' and 'per_gop' by multiplying both values by 100 and round them to the nearest whole number.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Insert a new column 'total_per' that combines the 'per_dem' and 'per_gop' by multiplying both values by 100 and round them to the nearest whole number.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data \ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Create a new column 'total_per' by multiplying 'per_dem' and 'per_gop' by 100 and rounding to the nearest whole number\ndf['total_per'] = np.round((df['per_dem'] + df['per_gop']) * 100)\n\n# Print out the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Insert a new column 'total_per' that combines the 'per_dem' and 'per_gop' \n# by multiplying both values by 100 and round them to the nearest whole number.\ndf['total_per'] = (df['per_dem']*100).round() + (df['per_gop']*100).round()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Add a new column 'votes_total' that is the sum of 'votes_dem', 'votes_gop', and 'votes_ind'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Add a new column 'votes_total' that is the sum of 'votes_dem', 'votes_gop', and 'votes_ind'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Checking to see if there is no 'votes_ind' column\nif 'votes_ind' not in df.columns:\n    df['votes_ind'] = 0\n\n# Creating new column 'votes_total' by summing 'votes_dem','votes_gop' and 'votes_ind'\ndf['votes_total'] = df['votes_dem'] + df['votes_gop'] + df['votes_ind']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column 'votes_total' that is the sum of 'votes_dem', 'votes_gop', and 'votes_ind'\ndf['votes_total'] = df['votes_dem'] + df['votes_gop'] \n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Insert a new column 'Diff_EQ1_EX1' which is the difference between 'EQ1' and 'EX1'.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Insert a new column 'Diff_EQ1_EX1' which is the difference between 'EQ1' and 'EX1'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# calculate the difference between 'EQ1' and 'EX1' and assign to new column 'Diff_EQ1_EX1'\ndf['Diff_EQ1_EX1'] = df['EQ1'] - df['EX1']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Insert a new column 'Diff_EQ1_EX1' which is the difference between 'EQ1' and 'EX1'\ndf['Diff_EQ1_EX1'] = df['EQ1'] - df['EX1']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Create a new column named 'Case Range' that calculates the difference between 'No. of cases_max' and 'No. of cases_min'.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Create a new column named 'Case Range' that calculates the difference between 'No. of cases_max' and 'No. of cases_min'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# add 'Case Range' column\ndf['Case Range'] = df['No. of cases_max'] - df['No. of cases_min']\n\n# output df\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Create a new column 'Case Range' that calculates the difference between 'No. of cases_max' and 'No. of cases_min'\ndata['Case Range'] = data['No. of cases_max'] - data['No. of cases_min']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Add a 'sentiment_score' column calculated as 'pos' minus 'neg'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Add a 'sentiment_score' column calculated as 'pos' minus 'neg'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Add a new column 'sentiment_score' calculated as 'pos' - 'neg'\ndf['sentiment_score'] = df['pos'] - df['neg']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column 'sentiment_score' calculated as 'pos' minus 'neg'\ndf['sentiment_score'] = df['pos'] - df['neg']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Create a new column 'overall_diffsel' which is the result of multiplying 'positive_diffsel' and 'negative_diffsel'.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Create a new column 'overall_diffsel' which is the result of multiplying 'positive_diffsel' and 'negative_diffsel'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Create the 'overall_diffsel' column\ndf['overall_diffsel'] = df['positive_diffsel'] * df['negative_diffsel']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'overall_diffsel' which is the result of multiplying 'positive_diffsel' and 'negative_diffsel'\ndf['overall_diffsel'] = df['positive_diffsel'] * df['negative_diffsel']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Add a new column called GDP where each data value is obtained by multiplying pop and gdppercap.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Add a new column called GDP where each data value is obtained by multiplying pop and gdppercap.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/35.csv')\n\n# Add a new column named 'GDP' by multiplying 'pop' and 'gdppercap'\ndata['GDP'] = data['pop'] * data['gdppercap']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Add a new column called GDP by multiplying pop and gdppercap\ndf['GDP'] = df['pop'] * df['gdppercap']\n\n# Print the dataframe with the new column\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Introduce a new column to the table that displays the percentage change in GDP per capita between the years 1952 and 2007 for every country.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Introduce a new column to the table that displays the percentage change in GDP per capita between the years 1952 and 2007 for every country.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Calculate the percentage change in GDP per capita from 1952 to 2007\ndf['percentage_change'] = ((df['gdpPercap_2007'] - df['gdpPercap_1952']) / df['gdpPercap_1952']) * 100\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the percentage change in GDP per capita between 1952 and 2007\ndf['percentage_change'] = ((df['gdpPercap_2007'] - df['gdpPercap_1952']) / df['gdpPercap_1952']) * 100\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Add a column 'Price Difference' which is calculated as the difference between 'High Price' and 'Low Price'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Add a column 'Price Difference' which is calculated as the difference between 'High Price' and 'Low Price'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the data into a DataFrame\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# calculate the difference between 'High Price' and 'Low Price' and assign it to new column 'Price Difference'\ndf['Price Difference'] = df['High Price'] - df['Low Price']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove leading and trailing whitespaces from 'High Price' and 'Low Price' columns\ndf['High Price'] = df['High Price'].str.strip()\ndf['Low Price'] = df['Low Price'].str.strip()\n\n# Convert 'High Price' and 'Low Price' columns to float type\ndf['High Price'] = df['High Price'].astype(float)\ndf['Low Price'] = df['Low Price'].astype(float)\n\n# Calculate 'Price Difference' and add it as a new column\ndf['Price Difference'] = df['High Price'] - df['Low Price']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Add a new column 'Price Range' calculated as 'High Price' minus 'Low Price'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Add a new column 'Price Range' calculated as 'High Price' minus 'Low Price'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Add a new column 'Price Range'\ndata['Price Range'] = data['High Price'] - data['Low Price']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove leading and trailing whitespaces from 'High Price' and 'Low Price' columns\ndf['High Price'] = df['High Price'].str.strip()\ndf['Low Price'] = df['Low Price'].str.strip()\n\n# Convert 'High Price' and 'Low Price' columns to numeric type\ndf['High Price'] = pd.to_numeric(df['High Price'])\ndf['Low Price'] = pd.to_numeric(df['Low Price'])\n\n# Calculate 'Price Range' and add it as a new column\ndf['Price Range'] = df['High Price'] - df['Low Price']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Insert new column 'BMI' and calculate it as 'Weight' in kg divided by square of 'Height' in m.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Insert new column 'BMI' and calculate it as 'Weight' in kg divided by square of 'Height' in m.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the csv file into pandas DataFrame\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# convert height from cm to m and weight from lb to kg for BMI calculation\ndf['Height'] = df['Height'] / 100\ndf['Weight'] = df['Weight'] * 0.453592\n\n# Calculate BMI as weight(kg) / height(m)^2 and add it as a new column\ndf['BMI'] = df['Weight'] / (df['Height']**2)\n\n# Handle cases where Height or Weight are undefined (-99 or other non-standard values) resulting in incorrect BMI calculation\ndf['BMI'] = df['BMI'].replace([np.inf,-np.inf],np.nan)\ndf['BMI'] = df['BMI'].fillna(-99)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'Height' column from meter to meter (for calculation purpose, assume Height is already in meter)\ndf['Height'] = df['Height']\n\n# Insert new column 'BMI' and calculate it as 'Weight' in kg divided by square of 'Height' in m\ndf['BMI'] = df['Weight'] / (df['Height'] ** 2)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Create a new column 'price_per_sqft' by dividing the 'price' column by the 'sqft_living' column.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Create a new column 'price_per_sqft' by dividing the 'price' column by the 'sqft_living' column.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Compute 'price_per_sqft'\ndf['price_per_sqft'] = df['price'] / df['sqft_living']\n\n# Check the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'price_per_sqft' by dividing the 'price' column by the 'sqft_living' column\ndf['price_per_sqft'] = df['price'] / df['sqft_living']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Create a new column named 'age' in the table, which represents the age of a building by subtracting the value in the 'yr_built' column from the current year, 2022.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Create a new column named 'age' in the table, which represents the age of a building by subtracting the value in the 'yr_built' column from the current year, 2022.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Calculate the age of buildings\ndf['age'] = 2022 - df['yr_built']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'age' representing the age of a building\ndf['age'] = 2022 - df['yr_built']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Insert a new column for 'importance.score' percentage, calculated as (importance.score/sum of all 'importance.score')*100.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Insert a new column for 'importance.score' percentage, calculated as (importance.score/sum of all 'importance.score')*100.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Calculate the sum of all 'importance.score'\ntotal_importance_score = df['importance.score'].sum()\n\n# Create a new column for 'importance.score' percentage\ndf['importance.score.percentage'] = (df['importance.score'] / total_importance_score) * 100\n\n# Print the df to see the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the sum of all 'importance.score'\nimportance_sum = df['importance.score'].sum()\n\n# Calculate the 'importance.score' percentage\ndf['importance.score_percentage'] = (df['importance.score'] / importance_sum) * 100\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Create a new column named \"Range\" in the table, which is the result of subtracting the \"Low\" values from the \"High\" values.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Create a new column named \"Range\" in the table, which is the result of subtracting the \"Low\" values from the \"High\" values.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Calculate the range and add it as a new column to the dataframe\ndf['Range'] = df['High'] - df['Low']\n\n# Output the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Create a new column \"Range\" by subtracting \"Low\" from \"High\"\ndf['Range'] = df['High'] - df['Low']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Create a new column that calculates the percentage change from the \"Open\" value to the \"Close\" value for each row. The formula for calculating the percentage change is ((Close-Open)/Open)*100.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Create a new column that calculates the percentage change from the \"Open\" value to the \"Close\" value for each row. The formula for calculating the percentage change is ((Close-Open)/Open)*100.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/44.csv')\n\n# Compute percentage change and add as a new column\ndata['Percentage_Change'] = ((data['Close'] - data['Open']) / data['Open']) * 100\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the percentage change from the \"Open\" value to the \"Close\" value for each row\ndf['Percentage Change'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Generate a new column by multiplying the 'Open' value with the 'Volume' value for every row in the table.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Generate a new column by multiplying the 'Open' value with the 'Volume' value for every row in the table.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# create a new column that calculates the product of Open and Volume values\ndf['Open_Volume'] = df['Open'] * df['Volume']\n\n# print the first 5 rows of the new dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Multiply the 'Open' value with the 'Volume' value for every row and create a new column\ndata['OpenVolume'] = data['Open'] * data['Volume']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Add a column \"RoomsPerPopulation\" to the table which shows the average number of rooms per person for each population (calculate AveRooms divided by Population).", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Add a column \"RoomsPerPopulation\" to the table which shows the average number of rooms per person for each population (calculate AveRooms divided by Population).", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# calculate the average number of rooms per person for each population\ndf['RoomsPerPopulation'] = df['AveRooms'] / df['Population']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average number of rooms per person\ndf['RoomsPerPopulation'] = df['AveRooms'] / df['Population']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Add a column that calculates the 'Population' divided by 'AveRooms'.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Add a column that calculates the 'Population' divided by 'AveRooms'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/45.csv')\n\n# Add a new column that divides 'Population' by 'AveRooms'\ndata['Population_AveRooms_ratio'] = data['Population'] / data['AveRooms']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Perform the calculation and add the new column\ndf['Population_AveRooms'] = df['Population'] / df['AveRooms']\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Add a new column named \"PopulationDensity\" calculated as the division of Population by AveRooms.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Add a new column named \"PopulationDensity\" calculated as the division of Population by AveRooms.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/45.csv')\n\n# Add a new column \"PopulationDensity\" which is the result of dividing \"Population\" by \"AveRooms\"\ndata['PopulationDensity'] = data['Population'] / data['AveRooms']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Compute PopulationDensity and add it as a new column\ndata['PopulationDensity'] = data['Population'] / data['AveRooms']\n\n# Print the updated DataFrame\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Add a new column indicating the annual difference in 'Value' for each 'SUBJECT' and 'LOCATION'.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Add a new column indicating the annual difference in 'Value' for each 'SUBJECT' and 'LOCATION'.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load data from csv file\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Compute annual difference in 'Value' for each 'SUBJECT' and 'LOCATION'\ndf['Annual Difference'] = df.groupby(['LOCATION','SUBJECT'])['Value'].diff()\n\nprint(df)", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Sorting the data by 'TIME'\ndf = df.sort_values('TIME')\n\n# Transforming 'VALUE' column to numeric\ndf['Value'] = pd.to_numeric(df['Value'],errors='coerce')\n\n# Reseting the index\ndf = df.reset_index(drop=True)\n\n# Adding a new column with the annual difference in 'Value' for each 'SUBJECT' and 'LOCATION'\ndf['annual_difference'] = df.groupby(['LOCATION','SUBJECT','TIME'])['Value'].diff().shift(1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Create a new column named 'Biology_to_Physical_Sciences' that represents the proportion of 'Biology' to 'Physical Sciences' for each row in the table.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Create a new column named 'Biology_to_Physical_Sciences' that represents the proportion of 'Biology' to 'Physical Sciences' for each row in the table.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Add a new column 'Biology_to_Physical Sciences' which is ratio of 'Biology' to 'Physical Sciences' for each row\ndf['Biology_to_Physical Sciences'] = df['Biology'] / df['Physical Sciences']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Biology_to_Physical_Sciences' that represents the proportion of 'Biology' to 'Physical Sciences'\ndf['Biology_to_Physical_Sciences'] = df['Biology'] / df['Physical Sciences']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Calculate the price range for each time interval by subtracting the \"Low\" from the \"High\" and add a new column named \"Price Range\" containing the results.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Calculate the price range for each time interval by subtracting the \"Low\" from the \"High\" and add a new column named \"Price Range\" containing the results.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load data from the csv file.\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Compute Price Range as difference between High and Low for each row.\ndf['Price Range'] = df['High'] - df['Low']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the price range\ndf['Price Range'] = df['High'] - df['Low']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Create a new column called \"Price Change\" by subtracting the \"Close\" price of the previous row from the \"Close\" price of the current row.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Create a new column called \"Price Change\" by subtracting the \"Close\" price of the previous row from the \"Close\" price of the current row.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# create a new column 'Price Change'\ndf['Price Change'] = df['Close'].diff()\n\n# output the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Shift the \"Close\" column to create a lagged version (previous close price)\ndf['Previous Close'] = df['Close'].shift(1)\n\n# Create a new column \"Price Change\" by subtracting the \"Previous Close\" from the \"Close\"\ndf['Price Change'] = df['Close'] - df['Previous Close']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Calculate the total trade amount of each interval by multiplying \"Close\" and \"Volume\", and name this column as \"Trade Amount\".", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Calculate the total trade amount of each interval by multiplying \"Close\" and \"Volume\", and name this column as \"Trade Amount\".", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# calculate the Trade Amount\ndf['Trade Amount'] = df['Close'] * df['Volume']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the total trade amount of each interval\ndf['Trade Amount'] = df['Close'] * df['Volume']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Add a new column to the table which calculates the pressure to humidity ratio.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Add a new column to the table which calculates the pressure to humidity ratio.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load data from csv\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Calculate the pressure to humidity ratio and add it as a new column\ndf['pressure_humidity_ratio'] = df['pressure'] / df['humidity']\n\n# Output the resulting DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the pressure to humidity ratio and add it as a new column\ndf['pressure_to_humidity_ratio'] = df['pressure'] / df['humidity']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Create a new column indicating the product of the 'temp' and 'humidity' values.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Create a new column indicating the product of the 'temp' and 'humidity' values.", "instruction_type": "Insert-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/50.csv')\n\n# Create a new column 'product' which is the product of 'temp' and 'humidity'\ndata['product'] = data['temp'] * data['humidity']\n\n# Print the modified data\nprint(data)", "assistant_code": "# Importing necessary library\nimport pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column which is the product of 'temp' and 'humidity'\ndf['temp_humidity'] = df['temp'] * df['humidity']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Create a new column indicating whether the 'Weight' value is missing or not.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Create a new column indicating whether the 'Weight' value is missing or not.", "instruction_type": "Insert-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Create a new column 'Weight_Missing' indicating if 'Weight' value is missing or not\ndf['Weight_Missing'] = df['Weight'].isnull()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Weight_Missing' that indicates whether the 'Weight' value is missing or not\ndf['Weight_Missing'] = df['Weight'].isna()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Generate a new column to denote whether the 'EVENTMSGACTIONTYPE' represents a missed shot or not.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Generate a new column to denote whether the 'EVENTMSGACTIONTYPE' represents a missed shot or not.", "instruction_type": "Insert-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Create new column\ndf['Missed_Shot'] = df['EVENTMSGACTIONTYPE'].apply(lambda x: True if x != 0 else False)\n\n# Output table\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'IS_MISSED_SHOT' based on 'EVENTMSGACTIONTYPE'\ndf['IS_MISSED_SHOT'] = df['EVENTMSGACTIONTYPE'].apply(lambda x: 1 if 'MISS' in str(x) else 0)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Replace missing 'RELHUM' values with the median of the existing 'RELHUM' values.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Replace missing 'RELHUM' values with the median of the existing 'RELHUM' values.", "instruction_type": "Insert-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Replace missing 'RELHUM' values with the median\nmedian_relhum = data['RELHUM'].median()\ndata['RELHUM'].fillna(median_relhum,inplace=True)\n\nprint(data)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of the 'RELHUM' column\nrelhum_median = df['RELHUM'].median()\n\n# Replace missing 'RELHUM' values with the median\ndf['RELHUM'].fillna(relhum_median, inplace=True)\n\n# Print the resulting DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Create a new column indicating whether the 'gdpPercap_1987' value is a duplicate of any other 'gdpPercap_1987' value in the table.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Create a new column indicating whether the 'gdpPercap_1987' value is a duplicate of any other 'gdpPercap_1987' value in the table.", "instruction_type": "Insert-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/36.csv')\n\n# Create a new column 'is_duplicate' indicating if the 'gdpPercap_1987' value is a duplicate\ndata['is_duplicate'] = data['gdpPercap_1987'].duplicated()\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'is_duplicate_1987' that checks if the 'gdpPercap_1987' value is a duplicate\ndf['is_duplicate_1987'] = df['gdpPercap_1987'].duplicated()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Add a column to convert 'Date' into day of the week.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Add a column to convert 'Date' into day of the week.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Convert 'Date' to datetime\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Extract the day of the week from 'Date' and insert as a new column 'Weekday'\ndata['Weekday'] = data['Date'].dt.day_name()\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'Date' column to datetime and create a new column 'Day_of_Week'\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Day_of_Week'] = df['Date'].dt.day_name()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Add a new column that calculates the length of the 'Trips over the past 24-hours' data.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Add a new column that calculates the length of the 'Trips over the past 24-hours' data.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Add a new column 'Data Length'\ndata['Data Length'] = data['Trips over the past 24-hours (midnight to 11:59pm)'].apply(lambda x: len(str(x)))\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column that calculates the length of the 'Trips over the past 24-hours' data\ndf['Trips_length'] = df['Trips over the past 24-hours (midnight to 11:59pm)'].apply(len)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Insert a column that indicates the length of each country's name.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Insert a column that indicates the length of each country's name.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/3.csv')\n\n# Create new column for length of each country's name\ndata['Country Name Length'] = data['Country'].apply(len)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Add a new column to the dataframe that indicates the length of each country's name\ndf['Country Length'] = df['Country'].apply(lambda x: len(x))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Transform the 'timestamp' column into a DateTime format and separate it into distinct 'Date' and 'Time' columns.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Transform the 'timestamp' column into a DateTime format and separate it into distinct 'Date' and 'Time' columns.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Convert the 'timestamp' column to datetime format\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Split the 'timestamp' into 'Date' and 'Time' columns\ndf['Date'] = df['timestamp'].dt.date\ndf['Time'] = df['timestamp'].dt.time\n\nprint(df)", "assistant_code": "# importing pandas library\nimport pandas as pd\n\n# loading the data from csv\ndf = pd.read_csv('data.csv')\n\n# converting the 'timestamp' column to datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# separating datetime into distinct 'Date' and 'Time' columns\ndf['Date'] = df['timestamp'].dt.date\ndf['Time'] = df['timestamp'].dt.time\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Retrieve the time from the WCTIMESTRING column, and show only the hours for each row.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Retrieve the time from the WCTIMESTRING column, and show only the hours for each row.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Extract the hour from the WCTIMESTRING column\ndf['Hour'] = df['WCTIMESTRING'].apply(lambda x: x.split(':')[0])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Extract the hours from the 'WCTIMESTRING' column\ndf['WCTIMESTRING'] = df['WCTIMESTRING'].str.split(':').str[0]\n\nprint(df['WCTIMESTRING'])"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Based on the shell weight being between 0.1 and 0.2, create a new column named \"Weight Category\" and fill it with the value 'Medium'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Based on the shell weight being between 0.1 and 0.2, create a new column named \"Weight Category\" and fill it with the value 'Medium'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# apply conditions to the data and insert new column \"Weight Category\" \ndf.loc[(df[\"Shell weight\"] > 0.1) & (df[\"Shell weight\"] < 0.2),\"Weight Category\"] = \"Medium\"\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Weight Category' and fill it with 'Medium' where shell weight is between 0.1 and 0.2\ndf['Weight Category'] = df['Shell weight'].apply(lambda x: 'Medium' if 0.1 <= x <= 0.2 else 'Light' if x < 0.1 else 'Heavy')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Insert a column that calculates the total seconds equivalent of the timestamp in hours-minutes-seconds format for more convenient time analysis.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Insert a column that calculates the total seconds equivalent of the timestamp in hours-minutes-seconds format for more convenient time analysis.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# function to convert hour-min-sec format to total seconds\ndef hms_to_seconds(hms):\n    h,m,s = hms.split('-')\n    h = int(h.strip('h'))\n    m = int(m.strip('m'))\n    s = int(s.strip('s'))\n    total_seconds = h*3600 + m*60 + s\n    return total_seconds\n\n# apply the function to the 'timestamp' column and create a new 'total_seconds' column\ndf['total_seconds'] = df['timestamp'].apply(hms_to_seconds)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('data.csv')\n\n# Function to convert h-m-s to total seconds\ndef convert_to_seconds(time_str):\n    h, m, s = time_str.split('-')\n    return int(h) * 3600 + int(m) * 60 + int(s)\n\n# Apply the function to the 'timestamp' column\ndf['timestamp_seconds'] = df['timestamp'].apply(convert_to_seconds)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Create a new column 'Offense_Detail' by combining 'Offense' and 'Offense_Type' values with a hyphen(-).", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Create a new column 'Offense_Detail' by combining 'Offense' and 'Offense_Type' values with a hyphen(-).", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Loading the data\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Adding the new column 'Offense_Detail'\ndf['Offense_Detail'] = df['Offense'] + '-' + df['Offense_Type']\n\nprint(df)", "assistant_code": "# Importing the necessary library\nimport pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Create a new column 'Offense_Detail' by combining 'Offense' and 'Offense_Type' values with a hyphen(-)\ndata['Offense_Detail'] = data['Offense'] + '-' + data['Offense_Type']\n\n# Print the updated dataframe\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Create a new column 'HIRE_YEAR' by extracting the year from 'HIRE_DT'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Create a new column 'HIRE_YEAR' by extracting the year from 'HIRE_DT'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Converting 'HIRE_DT' to datetime format\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'])\n\n# Extracting the year from 'HIRE_DT' and storing it in a new column 'HIRE_YEAR'\ndf['HIRE_YEAR'] = df['HIRE_DT'].dt.year\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'HIRE_DT' to datetime format\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'])\n\n# Extract the year from 'HIRE_DT' and create a new column 'HIRE_YEAR'\ndf['HIRE_YEAR'] = df['HIRE_DT'].dt.year\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Create a new column displaying the length of 'DATE TIME' values.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Create a new column displaying the length of 'DATE TIME' values.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\ndf = pd.read_csv('infiagent/csv/13.csv')\n\ndf['Length of DATE TIME'] = df['DATE TIME'].str.len()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Create a new column with the length of 'DATE TIME' values\ndf['DATE_TIME_LENGTH'] = df['DATE TIME'].apply(lambda x: len(str(x)))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Add a column to convert 'DATE TIME' to epoch format (the number of seconds that have elapsed since January 1, 1970).", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Add a column to convert 'DATE TIME' to epoch format (the number of seconds that have elapsed since January 1, 1970).", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv(\"infiagent/csv/13.csv\")\n\n# Convert 'DATE TIME' to datetime format\ndf['DATE TIME'] = pd.to_datetime(df['DATE TIME'])\n\n# Add a column to convert 'DATE TIME' to epoch format\ndf['DATE TIME EPOCH'] = (df['DATE TIME'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'DATE TIME' to datetime format\ndf['DATE TIME'] = pd.to_datetime(df['DATE TIME'])\n\n# Convert datetime to epoch format\ndf['DATE TIME_EPOCH'] = df['DATE TIME'].apply(lambda x: x.timestamp())\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Add a column that flags a player as 'Veteran' if they have more than 100 number_of_runs_batted_in and 'Rookie' if not.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Add a column that flags a player as 'Veteran' if they have more than 100 number_of_runs_batted_in and 'Rookie' if not.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Add a new column 'Level' that flags a player as 'Veteran' if they have more than 100 number_of_runs_batted_in and 'Rookie' if not\ndf['Level'] = df['number_of_runs_batted_in'].apply(lambda x: 'Veteran' if x > 100 else 'Rookie')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Add the 'Veteran_Rookie' column based on the condition\ndf['Veteran_Rookie'] = df['number_of_runs_batted_in'].apply(lambda x: 'Veteran' if x > 100 else 'Rookie')\n\n# Print the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Add a new column 'github_dept_id' by concatenating the prefix 'G-' with the 'coa_dept_id'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Add a new column 'github_dept_id' by concatenating the prefix 'G-' with the 'coa_dept_id'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Preprocess 'coa_dept_id' column\ndf['coa_dept_id'] = df['coa_dept_id'].astype(str)  \n\n# Add 'github_dept_id' column\ndf['github_dept_id'] = 'G-' + df['coa_dept_id']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Add a new column 'github_dept_id' by concatenating the prefix 'G-' with the 'coa_dept_id'\ndf['github_dept_id'] = 'G-' + df['coa_dept_id'].astype(str)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Insert a new column 'Date Range', which concatenates 'Start Date' and 'End Date' with a delimiter '-'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Insert a new column 'Date Range', which concatenates 'Start Date' and 'End Date' with a delimiter '-'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# insert a new column 'Date Range',which concatenates 'Start Date' and 'End Date' with a delimiter '-'\ndf['Date Range'] = df['Start Date'] + '-' + df['End Date']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Concatenate 'Start Date' and 'End Date' with a delimiter '-'\ndf['Date Range'] = df['Start Date'] + '-'+ df['End Date']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Add a new column which shows the month from the date column.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Add a new column which shows the month from the date column.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the dataframe\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Convert the 'date' column to datetime type\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create a new column 'month' by extracting month from the 'date' column\ndf['month'] = df['date'].dt.month\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Extract the month from the date and add it as a new column\ndf['month'] = df['date'].dt.month\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Convert the 'Student' column into a binary format, with 'Yes' represented as 1 and 'No' as 0, and store the resulting values in a new column called 'Student Binary'.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Convert the 'Student' column into a binary format, with 'Yes' represented as 1 and 'No' as 0, and store the resulting values in a new column called 'Student Binary'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Change the 'Student' column to binary form: 'Yes' equals to 1 and 'No' equals to 0\ndf['Student Binary'] = df['Student'].map({'Yes': 1,'No': 0})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Student' column into binary format\ndf['Student Binary'] = df['Student'].map({'Yes': 1, 'No': 0})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Generate a new column 'SCHOOL_SHORTNAME' by extracting the initial three characters from each school's name.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Generate a new column 'SCHOOL_SHORTNAME' by extracting the initial three characters from each school's name.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Create a new column with the first three letters of each school's name\ndata['SCHOOL_SHORTNAME'] = data['School'].apply(lambda x: x[:3])\n\nprint(data)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Generate a new column 'SCHOOL_SHORTNAME' by extracting the initial three characters from each school's name\ndf['SCHOOL_SHORTNAME'] = df['School'].apply(lambda x: x[:3])\n\n# Output the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Extract the year from the 'Calendar Date (TDB)' and create a new column named 'Year'.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Extract the year from the 'Calendar Date (TDB)' and create a new column named 'Year'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data table\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Create new column 'Year' by extracting year from 'Calendar Date (TDB)' column\ndf['Year'] = df['Calendar Date (TDB)'].str.extract('(\\d{4})',expand=False)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Extract year from 'Calendar Date (TDB)' and create a new column 'Year'\ndf['Year'] = pd.to_datetime(df['Calendar Date (TDB)']).dt.year\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Insert a new column 'word_count' into the table to store the number of words in each 'description'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Insert a new column 'word_count' into the table to store the number of words in each 'description'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Apply a lambda function on the 'description' column to count the number of words\ndf['word_count'] = df['description'].apply(lambda x: len(str(x).split()))\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'word_count' \ndf['word_count'] = df['description'].apply(lambda x: len(str(x).split(\" \")))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Create a new column 'Text_Length' that calculates the character count of the 'text' field.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Create a new column 'Text_Length' that calculates the character count of the 'text' field.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Create a new column 'Text_Length' to calculate the length of the 'text' in characters\ndf['Text_Length'] = df['text'].apply(lambda x: len(x))\n\n# Print the resulting dataframe\nprint(df)", "assistant_code": "# Importing required library\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Text_Length' that calculates the character count of the 'text' field\ndf['Text_Length'] = df['text'].apply(len)\n\n# Print the DataFrame with the newly created column\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Add a new column 'description_length' to store the character counts for each description.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Add a new column 'description_length' to store the character counts for each description.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Create a new column 'description_length' to store the character count for each description\ndf['description_length'] = df['description'].apply(len)\n\n# Print the dataframe to check the new column\nprint(df)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Add a new column 'description_length' to store the character counts for each description\ndf['description_length'] = df['description'].apply(len)\n\n# Print the updated DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Create a new column 'Text_Length' that calculates the character count of the 'text' field.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Create a new column 'Text_Length' that calculates the character count of the 'text' field.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Create a new column 'Text_Length' to calculate the length of the 'text' in characters\ndf['Text_Length'] = df['text'].apply(lambda x: len(x))\n\n# Print the resulting dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Calculate the character count of the 'text' field and create a new column 'Text_Length'\ndata['Text_Length'] = data['text'].apply(len)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Create a new column named 'text_summary' and populate each cell with the first 50 characters of the content in the 'text' column.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Create a new column named 'text_summary' and populate each cell with the first 50 characters of the content in the 'text' column.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv data\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# Add a new column 'text_summary' containing the first 50 characters of the 'text' column content\ndata['text_summary'] = data['text'].str[:50]\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'text_summary' with the first 50 characters of the 'text' column\ndf['text_summary'] = df['text'].str[:50]\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Create a new column 'Title_Uppercase' to store the uppercase version of the 'title'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Create a new column 'Title_Uppercase' to store the uppercase version of the 'title'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# Create a new column 'Title_Uppercase'\ndf['Title_Uppercase'] = df['title'].str.upper()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Title_Uppercase' to store the uppercase version of the 'title'\ndf['Title_Uppercase'] = df['title'].str.upper()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Add a new column that categorizes countries into developed and developing, based on the GDP per capita in 2007 (use $20,000 as the threshold).", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Add a new column that categorizes countries into developed and developing, based on the GDP per capita in 2007 (use $20,000 as the threshold).", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Create a new column for country category\ndf['Country_Category'] = df['gdpPercap_2007'].apply(lambda x: 'Developed' if x >= 20000 else 'Developing')\n\n# Print the DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column based on the GDP per capita in 2007\ndf['developed_country'] = df['gdpPercap_2007'].apply(lambda x: 'Developed' if x >= 20000 else 'Developing')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Create a new column named 'gloss_word_count' that calculates the number of words in the 'gloss' column for each row.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Create a new column named 'gloss_word_count' that calculates the number of words in the 'gloss' column for each row.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Create a new column 'gloss_word_count'\ndf['gloss_word_count'] = df['gloss'].apply(lambda x: len(str(x).split(' ')))\n\n# Output the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'gloss_word_count' by counting the number of words in the 'gloss' column\ndf['gloss_word_count'] = df['gloss'].apply(lambda x: len(str(x).split(' ')))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Add a new column 'average_age' that calculates the average 'Age' of records with the same 'transcript_id'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Add a new column 'average_age' that calculates the average 'Age' of records with the same 'transcript_id'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Calculate average 'Age' for each 'transcript_id' and prepare a DataFrame\naverage_age_df = df.groupby('transcript_id')['Age'].mean().reset_index().rename(columns={'Age':'average_age'})\n\n# Merge the average_age_df DataFrame with the original DataFrame to add the 'average_age' column\ndf = pd.merge(df,average_age_df,on='transcript_id',how='left')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average age of records with the same transcript_id\ndf['average_age'] = df.groupby('transcript_id')['Age'].transform('mean')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Create a new column 'speech_count' which is the count of different 'part_of_speech' in each 'transcript_id'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Create a new column 'speech_count' which is the count of different 'part_of_speech' in each 'transcript_id'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Count the unique 'part_of_speech' for each 'transcript_id' and create the 'speech_count' column\ndf['speech_count'] = df.groupby('transcript_id')['part_of_speech'].transform('nunique')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'speech_count' which is the count of different 'part_of_speech' in each 'transcript_id'\ndf['speech_count'] = df.groupby('transcript_id')['part_of_speech'].transform('nunique')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Generate a new column titled 'Eye-Hair Color Combination' by combining the values of 'Eye color' and 'Hair color'.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Generate a new column titled 'Eye-Hair Color Combination' by combining the values of 'Eye color' and 'Hair color'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Adding a new column \"Eye-Hair Color Combination\"\ndf['Eye-Hair Color Combination'] = df['Eye color'] + \"-\" + df['Hair color']\n\n# Print the dataframe with new column\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Eye-Hair Color Combination' by combining 'Eye color' and 'Hair color'\ndf['Eye-Hair Color Combination'] = df['Eye color'] + '-' + df['Hair color']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Create a new column converting the 'lat' and 'long' values into a single formatted coordinate string.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Create a new column converting the 'lat' and 'long' values into a single formatted coordinate string.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv using pandas\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# create new column for formatted coordinates\ndf['coordinates'] = df.apply(lambda x: f\"({x['lat']},{x['long']})\",axis=1)\n\n# output the results\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column with formatted 'lat' and 'long' values as single coordinate string\ndf['coordinate'] = df.apply(lambda row: str(row.lat) + ', ' + str(row.long), axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Add a new column to indicate if the 'standard_indentification_level_1' value includes the word 'Match'.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Add a new column to indicate if the 'standard_indentification_level_1' value includes the word 'Match'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Create a new column 'contains_match' indicating whether 'standard_indentification_level_1' contains the word 'Match'\ndf['contains_match'] = df['standard_indentification_level_1'].str.contains('Match')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column to indicate if the 'standard_indentification_level_1' value includes the word 'Match'\ndf['includes_match'] = df['standard_indentification_level_1'].apply(lambda x: 'Match' in str(x))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Convert the age column to age_range column such that if age is below 25 it's 'young', between 25-50 it's 'middle-aged', and above 50 it's 'senior'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Convert the age column to age_range column such that if age is below 25 it's 'young', between 25-50 it's 'middle-aged', and above 50 it's 'senior'.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Define function to convert ages to age ranges\ndef age_to_range(age):\n    if age < 25:\n        return 'young'\n    elif 25 <= age <= 50:\n        return 'middle-aged'\n    else:\n        return 'senior'\n\n# Apply the function to the age column and assign the result to a new column named 'age_range'\ndf['age'] = df['age'].apply(age_to_range)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize age\ndef categorize_age(age):\n    if age < 25:\n        return 'young'\n    elif 25 <= age <= 50:\n        return 'middle-aged'\n    else:\n        return 'senior'\n\n# Apply the function to the 'age' column\ndf['age_range'] = df['age'].apply(categorize_age)\n\n# Drop the original 'age' column\ndf = df.drop('age', axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Create a new column named \"Weekday\" by converting the values in the \"Date\" column into weekdays.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Create a new column named \"Weekday\" by converting the values in the \"Date\" column into weekdays.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Convert 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'],format='%d-%b-%y')\n\n# Add a new column 'Weekday',converting 'Date' into weekdays\ndf['Weekday'] = df['Date'].dt.day_name()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# create a new column 'Weekday' by extracting the weekday from the 'Date' column\ndf['Weekday'] = df['Date'].dt.day_name()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Include a new column 'DATE' which is obtained by parsing the 'TIME' column.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Include a new column 'DATE' which is obtained by parsing the 'TIME' column.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from CSV file\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Parse 'TIME' column to datetime and create a new 'DATE' column\ndf['DATE'] = pd.to_datetime(df['TIME'],format='%Y')\n\nprint(df)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'TIME' column to datetime format and assign it to a new column 'DATE'\ndf['DATE'] = pd.to_datetime(df['TIME'], format='Y')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Add a new column 'Total' which is the sum of all the fields excluding 'Year' for each row.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Add a new column 'Total' which is the sum of all the fields excluding 'Year' for each row.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Add a new column 'Total'\ndf['Total'] = df.sum(axis=1) - df['Year']\n\n# Display the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the sum of all the fields excluding 'Year' and create a new column 'Total'\ndf['Total'] = df.loc[:, df.columns != 'Year'].sum(axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Add a new column to indicate if the 'Close' value is an outlier or not, where an outlier is defined as a value that is greater than 2 standard deviations away from the mean.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Add a new column to indicate if the 'Close' value is an outlier or not, where an outlier is defined as a value that is greater than 2 standard deviations away from the mean.", "instruction_type": "Insert-Data Processing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Calculate the mean and standard deviation of the 'Close' column\nmean = df['Close'].mean()\nstd = df['Close'].std()\n\n# Create a new column 'Is_Outlier' and assign 1 if 'Close' value is an outlier,otherwise 0\ndf['Is_Outlier'] = np.where(df['Close'] > (mean + 2*std),1,0)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean and standard deviation of the 'Close' column\nmean_close = df['Close'].mean()\nstd_close = df['Close'].std()\n\n# Define the lower and upper bounds for outliers\nlower_bound = mean_close - 2 * std_close\nupper_bound = mean_close + 2 * std_close\n\n# Create a new column 'Outlier' that indicates whether the 'Close' value is an outlier or not\ndf['Outlier'] = df['Close'].apply(lambda x: True if (x > upper_bound) else False)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Add a new column 'is_high_income' where the value is True if 'income' is ' >50K', False otherwise.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Add a new column 'is_high_income' where the value is True if 'income' is ' >50K', False otherwise.", "instruction_type": "Insert-Data processing", "reference_code": "import pandas as pd\n\n# loading the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# adding a new column 'is_high_income'\ndf['is_high_income'] = df['income'].apply(lambda x: True if x == ' >50K' else False)\n\n# print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Process the 'income' column to convert it to boolean\ndf['income'] = df['income'].apply(lambda x: True if x == '>50K' else False)\n\n# Create the 'is_high_income' column\ndf['is_high_income'] = df['income']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Create a new column and set the default value to 'N/A' for all existing records.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Create a new column and set the default value to 'N/A' for all existing records.", "instruction_type": "Insert-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/13.csv\")\n\n# Add a new column with a default value of 'N/A'\ndf['NEW_COLUMN'] = 'N/A'\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column and set the default value to 'N/A' for all existing records\ndf['NEW_COLUMN'] = 'N/A'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Add a new column 'UNIQUE_ID' ensuring each row has a unique identifier.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Add a new column 'UNIQUE_ID' ensuring each row has a unique identifier.", "instruction_type": "Insert-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Add a 'UNIQUE_ID' column\ndata['UNIQUE_ID'] = range(1,len(data) + 1)\n\nprint(data)", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Loading the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Generating a unique identifier for each row\ndf['UNIQUE_ID'] = df.index\n\n# Printing the dataframe with the new column\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Insert a new row at the start with the values \"11, 0, 0.5, 30, 0, 0.1, 2000, 3, 0, 0, 0\".", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Insert a new row at the start with the values \"11, 0, 0.5, 30, 0, 0.1, 2000, 3, 0, 0, 0\".", "instruction_type": "Insert-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Insert a new row at the beginning of the DataFrame\nnew_row = pd.DataFrame({'': [11],'SeriousDlqin2yrs': [0],'RevolvingUtilizationOfUnsecuredLines': [0.5],'age': [30],'NumberOfTime30-59DaysPastDueNotWorse': [0],'DebtRatio': [0.1],'MonthlyIncome': [2000],'NumberOfOpenCreditLinesAndLoans': [3],'NumberOfTimes90DaysLate': [0],'NumberRealEstateLoansOrLines': [0],'NumberOfTime60-89DaysPastDueNotWorse': [0],'NumberOfDependents': [0]})\ndf = pd.concat([new_row,df],ignore_index=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Insert the new row at the start\ndf.loc = df.loc[0:0] = [11, 0, 0.5, 30, 0, 0.1, 2000, 3, 0, 0, 0]\n\n# Output the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Insert a new record to the table but if a duplicate 'site' value exists, ignore the insert.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Insert a new record to the table but if a duplicate 'site' value exists, ignore the insert.", "instruction_type": "Insert-Format and Content", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# define new record\nnew_record = {'site': '(HA2)121','abs_diffsel': 10.5,'positive_diffsel': 5.5,'negative_diffsel': -5,'max_diffsel': 2,'min_diffsel': -1}\n\n# check if 'site' value of the new record exists in the table\nif new_record['site'] not in df['site'].values:\n    # if not exists,append the new record\n    df = df.append(new_record,ignore_index=True)\n    \nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# New record to be inserted\nnew_record = {'site': '326', 'abs_diffsel': 9.002764774505879, 'positive_diffsel': 3.6156009948803702, 'negative_diffsel': -5.387163779625509, 'max_diffsel': 0.7169223975734413, 'min_diffsel': -1.2184218611180495}\n\n# Check if 'site' value in new record already exists in df\nrecord_exists = df['site'].eq(new_record['site']).any()\n\n# If 'site' value does not exist, append new record to df\nif not record_exists:\n    df = df.append(new_record, ignore_index=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Create a new column in the table to assign grades to countries based on their 'Happiness Score'. The grading scale will be A, B, C, D, and F.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Create a new column in the table to assign grades to countries based on their 'Happiness Score'. The grading scale will be A, B, C, D, and F.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/3.csv')\n\n# Define the function to grade the score \ndef grade(score):\n    if score >= 7:\n        return 'A'\n    elif score >= 6:\n        return 'B'\n    elif score >= 5:\n        return 'C'\n    elif score >= 4:\n        return 'D'\n    else:\n        return 'F'\n\n# Apply the function to the \"Happiness Score\" and add a new column \"Grade\" to the data.\ndata['Grade'] = data['Happiness Score'].apply(grade)\n\n#Print the modified data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Define a function to assign grades based on happiness score\ndef assign_grade(score):\n    if score >= 7.9:\n        return 'A'\n    elif score >= 7.5:\n        return 'B'\n    elif score >= 7:\n        return 'C'\n    elif score >= 6.5:\n        return 'D'\n    else:\n        return 'F'\n\n# Apply the function to the 'Happiness Score' column to create a new column 'Grade'\ndf['Grade'] = df['Happiness Score'].apply(assign_grade)\n\n# Print the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Add a new column 'Is_Directed' to the table, where it will display 'Yes' if the 'Type' column is 'Directed', and 'No' if it's not.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Add a new column 'Is_Directed' to the table, where it will display 'Yes' if the 'Type' column is 'Directed', and 'No' if it's not.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Add a new column 'Is_Directed' based on the condition in column 'Type'\ndf['Is_Directed'] = df['Type'].apply(lambda x: 'Yes' if x == 'Directed' else 'No')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'Is_Directed'\ndf['Is_Directed'] = df['Type'].apply(lambda x: 'Yes' if x == 'Directed' else 'No')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Create a new column categorizing 'Weight' into different weight classes (e.g., light, medium, heavy).", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Create a new column categorizing 'Weight' into different weight classes (e.g., light, medium, heavy).", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv('infiagent/csv/5.csv')\n\n# Define the weight classes\ndef weight_class(weight):\n    if weight < 10:\n        return 'light'\n    elif weight < 100:\n        return 'medium'\n    else:\n        return 'heavy'\n\n# Add a new column 'Weight Class' based on the 'Weight' column\ndata['Weight Class'] = data['Weight'].apply(weight_class)\n\n# Print the updated dataframe\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize weight\ndef categorize_weight(weight):\n    if weight < 50:\n        return 'light'\n    elif weight < 70:\n        return 'medium'\n    else:\n        return 'heavy'\n\n# Apply the function to the 'WEIGHT' column to create the new column 'Weight_class'\ndf['Weight_class'] = df['WEIGHT'].apply(categorize_weight)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Add a new column 'Age Category' to the table and categorize the values in the 'Rings' column as 'Young' if they are less than 10, 'Adult' if they are between 10 and 20, and 'Old' if they are more than 20.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Add a new column 'Age Category' to the table and categorize the values in the 'Rings' column as 'Young' if they are less than 10, 'Adult' if they are between 10 and 20, and 'Old' if they are more than 20.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Define a function to categorize based on the values\ndef categorize_age(rings):\n    if rings < 10:\n        return 'Young'\n    elif 10 <= rings <= 20:\n        return 'Adult'\n    else:\n        return 'Old'\n\n# Use apply function to categorize 'Rings' and create a new column 'Age Category'\ndata['Age Category'] = data['Rings'].apply(categorize_age)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize the 'Rings' column\ndef categorize_age(rings):\n    if rings < 10:\n        return 'Young'\n    elif 10 <= rings <= 20:\n        return 'Adult'\n    else:\n        return 'Old'\n\n# Apply the function to the 'Rings' column and create a new column 'Age Category'\ndf['Age Category'] = df['Rings'].apply(categorize_age)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Create a new column to classify cars according to their 'weight' ('light' for weights under 3000, 'medium' for weights between 3000 and 4000, and 'heavy' for weights over 4000).", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Create a new column to classify cars according to their 'weight' ('light' for weights under 3000, 'medium' for weights between 3000 and 4000, and 'heavy' for weights over 4000).", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/11.csv')\n\n# Define function to categorize weight\ndef categorize_weight(weight):\n    if weight < 3000:\n        return 'light'\n    elif 3000 <= weight <= 4000:\n        return 'medium'\n    else:\n        return 'heavy'\n\n# Apply function to weight column and create new column\ndata['weight_category'] = data['weight'].apply(categorize_weight)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Define a function to classify the weight of the cars\ndef classify_weight(weight):\n    if weight < 3000:\n        return 'light'\n    elif 3000 <= weight < 4000:\n        return 'medium'\n    else:\n        return 'heavy'\n\n# Apply the function to the 'weight' column to create the new column\ndf['weight_class'] = df['weight'].apply(classify_weight)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Create a new column that will receive the value \"yes\" if the 'number_of_runs' exceeds 50, and \"no\" otherwise.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Create a new column that will receive the value \"yes\" if the 'number_of_runs' exceeds 50, and \"no\" otherwise.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load data from csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Create new column based on condition\ndf['new_column'] = ['yes' if x > 50 else 'no' for x in df['number_of_runs']]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column based on the condition\ndf['exceeds_50_runs'] = df['number_of_runs'].apply(lambda x: 'yes' if x > 50 else 'no')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Add a column 'income group' to the table, wherein those with 'wage' greater than 7 are classified as 'high', those with 'wage' between 5 and 7 are 'middle', and those with 'wage' less than 5 are 'low'.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Add a column 'income group' to the table, wherein those with 'wage' greater than 7 are classified as 'high', those with 'wage' between 5 and 7 are 'middle', and those with 'wage' less than 5 are 'low'.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# create a new column 'income group' based on the 'wage' column\ndf['income group'] = pd.cut(df['wage'],bins=[0,5,7,df['wage'].max()],labels=['low','middle','high'])\n\n# print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'income group' based on the 'wage' column\ndf['income group'] = pd.cut(df['wage'], bins=[0, 5, 7, float('inf')], labels=['low', 'middle', 'high'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Create a new column 'avgwage_community' in the table, which represents the average wage of individuals residing in the same city (identified by 'bigcity' and 'smllcity' columns).", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Create a new column 'avgwage_community' in the table, which represents the average wage of individuals residing in the same city (identified by 'bigcity' and 'smllcity' columns).", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/15.csv')\n\n# Group by 'bigcity' and 'smllcity' and calculate the mean wage\nmean_wage = data.groupby(['bigcity','smllcity'])['wage'].mean()\n\n# Map the average wage to a new column in the original dataset\ndata['avgwage_community'] = data.set_index(['bigcity','smllcity']).index.map(mean_wage.to_dict())\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'avgwage_community' that represents the average wage of individuals residing in the same city\ndf['avgwage_community'] = df.groupby(['bigcity', 'smllcity'])['wage'].transform('mean')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Add a new column that shows whether the 'Open' value surpasses a specific threshold, such as 120, by assigning a value of 1 if true and 0 if false.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Add a new column that shows whether the 'Open' value surpasses a specific threshold, such as 120, by assigning a value of 1 if true and 0 if false.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/17.csv')\n\n# Create a new column indicating if the 'Open' value is above 120\ndata['Above_Threshold'] = data['Open'] > 120\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add a new column based on the condition\ndf['Open_surpasses_threshold'] = df['Open'].apply(lambda x: 1 if x > 120 else 0)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Create a new column that divides the values in the 'Open' column into different categories, such as 'Low', 'Medium', and 'High'.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Create a new column that divides the values in the 'Open' column into different categories, such as 'Low', 'Medium', and 'High'.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Create a new column categorizing the 'Open' values into different groups\ndf['Group'] = pd.cut(df['Open'],bins=[0,100,120,float('inf')],labels=['Low','Medium','High'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize the values in the 'Open' column\ndef categorize_open(value):\n    if value < df['Low'].quantile(0.25):\n        return 'Low'\n    elif value < df['Open'].quantile(0.75):\n        return 'Medium'\n    else:\n        return 'High'\n\n# Apply the function to the 'Open' column and create a new column\ndf['Open_Category'] = df['Open'].apply(categorize_open)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Introduce a new column 'education_level' in the table, which classifies individuals into 'Low' if 'education-num' is 5 or less, 'Medium' if it is greater than 5 and less than or equal to 9, and 'High' if it is greater than 9.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Introduce a new column 'education_level' in the table, which classifies individuals into 'Low' if 'education-num' is 5 or less, 'Medium' if it is greater than 5 and less than or equal to 9, and 'High' if it is greater than 9.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Define a function to categorize education level\ndef categorize_education(num):\n    if num <= 5:\n        return 'Low'\n    elif num > 5 and num <= 9:\n        return 'Medium'\n    else:\n        return 'High'\n\n# Apply the function to 'education-num' column and create a new column 'education_level'\ndf['education_level'] = df['education-num'].apply(categorize_education)\n\n# Output the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Define a function to classify education level\ndef classify_education(education_num):\n    if education_num <= 5:\n        return 'Low'\n    elif 5 < education_num <= 9:\n        return 'Medium'\n    else:\n        return 'High'\n\n# Apply the function to the 'education-num' column to create the new 'education_level' column\ndf['education_level'] = df['education-num'].apply(classify_education)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: In the table, add a new column that shows whether the 'hour-per-week' value is less than the average value or not.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "In the table, add a new column that shows whether the 'hour-per-week' value is less than the average value or not.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Calculate the average of 'hour-per-week' column\naverage_hour = data['hour-per-week'].mean()\n\n# Create a new column 'below_average_hours' indicating whether the 'hour-per-week' value is below the average or not\ndata['below_average_hours'] = data['hour-per-week'] < average_hour\n\n# Print the updated data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'hour-per-week' column\naverage_hour = df['hour-per-week'].mean()\n\n# Create a new column 'is_less_than_average' that shows whether the 'hour-per-week' value is less than the average or not\ndf['is_less_than_average'] = df['hour-per-week'] < average_hour\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Create a 'working_hours_category' column that categorizes individuals as 'Part-time' if they work less than 30 hours per week, 'Full-time' if they work between 30-40 hours, and 'Over-time' if they work more than 40 hours.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Create a 'working_hours_category' column that categorizes individuals as 'Part-time' if they work less than 30 hours per week, 'Full-time' if they work between 30-40 hours, and 'Over-time' if they work more than 40 hours.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Add 'working_hours_category' column \ndata['working_hours_category'] = pd.cut(data['hour-per-week'],bins=[0,29,40,1000],labels=['Part-time','Full-time','Over-time'])\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create 'working_hours_category' column based on 'hour-per-week' column\ndf['working_hours_category'] = pd.cut(df['hour-per-week'], \n                                      bins=[0, 30, 40, float('inf')], \n                                      labels=['Part-time', 'Full-time', 'Over-time'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Introduce a new column 'department_size' that categorizes departments as 'Small', 'Medium', or 'Large' in accordance with their 'coa_dept_id' values.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Introduce a new column 'department_size' that categorizes departments as 'Small', 'Medium', or 'Large' in accordance with their 'coa_dept_id' values.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Define a function to classify 'department_size' based on the 'coa_dept_id'\ndef classify_size(id):\n    if id <= 30:\n        return 'Small'\n    elif id > 30 and id <= 60:\n        return 'Medium'\n    else:\n        return 'Large'\n\n# Use the function to create a new column 'department_size'\ndf['department_size'] = df['coa_dept_id'].apply(classify_size)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize department size\ndef categorize_size(size):\n    if size < 100:\n        return 'Small'\n    elif size < 1000:\n        return 'Medium'\n    else:\n        return 'Large'\n\n# Apply the function to the 'coa_dept_id' column to create the 'department_size' column\ndf['department_size'] = df['coa_dept_id'].apply(categorize_size)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Create a new column named 'Hospital Rating' based on the 'Excess Readmission Ratio' of each hospital. If the 'Excess Readmission Ratio' is less than 1.5, assign a 'Good' rating; if it's between 1.5 and 1.6, assign an 'Average' rating; and if it's greater than 1.6, assign a 'Poor' rating.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Create a new column named 'Hospital Rating' based on the 'Excess Readmission Ratio' of each hospital. If the 'Excess Readmission Ratio' is less than 1.5, assign a 'Good' rating; if it's between 1.5 and 1.6, assign an 'Average' rating; and if it's greater than 1.6, assign a 'Poor' rating.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Add a new column named 'Hospital Rating' and assign rating based on 'Excess Readmission Ratio'\ndf['Hospital Rating'] = pd.cut(df['Excess Readmission Ratio'],bins=[0,1.5,1.6,float('inf')],labels=['Good','Average','Poor'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define a function to assign the ratings\ndef assign_rating(excess_readmission_ratio):\n    if excess_readmission_ratio < 1.5:\n        return 'Good'\n    elif 1.5 <= excess_readmission_ratio <= 1.6:\n        return 'Average'\n    else:\n        return 'Poor'\n\n# Apply the function to the 'Excess Readmission Ratio' column to create the new 'Hospital Rating' column\ndf['Hospital Rating'] = df['Excess Readmission Ratio'].apply(assign_rating)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Introduce a new column to denote whether the value of 'deaths' is missing or not.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Introduce a new column to denote whether the value of 'deaths' is missing or not.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/21.csv')\n\n# Create a new column indicating whether the 'deaths' value is missing or not\ndata['deaths_missing'] = data['deaths'].isnull()\n\n# Output the results\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# handle the non-standard data in 'deaths' column\ndf['deaths'] = df['deaths'].replace('', np.nan)\n\n# create a new column 'deaths_missing' to denote whether the 'deaths' value is missing or not\ndf['deaths_missing'] = df['deaths'].isnull()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Insert a new column named 'income_level', classifying 'monthly_income' with criteria: 'low' if less than 3000, 'medium' from 3000 to 10000, and 'high' if more than 10000.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Insert a new column named 'income_level', classifying 'monthly_income' with criteria: 'low' if less than 3000, 'medium' from 3000 to 10000, and 'high' if more than 10000.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# load data from csv file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# apply the income level calculation\ndf['income_level'] = pd.cut(df['monthly_income'],bins=[-1,3000,10000,float('inf')],labels=['low','medium','high'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize income\ndef categorize_income(income):\n    if income < 3000:\n        return 'low'\n    elif 3000 <= income <= 10000:\n        return 'medium'\n    else:\n        return 'high'\n\n# Apply the function to the 'monthly_income' column and create a new column 'income_level'\ndf['income_level'] = df['monthly_income'].apply(categorize_income)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Create a new column in the table named 'is_high_risk' that indicates whether 'serious_dlqin2yrs' is equal to 1 and 'number_of_time30-59_days_past_due_not_worse' is greater than or equal to 3.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Create a new column in the table named 'is_high_risk' that indicates whether 'serious_dlqin2yrs' is equal to 1 and 'number_of_time30-59_days_past_due_not_worse' is greater than or equal to 3.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# generate a new column 'is_high_risk'\n# where 'serious_dlqin2yrs' is 1 and 'number_of_time30-59_days_past_due_not_worse' is >= 3\ndf['is_high_risk'] = (df['serious_dlqin2yrs'] == 1) & (df['number_of_time30-59_days_past_due_not_worse'] >= 3)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'is_high_risk' based on the conditions\ndf['is_high_risk'] = df.apply(lambda row: 1 if (row['serious_dlqin2yrs'] == 1 and row['number_of_time30-59_days_past_due_not_worse'] >= 3) else 0, axis=1)\n\n# Print the resulting DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Group the data by 'Married' and 'Student' status and add a new column depicting the count of each group.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Group the data by 'Married' and 'Student' status and add a new column depicting the count of each group.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Group by 'Married' and 'Student' status and count\ngrouped_df = df.groupby(['Married','Student']).size().reset_index(name='Count')\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'Married' and 'Student' status\ngrouped_df = df.groupby(['Married', 'Student']).size().reset_index(name='Count')\n\n# Print the result\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Create a new column 'IncomeBracket' by categorizing the 'MonthlyIncome' into four brackets: 'Low', 'Average', 'High' and 'Very High' based on quartiles.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Create a new column 'IncomeBracket' by categorizing the 'MonthlyIncome' into four brackets: 'Low', 'Average', 'High' and 'Very High' based on quartiles.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Handle non-standard data\ndf['MonthlyIncome'] = df['MonthlyIncome'].replace('NA',np.nan)\ndf['MonthlyIncome'] = df['MonthlyIncome'].astype(float)\n\n# Create Income Bracket column based on the quartiles of MonthlyIncome\ndf['IncomeBracket'] = pd.qcut(df['MonthlyIncome'],q=4,labels=['Low','Average','High','Very High'])\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Replace 'NA' with np.nan\ndf.replace('NA', np.nan, inplace=True)\n\n# Convert 'MonthlyIncome' to numeric\ndf['MonthlyIncome'] = pd.to_numeric(df['MonthlyIncome'], errors='coerce')\n\n# Calculate the quartiles\nq1 = df['MonthlyIncome'].quantile(0.25)\nq2 = df['MonthlyIncome'].quantile(0.5)\nq3 = df['MonthlyIncome'].quantile(0.75)\n\n# Define the brackets\nbrackets = [(df['MonthlyIncome'] <= q1), (df['MonthlyIncome'] > q1) & (df['MonthlyIncome'] <= q2), \n           (df['MonthlyIncome'] > q2) & (df['MonthlyIncome'] <= q3), df['MonthlyIncome'] > q3)]\n\n# Define the income bracket categories\nincome_bracket_categories = ['Low', 'Average', 'High', 'Very High']\n\n# Create the new 'IncomeBracket' column\ndf['IncomeBracket'] = np.select(brackets, income_bracket_categories, default='NA')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Categorize the 'age' column into 'Young' (18-30), 'Adult' (31-60), and 'Senior' (60+) by adding a new column 'AgeGroup'.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Categorize the 'age' column into 'Young' (18-30), 'Adult' (31-60), and 'Senior' (60+) by adding a new column 'AgeGroup'.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Define a function to categorize the age\ndef age_categorizer(age):\n    if 18 <= age <= 30:\n        return 'Young'\n    elif 31 <= age <= 60:\n        return 'Adult'\n    else:\n        return 'Senior'\n\n# Apply the function to the 'age' column\ndata['AgeGroup'] = data['age'].apply(age_categorizer)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize age\ndef categorize_age(age):\n    if age <= 30:\n        return 'Young'\n    elif 30 < age <= 60:\n        return 'Adult'\n    else:\n        return 'Senior'\n\n# Apply the function to the 'age' column to create the 'AgeGroup' column\ndf['AgeGroup'] = df['age'].apply(categorize_age)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Insert a new column 'Extremely High Volume' that contains the value 'Yes' if 'Volume' is more than 100000000 and 'No' otherwise.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Insert a new column 'Extremely High Volume' that contains the value 'Yes' if 'Volume' is more than 100000000 and 'No' otherwise.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the dataframe\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Add a new column 'Extremely High Volume' based on the 'Volume'\ndf['Extremely High Volume'] = df['Volume'].apply(lambda x: 'Yes' if x > 100000000 else 'No')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Insert new column 'Extremely High Volume'\ndf['Extremely High Volume'] = df['Volume'].apply(lambda x: 'Yes' if x > 100000000 else 'No')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Insert a new column that classifies properties into categories according to their year built (yr_built): 'Vintage' for properties built before 1900, 'Old' for properties built between 1900 and 1970, and 'Modern' for properties built after 1970.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Insert a new column that classifies properties into categories according to their year built (yr_built): 'Vintage' for properties built before 1900, 'Old' for properties built between 1900 and 1970, and 'Modern' for properties built after 1970.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Define a function to classify the properties\ndef classify_property(year):\n    if year < 1900:\n        return 'Vintage'\n    elif 1900 <= year <= 1970:\n        return 'Old'\n    else:\n        return 'Modern'\n\n# Apply the function to the 'yr_built' column and create a new column\ndf['property_class'] = df['yr_built'].apply(classify_property)\n\n# Print the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize year built\ndef categorize_year_built(year):\n    if year < 1900:\n        return 'Vintage'\n    elif 1900 <= year <= 1970:\n        return 'Old'\n    else:\n        return 'Modern'\n\n# Apply the function to the 'yr_built' column\ndf['year_built_category'] = df['yr_built'].apply(categorize_year_built)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Add a column with group IDs, grouping records based on 'LibraryID'. If 'LibraryID' is not provided, assign to a default group.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Add a column with group IDs, grouping records based on 'LibraryID'. If 'LibraryID' is not provided, assign to a default group.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# checking if 'LibraryID' column has any missing values\ndf['LibraryID'].isna().sum()\n\n# creating a new column 'GroupID'\ndf['GroupID'] = df['LibraryID']\n\n# filling the missing values in 'GroupID' column with 'default group'\ndf['GroupID'].fillna('default group',inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill NaN values in 'LibraryID' column with a placeholder\ndf['LibraryID'].fillna('Default Group', inplace=True)\n\n# Add a column with group IDs, grouping by 'LibraryID'\ndf['GroupID'] = df['LibraryID']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Add a new column 'bmi_category' to the table such that if bmi is less than 18.5 it is 'Underweight', between 18.5 & 24.9 it is 'Normal', between 25 & 29.9 it is 'Overweight' and 30 or more it is 'Obese'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Add a new column 'bmi_category' to the table such that if bmi is less than 18.5 it is 'Underweight', between 18.5 & 24.9 it is 'Normal', between 25 & 29.9 it is 'Overweight' and 30 or more it is 'Obese'.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Create a new column 'bmi_category' based on the 'bmi' column values\ndf['bmi_category'] = pd.cut(df['bmi'],bins=[0,18.5,24.9,29.9,float('inf')],labels=['Underweight','Normal','Overweight','Obese'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Define a function to categorize BMI\ndef categorize_bmi(bmi):\n    if bmi < 18.5:\n        return 'Underweight'\n    elif 18.5 <= bmi <= 24.9:\n        return 'Normal'\n    elif 25 <= bmi <= 29.9:\n        return 'Overweight'\n    else:\n        return 'Obese'\n\n# Apply the function to the 'bmi' column to create the 'bmi_category' column\ndf['bmi_category'] = df['bmi'].apply(categorize_bmi)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Create a new class, 'class10', that encompasses 'class1' and 'class2', and assign a value of 1 when both are identical and 0 otherwise.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Create a new class, 'class10', that encompasses 'class1' and 'class2', and assign a value of 1 when both are identical and 0 otherwise.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Create the new column 'class10' \ndf['class10'] = df.apply(lambda row: 1 if row['class1'] == row['class2'] else 0,axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'class10' that encompasses 'class1' and 'class2'\ndf['class10'] = df.apply(lambda row: 1 if row['class1'] == row['class2'] else 0, axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Create a new column indicating whether the 'wind_speed' value is above the average wind speed.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Create a new column indicating whether the 'wind_speed' value is above the average wind speed.", "instruction_type": "Insert-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/50.csv')\n\n# Calculate the average wind speed\naverage_wind_speed = data['wind_speed'].mean()\n\n# Create a new column 'above_average_wind_speed' to indicate whether the 'wind_speed' value is above the average wind speed\ndata['above_average_wind_speed'] = data['wind_speed'] > average_wind_speed\n\n# Print the updated table\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average wind speed\naverage_wind_speed = df['wind_speed'].mean()\n\n# Create a new column indicating whether the 'wind_speed' value is above the average\ndf['above_average_wind'] = df['wind_speed'] > average_wind_speed\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Add a column displaying the final word of each 'gloss'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Add a column displaying the final word of each 'gloss'.", "instruction_type": "Insert-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Identify the last word in each 'gloss'\ndf['last_word'] = df['gloss'].str.split().str[-1]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Add a column displaying the final word of each 'gloss'\ndf['final_word'] = df['gloss'].apply(lambda x: x.split(' ')[-1])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Add a column to capture the hotel's location category based on city name: 'Major' if the city is 'New York City'; otherwise, 'Other'.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Add a column to capture the hotel's location category based on city name: 'Major' if the city is 'New York City'; otherwise, 'Other'.", "instruction_type": "Insert-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Define a function to categorize the location\ndef categorize_location(city):\n    if city == 'New York City':\n        return 'Major'\n    else:\n        return 'Other'\n\n# Apply the function to the 'city_name' column to create the 'location_category' column\ndf['location_category'] = df['city_name'].apply(categorize_location)\n\n# Print the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Add the 'location_category' column\ndf['location_category'] = df['city_name'].apply(lambda x: 'Major' if x == 'New York City' else 'Other')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Add two new columns with the average of 'High' and 'Low' for the date 'Sep 15, 2017' derived from a subquery.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Add two new columns with the average of 'High' and 'Low' for the date 'Sep 15, 2017' derived from a subquery.", "instruction_type": "Insert-Simple Selection", "reference_code": "import pandas as pd\n\n# load the data from csv\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove \",\" from Volume and Market Cap and convert to int\ndf['Volume'] = df['Volume'].str.replace(',','').astype(int)\ndf['Market Cap'] = df['Market Cap'].str.replace(',','').astype(int)\n\n# convert Date to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Subquery to calculate the average 'High' and 'Low' for the date 'Sep 15,2017'\naverage_high_low_15_sep = df[df['Date'] == '2017-09-15'][['High','Low']].mean()\n\n# Assign new columns with values of subquery\ndf['Average High for Sep 15,2017'] = average_high_low_15_sep['High']\ndf['Average Low for Sep 15,2017'] = average_high_low_15_sep['Low']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove commas from the 'Date' column and convert it to datetime type\ndata['Date'] = pd.to_datetime(data['Date'].str.replace(',', ''))\n\n# Filter the data for 'Sep 15, 2017'\nsubquery_data = data[data['Date'] == '2017-09-15']\n\n# Calculate the average of 'High' and 'Low' for 'Sep 15, 2017'\navg_high = subquery_data['High'].mean()\navg_low = subquery_data['Low'].mean()\n\n# Add two new columns 'Avg_High' and 'Avg_Low' to the original dataframe\ndata['Avg_High'] = avg_high\ndata['Avg_Low'] = avg_low\n\n# Output the result\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Insert a new row with data from the row with the maximum 'importance.score' from a subset of data where 'row retention time' is less than the average 'row retention time'.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Insert a new row with data from the row with the maximum 'importance.score' from a subset of data where 'row retention time' is less than the average 'row retention time'.", "instruction_type": "Insert-Subquery", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# calculate the average 'row retention time'\naverage_retention = df['row retention time'].mean()\n\n# Filter the rows where 'row retention time' is less than the average 'row retention time'\nsubset_df = df[df['row retention time'] < average_retention]\n\n# Get row with maximum 'importance.score' in subset_df\nmax_score_row = subset_df[subset_df['importance.score'] == subset_df['importance.score'].max()]\n\n# Insert the new row to the original dataframe\ndf = pd.concat([df,max_score_row])\n\n# assume that the indexes need to be reset\ndf.reset_index(drop=True,inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'row retention time'\naverage_retention_time = df['row retention time'].mean()\n\n# Create a subset dataframe where 'row retention time' is less than the average\nsubset_df = df[df['row retention time'] < average_retention_time]\n\n# Find the row with the maximum 'importance.score' in the subset dataframe\nmax_score_row = subset_df[subset_df['importance.score'] == subset_df['importance.score'].max()]\n\n# Insert the row with the maximum 'importance.score' from the subset dataframe into the original dataframe\ndf = pd.concat([df, max_score_row])\n\nprint(df)"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nact_arr_time,start_time,origin_station_code,fleet_number,route_code,sch_dep_time,origin_station\n,21/09/15 05:01:00,MAM,3088.0,4,21/09/15 05:01:00,Machine Mist\n,21/09/15 07:20:00,ATP,3088.0,1,21/09/15 07:20:00,Attempt Pin\n,21/09/15 05:06:30,STSTb,3063.0,73,21/09/15 05:06:30,Step Scarecrow Turnback\n,21/09/15 05:49:00,JAV,3063.0,20,21/09/15 05:49:00,Jail Vest\n,21/09/15 06:38:00,CHATb,3063.0,74,21/09/15 06:38:00,Children Cast Turnback\n,21/09/15 06:32:00,BRB,3006.0,10,21/09/15 06:32:00,Bridge Bottle\n,21/09/15 07:32:00,CRT,3017.0,58,21/09/15 07:32:00,Crib Team\n,21/09/15 23:58:00,CLG,,83,21/09/15 23:58:00,Clouds Goose\n,22/09/15 00:20:00,SKH,3099.0,91,22/09/15 00:20:00,Skin Shape\n\nHeader and first few lines of CSV file 2:\nstation,station_type,end_time,act_dep_time,station_code,platform,origin_station_code,sch_arr_time\nMachine Mist,Passenger,21/09/15 05:41:00,21/09/15 04:59:19,MAM,Outbound,MAM,\nRoll Test,Passenger,21/09/15 07:14:00,21/09/15 05:50:38,ROT,Platform A,ROT,\nAttempt Pin,Passenger,21/09/15 08:42:00,21/09/15 07:21:38,ATP,Platform A,ATP,\nStep Scarecrow Turnback,Vehicle,21/09/15 05:44:00,21/09/15 05:00:01,STSTb,Platform 3,STSTb,\nChildren Cast Turnback,Vehicle,21/09/15 07:20:00,21/09/15 06:36:58,CHATb,Platform 1,CHATb,\nBridge Bottle,Passenger,21/09/15 07:27:00,21/09/15 06:40:32,BRB,Platform A,BRB,\nCrib Team,Passenger,21/09/15 08:27:00,21/09/15 07:38:27,CRT,Platform B,CRT,\nClouds Goose,Passenger,22/09/15 00:05:30,,CLG,Outbound,CLG,\nSkin Shape,Passenger,22/09/15 00:46:00,22/09/15 00:21:21,SKH,Inbound Siding,SKH,\n\nQuestion: Merge all rows in the two tables that the length of 'sch_dep_time' is less than 17 and the length of 'station' is less than 12, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "act_arr_time,start_time,origin_station_code,fleet_number,route_code,sch_dep_time,origin_station\n,21/09/15 05:01:00,MAM,3088.0,4,21/09/15 05:01:00,Machine Mist\n,21/09/15 07:20:00,ATP,3088.0,1,21/09/15 07:20:00,Attempt Pin\n,21/09/15 05:06:30,STSTb,3063.0,73,21/09/15 05:06:30,Step Scarecrow Turnback\n,21/09/15 05:49:00,JAV,3063.0,20,21/09/15 05:49:00,Jail Vest\n,21/09/15 06:38:00,CHATb,3063.0,74,21/09/15 06:38:00,Children Cast Turnback\n,21/09/15 06:32:00,BRB,3006.0,10,21/09/15 06:32:00,Bridge Bottle\n,21/09/15 07:32:00,CRT,3017.0,58,21/09/15 07:32:00,Crib Team\n,21/09/15 23:58:00,CLG,,83,21/09/15 23:58:00,Clouds Goose\n,22/09/15 00:20:00,SKH,3099.0,91,22/09/15 00:20:00,Skin Shape\n,21/09/15 05:49:00,SKSTb,3052.0,7,21/09/15 05:49:00,Skate Stone Turnback\n", "csv2_example": "station,station_type,end_time,act_dep_time,station_code,platform,origin_station_code,sch_arr_time\nMachine Mist,Passenger,21/09/15 05:41:00,21/09/15 04:59:19,MAM,Outbound,MAM,\nRoll Test,Passenger,21/09/15 07:14:00,21/09/15 05:50:38,ROT,Platform A,ROT,\nAttempt Pin,Passenger,21/09/15 08:42:00,21/09/15 07:21:38,ATP,Platform A,ATP,\nStep Scarecrow Turnback,Vehicle,21/09/15 05:44:00,21/09/15 05:00:01,STSTb,Platform 3,STSTb,\nChildren Cast Turnback,Vehicle,21/09/15 07:20:00,21/09/15 06:36:58,CHATb,Platform 1,CHATb,\nBridge Bottle,Passenger,21/09/15 07:27:00,21/09/15 06:40:32,BRB,Platform A,BRB,\nCrib Team,Passenger,21/09/15 08:27:00,21/09/15 07:38:27,CRT,Platform B,CRT,\nClouds Goose,Passenger,22/09/15 00:05:30,,CLG,Outbound,CLG,\nSkin Shape,Passenger,22/09/15 00:46:00,22/09/15 00:21:21,SKH,Inbound Siding,SKH,\nSkate Stone Turnback,Vehicle,21/09/15 06:18:30,,SKSTb,Platform 1,SKSTb,\n", "csv1_path": "infiagent/merge_test/2_0_0.csv", "csv2_path": "infiagent/merge_test/2_0_1.csv", "instruction": "Merge all rows in the two tables that the length of 'sch_dep_time' is less than 17 and the length of 'station' is less than 12, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/2_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/2_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['sch_dep_time'].str.len() < 17]\ndf = df[df['station'].str.len() < 12]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['sch_dep_time'].str.len() < 17]\ndf = df[df['station'].str.len() < 12]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsqft_lot15,sqft_lot,waterfront,sqft_basement,view,yr_built,zipcode,price,yr_renovated,Unnamed: 0\n5650,5650,0,0,0,1955,98178,221900.0,0,0\n8062,10000,0,0,0,1933,98028,180000.0,0,2\n5000,5000,0,910,0,1965,98136,604000.0,0,3\n7503,8080,0,0,0,1987,98074,510000.0,0,4\n6819,6819,0,0,0,1995,98003,257500.0,0,6\n7570,6560,0,0,0,2003,98038,323000.0,0,9\n6000,6000,0,300,0,1942,98115,468000.0,0,11\n12697,19901,0,0,0,1927,98028,310000.0,0,12\n10208,9680,0,0,0,1977,98074,400000.0,0,13\n\nHeader and first few lines of CSV file 2:\nlat,floors,sqft_above,Unnamed: 0\n47.5112,1.0,1180,0\n47.721,2.0,2170,1\n47.7379,1.0,770,2\n47.6168,1.0,1680,4\n47.3097,2.0,1715,6\n47.4095,1.0,1060,7\n47.3684,2.0,1890,9\n47.6007,1.0,1860,10\n47.69,1.0,860,11\n\nQuestion: Combine all rows from both tables where the value of 'sqft_above' is not '1300', joining on shared column values and replacing missing values with NA.", "csv1_example": "sqft_lot15,sqft_lot,waterfront,sqft_basement,view,yr_built,zipcode,price,yr_renovated,Unnamed: 0\n5650,5650,0,0,0,1955,98178,221900.0,0,0\n8062,10000,0,0,0,1933,98028,180000.0,0,2\n5000,5000,0,910,0,1965,98136,604000.0,0,3\n7503,8080,0,0,0,1987,98074,510000.0,0,4\n6819,6819,0,0,0,1995,98003,257500.0,0,6\n7570,6560,0,0,0,2003,98038,323000.0,0,9\n6000,6000,0,300,0,1942,98115,468000.0,0,11\n12697,19901,0,0,0,1927,98028,310000.0,0,12\n10208,9680,0,0,0,1977,98074,400000.0,0,13\n4850,4850,0,0,0,1900,98107,530000.0,0,14\n", "csv2_example": "lat,floors,sqft_above,Unnamed: 0\n47.5112,1.0,1180,0\n47.721,2.0,2170,1\n47.7379,1.0,770,2\n47.6168,1.0,1680,4\n47.3097,2.0,1715,6\n47.4095,1.0,1060,7\n47.3684,2.0,1890,9\n47.6007,1.0,1860,10\n47.69,1.0,860,11\n47.7558,1.5,1430,12\n", "csv1_path": "infiagent/merge_test/41_1_0.csv", "csv2_path": "infiagent/merge_test/41_1_1.csv", "instruction": "Combine all rows from both tables where the value of 'sqft_above' is not '1300', joining on shared column values and replacing missing values with NA.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/41_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/41_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['sqft_above'] != 1300]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['sqft_above'] != '1300']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nFlag Codes,TIME,LOCATION,FREQUENCY\n,2012,AUS,A\n,2012,AUT,A\n,2012,BEL,A\n,2012,CAN,A\n,2012,CZE,A\nM,2012,DNK,A\n,2012,FIN,A\n,2012,DEU,A\nM,2012,GRC,A\n\nHeader and first few lines of CSV file 2:\nValue,LOCATION\n1.6,AUS\n1.4,BEL\n2.5,CAN\n,DNK\n1.4,FRA\n1.2,DEU\n,GRC\n1.2,HUN\n1.4,IRL\n\nQuestion: Combine all rows from both tables, display the values of TIME and LOCATION columns, and group them by the shared column names. Only display the merged rows that were successful.", "csv1_example": "Flag Codes,TIME,LOCATION,FREQUENCY\n,2012,AUS,A\n,2012,AUT,A\n,2012,BEL,A\n,2012,CAN,A\n,2012,CZE,A\nM,2012,DNK,A\n,2012,FIN,A\n,2012,DEU,A\nM,2012,GRC,A\n,2012,HUN,A\n", "csv2_example": "Value,LOCATION\n1.6,AUS\n1.4,BEL\n2.5,CAN\n,DNK\n1.4,FRA\n1.2,DEU\n,GRC\n1.2,HUN\n1.4,IRL\n0.9,ITA\n", "csv1_path": "infiagent/merge_test/46_2_0.csv", "csv2_path": "infiagent/merge_test/46_2_1.csv", "instruction": "Combine all rows from both tables, display the values of TIME and LOCATION columns, and group them by the shared column names. Only display the merged rows that were successful.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/46_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/46_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"TIME\", \"LOCATION\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"TIME\", \"LOCATION\"]]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhour-per-week,capital-loos,final-weight,education-num,age,native-country,marital-status,relationship\n40,0,77516,13,39, United-States, Never-married, Not-in-family\n13,0,83311,13,50, United-States, Married-civ-spouse, Husband\n40,0,215646,9,38, United-States, Divorced, Not-in-family\n40,0,234721,7,53, United-States, Married-civ-spouse, Husband\n40,0,338409,13,28, Cuba, Married-civ-spouse, Wife\n40,0,284582,14,37, United-States, Married-civ-spouse, Wife\n16,0,160187,5,49, Jamaica, Married-spouse-absent, Not-in-family\n45,0,209642,9,52, United-States, Married-civ-spouse, Husband\n50,0,45781,14,31, United-States, Never-married, Not-in-family\n\nHeader and first few lines of CSV file 2:\nincome,age,workclass,occupation\n <=50K,39, State-gov, Adm-clerical\n <=50K,38, Private, Handlers-cleaners\n <=50K,53, Private, Handlers-cleaners\n <=50K,28, Private, Prof-specialty\n <=50K,37, Private, Exec-managerial\n <=50K,49, Private, Other-service\n >50K,52, Self-emp-not-inc, Exec-managerial\n >50K,42, Private, Exec-managerial\n >50K,30, State-gov, Prof-specialty\n\nQuestion: Combine the tables by aligning rows with matching column names, where the value of 'age' is greater than 53 and the length of 'income' is greater than 6, keeping only the successfully merged portions.", "csv1_example": "hour-per-week,capital-loos,final-weight,education-num,age,native-country,marital-status,relationship\n40,0,77516,13,39, United-States, Never-married, Not-in-family\n13,0,83311,13,50, United-States, Married-civ-spouse, Husband\n40,0,215646,9,38, United-States, Divorced, Not-in-family\n40,0,234721,7,53, United-States, Married-civ-spouse, Husband\n40,0,338409,13,28, Cuba, Married-civ-spouse, Wife\n40,0,284582,14,37, United-States, Married-civ-spouse, Wife\n16,0,160187,5,49, Jamaica, Married-spouse-absent, Not-in-family\n45,0,209642,9,52, United-States, Married-civ-spouse, Husband\n50,0,45781,14,31, United-States, Never-married, Not-in-family\n40,0,159449,13,42, United-States, Married-civ-spouse, Husband\n", "csv2_example": "income,age,workclass,occupation\n <=50K,39, State-gov, Adm-clerical\n <=50K,38, Private, Handlers-cleaners\n <=50K,53, Private, Handlers-cleaners\n <=50K,28, Private, Prof-specialty\n <=50K,37, Private, Exec-managerial\n <=50K,49, Private, Other-service\n >50K,52, Self-emp-not-inc, Exec-managerial\n >50K,42, Private, Exec-managerial\n >50K,30, State-gov, Prof-specialty\n <=50K,23, Private, Adm-clerical\n", "csv1_path": "infiagent/merge_test/18_2_0.csv", "csv2_path": "infiagent/merge_test/18_2_1.csv", "instruction": "Combine the tables by aligning rows with matching column names, where the value of 'age' is greater than 53 and the length of 'income' is greater than 6, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/18_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/18_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['age'] > 53]\ndf = df[df['income'].str.len() > 6]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['age'] > 53]\ndf = df[df['income'].str.len() > 6]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents,RevolvingUtilizationOfUnsecuredLines\n2,0,2.0,0.463295269\n3,0,2.0,0.043275036\n4,0,0.0,0.280308229\n5,0,1.0,0.9999999\n6,0,1.0,0.509791452\n7,0,3.0,0.587778161\n8,0,1.0,0.046148938\n11,0,1.0,0.0284849\n12,0,0.0,0.085076597\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,NumberRealEstateLoansOrLines,SeriousDlqin2yrs,DebtRatio,age,NumberOfOpenCreditLinesAndLoans\n1,0,,0.177512717,43,4\n4,2,,0.925960637,38,7\n5,0,,0.019917227,27,4\n6,0,,0.342429365,63,4\n7,0,,1048.0,50,5\n8,1,,0.3691702,79,8\n9,1,,2024.0,68,4\n10,0,,0.0,23,0\n12,2,,0.903552105,52,6\n\nQuestion: Merge all rows in the two tables that the value of 'NumberOfDependents' is greater than 1.0, show the value of  NumberOfTime60-89DaysPastDueNotWorse, NumberOfDependents , RevolvingUtilizationOfUnsecuredLines , DebtRatio , NumberOfOpenCreditLinesAndLoans , NumberRealEstateLoansOrLines , age  and Unnamed: 0, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "Unnamed: 0,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents,RevolvingUtilizationOfUnsecuredLines\n2,0,2.0,0.463295269\n3,0,2.0,0.043275036\n4,0,0.0,0.280308229\n5,0,1.0,0.9999999\n6,0,1.0,0.509791452\n7,0,3.0,0.587778161\n8,0,1.0,0.046148938\n11,0,1.0,0.0284849\n12,0,0.0,0.085076597\n13,0,0.0,0.010596142\n", "csv2_example": "Unnamed: 0,NumberRealEstateLoansOrLines,SeriousDlqin2yrs,DebtRatio,age,NumberOfOpenCreditLinesAndLoans\n1,0,,0.177512717,43,4\n4,2,,0.925960637,38,7\n5,0,,0.019917227,27,4\n6,0,,0.342429365,63,4\n7,0,,1048.0,50,5\n8,1,,0.3691702,79,8\n9,1,,2024.0,68,4\n10,0,,0.0,23,0\n12,2,,0.903552105,52,6\n13,0,,0.007782101,68,16\n", "csv1_path": "infiagent/merge_test/25_0_0.csv", "csv2_path": "infiagent/merge_test/25_0_1.csv", "instruction": "Merge all rows in the two tables that the value of 'NumberOfDependents' is greater than 1.0, show the value of  NumberOfTime60-89DaysPastDueNotWorse, NumberOfDependents , RevolvingUtilizationOfUnsecuredLines , DebtRatio , NumberOfOpenCreditLinesAndLoans , NumberRealEstateLoansOrLines , age  and Unnamed: 0, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/25_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/25_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['NumberOfDependents'] > 1.0]\ndf = df[[\"NumberOfTime60-89DaysPastDueNotWorse\", \"NumberOfDependents\", \"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\", \"NumberOfOpenCreditLinesAndLoans\", \"NumberRealEstateLoansOrLines\", \"age\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['NumberOfDependents'] > 1.0]\ndf = df[[\"NumberOfTime60-89DaysPastDueNotWorse\", \"NumberOfDependents\", \"RevolvingUtilizationOfUnsecuredLines\", \"DebtRatio\", \"NumberOfOpenCreditLinesAndLoans\", \"NumberRealEstateLoansOrLines\", \"age\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDATE TIME,WINDSPEED,BARO\n01/01/2015 01:00,3.89,1022.7\n01/01/2015 02:00,4.86,1022.1\n01/01/2015 03:00,4.47,1021.4\n01/01/2015 04:00,4.08,1020.9\n01/01/2015 05:00,5.64,1020.5\n01/01/2015 07:00,7.58,1019.1\n01/01/2015 08:00,6.61,1018.9\n01/01/2015 09:00,9.14,1018.5\n01/01/2015 10:00,7.19,1019.0\n\nHeader and first few lines of CSV file 2:\nGUSTS,RELHUM,VIS,DATE TIME,AT\n5.25,,,01/01/2015 00:00,27.7\n7.0,,,01/01/2015 01:00,26.8\n6.41,,,01/01/2015 02:00,27.0\n7.19,,,01/01/2015 04:00,25.9\n11.47,,,01/01/2015 06:00,26.1\n10.3,,,01/01/2015 08:00,25.2\n13.41,,,01/01/2015 09:00,25.2\n10.11,,,01/01/2015 10:00,25.5\n13.61,,,01/01/2015 11:00,25.3\n\nQuestion: Merge all rows in the two tables that the value of 'WINDSPEED' is less than 7.0, show the value of VIS and DATE TIME, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "DATE TIME,WINDSPEED,BARO\n01/01/2015 01:00,3.89,1022.7\n01/01/2015 02:00,4.86,1022.1\n01/01/2015 03:00,4.47,1021.4\n01/01/2015 04:00,4.08,1020.9\n01/01/2015 05:00,5.64,1020.5\n01/01/2015 07:00,7.58,1019.1\n01/01/2015 08:00,6.61,1018.9\n01/01/2015 09:00,9.14,1018.5\n01/01/2015 10:00,7.19,1019.0\n01/01/2015 12:00,7.19,1019.4\n", "csv2_example": "GUSTS,RELHUM,VIS,DATE TIME,AT\n5.25,,,01/01/2015 00:00,27.7\n7.0,,,01/01/2015 01:00,26.8\n6.41,,,01/01/2015 02:00,27.0\n7.19,,,01/01/2015 04:00,25.9\n11.47,,,01/01/2015 06:00,26.1\n10.3,,,01/01/2015 08:00,25.2\n13.41,,,01/01/2015 09:00,25.2\n10.11,,,01/01/2015 10:00,25.5\n13.61,,,01/01/2015 11:00,25.3\n11.66,,,01/01/2015 12:00,25.5\n", "csv1_path": "infiagent/merge_test/13_3_0.csv", "csv2_path": "infiagent/merge_test/13_3_1.csv", "instruction": "Merge all rows in the two tables that the value of 'WINDSPEED' is less than 7.0, show the value of VIS and DATE TIME, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/13_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/13_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['WINDSPEED'] < 7.0]\ndf = df[[\"VIS\", \"DATE TIME\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['WINDSPEED'] < 7.0]\ndf = df[[\"VIS\", \"DATE TIME\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnumber_of_strike_outs,number_of_runs_batted_in,number_of_home_runs,batting_average,salary_in_thousands_of_dollars,number_of_triples,number_of_runs\n80.0,104,31.0,0.272,3300,4,69\n69.0,66,18.0,0.269,2600,2,58\n64.0,50,12.0,0.26,2475,7,59\n89.0,100,26.0,0.291,2175,2,104\n32.0,21,3.0,0.228,460,2,16\n26.0,18,1.0,0.25,240,0,40\n96.0,33,10.0,0.203,200,1,39\n18.0,10,0.0,0.262,177,0,7\n56.0,22,6.0,0.222,140,0,21\n\nHeader and first few lines of CSV file 2:\nsalary_in_thousands_of_dollars,number_of_hits,number_of_errors,number_of_stolen_bases,indicator_of_arbitration_in_1991_1992,indicator_of_free_agency_eligibility,number_of_walks,on_base_percentage\n3300,153,3.0,4,0.0,1.0,22,0.302\n2600,111,3.0,0,0.0,1.0,39,0.335\n2475,128,21.0,21,0.0,0.0,23,0.292\n2313,169,8.0,3,0.0,0.0,70,0.346\n2175,170,4.0,22,0.0,1.0,87,0.379\n600,86,10.0,0,0.0,1.0,15,0.37\n460,38,3.0,2,0.0,0.0,11,0.279\n240,61,2.0,14,0.0,0.0,24,0.327\n200,64,6.0,13,0.0,0.0,14,0.24\n\nQuestion: Combine the data from both tables where the value of 'number_of_errors' is more than 6.0 and the value of 'salary_in_thousands_of_dollars' is below 990, and merge it based on the same column names. Only retain the rows that were successfully merged.", "csv1_example": "number_of_strike_outs,number_of_runs_batted_in,number_of_home_runs,batting_average,salary_in_thousands_of_dollars,number_of_triples,number_of_runs\n80.0,104,31.0,0.272,3300,4,69\n69.0,66,18.0,0.269,2600,2,58\n64.0,50,12.0,0.26,2475,7,59\n89.0,100,26.0,0.291,2175,2,104\n32.0,21,3.0,0.228,460,2,16\n26.0,18,1.0,0.25,240,0,40\n96.0,33,10.0,0.203,200,1,39\n18.0,10,0.0,0.262,177,0,7\n56.0,22,6.0,0.222,140,0,21\n1.0,3,1.0,0.227,117,0,4\n", "csv2_example": "salary_in_thousands_of_dollars,number_of_hits,number_of_errors,number_of_stolen_bases,indicator_of_arbitration_in_1991_1992,indicator_of_free_agency_eligibility,number_of_walks,on_base_percentage\n3300,153,3.0,4,0.0,1.0,22,0.302\n2600,111,3.0,0,0.0,1.0,39,0.335\n2475,128,21.0,21,0.0,0.0,23,0.292\n2313,169,8.0,3,0.0,0.0,70,0.346\n2175,170,4.0,22,0.0,1.0,87,0.379\n600,86,10.0,0,0.0,1.0,15,0.37\n460,38,3.0,2,0.0,0.0,11,0.279\n240,61,2.0,14,0.0,0.0,24,0.327\n200,64,6.0,13,0.0,0.0,14,0.24\n177,38,7.0,2,0.0,0.0,5,0.283\n", "csv1_path": "infiagent/merge_test/14_3_0.csv", "csv2_path": "infiagent/merge_test/14_3_1.csv", "instruction": "Combine the data from both tables where the value of 'number_of_errors' is more than 6.0 and the value of 'salary_in_thousands_of_dollars' is below 990, and merge it based on the same column names. Only retain the rows that were successfully merged.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/14_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/14_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['number_of_errors'] > 6.0]\ndf = df[df['salary_in_thousands_of_dollars'] < 990]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['number_of_errors'] > 6.0]\ndf = df[df['salary_in_thousands_of_dollars'] < 990]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nrevolving_utilization_of_unsecured_lines,serious_dlqin2yrs,debt_ratio,number_of_dependents\n0.7661266089999998,1,0.8029821290000001,2.0\n0.957151019,0,0.121876201,1.0\n0.65818014,0,0.085113375,0.0\n0.233809776,0,0.036049682,0.0\n0.9072394,0,0.024925695,0.0\n0.213178682,0,0.375606969,1.0\n0.305682465,0,5710.0,0.0\n0.7544636479999999,0,0.209940017,0.0\n0.116950644,0,46.0,0.0\n\nHeader and first few lines of CSV file 2:\nnumber_of_open_credit_lines_and_loans,revolving_utilization_of_unsecured_lines,number_of_time60-89_days_past_due_not_worse\n13,0.7661266089999998,0\n4,0.957151019,0\n2,0.65818014,0\n5,0.233809776,0\n7,0.9072394,0\n3,0.213178682,0\n8,0.305682465,0\n8,0.7544636479999999,0\n2,0.116950644,0\n\nQuestion: Merge all rows in the two tables that the value of 'number_of_dependents' is not less than 0.0 and the value of 'number_of_open_credit_lines_and_loans' is not greater than 8, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "revolving_utilization_of_unsecured_lines,serious_dlqin2yrs,debt_ratio,number_of_dependents\n0.7661266089999998,1,0.8029821290000001,2.0\n0.957151019,0,0.121876201,1.0\n0.65818014,0,0.085113375,0.0\n0.233809776,0,0.036049682,0.0\n0.9072394,0,0.024925695,0.0\n0.213178682,0,0.375606969,1.0\n0.305682465,0,5710.0,0.0\n0.7544636479999999,0,0.209940017,0.0\n0.116950644,0,46.0,0.0\n0.189169052,0,0.606290901,2.0\n", "csv2_example": "number_of_open_credit_lines_and_loans,revolving_utilization_of_unsecured_lines,number_of_time60-89_days_past_due_not_worse\n13,0.7661266089999998,0\n4,0.957151019,0\n2,0.65818014,0\n5,0.233809776,0\n7,0.9072394,0\n3,0.213178682,0\n8,0.305682465,0\n8,0.7544636479999999,0\n2,0.116950644,0\n9,0.189169052,0\n", "csv1_path": "infiagent/merge_test/23_1_0.csv", "csv2_path": "infiagent/merge_test/23_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'number_of_dependents' is not less than 0.0 and the value of 'number_of_open_credit_lines_and_loans' is not greater than 8, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/23_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/23_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['number_of_dependents'] <= 0.0]\ndf = df[df['number_of_open_credit_lines_and_loans'] >= 8]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['number_of_dependents'] <= 0.0]\ndf = df[df['number_of_open_credit_lines_and_loans'] >= 8]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nmpg,displacement\n18.0,307.0\n15.0,350.0\n16.0,304.0\n14.0,454.0\n21.0,200.0\n27.0,97.0\n26.0,97.0\n10.0,360.0\n28.0,140.0\n\nHeader and first few lines of CSV file 2:\norigin,horsepower,modelyear,mpg\n1,130.0,70,18.0\n1,165.0,70,15.0\n1,150.0,70,16.0\n1,140.0,70,17.0\n1,220.0,70,14.0\n1,95.0,70,22.0\n1,85.0,70,21.0\n3,88.0,70,27.0\n2,46.0,70,26.0\n\nQuestion: Merge all rows in the two tables, show the value of  origin, modelyear , horsepower  and mpg, merging by entries with the same column name,  and fill in the blanks with NAN.", "csv1_example": "mpg,displacement\n18.0,307.0\n15.0,350.0\n16.0,304.0\n14.0,454.0\n21.0,200.0\n27.0,97.0\n26.0,97.0\n10.0,360.0\n28.0,140.0\n19.0,232.0\n", "csv2_example": "origin,horsepower,modelyear,mpg\n1,130.0,70,18.0\n1,165.0,70,15.0\n1,150.0,70,16.0\n1,140.0,70,17.0\n1,220.0,70,14.0\n1,95.0,70,22.0\n1,85.0,70,21.0\n3,88.0,70,27.0\n2,46.0,70,26.0\n2,87.0,70,25.0\n", "csv1_path": "infiagent/merge_test/11_3_0.csv", "csv2_path": "infiagent/merge_test/11_3_1.csv", "instruction": "Merge all rows in the two tables, show the value of  origin, modelyear , horsepower  and mpg, merging by entries with the same column name,  and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/11_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/11_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"origin\", \"modelyear\", \"horsepower\", \"mpg\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"origin\", \"modelyear\", \"horsepower\", \"mpg\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nweekday,instant,casual,hr,weathersit,cnt\n6,1,3,0,1,16\n6,2,8,1,1,40\n6,3,5,2,1,32\n6,4,3,3,1,13\n6,5,0,4,1,1\n6,6,0,5,2,1\n6,7,2,6,1,2\n6,8,1,7,1,3\n6,10,8,9,1,14\n\nHeader and first few lines of CSV file 2:\ndteday,atemp,mnth,instant,temp,yr\n2011-01-01,0.2879,1,1,0.24,0\n2011-01-01,0.2727,1,2,0.22,0\n2011-01-01,0.2727,1,3,0.22,0\n2011-01-01,0.2879,1,4,0.24,0\n2011-01-01,0.2879,1,5,0.24,0\n2011-01-01,0.2576,1,6,0.24,0\n2011-01-01,0.2727,1,7,0.22,0\n2011-01-01,0.2576,1,8,0.2,0\n2011-01-01,0.2879,1,9,0.24,0\n\nQuestion: Combine rows from the two tables where 'casual' is not '0', merging by entries with the same column names, and retain only the successfully merged data.", "csv1_example": "weekday,instant,casual,hr,weathersit,cnt\n6,1,3,0,1,16\n6,2,8,1,1,40\n6,3,5,2,1,32\n6,4,3,3,1,13\n6,5,0,4,1,1\n6,6,0,5,2,1\n6,7,2,6,1,2\n6,8,1,7,1,3\n6,10,8,9,1,14\n6,11,12,10,1,36\n", "csv2_example": "dteday,atemp,mnth,instant,temp,yr\n2011-01-01,0.2879,1,1,0.24,0\n2011-01-01,0.2727,1,2,0.22,0\n2011-01-01,0.2727,1,3,0.22,0\n2011-01-01,0.2879,1,4,0.24,0\n2011-01-01,0.2879,1,5,0.24,0\n2011-01-01,0.2576,1,6,0.24,0\n2011-01-01,0.2727,1,7,0.22,0\n2011-01-01,0.2576,1,8,0.2,0\n2011-01-01,0.2879,1,9,0.24,0\n2011-01-01,0.3485,1,10,0.32,0\n", "csv1_path": "infiagent/merge_test/16_1_0.csv", "csv2_path": "infiagent/merge_test/16_1_1.csv", "instruction": "Combine rows from the two tables where 'casual' is not '0', merging by entries with the same column names, and retain only the successfully merged data.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/16_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/16_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['casual'] != 0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['casual'] != '0']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ngithub-dept-code,budget_year_start,Department Name\nANM,10/1/2016,Animal Services\nCOD,10/1/2016,Austin Code\nCON,10/1/2016,Austin Convention Center\nENE,10/1/2016,Austin Energy\nLIB,10/1/2016,Austin Public Library\nRES,10/1/2016,Austin Resource Recovery\nTRA,10/1/2016,Austin Transportation\nWAT,10/1/2016,Austin Water\nAVI,10/1/2016,Aviation\n\nHeader and first few lines of CSV file 2:\nDepartment Name,budget_year_end,coa_dept_id\nAnimal Services,9/30/2017,92\nAustin Code,9/30/2017,16\nAustin Convention Center,9/30/2017,88\nAustin Energy,9/30/2017,11\nAustin Public Library,9/30/2017,85\nAustin Resource Recovery,9/30/2017,15\nAustin Transportation,9/30/2017,24\nAustin Water,9/30/2017,22\nAviation,9/30/2017,81\n\nQuestion: Combine the rows in two tables where the 'Department Name' length is greater than 20 and 'budget_year_end' is not '9/30/2017'. Merge entries with the same column name and retain only the successfully merged segments.", "csv1_example": "github-dept-code,budget_year_start,Department Name\nANM,10/1/2016,Animal Services\nCOD,10/1/2016,Austin Code\nCON,10/1/2016,Austin Convention Center\nENE,10/1/2016,Austin Energy\nLIB,10/1/2016,Austin Public Library\nRES,10/1/2016,Austin Resource Recovery\nTRA,10/1/2016,Austin Transportation\nWAT,10/1/2016,Austin Water\nAVI,10/1/2016,Aviation\nTEC,10/1/2016,Communication and Technology Management\n", "csv2_example": "Department Name,budget_year_end,coa_dept_id\nAnimal Services,9/30/2017,92\nAustin Code,9/30/2017,16\nAustin Convention Center,9/30/2017,88\nAustin Energy,9/30/2017,11\nAustin Public Library,9/30/2017,85\nAustin Resource Recovery,9/30/2017,15\nAustin Transportation,9/30/2017,24\nAustin Water,9/30/2017,22\nAviation,9/30/2017,81\nBuilding Services,9/30/2017,75\n", "csv1_path": "infiagent/merge_test/19_1_0.csv", "csv2_path": "infiagent/merge_test/19_1_1.csv", "instruction": "Combine the rows in two tables where the 'Department Name' length is greater than 20 and 'budget_year_end' is not '9/30/2017'. Merge entries with the same column name and retain only the successfully merged segments.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/19_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/19_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Department Name'].str.len() > 20]\ndf = df[df['budget_year_end'] != '9/30/2017']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Department Name'].str.len() > 20]\ndf = df[df['budget_year_end'] != '9/30/2017']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhappiness,index,blur,fear,name\n0.0,0,0.0,0.0,alckmin\n0.001,12845,0.05,0.0,boulos\n0.0,19125,0.3,0.0,ciro\n0.126,25592,0.0,0.0,marina\n0.0,32061,0.0,0.0,meirelles\n0.145,6397,0.31,0.0,alvaro\n0.432,25593,0.0,0.0,marina\n0.0,19126,0.18,0.0,ciro\n0.694,6398,0.89,0.0,alvaro\n\nHeader and first few lines of CSV file 2:\ndisgust,anger,contempt,index,neutral\n0.0,0.0,0.001,0,0.981\n0.0,0.001,0.001,12845,0.996\n0.0,0.003,0.01,19125,0.942\n0.003,0.0,0.002,25592,0.626\n0.0,0.0,0.003,6397,0.852\n0.0,0.0,0.005,25593,0.563\n0.0,0.0,0.0,12846,0.998\n0.0,0.0,0.0,32062,0.888\n0.0,0.0,0.0,6399,0.942\n\nQuestion: Merge all rows in the two tables that the value of 'contempt' is '0.0' and the value of 'anger' is '0.0', merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "happiness,index,blur,fear,name\n0.0,0,0.0,0.0,alckmin\n0.001,12845,0.05,0.0,boulos\n0.0,19125,0.3,0.0,ciro\n0.126,25592,0.0,0.0,marina\n0.0,32061,0.0,0.0,meirelles\n0.145,6397,0.31,0.0,alvaro\n0.432,25593,0.0,0.0,marina\n0.0,19126,0.18,0.0,ciro\n0.694,6398,0.89,0.0,alvaro\n0.0,32062,0.45,0.0,meirelles\n", "csv2_example": "disgust,anger,contempt,index,neutral\n0.0,0.0,0.001,0,0.981\n0.0,0.001,0.001,12845,0.996\n0.0,0.003,0.01,19125,0.942\n0.003,0.0,0.002,25592,0.626\n0.0,0.0,0.003,6397,0.852\n0.0,0.0,0.005,25593,0.563\n0.0,0.0,0.0,12846,0.998\n0.0,0.0,0.0,32062,0.888\n0.0,0.0,0.0,6399,0.942\n0.0,0.002,0.026,19127,0.922\n", "csv1_path": "infiagent/merge_test/9_2_0.csv", "csv2_path": "infiagent/merge_test/9_2_1.csv", "instruction": "Merge all rows in the two tables that the value of 'contempt' is '0.0' and the value of 'anger' is '0.0', merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/9_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/9_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['contempt'] == 0.0]\ndf = df[df['anger'] == 0.0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['contempt'] == '0.0']\ndf = df[df['anger'] == '0.0']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ncapital-loos,age,hour-per-week,education-num,occupation,marital-status,race,education,income\n0,39,40,13, Adm-clerical, Never-married, White, Bachelors, <=50K\n0,50,13,13, Exec-managerial, Married-civ-spouse, White, Bachelors, <=50K\n0,38,40,9, Handlers-cleaners, Divorced, White, HS-grad, <=50K\n0,53,40,7, Handlers-cleaners, Married-civ-spouse, Black, 11th, <=50K\n0,28,40,13, Prof-specialty, Married-civ-spouse, Black, Bachelors, <=50K\n0,37,40,14, Exec-managerial, Married-civ-spouse, White, Masters, <=50K\n0,49,16,5, Other-service, Married-spouse-absent, Black, 9th, <=50K\n0,52,45,9, Exec-managerial, Married-civ-spouse, White, HS-grad, >50K\n0,31,50,14, Prof-specialty, Never-married, White, Masters, >50K\n\nHeader and first few lines of CSV file 2:\ncapital-gain,final-weight,age,native-country,workclass,relationship\n2174,77516,39, United-States, State-gov, Not-in-family\n0,83311,50, United-States, Self-emp-not-inc, Husband\n0,215646,38, United-States, Private, Not-in-family\n0,338409,28, Cuba, Private, Wife\n0,284582,37, United-States, Private, Wife\n0,160187,49, Jamaica, Private, Not-in-family\n0,209642,52, United-States, Self-emp-not-inc, Husband\n14084,45781,31, United-States, Private, Not-in-family\n5178,159449,42, United-States, Private, Husband\n\nQuestion: Combine rows from both tables where the length of the 'education' field exceeds 8, and display the values of 'income', 'hour-per-week', 'workclass', 'capital-loos', 'marital-status', and 'age' during the merge process. The merged rows will be outputted, preserving only the successfully merged portions, with matching column names used as the merge key.", "csv1_example": "capital-loos,age,hour-per-week,education-num,occupation,marital-status,race,education,income\n0,39,40,13, Adm-clerical, Never-married, White, Bachelors, <=50K\n0,50,13,13, Exec-managerial, Married-civ-spouse, White, Bachelors, <=50K\n0,38,40,9, Handlers-cleaners, Divorced, White, HS-grad, <=50K\n0,53,40,7, Handlers-cleaners, Married-civ-spouse, Black, 11th, <=50K\n0,28,40,13, Prof-specialty, Married-civ-spouse, Black, Bachelors, <=50K\n0,37,40,14, Exec-managerial, Married-civ-spouse, White, Masters, <=50K\n0,49,16,5, Other-service, Married-spouse-absent, Black, 9th, <=50K\n0,52,45,9, Exec-managerial, Married-civ-spouse, White, HS-grad, >50K\n0,31,50,14, Prof-specialty, Never-married, White, Masters, >50K\n0,42,40,13, Exec-managerial, Married-civ-spouse, White, Bachelors, >50K\n", "csv2_example": "capital-gain,final-weight,age,native-country,workclass,relationship\n2174,77516,39, United-States, State-gov, Not-in-family\n0,83311,50, United-States, Self-emp-not-inc, Husband\n0,215646,38, United-States, Private, Not-in-family\n0,338409,28, Cuba, Private, Wife\n0,284582,37, United-States, Private, Wife\n0,160187,49, Jamaica, Private, Not-in-family\n0,209642,52, United-States, Self-emp-not-inc, Husband\n14084,45781,31, United-States, Private, Not-in-family\n5178,159449,42, United-States, Private, Husband\n0,122272,23, United-States, Private, Own-child\n", "csv1_path": "infiagent/merge_test/18_3_0.csv", "csv2_path": "infiagent/merge_test/18_3_1.csv", "instruction": "Combine rows from both tables where the length of the 'education' field exceeds 8, and display the values of 'income', 'hour-per-week', 'workclass', 'capital-loos', 'marital-status', and 'age' during the merge process. The merged rows will be outputted, preserving only the successfully merged portions, with matching column names used as the merge key.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/18_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/18_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['education'].str.len() > 8]\ndf = df[[\"income\", \"hour-per-week\", \"workclass\", \"capital-loos\", \"marital-status\", \"age\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['education'].str.len() > 8]\ndf = df[[\"income\", \"hour-per-week\", \"workclass\", \"capital-loos\", \"marital-status\", \"age\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nYear,Agriculture,Psychology,Social Sciences and History,Computer Science,English,Education,Architecture,Physical Sciences,Art and Performance\n1970,4.22979798,44.4,36.8,13.6,65.57092343,74.53532758,11.92100539,13.8,59.7\n1971,5.452796685,46.2,36.2,13.6,64.55648516,74.14920369,12.00310559,14.9,59.9\n1972,7.42071022,47.6,36.1,14.9,63.6642632,73.55451996,13.21459351,14.8,60.4\n1973,9.653602412,50.4,36.4,16.4,62.94150212,73.50181443,14.7916134,16.5,60.2\n1974,14.07462346,52.6,37.3,18.9,62.41341209,73.33681143,17.44468758,18.2,61.9\n1975,18.33316153,54.5,37.7,19.8,61.64720641,72.80185448,19.13404767,19.1,60.9\n1977,24.6401766,59.0,40.5,25.7,62.72306675,72.45639481,23.74054054,21.3,62.0\n1979,29.63336549,63.3,43.6,30.2,65.08838972,73.82114234,27.77047744,23.7,63.2\n1980,30.75938956,65.1,44.2,32.5,65.28413007,74.98103152,28.08038075,24.6,63.4\n\nHeader and first few lines of CSV file 2:\nYear,Biology,Engineering,Public Administration\n1970,29.08836297,0.8,68.4\n1971,29.39440285,1.0,65.5\n1973,31.14791477,1.6,64.3\n1975,34.44990213,3.2,63.0\n1978,40.11249564,8.4,71.5\n1979,42.06555109,9.4,73.3\n1980,43.99925716,10.3,74.6\n1981,45.24951206,11.6,74.7\n1982,45.96733794,12.4,76.8\n\nQuestion: Merge all rows in the two tables that the value of 'Engineering' is not less than 14.1, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "Year,Agriculture,Psychology,Social Sciences and History,Computer Science,English,Education,Architecture,Physical Sciences,Art and Performance\n1970,4.22979798,44.4,36.8,13.6,65.57092343,74.53532758,11.92100539,13.8,59.7\n1971,5.452796685,46.2,36.2,13.6,64.55648516,74.14920369,12.00310559,14.9,59.9\n1972,7.42071022,47.6,36.1,14.9,63.6642632,73.55451996,13.21459351,14.8,60.4\n1973,9.653602412,50.4,36.4,16.4,62.94150212,73.50181443,14.7916134,16.5,60.2\n1974,14.07462346,52.6,37.3,18.9,62.41341209,73.33681143,17.44468758,18.2,61.9\n1975,18.33316153,54.5,37.7,19.8,61.64720641,72.80185448,19.13404767,19.1,60.9\n1977,24.6401766,59.0,40.5,25.7,62.72306675,72.45639481,23.74054054,21.3,62.0\n1979,29.63336549,63.3,43.6,30.2,65.08838972,73.82114234,27.77047744,23.7,63.2\n1980,30.75938956,65.1,44.2,32.5,65.28413007,74.98103152,28.08038075,24.6,63.4\n1981,31.31865519,66.9,44.6,34.8,65.83832154,75.84512345,29.84169408,25.7,63.3\n", "csv2_example": "Year,Biology,Engineering,Public Administration\n1970,29.08836297,0.8,68.4\n1971,29.39440285,1.0,65.5\n1973,31.14791477,1.6,64.3\n1975,34.44990213,3.2,63.0\n1978,40.11249564,8.4,71.5\n1979,42.06555109,9.4,73.3\n1980,43.99925716,10.3,74.6\n1981,45.24951206,11.6,74.7\n1982,45.96733794,12.4,76.8\n1983,46.71313451,13.1,76.1\n", "csv1_path": "infiagent/merge_test/47_3_0.csv", "csv2_path": "infiagent/merge_test/47_3_1.csv", "instruction": "Merge all rows in the two tables that the value of 'Engineering' is not less than 14.1, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/47_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/47_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Engineering'] <= 14.1]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Engineering'] <= 14.1]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnum. calls answered,avg. abandonment time,avg. num. agents talking,timestamp\n0,00:00:00,0.0,Apr 13  2017 12:00:00 AM\n0,00:00:00,0.0,Apr 13  2017 12:15:00 AM\n0,00:00:00,0.0,Apr 13  2017 12:45:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:00:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:15:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:30:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:45:00 AM\n0,00:00:00,0.0,Apr 13  2017 2:15:00 AM\n0,00:00:00,0.0,Apr 13  2017 2:30:00 AM\n\nHeader and first few lines of CSV file 2:\navg. num. agents staffed,timestamp\n4,Apr 13  2017 12:00:00 AM\n4,Apr 13  2017 12:30:00 AM\n4,Apr 13  2017 12:45:00 AM\n4,Apr 13  2017 1:00:00 AM\n4,Apr 13  2017 1:15:00 AM\n4,Apr 13  2017 1:30:00 AM\n4,Apr 13  2017 1:45:00 AM\n4,Apr 13  2017 2:00:00 AM\n4,Apr 13  2017 2:15:00 AM\n\nQuestion: Combine rows from both tables where the length of the 'timestamp' column is less than 23 and the value of 'num. calls answered' is greater than 0, joining on common column names, and leaving blank values as NAN.", "csv1_example": "num. calls answered,avg. abandonment time,avg. num. agents talking,timestamp\n0,00:00:00,0.0,Apr 13  2017 12:00:00 AM\n0,00:00:00,0.0,Apr 13  2017 12:15:00 AM\n0,00:00:00,0.0,Apr 13  2017 12:45:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:00:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:15:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:30:00 AM\n0,00:00:00,0.0,Apr 13  2017 1:45:00 AM\n0,00:00:00,0.0,Apr 13  2017 2:15:00 AM\n0,00:00:00,0.0,Apr 13  2017 2:30:00 AM\n0,00:00:00,0.0,Apr 13  2017 2:45:00 AM\n", "csv2_example": "avg. num. agents staffed,timestamp\n4,Apr 13  2017 12:00:00 AM\n4,Apr 13  2017 12:30:00 AM\n4,Apr 13  2017 12:45:00 AM\n4,Apr 13  2017 1:00:00 AM\n4,Apr 13  2017 1:15:00 AM\n4,Apr 13  2017 1:30:00 AM\n4,Apr 13  2017 1:45:00 AM\n4,Apr 13  2017 2:00:00 AM\n4,Apr 13  2017 2:15:00 AM\n4,Apr 13  2017 2:30:00 AM\n", "csv1_path": "infiagent/merge_test/4_3_0.csv", "csv2_path": "infiagent/merge_test/4_3_1.csv", "instruction": "Combine rows from both tables where the length of the 'timestamp' column is less than 23 and the value of 'num. calls answered' is greater than 0, joining on common column names, and leaving blank values as NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/4_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/4_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['timestamp'].str.len() < 23]\ndf = df[df['num. calls answered'] > 0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['timestamp'].str.len() < 23]\ndf = df[df['num. calls answered'] > 0]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nCountry,Freedom,Happiness Score\nIceland,0.62877,7.561\nDenmark,0.64938,7.527\nNorway,0.66973,7.522\nFinland,0.64169,7.406\nSweden,0.6598,7.364\nNew Zealand,0.63938,7.286\nIsrael,0.41319,7.278\nCosta Rica,0.63376,7.226\nAustria,0.62433,7.2\n\nHeader and first few lines of CSV file 2:\nStandard Error,Trust (Government Corruption),Generosity,Economy (GDP per Capita),Happiness Rank,Dystopia Residual,Country,Health (Life Expectancy),Family\n0.03411,0.41978,0.29678,1.39651,1,2.51738,Switzerland,0.94143,1.34951\n0.03328,0.48357,0.34139,1.32548,3,2.49204,Denmark,0.87464,1.36058\n0.0388,0.36503,0.34699,1.459,4,2.46531,Norway,0.88521,1.33095\n0.03553,0.32957,0.45811,1.32629,5,2.45176,Canada,0.90563,1.32261\n0.0314,0.41372,0.23351,1.29025,6,2.61955,Finland,0.88911,1.31826\n0.02799,0.31814,0.4761,1.32944,7,2.4657,Netherlands,0.89284,1.28017\n0.03157,0.43844,0.36262,1.33171,8,2.37119,Sweden,0.91087,1.28907\n0.04083,0.35637,0.43562,1.33358,10,2.26646,Australia,0.93156,1.30923\n0.0347,0.07785,0.33172,1.22857,11,3.08854,Israel,0.91387,1.22393\n\nQuestion: Combine the rows from both tables where the 'Generosity' value is greater than or equal to 0.21684, using matching column names, and replace any missing values with NaN.", "csv1_example": "Country,Freedom,Happiness Score\nIceland,0.62877,7.561\nDenmark,0.64938,7.527\nNorway,0.66973,7.522\nFinland,0.64169,7.406\nSweden,0.6598,7.364\nNew Zealand,0.63938,7.286\nIsrael,0.41319,7.278\nCosta Rica,0.63376,7.226\nAustria,0.62433,7.2\nMexico,0.48181,7.187\n", "csv2_example": "Standard Error,Trust (Government Corruption),Generosity,Economy (GDP per Capita),Happiness Rank,Dystopia Residual,Country,Health (Life Expectancy),Family\n0.03411,0.41978,0.29678,1.39651,1,2.51738,Switzerland,0.94143,1.34951\n0.03328,0.48357,0.34139,1.32548,3,2.49204,Denmark,0.87464,1.36058\n0.0388,0.36503,0.34699,1.459,4,2.46531,Norway,0.88521,1.33095\n0.03553,0.32957,0.45811,1.32629,5,2.45176,Canada,0.90563,1.32261\n0.0314,0.41372,0.23351,1.29025,6,2.61955,Finland,0.88911,1.31826\n0.02799,0.31814,0.4761,1.32944,7,2.4657,Netherlands,0.89284,1.28017\n0.03157,0.43844,0.36262,1.33171,8,2.37119,Sweden,0.91087,1.28907\n0.04083,0.35637,0.43562,1.33358,10,2.26646,Australia,0.93156,1.30923\n0.0347,0.07785,0.33172,1.22857,11,3.08854,Israel,0.91387,1.22393\n0.04454,0.10583,0.25497,0.95578,12,3.17728,Costa Rica,0.86027,1.23788\n", "csv1_path": "infiagent/merge_test/3_1_0.csv", "csv2_path": "infiagent/merge_test/3_1_1.csv", "instruction": "Combine the rows from both tables where the 'Generosity' value is greater than or equal to 0.21684, using matching column names, and replace any missing values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/3_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/3_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Generosity'] <= 0.21684]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Generosity'] >= 0.21684]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nbrand_name,hotel_name,hotel_id\nNone,Heritage Hotel New York City,93401\nClarion,Clarion Hotel Park Avenue,224217\nThe Leading Hotels of the World,Greenwich Hotel,1028569\n,Sutton Court Hotel Residences,1733337\nLangham,The Langham New York Fifth Avenue,1776857\nNone,Brooklyn Hostel - Utica Avenue,7379289\n,Interfaith Retreats,10036825\n,Kennedy Suites,15646553\n,Studio Apartments,15697497\n\nHeader and first few lines of CSV file 2:\nparent_brand_name,city_name,hotel_id,bubble_score\nWyndham Hotel Group,New York City,75737,40.0\n,New York City,93401,35.0\nChoice Hotels International, Inc.,New York City,488793,40.0\nThe Leading Hotels of the World, Ltd,New York City,1028569,45.0\nChoice Hotels International, Inc.,New York City,1383001,40.0\nLangham Hospitality Group,New York City,1776857,45.0\n,New York City,7148761,45.0\nMarriott International, Inc.,New York City,8541913,45.0\n,New York City,9608281,45.0\n\nQuestion: Merge all rows in the two tables that the value of 'city_name' is 'None', merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "brand_name,hotel_name,hotel_id\nNone,Heritage Hotel New York City,93401\nClarion,Clarion Hotel Park Avenue,224217\nThe Leading Hotels of the World,Greenwich Hotel,1028569\n,Sutton Court Hotel Residences,1733337\nLangham,The Langham New York Fifth Avenue,1776857\nNone,Brooklyn Hostel - Utica Avenue,7379289\n,Interfaith Retreats,10036825\n,Kennedy Suites,15646553\n,Studio Apartments,15697497\n,Lafayette International Hostel,252972\n", "csv2_example": "parent_brand_name,city_name,hotel_id,bubble_score\nWyndham Hotel Group,New York City,75737,40.0\n,New York City,93401,35.0\nChoice Hotels International, Inc.,New York City,488793,40.0\nThe Leading Hotels of the World, Ltd,New York City,1028569,45.0\nChoice Hotels International, Inc.,New York City,1383001,40.0\nLangham Hospitality Group,New York City,1776857,45.0\n,New York City,7148761,45.0\nMarriott International, Inc.,New York City,8541913,45.0\n,New York City,9608281,45.0\n,New York City,10036825,30.0\n", "csv1_path": "infiagent/merge_test/40_1_0.csv", "csv2_path": "infiagent/merge_test/40_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'city_name' is 'None', merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/40_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/40_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['city_name'] == 'None']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['city_name'] == 'None']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nEducation,Income,Unnamed: 0\n11,14.891,1\n15,106.025,2\n11,104.593,3\n11,148.924,4\n16,55.882,5\n10,80.18,6\n12,20.996,7\n9,71.408,8\n13,15.125,9\n\nHeader and first few lines of CSV file 2:\nLimit,Student,Unnamed: 0,Ethnicity,Rating,Balance,Cards\n3606,No,1,Caucasian,283,333,2\n6645,Yes,2,Asian,483,903,3\n7075,No,3,Asian,514,580,4\n4897,No,5,Caucasian,357,331,2\n8047,No,6,Caucasian,569,1151,4\n3388,No,7,African American,259,203,2\n7114,No,8,Asian,512,872,2\n3300,No,9,Caucasian,266,279,5\n6819,Yes,10,African American,491,1350,3\n\nQuestion: Merge all rows in the two tables, show the value of  Limit, Cards , Income , Education  and Unnamed: 0, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "Education,Income,Unnamed: 0\n11,14.891,1\n15,106.025,2\n11,104.593,3\n11,148.924,4\n16,55.882,5\n10,80.18,6\n12,20.996,7\n9,71.408,8\n13,15.125,9\n19,71.061,10\n", "csv2_example": "Limit,Student,Unnamed: 0,Ethnicity,Rating,Balance,Cards\n3606,No,1,Caucasian,283,333,2\n6645,Yes,2,Asian,483,903,3\n7075,No,3,Asian,514,580,4\n4897,No,5,Caucasian,357,331,2\n8047,No,6,Caucasian,569,1151,4\n3388,No,7,African American,259,203,2\n7114,No,8,Asian,512,872,2\n3300,No,9,Caucasian,266,279,5\n6819,Yes,10,African American,491,1350,3\n8117,No,11,Caucasian,589,1407,4\n", "csv1_path": "infiagent/merge_test/24_2_0.csv", "csv2_path": "infiagent/merge_test/24_2_1.csv", "instruction": "Merge all rows in the two tables, show the value of  Limit, Cards , Income , Education  and Unnamed: 0, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/24_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/24_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Limit\", \"Cards\", \"Income\", \"Education\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Limit\", \"Cards\", \"Income\", \"Education\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnumber_of_doubles,salary_in_thousands_of_dollars,indicator_of_free_agency_eligibility,number_of_runs,number_of_runs_batted_in,batting_average,indicator_of_arbitration_in_1991_1992,number_of_hits,number_of_errors,number_of_strike_outs\n17,2600,1.0,58,66,0.269,0.0,111,3.0,69.0\n15,2500,1.0,54,73,0.249,0.0,115,5.0,116.0\n22,2475,0.0,59,50,0.26,0.0,128,21.0,64.0\n14,600,1.0,34,38,0.258,0.0,86,10.0,45.0\n7,460,0.0,16,21,0.228,0.0,38,3.0,32.0\n10,200,0.0,39,33,0.203,0.0,64,6.0,96.0\n5,177,0.0,7,10,0.262,0.0,38,7.0,18.0\n9,140,0.0,21,22,0.222,0.0,45,3.0,56.0\n2,117,0.0,4,3,0.227,0.0,5,0.0,1.0\n\nHeader and first few lines of CSV file 2:\nindicator_of_free_agent_in_1991_1992,on_base_percentage,number_of_triples,indicator_of_arbitration_eligibility,salary_in_thousands_of_dollars\n0,0.302,4,0.0,3300\n1,0.335,2,0.0,2600\n0,0.292,7,1.0,2475\n0,0.346,5,1.0,2313\n0,0.379,2,0.0,2175\n0,0.37,1,0.0,600\n0,0.279,2,0.0,460\n0,0.24,1,0.0,200\n0,0.307,0,0.0,140\n\nQuestion: Combine the rows in two tables where 'salary_in_thousands_of_dollars' is less than or equal to 990, display the values of number_of_runs_batted_in, number_of_errors, number_of_strike_outs, number_of_runs, number_of_hits, number_of_doubles, batting_average, number_of_triples, on_base_percentage, and salary_in_thousands_of_dollars. Merge entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "number_of_doubles,salary_in_thousands_of_dollars,indicator_of_free_agency_eligibility,number_of_runs,number_of_runs_batted_in,batting_average,indicator_of_arbitration_in_1991_1992,number_of_hits,number_of_errors,number_of_strike_outs\n17,2600,1.0,58,66,0.269,0.0,111,3.0,69.0\n15,2500,1.0,54,73,0.249,0.0,115,5.0,116.0\n22,2475,0.0,59,50,0.26,0.0,128,21.0,64.0\n14,600,1.0,34,38,0.258,0.0,86,10.0,45.0\n7,460,0.0,16,21,0.228,0.0,38,3.0,32.0\n10,200,0.0,39,33,0.203,0.0,64,6.0,96.0\n5,177,0.0,7,10,0.262,0.0,38,7.0,18.0\n9,140,0.0,21,22,0.222,0.0,45,3.0,56.0\n2,117,0.0,4,3,0.227,0.0,5,0.0,1.0\n0,115,0.0,1,2,0.261,0.0,6,0.0,3.0\n", "csv2_example": "indicator_of_free_agent_in_1991_1992,on_base_percentage,number_of_triples,indicator_of_arbitration_eligibility,salary_in_thousands_of_dollars\n0,0.302,4,0.0,3300\n1,0.335,2,0.0,2600\n0,0.292,7,1.0,2475\n0,0.346,5,1.0,2313\n0,0.379,2,0.0,2175\n0,0.37,1,0.0,600\n0,0.279,2,0.0,460\n0,0.24,1,0.0,200\n0,0.307,0,0.0,140\n0,0.321,8,1.0,1190\n", "csv1_path": "infiagent/merge_test/14_1_0.csv", "csv2_path": "infiagent/merge_test/14_1_1.csv", "instruction": "Combine the rows in two tables where 'salary_in_thousands_of_dollars' is less than or equal to 990, display the values of number_of_runs_batted_in, number_of_errors, number_of_strike_outs, number_of_runs, number_of_hits, number_of_doubles, batting_average, number_of_triples, on_base_percentage, and salary_in_thousands_of_dollars. Merge entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/14_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/14_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['salary_in_thousands_of_dollars'] >= 990]\ndf = df[[\"number_of_runs_batted_in\", \"number_of_errors\", \"number_of_strike_outs\", \"number_of_runs\", \"number_of_hits\", \"number_of_doubles\", \"batting_average\", \"number_of_triples\", \"on_base_percentage\", \"salary_in_thousands_of_dollars\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['salary_in_thousands_of_dollars'] <= 990]\ndf = df[[\"number_of_runs_batted_in\", \"number_of_errors\", \"number_of_strike_outs\", \"number_of_runs\", \"number_of_hits\", \"number_of_doubles\", \"batting_average\", \"number_of_triples\", \"on_base_percentage\", \"salary_in_thousands_of_dollars\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,48\n10/2/2014,86\n10/3/2014,107\n10/4/2014,26\n10/5/2014,90\n10/6/2014,99\n10/7/2014,81\n10/8/2014,86\n10/9/2014,71\n\nHeader and first few lines of CSV file 2:\nDate,Cumulative trips (since launch):,Total Annual Memberships Sold,Trips over the past 24-hours (midnight to 11:59pm)\n10/1/2014,13296973,124846,31197\n10/2/2014,13335259,124959,38286\n10/3/2014,13374215,125024,38956\n10/4/2014,13389303,125058,15088\n10/5/2014,13415550,125109,26247\n10/6/2014,13451201,125203,35651\n10/7/2014,13488132,125262,36931\n10/8/2014,13526002,125354,37870\n10/9/2014,13562429,125407,36427\n\nQuestion: Merge all rows in the two tables that the length of 'Date' is greater than 10, show the value of  Cumulative trips (since launch): and Date, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "Date,7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,48\n10/2/2014,86\n10/3/2014,107\n10/4/2014,26\n10/5/2014,90\n10/6/2014,99\n10/7/2014,81\n10/8/2014,86\n10/9/2014,71\n10/10/2014,78\n", "csv2_example": "Date,Cumulative trips (since launch):,Total Annual Memberships Sold,Trips over the past 24-hours (midnight to 11:59pm)\n10/1/2014,13296973,124846,31197\n10/2/2014,13335259,124959,38286\n10/3/2014,13374215,125024,38956\n10/4/2014,13389303,125058,15088\n10/5/2014,13415550,125109,26247\n10/6/2014,13451201,125203,35651\n10/7/2014,13488132,125262,36931\n10/8/2014,13526002,125354,37870\n10/9/2014,13562429,125407,36427\n10/10/2014,13597119,125464,34690\n", "csv1_path": "infiagent/merge_test/1_3_0.csv", "csv2_path": "infiagent/merge_test/1_3_1.csv", "instruction": "Merge all rows in the two tables that the length of 'Date' is greater than 10, show the value of  Cumulative trips (since launch): and Date, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/1_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/1_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Date'].str.len() > 10]\ndf = df[[\"Cumulative trips (since launch):\", \"Date\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Date'].str.len() > 10]\ndf = df[[\"Cumulative trips (since launch):\", \"Date\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntemp,wind_deg,pressure,description,city,Unnamed: 0,dist\n32.18000000000001,330.003,1010,moderate rain,Ravenna,0,8\n32.370000000000005,20.0,1010,moderate rain,Ravenna,1,8\n32.09000000000003,80.0,1008,moderate rain,Ravenna,5,8\n31.32000000000005,135.0,1007,moderate rain,Ravenna,8,8\n30.52000000000004,130.0,1007,moderate rain,Ravenna,9,8\n28.680000000000007,130.501,1008,moderate rain,Ravenna,11,8\n27.78000000000003,190.0,1007,moderate rain,Ravenna,12,8\n27.230000000000015,180.0,1007,moderate rain,Ravenna,13,8\n26.49000000000001,193.502,1007,moderate rain,Ravenna,14,8\n\nHeader and first few lines of CSV file 2:\nwind_speed,dt,Unnamed: 0\n2.11,1437730851,0\n5.1,1437741580,3\n3.1,1437745188,4\n2.06,1437759569,8\n1.5,1437763244,9\n1.5,1437766819,10\n2.11,1437770378,11\n1.5,1437774028,12\n0.5,1437777556,13\n\nQuestion: Combine the rows in the two tables where 'wind_deg' is greater than or equal to 180.0, with columns having the same name, and replace missing values with NaN.", "csv1_example": "temp,wind_deg,pressure,description,city,Unnamed: 0,dist\n32.18000000000001,330.003,1010,moderate rain,Ravenna,0,8\n32.370000000000005,20.0,1010,moderate rain,Ravenna,1,8\n32.09000000000003,80.0,1008,moderate rain,Ravenna,5,8\n31.32000000000005,135.0,1007,moderate rain,Ravenna,8,8\n30.52000000000004,130.0,1007,moderate rain,Ravenna,9,8\n28.680000000000007,130.501,1008,moderate rain,Ravenna,11,8\n27.78000000000003,190.0,1007,moderate rain,Ravenna,12,8\n27.230000000000015,180.0,1007,moderate rain,Ravenna,13,8\n26.49000000000001,193.502,1007,moderate rain,Ravenna,14,8\n25.890000000000043,193.502,1007,moderate rain,Ravenna,15,8\n", "csv2_example": "wind_speed,dt,Unnamed: 0\n2.11,1437730851,0\n5.1,1437741580,3\n3.1,1437745188,4\n2.06,1437759569,8\n1.5,1437763244,9\n1.5,1437766819,10\n2.11,1437770378,11\n1.5,1437774028,12\n0.5,1437777556,13\n2.36,1437781278,14\n", "csv1_path": "infiagent/merge_test/50_1_0.csv", "csv2_path": "infiagent/merge_test/50_1_1.csv", "instruction": "Combine the rows in the two tables where 'wind_deg' is greater than or equal to 180.0, with columns having the same name, and replace missing values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/50_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/50_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['wind_deg'] <= 180.0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['wind_deg'] >= 180.0]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNAME,ANNUAL_RT\nAKROFI,BERNARD,37407.0\nAaron,Keairah T,11310.0\nAaron,Patricia G,51862.0\nAaron,Petra L,64000.0\nAbaineh,Yohannes T,57900.0\nAbbey,Emmanuel,52000.0\nAbdal-Rahim,Naim A,40650.0\nAbdi,Ezekiel W,68847.0\nAbdul Adl,Attrice A,41194.0\n\nHeader and first few lines of CSV file 2:\nGross,NAME\n14387.3,AKROFI,BERNARD\n,Aaron,Keairah T\n,Aaron,Keontae E\n51771.01,Aaron,Patricia G\n3477.0,Abbey,Emmanuel\n44159.59,Abdal-Rahim,Naim A\n66496.24,Abdi,Ezekiel W\n46254.02,Abdul Adl,Attrice A\n32186.22,Abdul Saboor,Jamillah\n\nQuestion: Combine the rows from the two tables where 'ANNUAL_RT' is not equal to '11310.0' and the length of 'NAME' exceeds 15 characters, merging entries with the same column names while retaining only the merged portions that are successful.", "csv1_example": "NAME,ANNUAL_RT\nAKROFI,BERNARD,37407.0\nAaron,Keairah T,11310.0\nAaron,Patricia G,51862.0\nAaron,Petra L,64000.0\nAbaineh,Yohannes T,57900.0\nAbbey,Emmanuel,52000.0\nAbdal-Rahim,Naim A,40650.0\nAbdi,Ezekiel W,68847.0\nAbdul Adl,Attrice A,41194.0\nAbdul Saboor,Jamillah,31741.0\n", "csv2_example": "Gross,NAME\n14387.3,AKROFI,BERNARD\n,Aaron,Keairah T\n,Aaron,Keontae E\n51771.01,Aaron,Patricia G\n3477.0,Abbey,Emmanuel\n44159.59,Abdal-Rahim,Naim A\n66496.24,Abdi,Ezekiel W\n46254.02,Abdul Adl,Attrice A\n32186.22,Abdul Saboor,Jamillah\n,Abdul Wajid,Amani B\n", "csv1_path": "infiagent/merge_test/12_2_0.csv", "csv2_path": "infiagent/merge_test/12_2_1.csv", "instruction": "Combine the rows from the two tables where 'ANNUAL_RT' is not equal to '11310.0' and the length of 'NAME' exceeds 15 characters, merging entries with the same column names while retaining only the merged portions that are successful.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/12_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/12_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['ANNUAL_RT'] != 11310.0]\ndf = df[df['NAME'].str.len() > 15]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['ANNUAL_RT'] != '11310.0']\ndf = df[df['NAME'].str.len() > 15]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nBalance,Cards,Unnamed: 0\n903,3,2\n580,4,3\n964,3,4\n331,2,5\n1151,4,6\n203,2,7\n279,5,9\n0,3,12\n204,1,13\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,Age,Gender,Married,Limit,Rating,Income\n1,34,Male,Yes,3606,283,14.891\n3,71,Male,No,7075,514,104.593\n4,36,Female,No,9504,681,148.924\n5,68,Male,Yes,4897,357,55.882\n6,77,Male,No,8047,569,80.18\n7,37,Female,No,3388,259,20.996\n8,87,Male,No,7114,512,71.408\n9,66,Female,No,3300,266,15.125\n10,41,Female,Yes,6819,491,71.061\n\nQuestion: Combine all rows from the two tables, display the values of Cards and Unnamed: 0, merging by rows with the same column names, and fill in missing values with NAN.", "csv1_example": "Balance,Cards,Unnamed: 0\n903,3,2\n580,4,3\n964,3,4\n331,2,5\n1151,4,6\n203,2,7\n279,5,9\n0,3,12\n204,1,13\n1081,1,14\n", "csv2_example": "Unnamed: 0,Age,Gender,Married,Limit,Rating,Income\n1,34,Male,Yes,3606,283,14.891\n3,71,Male,No,7075,514,104.593\n4,36,Female,No,9504,681,148.924\n5,68,Male,Yes,4897,357,55.882\n6,77,Male,No,8047,569,80.18\n7,37,Female,No,3388,259,20.996\n8,87,Male,No,7114,512,71.408\n9,66,Female,No,3300,266,15.125\n10,41,Female,Yes,6819,491,71.061\n11,30,Male,Yes,8117,589,63.095\n", "csv1_path": "infiagent/merge_test/24_1_0.csv", "csv2_path": "infiagent/merge_test/24_1_1.csv", "instruction": "Combine all rows from the two tables, display the values of Cards and Unnamed: 0, merging by rows with the same column names, and fill in missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/24_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/24_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Cards\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Cards\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nSource,lat_org,Target\n0,16.960160000000002,1.0\n1,17.001061,0.0\n3,17.114487,2.0\n4,17.116356,2.0\n5,17.048482999999994,1.0\n6,17.032705,2.0\n7,17.059815,2.0\n9,16.889872,15.0\n10,17.030970999999994,1.0\n\nHeader and first few lines of CSV file 2:\nlng_org,Type,Source,lng_dest\n51.089356,Directed,1,51.048332\n51.102301,Directed,4,51.13283300000001\n51.085218,Directed,5,51.089356\n51.139829,Directed,6,51.13283300000001\n51.141671,Directed,8,51.089356\n51.143921,Directed,9,51.116737\n51.126526,Directed,12,51.089356\n51.053396,Directed,13,51.048332\n51.116737,Directed,15,51.13283300000001\n\nQuestion: Merge all rows in the two tables that the value of 'Source' is less than 102, merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "Source,lat_org,Target\n0,16.960160000000002,1.0\n1,17.001061,0.0\n3,17.114487,2.0\n4,17.116356,2.0\n5,17.048482999999994,1.0\n6,17.032705,2.0\n7,17.059815,2.0\n9,16.889872,15.0\n10,17.030970999999994,1.0\n11,17.057259,0.0\n", "csv2_example": "lng_org,Type,Source,lng_dest\n51.089356,Directed,1,51.048332\n51.102301,Directed,4,51.13283300000001\n51.085218,Directed,5,51.089356\n51.139829,Directed,6,51.13283300000001\n51.141671,Directed,8,51.089356\n51.143921,Directed,9,51.116737\n51.126526,Directed,12,51.089356\n51.053396,Directed,13,51.048332\n51.116737,Directed,15,51.13283300000001\n51.117021,Directed,16,51.089356\n", "csv1_path": "infiagent/merge_test/5_3_0.csv", "csv2_path": "infiagent/merge_test/5_3_1.csv", "instruction": "Merge all rows in the two tables that the value of 'Source' is less than 102, merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/5_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/5_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Source'] < 102]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Source'] < 102]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLow,Unnamed: 0,Date\n89.66,0,19-Jan-18\n89.66,1,18-Jan-18\n88.75,2,17-Jan-18\n88.01,3,16-Jan-18\n88.45,4,12-Jan-18\n87.86,7,9-Jan-18\n87.6,8,8-Jan-18\n87.43,9,5-Jan-18\n86.57,10,4-Jan-18\n\nHeader and first few lines of CSV file 2:\nClose,Open,Volume,Unnamed: 0\n90.0,90.14,36875013,0\n90.1,89.8,24159683,1\n90.14,89.08,25621164,2\n88.35,90.1,36599736,3\n89.6,88.67,24271531,4\n88.08,88.13,17808877,5\n88.22,88.65,19484317,7\n88.28,88.2,22113049,8\n86.35,86.06,26061439,11\n\nQuestion: Merge all rows in the two tables, show the value of  Open, Volume , Close  and Unnamed: 0, merging by entries with the same column name,  and fill in the blanks with NAN.", "csv1_example": "Low,Unnamed: 0,Date\n89.66,0,19-Jan-18\n89.66,1,18-Jan-18\n88.75,2,17-Jan-18\n88.01,3,16-Jan-18\n88.45,4,12-Jan-18\n87.86,7,9-Jan-18\n87.6,8,8-Jan-18\n87.43,9,5-Jan-18\n86.57,10,4-Jan-18\n85.97,11,3-Jan-18\n", "csv2_example": "Close,Open,Volume,Unnamed: 0\n90.0,90.14,36875013,0\n90.1,89.8,24159683,1\n90.14,89.08,25621164,2\n88.35,90.1,36599736,3\n89.6,88.67,24271531,4\n88.08,88.13,17808877,5\n88.22,88.65,19484317,7\n88.28,88.2,22113049,8\n86.35,86.06,26061439,11\n85.95,86.12,22483797,12\n", "csv1_path": "infiagent/merge_test/44_0_0.csv", "csv2_path": "infiagent/merge_test/44_0_1.csv", "instruction": "Merge all rows in the two tables, show the value of  Open, Volume , Close  and Unnamed: 0, merging by entries with the same column name,  and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/44_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/44_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Open\", \"Volume\", \"Close\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Open\", \"Volume\", \"Close\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nShucked weight,Height,Whole weight,Length\n0.2245,0.095,0.514,0.455\n0.0995,0.09,0.2255,0.35\n0.2565,0.135,0.677,0.53\n0.2155,0.125,0.516,0.44\n0.0895,0.08,0.205,0.33\n0.141,0.095,0.3515,0.425\n0.294,0.125,0.768,0.545\n0.2165,0.125,0.5095,0.475\n0.3145,0.15,0.8945,0.55\n\nHeader and first few lines of CSV file 2:\nLength,Viscera weight\n0.455,0.101\n0.35,0.0485\n0.53,0.1415\n0.44,0.114\n0.33,0.0395\n0.425,0.0775\n0.545,0.1495\n0.475,0.1125\n0.55,0.151\n\nQuestion: Combine rows from both tables where the 'Length' is greater than or equal to 0.46 and the 'Shucked weight' is greater than or equal to 0.194, merging by entries with the same column name, and fill in missing values with NaN.", "csv1_example": "Shucked weight,Height,Whole weight,Length\n0.2245,0.095,0.514,0.455\n0.0995,0.09,0.2255,0.35\n0.2565,0.135,0.677,0.53\n0.2155,0.125,0.516,0.44\n0.0895,0.08,0.205,0.33\n0.141,0.095,0.3515,0.425\n0.294,0.125,0.768,0.545\n0.2165,0.125,0.5095,0.475\n0.3145,0.15,0.8945,0.55\n0.194,0.14,0.6065,0.525\n", "csv2_example": "Length,Viscera weight\n0.455,0.101\n0.35,0.0485\n0.53,0.1415\n0.44,0.114\n0.33,0.0395\n0.425,0.0775\n0.545,0.1495\n0.475,0.1125\n0.55,0.151\n0.525,0.1475\n", "csv1_path": "infiagent/merge_test/8_2_0.csv", "csv2_path": "infiagent/merge_test/8_2_1.csv", "instruction": "Combine rows from both tables where the 'Length' is greater than or equal to 0.46 and the 'Shucked weight' is greater than or equal to 0.194, merging by entries with the same column name, and fill in missing values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/8_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/8_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Length'] <= 0.46]\ndf = df[df['Shucked weight'] <= 0.194]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Length'] >= 0.46]\ndf = df[df['Shucked weight'] >= 0.194]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nneu,Unnamed: 0\n0.733,0\n0.735,1\n0.847,2\n\nHeader and first few lines of CSV file 2:\nurlToImage,publishedAt,text,Unnamed: 0,author\nhttps://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,2018-08-29 01:22:02,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}},1,ABC News\nhttps://s.abcnews.com/images/Nightline/180827_ntl_az_hpMain_16x9_992.jpg,2018-08-28 02:31:59,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Junior Arizona senator: McCain 'put service... over and above self interest' Sen. Jeff Flake, R-Ariz., honors his late colleague Sen. John McCain, R-Ariz., on \"This Week.\" Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Junior Arizona senator: McCain 'put service... over and above self interest' Now Playing: Flags fly at half staff at US Capitol, tributes pour in for Sen. John McCain Now Playing: Louis C.K. performs first stand-up comedy set since admitting to sexual misconduct Now Playing: Behind the scenes with Ed Sheeran as he writes some of his hit songs Now Playing: When sharks attack: What you need to know to protect yourself Now Playing: How false eyelashes become a must-have, everyday accessory and a booming market Now Playing: The three Republicans vying to be the next senator from McCain's home state Now Playing: John McCain's lighter side, 'SNL' cameos, family man Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: What Aaron Hernandez' fiancee, lawyer think of his final notes before his suicide Now Playing: Back home in Staten Island with 'The Way I Are' singer Bebe Rexha Now Playing: Coach, kids rescued from Thai cave on the moment they were found: Part 1 Now Playing: After rescue from Thai cave, Coach, boys share message to the world: Part 2 Now Playing: South Africans grapple with land expropriation Now Playing: Inside Afghanistan: On the ground with US troops Now Playing: 'It's just like a feeling': Ariana Grande on how she knew Pete Davidson was the one Now Playing: Cohen pleads guilty, Manafort found guilty on 8 counts Now Playing: Body found in search for missing Iowa jogger Mollie Tibbetts Now Playing: Friend of woman allegedly killed by husband on why something 'didn't seem right' Now Playing: {{itm.title}},3,ABC News\nhttps://s.abcnews.com/images/Politics/kelli-ward-ap-171025_hpMain_5_16x9_992.jpg,2018-08-28 02:27:43,\n\nQuestion: Merge all rows in the two tables that the length of 'text' is greater than 4228, show the value of  publishedAt, text , urlToImage , author  and Unnamed: 0, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "neu,Unnamed: 0\n0.733,0\n0.735,1\n0.847,2\n0.762,3\n0.851,4\n0.719,5\n0.807,7\n0.81,8\n0.792,9\n0.848,10\n", "csv2_example": "urlToImage,publishedAt,text,Unnamed: 0,author\nhttps://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,2018-08-29 01:22:02,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}},1,ABC News\nhttps://s.abcnews.com/images/Nightline/180827_ntl_az_hpMain_16x9_992.jpg,2018-08-28 02:31:59,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Junior Arizona senator: McCain 'put service... over and above self interest' Sen. Jeff Flake, R-Ariz., honors his late colleague Sen. John McCain, R-Ariz., on \"This Week.\" Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Junior Arizona senator: McCain 'put service... over and above self interest' Now Playing: Flags fly at half staff at US Capitol, tributes pour in for Sen. John McCain Now Playing: Louis C.K. performs first stand-up comedy set since admitting to sexual misconduct Now Playing: Behind the scenes with Ed Sheeran as he writes some of his hit songs Now Playing: When sharks attack: What you need to know to protect yourself Now Playing: How false eyelashes become a must-have, everyday accessory and a booming market Now Playing: The three Republicans vying to be the next senator from McCain's home state Now Playing: John McCain's lighter side, 'SNL' cameos, family man Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: What Aaron Hernandez' fiancee, lawyer think of his final notes before his suicide Now Playing: Back home in Staten Island with 'The Way I Are' singer Bebe Rexha Now Playing: Coach, kids rescued from Thai cave on the moment they were found: Part 1 Now Playing: After rescue from Thai cave, Coach, boys share message to the world: Part 2 Now Playing: South Africans grapple with land expropriation Now Playing: Inside Afghanistan: On the ground with US troops Now Playing: 'It's just like a feeling': Ariana Grande on how she knew Pete Davidson was the one Now Playing: Cohen pleads guilty, Manafort found guilty on 8 counts Now Playing: Body found in search for missing Iowa jogger Mollie Tibbetts Now Playing: Friend of woman allegedly killed by husband on why something 'didn't seem right' Now Playing: {{itm.title}},3,ABC News\nhttps://s.abcnews.com/images/Politics/kelli-ward-ap-171025_hpMain_5_16x9_992.jpg,2018-08-28 02:27:43,\nOne of the Republicans running for Senate in Arizona spent time on the last day before the primary election to parse through whether or not she meant to offend people in her recent social media posts, one of which included a suggestion there was a narrative at play when Sen. John McCain announced he was stopping his cancer treatment.\n \nKelli Ward, one of the three candidates looking to fill Sen. Jeff Flakeâs seat, had a long history of being critical of Sen. John McCain before he passed away on Saturday, and now sheâs caught up in a battle of semantics about her comments about the late senator in the two days following his death.\n \nIt all started on Friday, when the McCain family announced that he was ending treatment for brain cancer. That same day, Ward launched a two-day campaign bus tour. One of her aides later wrote on Facebook that the timing of the McCain announcement was planned to hurt Wardâs campaign, and Ward then commented on that post, writing: \"I think they wanted to have a particular narrative that they hope is negative to me.\"\n \nThe post and her comment, which were posted hours before McCainâs death was announced, have since been deleted, but screengrabs of the post began circulating.\n \nWard has placed the blame on the media.\n \n \n \nAdding to this, another of Ward's social media posts caused controversy today when she tweeted that \"political correctness is like a cancer!\"\n \n Political correctness is like a cancer! \n \nAt a news conference this afternoon, when asked to address the criticism she is receiving over the comment, especially in light of McCainâs fatal brain cancer, she said \"They have nothing to do with each other really, but political correctness is like a cancer.\"\n \n \n \nWhen asked if the \"political correctness\" tweet was intended to troll social media users, she said \"I canât see how it would be trolling, but I can see how you might misunderstand it.\"\n \n .@kelliwardaz defends her earlier tweet saying that âpolitical correctness is like a cancerâ pic.twitter.com/Ob6LcWr3cX \nShe also debated the fact that she previously called on McCain to step down in light of his glioblastoma diagnosis, with her argument being that since she said anyone who is unable to serve - be it for health or financial or personal reasons - should step down, and McCain was included in that group, she didn't see that as calling on him to step down.\n \n \n \nShe also revisited the Facebook comment controversy, saying she wanted to \"set the record straight.\"\n \n\"The media quickly ran with a false narrative that I was being insensitive to Senator McCain at a time when he found himself in dire straights. To be clear, my comments were in no way directed at Senator McCain or his family or his team. My comments were in reference to the media, and a lot of you know Iâve been kinda critical of the media â not quite as critical as the president has been but Iâve been critical of you as well â and that the media sometimes might hope for a narrative that might hinder the momentum of our campaign. And that momentum is huge,\" she said.\n \n \n \n\"Our differences were purely politicalâ¦ the media they tried to make them out as personal,\" she said of she and McCain.\n \n \n \nAt one point, someone asked if she were to win Flake's Senate seat, and if Senator Chuck Schumer's proposal to rename the Senate building after McCain passes, then she could be working in a building named after McCain. In response, she seemed to shrug.\n \n\"I mean you know thatâs going to be up to whoeverâs there and weâll see what happens. You know I have to fly out of the McCain terminal sometimes here so...\" she said, referencing one of the Phoenix airport terminals named after him.,4,Meghan Keneally\nhttps://s.abcnews.com/images/Technology/WireAP_86828a77b1c1446e96826da60a57aed8_16x9_992.jpg,2018-08-27 10:26:19,\n \nFacebook said Monday that it is banning Myanmar's powerful military chief and 19 other individuals and organizations from its site to prevent the spread of hate and misinformation. \nThe social media giant was heavily criticized for permitting itself to be used to inflame ethnic and religious conflict in the country, particularly against minority Rohingya Muslims. It has been accused of being lax in fighting online misinformation and manipulation in many countries, but Myanmar is one where it has been most closely tied to deadly violence. \nSome 700,000 Rohingya have fled from Myanmar's western state of Rakhine over the past year in response to a brutal counterinsurgency campaign by the military, which has been accused of massive human rights violations. Critics accuse the military of carrying out ethnic cleansing, or even genocide, an allegation denied by the government, which says it was responding to attacks on security forces. \nFacebook said it also targeted pages and accounts that pretended to provide independent news and opinion, while covertly promoting messages of Myanmar's military. It said it was deleting 18 Facebook accounts, one Instagram account and 52 Facebook pages. \nA separate report by investigators working for the U.N.'s top human rights body, released Monday, charged that \"Facebook has been a useful instrument for those seeking to spread hate, in a context where for most users Facebook is the internet.\" \n\"Although improved in recent months, Facebook's response has been slow and ineffective,\" said the report by the Fact-Finding Mission on Myanmar, authorized by the U.N. Human Rights Council. \"The extent to which Facebook posts and messages have led to real-world discrimination and violence must be independently and thoroughly examined.\" \nFour high-ranking officers and two military units targeted by Facebook were also put on a U.S. government blacklist earlier this month for human rights abuses. The sanctions block any property they own within the U.S. and prohibit U.S. citizens from engaging in transactions with them. The U.S. already maintains restrictions on visas, arms sales and assistance to Myanmar's military. In June, the EU imposed similar sanctions on seven senior army and police officers, all of whom are on Facebook's blacklist. \nSix officers on Facebook's list were also named in the U.N. human rights report, which said Myanmar's top leaders should be prosecuted for genocide. Those it recommended as \"priority subjects for investigation and prosecution\" included top commander Senior Gen. Min Aung Hlaing. \nIn a statement, Facebook referred to the U.N. report, which it said \"found evidence that many of these individuals and organizations committed or enabled serious human rights abuses in the country. And we want to prevent them from using our service to further inflame ethnic and religious tensions.\" Discrimination against the Rohingya ran deep and wide even before the spread of Facebook. \nFacebook has been under pressure for several months to take action on the problem, especially after civic and rights groups in Myanmar said in April that it had failed to adequately act against online hate speech that incited violence against the country's Muslim minorities, neglecting to effectively enforce its own rules. \n\"The ethnic violence in Myanmar has been truly horrific,\" Facebook said in its statement. \"While we were too slow to act, we're now making progress â with better technology to identify hate speech, improved reporting tools, and more people to review content.\" \n\"We continue to work to prevent the misuse of Facebook in Myanmar â including through the independent human rights impact assessment we commissioned earlier in the year. This is a huge responsibility given so many people there rely on Facebook for information â more so than in almost any other country given the nascent state of the news media and the recent rapid adoption of mobile phones. It's why we're so determined to do better in the future.\" \nYangon-based political analyst David Mathieson said that Facebook's action, together with the damning U.N. report, force Myanmar's military brass \"into an isolation they're not going to like.\" \n\"They have to find alternative ways to communicate with the Myanmar population, because Facebook really is the internet for many people here. And Facebook just excommunicated the commander in chief from the worldwide web,\" he told The Associated Press.,6,The Associated Press\nhttps://s.abcnews.com/images/US/shawn-richard-christy-ht-001-jpo-180823_hpMain_16x9_992.jpg,2018-08-23 20:39:58,\nA Pennsylvania man who threatened to kill President Donald Trump back in June is still on the run and was last seen in Cumberland, Maryland, earlier this week in a stolen truck, the U.S. Marshals Service said.\n \nA warrant was issued June 19 for 27-year-old Shawn Richard Christy and he remains wanted by federal authorities for threatening other elected officials as well.\n \nAuthorities have not been able to locate Christy since the warrant was issued. He drew federal authorities attention with his remarks about the president on Facebook and his previous behavior.\n \nAccording to court documents, Christy, in a threatening post on Facebook to Northampton County, Pennsylvania District Attorney John Morganelli, wrote: âKeep it up Morganelli, I promise Iâll put a bullet in your head as soon as I put one in the head of President Donald J. Trump.â\n \nSupervisory Deputy U.S. Marshal Robert Clark told ABC News the stolen red pickup truck was recovered Tuesday, in Tamaqua, Pennsylvania, near Christy's hometown of McAdoo.\n \nChristy is considered by the U.S. Marshals, FBI and U.S. Secret Service to be armed and dangerous.\n \nAuthorities say Christy has a distinct lisp and a barbed wire cross tattoo on his upper right arm. He also, according to authorities, considers himself a survivalist.\n \nChristy also said that he was going to use \"lethal force\" on any law enforcement attempting to detain him. He made the comments between June 3-12.\n \nThe reward for any information is now up to $20,000 and the three agencies are asking for the public's help with any information leading to Christy.\n \nHe is also wanted in Pennsylvania for burglary, probation violation and failure to appear for an aggravated assault case.\n \n\"Persons having information should contact the U.S. Marshals at 1-877-Wanted-2 (1-877-926-8332) or the FBI at 215-418-4000,\" a release from the Marshals said.\n \n\"Individuals should not attempt to arrest Christy themselves,\" the release said.\n \n \n \nThis is not the only time Christy has made threats against a public official.\n \nIn an interview with Newsweek, Shawn Christy's father, Craig, said that his son explained to him how he hid from law enforcement and noted that he has various guns on his possession that he had taken from the home.\n \nLast year, according to the Associated Press he swung a stick at the McAdoo mayor over a dispute about snow plowing.\n \nHe's also had problems with former Alaska governor and vice presidential candidate Sarah Palin, who filed a restraining order against then 18-year-old Christy in 2010.\n \nA year later he pleaded guilty to making harassing phone calls to Palin, her family members and lawyers.\n \n\"This is one of the stalkers who has tormented my family for years. Threatening my kids and my parents, following my daughters, moving to Alaska, then following Bristol to Texas to more aggressively physically invade and intimidate... itâs been a hellish ride with these stalkers... as weâve informed the FBI and law enforcement for years,\" Palin said in a Facebook post August 20.\n \n\"Finally, perhaps, in this particular case with one of the stalkers - Shawn Christy (having recently threatened the President) justice may prevail.\"\n \nABC News' Jack Date contributed to this report.,10,Luke Barr\nhttps://s.abcnews.com/images/Politics/facebook-headquarters-01-as-gty-180822_hpMain_16x9_992.jpg,2018-08-23 19:21:31,\nIn the 2016 presidential election Facebook and other social media platforms were utilized by political campaigns and interest groups to run advertisements with very few rules regulating their disclosure and little public information about their source.\n \nThe lack of knowledge about these advertisements underscored the ease with which political actors, both foreign and domestic, were able to manipulate platforms like Facebook to disseminate misleading and often false information to Facebook users in the United States and around the world.\n \nThe spread of misinformation meant to foment political discord and stir unrest continues to be a concern less than three months out from a consequential midterm election. Just this week Facebook announced it removed \"652 pages, groups and accounts for coordinated inauthentic behavior that originated in Iran and targeted people across multiple internet services in the Middle East, Latin America, UK and US.\"\n \nFacebook also said it \"removed multiple pages, groups and accounts\" linked to \"inauthentic behavior\" on its platforms, including actions that originated in Russia.\n \n \n \nAside from the threat from nefarious actors, Facebook's influence as a medium through which American politicians and campaigns communicate with voters has skyrocketed in the last decade. But exactly how that influence and power is being yielded by politicians, campaigns and outside groups from all sides of the political spectrum still remains somewhat of a mystery due to the lack of rules governing the disclosure of that information.\n \n \n \nIn order to better understand the tactics and strategies of all political actors on Facebook, ABC News is partnering with ProPublica, a nonprofit, award-winning news organization that has built a tool to collect information and data on political ads that are being targeted to users of the social media giant.\n \nProPublica's tool is a simple, free browser plugin on a Firefox or Google Chrome web browser, that Facebook users can opt into and report political advertisements they see on the site. Those advertisements will then be collected by ProPublica in a public database to track and analyze advertisements on Facebook.\n \nABC News and other news organizations will then use that database and information to better understand and report on how political actors are utilizing one of the most powerful communication mediums on the planet.\n \nThus far the tool has gathered almost 60,000 individual advertisements from over 12,500 contributors, according to ProPublica.\n \nProPublica clearly states: \"The tool doesn't collect your personal information,\" and privacy controls ensure that the information that is being collected relates only to the advertisement that appeared on the site.\n \n \n \nEarlier this year Facebook announced steps to make the source of political advertisements on the site more transparent and understandable, but has still not given news organizations or academic institutions full and total access to all of the data behind what goes into purchasing and targeting those ads.\n \nThe site has created its own archive of political advertisements that allows users to obtain \"more information about some of the ads they see and the advertisers who are funding them,\" according to Facebook. But that archive remains an incomplete picture of how political advertisements are tailored to certain users because it does not show who was targeted with the ad.\n \nBy utilizing ProPublica's tool to track advertisements, Facebook users can more easily report political advertising, giving news organizations more data to gather and a greater ability to understand, analyze, fact-check and investigate political communication on the site.\n \nAlready the tool has helped locate political ads that were actually scams and malware, political ads that were ignore federal election rules by not disclosing who funded them, and uncovered a fake Facebook page claiming to belong to Special Counsel Robert Mueller with more than 100,000 followers.\n \nThe midterms are fast approaching, and ProPublica's tool will be a key asset in understanding how politicians, their campaigns, and powerful outside interest groups are using the medium to communicate with voters.\n \n \n,12,John Verhovek\n,2018-08-23 13:44:38,\n \nGoogle announced Thursday it had disabled dozens of YouTube channels and other accounts linked to a state-run Iranian broadcaster for a political influence campaign. \nThe security firm FireEye, which alerted tech companies to some of the suspicious activity, said in a report this week that the overall operation originates from Iran and promotes Iranian interests to audiences in the U.S. and elsewhere. \nGoogle said its own forensic research shows the accounts were set up by people associated with the state-run Islamic Republic of Iran Broadcasting, or IRIB. \nThe broadcaster didn't immediately respond to an emailed request for comment Thursday. FireEye said it's a sign that it's no longer just Russia conducting disguised political influence campaigns. \nOn Tuesday, Facebook â which also works with FireEye â revealed that it had removed 652 suspicious pages, groups and accounts linked to Russia and Iran. Twitter made a similar announcement shortly thereafter. \nTech companies have become much more proactive about sleuthing out and dealing with political influence campaigns since last year, when Facebook, Google and Twitter acknowledged allowing Russian agents to spread propaganda on their networks during the 2016 presidential campaign. \nSeveral are going further, offering specific help to protect U.S. political candidates from Russian hackers and other bad actors ahead of the 2018 midterm elections. In so doing, they're confronting another question: Could that free help count as an illegal campaign contribution? \nMicrosoft, for instance, has gone so far as to request an advisory opinion from the Federal Election Commission to make sure its new free package of online security protections for \"election-sensitive\" customers doesn't count as an in-kind campaign contribution. Companies are typically prohibited from contributing to federal candidates and political committees under federal law. \nMicrosoft said this week it's offering its AccountGuard service on a nonpartisan basis to federal, state and local candidates, party committees and certain nonprofit groups. The company told FEC it might also work with other tech firms such as Facebook and Twitter on coordinated election security efforts, though no agreements have been made. \nGoogle last year also launched what it calls its Advanced Protection Program, which uses security keys to protect high-risk potential hacking targets such as politicians, as well as journalists and business leaders. \nAt least one prominent security expert believes it may be too late to protect November's midterms from further interference. Alex Stamos, who stepped down as Facebook's security chief last week, said in an online essay that U.S. officials haven't taken the threats seriously enough. \nHe cited Microsoft's revelation early this week that it discovered efforts by a hacking group tied to the Russian government to spoof websites belonging to the U.S. Senate and two conservative institutions. Such fake websites have previously been used by the group known as Fancy Bear to trick targeted victims into letting their computers be infiltrated. \nStamos said that \"In some ways, the United States has broadcast to the world that it doesn't take these issues seriously and that any perpetrators of information warfare against the West will get, at most, a slap on the wrist.\" \nHe said \"this failure has left the U.S. unprepared to protect the 2018 elections,\" though there's \"still a chance to defend American democracy in 2020.\" \nObtaining the FEC's opinion could take Microsoft a few months, but the company said that won't stop it from moving ahead with the service immediately. Microsoft said it believes there's precedent for charging political and non-political customers different rates. \nThe midterm election is on Nov. 6, though many states have already held their primaries. \nCompany lawyers told the FEC that along with trying to help democracy, Microsoft has a \"compelling business interest in maintaining its brand reputation\" amid continued public focus on Russian efforts to influence this year's election. They said Microsoft's reputation would suffer if hackers breached Microsoft accounts belonging to election-sensitive customers.,13,The Associated Press\n,2018-08-23 10:40:41,\n \nPolice acting on a complaint by Thailand's ruling junta have charged the leaders of a new political party that strongly opposes military rule with violating the computer crime law, which could result in five-year prison terms. \nFuture Forward party leader Thanathorn Juangroongruangkit and two other senior party members were charged by police this week with violating a section of the law that makes it a crime to transmit false information or information that damages the country's stability, group spokeswoman Pannika Wanich said Thursday. \nShe said the charges involve a June 29 Facebook live video posted on the pages of Thanathorn and the party, but did not describe how it may have violated the law. She said police had issued summonses in the past few days for the three party members to turn themselves in on Friday, but they had busy schedules and would ask for the date to be postponed to the middle of next month. \n\"And since all three have now officially become defendants in the case, our legal team needs some time to prepare,\" she said. \nThe Future Forward party is a new grouping featuring young politicians and is viewed as offering a real alternative to the ruling junta and its allies in upcoming elections. \nThailand's military government has kept a tight lid on dissent since it seized power from an elected government in a 2014 coup. It has promised to hold elections by early next year in a contest its critics charge will be set up to ensure it continues its influence over the country's administration. \nThe junta has already pushed back several promised election dates. Prime Minister and junta leader Prayuth Chan-ocha said Tuesday that the next polls are still scheduled for Feb. 24, though he did not entirely rule out another delay. \nPannika said a junta official had filed a police complaint against Thanathorn and his colleagues and had provided a brief explanation of the charges to local media. \nThe country's biggest newspaper, Thai Rath, said Col. Burin Thongprapai, a legal officer for the junta, stated last month that he filed the charges because in the Facebook live broadcast Thanathorn had made accusations against the junta and twisted facts in a manner that amounted to an attack on Thailand's justice system.,14,The Associated Press\n,2018-08-23 09:41:21,\n \nFacebook has pulled one of its own products from Apple's app store because it didn't want to stop tracking what people were doing on their iPhones. Facebook also banned a quiz app from its social network for possible privacy intrusions on about 4 million users. \nThe twin developments come as Facebook is under intense scrutiny over privacy following the Cambridge Analytica scandal earlier this year. Allegations that the political consultancy used personal information harvested from 87 million Facebook accounts have dented Facebook's reputation. \nSince the scandal broke, Facebook has investigated thousands of apps and suspended more than 400 of them over data-sharing concerns. \nThe social media company said late Wednesday that it took action against the myPersonality quiz app, saying that its creators refused an inspection. But even as Facebook did that, it found its own Onavo Protect security app at odds with Apple's tighter rules for applications. \nOnavo Protect is a virtual-private network service aimed at helping users secure their personal information over public Wi-Fi networks. The app also alerts users when other apps use too much data. \nSince acquiring Onavo in 2013, Facebook has used it to track what apps people were using on phones. This surveillance helped Facebook detect trendy services, tipping off the company to startups it might want to buy and areas it might want to work on for upcoming features. \nFacebook said in a statement that it has \"always been clear when people download Onavo about the information that is collected and how it is used.\" \nBut Onavo fell out of compliance with Apple's app-store guidelines after they were tightened two months ago to protect the reservoir of personal information that people keep on their iPhones and iPads. \nApple's revised guidelines require apps to get users' express consent before recording and logging their activity on a device. According to Apple, the new rules also \"made it explicitly clear that apps should not collect information about which other apps are installed on a user's device for the purposes of analytics or advertising/marketing.\" \nFacebook will still be able to deploy Onavo on devices powered by Google's Android software. \nOnavo's ouster from Apple's app store widens the rift between two of the world's most popular companies. \nApple CEO Tim Cook has been outspoken in his belief that Facebook does a shoddy job of protecting its 2.2 billion users' privacy â something that he has framed as \"a fundamental human right.\"  \nCook sharpened his criticism following the Cambridge Analytica scandal. He emphasized that Apple would never be caught in the same situation as Facebook because it doesn't collect information about its customers to sell advertising. Facebook CEO Mark Zuckerberg fired back in a separate interview and called Cook's remarks \"extremely glib.\" Zuckerberg implied that Apple caters primarily to rich people with a line of products that includes the $1,000 iPhone X. \nLate Wednesday, Facebook said it moved to ban the myPersonality app after it found user information was shared with researchers and companies \"with only limited protections in place.\" The company said it would notify the app's users that their data may have been misused. \nIt said myPersonality was \"mainly active\" prior to 2012. Though Facebook has tightened its rules since then, it is only now reviewing those older apps following the Cambridge Analytica scandal. \nThe app was created in 2007 by researcher David Stillwell and allowed users to take a personality questionnaire and get feedback on the results. \n\"There was no misuse of personal data,\" Stillwell said in a statement, adding that \"this ban appears to be purely cosmetic.\" Stillwell said users gave their consent and the app's data was fully anonymized before it was used for academic research. He also rejected Facebook's assertion that he refused to submit to an audit.,15,The Associated Press\n,2018-08-22 20:07:08,\n \nFacebook, Twitter and Google routinely squabble for users, engineers and advertising money. Yet it makes sense for these tech giants to work together on security threats, elections meddling and other common ills. \nSuch cooperation was evident Tuesday when Facebook announced that it had removed 652 suspicious pages, groups and accounts linked to Russia and Iran. This was followed by similar news from Twitter. On Monday, meanwhile, Microsoft reported a new Russian effort to impersonate conservative U.S. websites, potentially as part of an espionage campaign. \nCooperation makes it easier for tech companies to combat fraudulent use of their services. It also makes them look good in the eyes of their users and regulators by showing that they take the threats seriously enough to set aside competitive differences. \nThey have little other choice if they want to avoid regulation and stay ahead of â or just keep up with â the malicious actors, who are getting smarter and smarter at evading the tech companies' controls. \nCase in point: While Facebook said there was no evidence that Russian and Iranian actors cooperated with each other in the latest efforts to create fake accounts to mislead users, the company said their tactics were similar. In other words, if the bad guys are learning from each other, the companies fighting them would need to do the same. \nFacebook has significantly stepped up policing of its services since last year, when it acknowledged that Russian agents successfully used Facebook to run political influence operations aimed at swaying the 2016 presidential election. \nOther social media companies have done likewise and continue to turn up fresh evidence of political disinformation campaigns. While some of the 2016 disruptions seemed to support certain candidates, more recent campaigns appear aimed at sowing discord and driving people to more extreme sides of the political stage. \nTech companies already share information to fight terrorism, child pornography, malware and spam. They are now adding global political threats from nation-states. In congressional hearings earlier this year, Facebook General Counsel Colin Stretch said Facebook, Twitter and Google have a \"long history\" of working together on such threats. He expressed hope that sharing information becomes \"industry standard practice.\" \nUnderstanding the threat requires understanding how the malicious actors communicate, operate and move among various services, Facebook said in a blog post on Tuesday. \"To help gather this information, we often share intelligence with other companies once we have a basic grasp of what's happening,\" the company wrote. \nEven with all the cooperation, disagreements exist. The companies don't always agree on when and how to go public with threats they uncover, for example. And while critics have called for a formal industry body to address issues such as elections meddling, misinformation and hate speech on social networks, no such broad-reaching organization exists.Â  \nThe closest is the Cybersecurity Tech Accord, which Microsoft, Facebook and other companies formed to protect businesses and users from internet crime. But bigshots such as Google and Twitter were noticeably missing. (Those companies did not respond to messages Wednesday asking if they have joined since).Â  \nNonetheless, cooperation has helped other industries stave off regulation. For example, the movie industry banded together to develop its own ratings system in the 1960s to ward off government censorship. \nJeff Bardin, chief information officer at the security firm Treadstone 71, said cooperation is one way to combat fake accounts without imposing tighter verification when users sign up. Of course, if Facebook started asking potential members for a government-issued ID and a home address, it would drive people away. \n\"There is no way they will do that upfront,\" he said. So, what's left is to continue to play the cat-and-mouse game, catching and removing the enemy and then learning its new tactics as it changes them.,16,The Associated Press\n", "csv1_path": "infiagent/merge_test/33_2_0.csv", "csv2_path": "infiagent/merge_test/33_2_1.csv", "instruction": "Merge all rows in the two tables that the length of 'text' is greater than 4228, show the value of  publishedAt, text , urlToImage , author  and Unnamed: 0, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/33_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/33_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['text'].str.len() > 4228]\ndf = df[[\"publishedAt\", \"text\", \"urlToImage\", \"author\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['text'].str.len() > 4228]\ndf = df[[\"publishedAt\", \"text\", \"urlToImage\", \"author\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nyear\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n\nHeader and first few lines of CSV file 2:\nyear,continent,country\n1952,asia,afghanistan\n1957,asia,afghanistan\n1962,asia,afghanistan\n1967,asia,afghanistan\n1972,asia,afghanistan\n1977,asia,afghanistan\n1987,asia,afghanistan\n1992,asia,afghanistan\n1997,asia,afghanistan\n\nQuestion: Merge all rows in the two tables that the value of 'year' is not greater than 1982, show the value of  country and year, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "year\n1952\n1957\n1962\n1967\n1972\n1977\n1982\n1987\n1992\n1997\n", "csv2_example": "year,continent,country\n1952,asia,afghanistan\n1957,asia,afghanistan\n1962,asia,afghanistan\n1967,asia,afghanistan\n1972,asia,afghanistan\n1977,asia,afghanistan\n1987,asia,afghanistan\n1992,asia,afghanistan\n1997,asia,afghanistan\n2002,asia,afghanistan\n", "csv1_path": "infiagent/merge_test/35_3_0.csv", "csv2_path": "infiagent/merge_test/35_3_1.csv", "instruction": "Merge all rows in the two tables that the value of 'year' is not greater than 1982, show the value of  country and year, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/35_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/35_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['year'] >= 1982]\ndf = df[[\"country\", \"year\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['year'] >= 1982]\ndf = df[[\"country\", \"year\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nTrust (Government Corruption),Dystopia Residual,Happiness Score,Country\n0.41978,2.51738,7.587,Switzerland\n0.14145,2.70201,7.561,Iceland\n0.36503,2.46531,7.522,Norway\n0.32957,2.45176,7.427,Canada\n0.41372,2.61955,7.406,Finland\n0.43844,2.37119,7.364,Sweden\n0.42922,2.26425,7.286,New Zealand\n0.35637,2.26646,7.284,Australia\n0.07785,3.08854,7.278,Israel\n\nHeader and first few lines of CSV file 2:\nHealth (Life Expectancy),Standard Error,Happiness Rank,Freedom,Economy (GDP per Capita),Country\n0.94143,0.03411,1,0.66557,1.39651,Switzerland\n0.87464,0.03328,3,0.64938,1.32548,Denmark\n0.88521,0.0388,4,0.66973,1.459,Norway\n0.90563,0.03553,5,0.63297,1.32629,Canada\n0.89284,0.02799,7,0.61576,1.32944,Netherlands\n0.91387,0.0347,11,0.41319,1.22857,Israel\n0.86027,0.04454,12,0.63376,0.95578,Costa Rica\n0.89042,0.03751,13,0.62433,1.33723,Austria\n0.81444,0.04176,14,0.48181,1.02054,Mexico\n\nQuestion: Combine the data from both tables, preserving the original column names, and consolidate the rows based on matching column names. Display the following values: Happiness Rank, Happiness Score, Economy (GDP per Capita), Freedom, Standard Error, Trust (Government Corruption), Health (Life Expectancy), Dystopia Residual, and Country. Fill any missing values with NaN.", "csv1_example": "Trust (Government Corruption),Dystopia Residual,Happiness Score,Country\n0.41978,2.51738,7.587,Switzerland\n0.14145,2.70201,7.561,Iceland\n0.36503,2.46531,7.522,Norway\n0.32957,2.45176,7.427,Canada\n0.41372,2.61955,7.406,Finland\n0.43844,2.37119,7.364,Sweden\n0.42922,2.26425,7.286,New Zealand\n0.35637,2.26646,7.284,Australia\n0.07785,3.08854,7.278,Israel\n0.10583,3.17728,7.226,Costa Rica\n", "csv2_example": "Health (Life Expectancy),Standard Error,Happiness Rank,Freedom,Economy (GDP per Capita),Country\n0.94143,0.03411,1,0.66557,1.39651,Switzerland\n0.87464,0.03328,3,0.64938,1.32548,Denmark\n0.88521,0.0388,4,0.66973,1.459,Norway\n0.90563,0.03553,5,0.63297,1.32629,Canada\n0.89284,0.02799,7,0.61576,1.32944,Netherlands\n0.91387,0.0347,11,0.41319,1.22857,Israel\n0.86027,0.04454,12,0.63376,0.95578,Costa Rica\n0.89042,0.03751,13,0.62433,1.33723,Austria\n0.81444,0.04176,14,0.48181,1.02054,Mexico\n0.86179,0.03839,15,0.54604,1.39451,United States\n", "csv1_path": "infiagent/merge_test/3_0_0.csv", "csv2_path": "infiagent/merge_test/3_0_1.csv", "instruction": "Combine the data from both tables, preserving the original column names, and consolidate the rows based on matching column names. Display the following values: Happiness Rank, Happiness Score, Economy (GDP per Capita), Freedom, Standard Error, Trust (Government Corruption), Health (Life Expectancy), Dystopia Residual, and Country. Fill any missing values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/3_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/3_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Happiness Rank\", \"Happiness Score\", \"Economy (GDP per Capita)\", \"Freedom\", \"Standard Error\", \"Trust (Government Corruption)\", \"Health (Life Expectancy)\", \"Dystopia Residual\", \"Country\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Happiness Rank\", \"Happiness Score\", \"Economy (GDP per Capita)\", \"Freedom\", \"Standard Error\", \"Trust (Government Corruption)\", \"Health (Life Expectancy)\", \"Dystopia Residual\", \"Country\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ngdpPercap_1977,gdpPercap_2007,country,gdpPercap_2002,gdpPercap_1992,gdpPercap_1962,gdpPercap_1972\n786.11336,974.5803384,Afghanistan,726.7340548,649.3413952,853.10071,739.9811058\n659.8772322,1391.253792,Bangladesh,1136.39043,837.8101643,686.3415538,630.2336265\n524.9721832,1713.778686,Cambodia,896.2260153,682.3031755,496.9136476,421.6240257\n813.337323,2452.210407,India,1746.769454,1164.406809,658.3471509,724.032527\n11888.59508,11605.71449,Iran,9240.761975,7235.653188,4187.329802,9613.818607\n14688.23507,4471.061906,Iraq,4390.717312,3745.640687,8341.737815,9576.037596\n13306.61921,25523.2771,Israel,21905.59514,18051.52254,7105.630706,12786.93223\n4106.301249,1593.06548,Korea Dem. Rep.,1646.758151,3726.063507,1621.693598,3701.621503\n4657.22102,23348.13973,Korea Rep.,19233.98818,12104.27872,1536.344387,3030.87665\n\nHeader and first few lines of CSV file 2:\ngdpPercap_1957,country,gdpPercap_1997\n820.8530296,Afghanistan,635.341351\n11635.79945,Bahrain,20292.01679\n661.6374577,Bangladesh,972.7700352\n434.0383364,Cambodia,734.28517\n575.9870009,China,2289.234136\n3629.076457,Hong Kong China,28377.63219\n590.061996,India,1458.817442\n858.9002707,Indonesia,3119.335603\n3290.257643,Iran,8263.590301\n\nQuestion: Combine the data from the two tables, displaying the values of gdpPercap_1957, gdpPercap_1992, gdpPercap_2007, gdpPercap_2002, gdpPercap_1972, and country, merging rows with the same column name, and filling in any missing values with NAN.", "csv1_example": "gdpPercap_1977,gdpPercap_2007,country,gdpPercap_2002,gdpPercap_1992,gdpPercap_1962,gdpPercap_1972\n786.11336,974.5803384,Afghanistan,726.7340548,649.3413952,853.10071,739.9811058\n659.8772322,1391.253792,Bangladesh,1136.39043,837.8101643,686.3415538,630.2336265\n524.9721832,1713.778686,Cambodia,896.2260153,682.3031755,496.9136476,421.6240257\n813.337323,2452.210407,India,1746.769454,1164.406809,658.3471509,724.032527\n11888.59508,11605.71449,Iran,9240.761975,7235.653188,4187.329802,9613.818607\n14688.23507,4471.061906,Iraq,4390.717312,3745.640687,8341.737815,9576.037596\n13306.61921,25523.2771,Israel,21905.59514,18051.52254,7105.630706,12786.93223\n4106.301249,1593.06548,Korea Dem. Rep.,1646.758151,3726.063507,1621.693598,3701.621503\n4657.22102,23348.13973,Korea Rep.,19233.98818,12104.27872,1536.344387,3030.87665\n59265.47714,47306.98978,Kuwait,35110.10566,34932.91959,95458.11176,109347.867\n", "csv2_example": "gdpPercap_1957,country,gdpPercap_1997\n820.8530296,Afghanistan,635.341351\n11635.79945,Bahrain,20292.01679\n661.6374577,Bangladesh,972.7700352\n434.0383364,Cambodia,734.28517\n575.9870009,China,2289.234136\n3629.076457,Hong Kong China,28377.63219\n590.061996,India,1458.817442\n858.9002707,Indonesia,3119.335603\n3290.257643,Iran,8263.590301\n6229.333562,Iraq,3076.239795\n", "csv1_path": "infiagent/merge_test/36_1_0.csv", "csv2_path": "infiagent/merge_test/36_1_1.csv", "instruction": "Combine the data from the two tables, displaying the values of gdpPercap_1957, gdpPercap_1992, gdpPercap_2007, gdpPercap_2002, gdpPercap_1972, and country, merging rows with the same column name, and filling in any missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/36_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/36_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"gdpPercap_1957\", \"gdpPercap_1992\", \"gdpPercap_2007\", \"gdpPercap_2002\", \"gdpPercap_1972\", \"country\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"gdpPercap_1957\", \"gdpPercap_1992\", \"gdpPercap_2007\", \"gdpPercap_2002\", \"gdpPercap_1972\", \"country\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsite\n(HA2)121\n326\n280\n9\n210\n192\n-12\n171\n(HA2)102\n\nHeader and first few lines of CSV file 2:\nmin_diffsel,max_diffsel,site\n-1.004167098795603,1.5787387471316894,(HA2)121\n-1.2184218611180495,0.7169223975734413,326\n-1.0182673783732994,0.9710714351184296,280\n-0.8471518556126452,1.0005541420596125,9\n-1.2405474792520252,1.37896385796,210\n-1.027758871160202,1.263069276940311,192\n-1.2624452625748657,0.6785731341245361,-12\n-1.9128310938041404,1.2322928805568882,171\n-1.663800562787797,0.9115078157304708,(HA2)102\n\nQuestion: Merge two tables based on shared column names, and display the values of max_diffsel, min_diffsel, and site. If there are no shared column names, fill in the blanks with NAN.", "csv1_example": "site\n(HA2)121\n326\n280\n9\n210\n192\n-12\n171\n(HA2)102\n312\n", "csv2_example": "min_diffsel,max_diffsel,site\n-1.004167098795603,1.5787387471316894,(HA2)121\n-1.2184218611180495,0.7169223975734413,326\n-1.0182673783732994,0.9710714351184296,280\n-0.8471518556126452,1.0005541420596125,9\n-1.2405474792520252,1.37896385796,210\n-1.027758871160202,1.263069276940311,192\n-1.2624452625748657,0.6785731341245361,-12\n-1.9128310938041404,1.2322928805568882,171\n-1.663800562787797,0.9115078157304708,(HA2)102\n-1.4217237593217844,1.3208015052141648,312\n", "csv1_path": "infiagent/merge_test/34_0_0.csv", "csv2_path": "infiagent/merge_test/34_0_1.csv", "instruction": "Merge two tables based on shared column names, and display the values of max_diffsel, min_diffsel, and site. If there are no shared column names, fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/34_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/34_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"max_diffsel\", \"min_diffsel\", \"site\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"max_diffsel\", \"min_diffsel\", \"site\"]]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhotel_id,review_count\n75737,2291\n93401,968\n224217,462\n488793,520\n1028569,582\n1383001,578\n1733337,72\n1776857,2608\n2643161,964\n\nHeader and first few lines of CSV file 2:\nhotel_type,parent_brand_name,hotel_id\nHotel,Choice Hotels International, Inc.,224217\nHotel,Choice Hotels International, Inc.,1383001\nHotel,,1733337\nHotel,Langham Hospitality Group,1776857\nHotel,,7148761\nHotel,Marriott International, Inc.,8541913\nHotel,,9608281\nHostel,,10036825\nCondo,,15646553\n\nQuestion: Combine the rows from both tables where 'review_count' is less than or equal to 177 and 'hotel_id' is greater than or equal to 1641016, consolidating entries with identical column names while preserving only the successfully merged portions.", "csv1_example": "hotel_id,review_count\n75737,2291\n93401,968\n224217,462\n488793,520\n1028569,582\n1383001,578\n1733337,72\n1776857,2608\n2643161,964\n7148761,1601\n", "csv2_example": "hotel_type,parent_brand_name,hotel_id\nHotel,Choice Hotels International, Inc.,224217\nHotel,Choice Hotels International, Inc.,1383001\nHotel,,1733337\nHotel,Langham Hospitality Group,1776857\nHotel,,7148761\nHotel,Marriott International, Inc.,8541913\nHotel,,9608281\nHostel,,10036825\nCondo,,15646553\nSpecialty Hotel,,15697497\n", "csv1_path": "infiagent/merge_test/40_2_0.csv", "csv2_path": "infiagent/merge_test/40_2_1.csv", "instruction": "Combine the rows from both tables where 'review_count' is less than or equal to 177 and 'hotel_id' is greater than or equal to 1641016, consolidating entries with identical column names while preserving only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/40_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/40_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['review_count'] >= 177]\ndf = df[df['hotel_id'] <= 1641016]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['review_count'] <= 177]\ndf = df[df['hotel_id'] >= 1641016]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,damage_USD,max_sust_wind,damage_imputed\n1,3000000.0,43.4488,0\n2,25000000.0,52.13856,0\n3,0.0,26.06928,0\n4,0.0,43.4488,0\n5,10000000.0,52.13856,0\n8,125000000000.0,112.96688,0\n9,64760000000.00001,156.41568,0\n12,0.0,99.93224,0\n13,91606000000.0,152.0708,0\n\nHeader and first few lines of CSV file 2:\ndeaths,max_storm_cat,min_p,year,Unnamed: 0\n2.0,1,1007.0,2017,1\n2.0,1,991.0,2017,2\n0.0,0,1009.0,2017,3\n0.0,1,1005.0,2017,4\n0.0,1,1001.0,2017,5\n0.0,2,981.0,2017,6\n107.0,5,937.0,2017,8\n134.0,6,914.0,2017,9\n3.0,3,972.0,2017,11\n\nQuestion: Merge all rows in the two tables that the value of 'year' is '2005', show the value of  damage_imputed, damage_USD , max_storm_cat , max_sust_wind , deaths , year  and Unnamed: 0, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "Unnamed: 0,damage_USD,max_sust_wind,damage_imputed\n1,3000000.0,43.4488,0\n2,25000000.0,52.13856,0\n3,0.0,26.06928,0\n4,0.0,43.4488,0\n5,10000000.0,52.13856,0\n8,125000000000.0,112.96688,0\n9,64760000000.00001,156.41568,0\n12,0.0,99.93224,0\n13,91606000000.0,152.0708,0\n15,13600000.0,99.93224,0\n", "csv2_example": "deaths,max_storm_cat,min_p,year,Unnamed: 0\n2.0,1,1007.0,2017,1\n2.0,1,991.0,2017,2\n0.0,0,1009.0,2017,3\n0.0,1,1005.0,2017,4\n0.0,1,1001.0,2017,5\n0.0,2,981.0,2017,6\n107.0,5,937.0,2017,8\n134.0,6,914.0,2017,9\n3.0,3,972.0,2017,11\n0.0,4,962.0,2017,12\n", "csv1_path": "infiagent/merge_test/21_2_0.csv", "csv2_path": "infiagent/merge_test/21_2_1.csv", "instruction": "Merge all rows in the two tables that the value of 'year' is '2005', show the value of  damage_imputed, damage_USD , max_storm_cat , max_sust_wind , deaths , year  and Unnamed: 0, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/21_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/21_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['year'] == 2005]\ndf = df[[\"damage_imputed\", \"damage_USD\", \"max_storm_cat\", \"max_sust_wind\", \"deaths\", \"year\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['year'] == '2005']\ndf = df[[\"damage_imputed\", \"damage_USD\", \"max_storm_cat\", \"max_sust_wind\", \"deaths\", \"year\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnumber_of_times90_days_late,number_of_time60-89_days_past_due_not_worse,revolving_utilization_of_unsecured_lines,number_real_estate_loans_or_lines,debt_ratio,serious_dlqin2yrs,age\n0,0,0.7661266089999998,6,0.8029821290000001,1,45\n0,0,0.957151019,0,0.121876201,0,40\n1,0,0.65818014,0,0.085113375,0,38\n0,0,0.233809776,0,0.036049682,0,30\n0,0,0.9072394,1,0.024925695,0,49\n0,0,0.213178682,1,0.375606969,0,74\n0,0,0.116950644,0,46.0,0,27\n0,0,0.189169052,4,0.606290901,0,57\n0,0,0.644225962,0,0.3094762099999999,0,30\n\nHeader and first few lines of CSV file 2:\nnumber_of_open_credit_lines_and_loans,monthly_income,revolving_utilization_of_unsecured_lines\n4,2600.0,0.957151019\n2,3042.0,0.65818014\n7,63588.0,0.9072394\n3,3500.0,0.213178682\n8,3500.0,0.7544636479999999\n2,,0.116950644\n9,23684.0,0.189169052\n5,2500.0,0.644225962\n7,6501.0,0.01879812\n\nQuestion: Combine the rows from both tables where the 'number_of_open_credit_lines_and_loans' value is less than 8. Display the values of 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', and 'revolving_utilization_of_unsecured_lines' for the merged entries. Merge the rows based on common column names and fill in any missing values with NAN.", "csv1_example": "number_of_times90_days_late,number_of_time60-89_days_past_due_not_worse,revolving_utilization_of_unsecured_lines,number_real_estate_loans_or_lines,debt_ratio,serious_dlqin2yrs,age\n0,0,0.7661266089999998,6,0.8029821290000001,1,45\n0,0,0.957151019,0,0.121876201,0,40\n1,0,0.65818014,0,0.085113375,0,38\n0,0,0.233809776,0,0.036049682,0,30\n0,0,0.9072394,1,0.024925695,0,49\n0,0,0.213178682,1,0.375606969,0,74\n0,0,0.116950644,0,46.0,0,27\n0,0,0.189169052,4,0.606290901,0,57\n0,0,0.644225962,0,0.3094762099999999,0,30\n0,0,0.01879812,2,0.53152876,0,51\n", "csv2_example": "number_of_open_credit_lines_and_loans,monthly_income,revolving_utilization_of_unsecured_lines\n4,2600.0,0.957151019\n2,3042.0,0.65818014\n7,63588.0,0.9072394\n3,3500.0,0.213178682\n8,3500.0,0.7544636479999999\n2,,0.116950644\n9,23684.0,0.189169052\n5,2500.0,0.644225962\n7,6501.0,0.01879812\n7,11362.0,0.548458062\n", "csv1_path": "infiagent/merge_test/23_3_0.csv", "csv2_path": "infiagent/merge_test/23_3_1.csv", "instruction": "Combine the rows from both tables where the 'number_of_open_credit_lines_and_loans' value is less than 8. Display the values of 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', and 'revolving_utilization_of_unsecured_lines' for the merged entries. Merge the rows based on common column names and fill in any missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/23_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/23_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['number_of_open_credit_lines_and_loans'] < 8]\ndf = df[[\"number_of_open_credit_lines_and_loans\", \"number_of_times90_days_late\", \"revolving_utilization_of_unsecured_lines\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['number_of_open_credit_lines_and_loans'] < 8]\ndf = df[[\"number_of_open_credit_lines_and_loans\", \"number_of_times90_days_late\", \"revolving_utilization_of_unsecured_lines\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nVolume,Date\n58791957,2014-01-02\n98303870,2014-01-03\n79432766,2014-01-07\n69905199,2014-01-09\n76320664,2014-01-10\n94860843,2014-01-13\n83734371,2014-01-14\n98472619,2014-01-15\n57471330,2014-01-16\n\nHeader and first few lines of CSV file 2:\nClose,Date\n79.02,2014-01-02\n77.7,2014-01-06\n77.15,2014-01-07\n76.65,2014-01-09\n76.13,2014-01-10\n76.53,2014-01-13\n78.06,2014-01-14\n79.62,2014-01-15\n79.18,2014-01-16\n\nQuestion: Combine rows from both tables where the value of 'Close' is less than or equal to 107.95 and the value of 'Volume' is less than 45696176, merging by common column values and inserting missing values with NAN.", "csv1_example": "Volume,Date\n58791957,2014-01-02\n98303870,2014-01-03\n79432766,2014-01-07\n69905199,2014-01-09\n76320664,2014-01-10\n94860843,2014-01-13\n83734371,2014-01-14\n98472619,2014-01-15\n57471330,2014-01-16\n108426689,2014-01-17\n", "csv2_example": "Close,Date\n79.02,2014-01-02\n77.7,2014-01-06\n77.15,2014-01-07\n76.65,2014-01-09\n76.13,2014-01-10\n76.53,2014-01-13\n78.06,2014-01-14\n79.62,2014-01-15\n79.18,2014-01-16\n77.24,2014-01-17\n", "csv1_path": "infiagent/merge_test/29_0_0.csv", "csv2_path": "infiagent/merge_test/29_0_1.csv", "instruction": "Combine rows from both tables where the value of 'Close' is less than or equal to 107.95 and the value of 'Volume' is less than 45696176, merging by common column values and inserting missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/29_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/29_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Close'] >= 107.95]\ndf = df[df['Volume'] < 45696176]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Close'] <= 107.95]\ndf = df[df['Volume'] < 45696176]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nYear,Agriculture,Public Administration\n1970,4.22979798,68.4\n1971,5.452796685,65.5\n1972,7.42071022,62.6\n1973,9.653602412,64.3\n1974,14.07462346,66.1\n1975,18.33316153,63.0\n1976,22.25276005,65.6\n1977,24.6401766,69.3\n1978,27.14619175,71.5\n\nHeader and first few lines of CSV file 2:\nArt and Performance,Year,Physical Sciences,Business,Architecture,Education,Math and Statistics\n59.7,1970,13.8,9.064438975,11.92100539,74.53532758,38.0\n60.4,1972,14.8,10.5589621,13.21459351,73.55451996,40.2\n60.2,1973,16.5,12.80460152,14.7916134,73.50181443,40.9\n61.9,1974,18.2,16.20485038,17.44468758,73.33681143,41.8\n60.9,1975,19.1,19.68624931,19.13404767,72.80185448,40.7\n62.5,1978,22.5,30.52751868,25.84923973,73.19282134,41.6\n63.2,1979,23.7,33.62163381,27.77047744,73.82114234,42.3\n63.4,1980,24.6,36.76572529,28.08038075,74.98103152,42.8\n63.1,1982,27.3,41.94937335,34.81624758,75.84364914,44.0\n\nQuestion: Merge all rows in the two tables that the value of 'Physical Sciences' is not greater than 32.6 and the value of 'Education' is not less than 75.95060123, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "Year,Agriculture,Public Administration\n1970,4.22979798,68.4\n1971,5.452796685,65.5\n1972,7.42071022,62.6\n1973,9.653602412,64.3\n1974,14.07462346,66.1\n1975,18.33316153,63.0\n1976,22.25276005,65.6\n1977,24.6401766,69.3\n1978,27.14619175,71.5\n1979,29.63336549,73.3\n", "csv2_example": "Art and Performance,Year,Physical Sciences,Business,Architecture,Education,Math and Statistics\n59.7,1970,13.8,9.064438975,11.92100539,74.53532758,38.0\n60.4,1972,14.8,10.5589621,13.21459351,73.55451996,40.2\n60.2,1973,16.5,12.80460152,14.7916134,73.50181443,40.9\n61.9,1974,18.2,16.20485038,17.44468758,73.33681143,41.8\n60.9,1975,19.1,19.68624931,19.13404767,72.80185448,40.7\n62.5,1978,22.5,30.52751868,25.84923973,73.19282134,41.6\n63.2,1979,23.7,33.62163381,27.77047744,73.82114234,42.3\n63.4,1980,24.6,36.76572529,28.08038075,74.98103152,42.8\n63.1,1982,27.3,41.94937335,34.81624758,75.84364914,44.0\n62.1,1984,28.0,45.12403027,35.45308311,75.86911601,46.2\n", "csv1_path": "infiagent/merge_test/47_1_0.csv", "csv2_path": "infiagent/merge_test/47_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'Physical Sciences' is not greater than 32.6 and the value of 'Education' is not less than 75.95060123, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/47_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/47_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Physical Sciences'] >= 32.6]\ndf = df[df['Education'] <= 75.95060123]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Physical Sciences'] >= 32.6]\ndf = df[df['Education'] <= 75.95060123]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nClose,Unnamed: 0\n90.0,0\n90.1,1\n90.14,2\n89.6,4\n88.08,5\n87.82,6\n88.28,8\n88.19,9\n87.11,10\n\nHeader and first few lines of CSV file 2:\nVolume,Unnamed: 0,Low\n36875013,0,89.66\n24159683,1,89.66\n25621164,2,88.75\n36599736,3,88.01\n17808877,5,87.24\n18652201,6,87.41\n19484317,7,87.86\n22113049,8,87.6\n23407110,9,87.43\n\nQuestion: Show the values of Close, Volume, and Unnamed: 0 for all merged rows in the two tables, where the merge is performed based on the column names with the same name, and only the successfully merged portions are kept.", "csv1_example": "Close,Unnamed: 0\n90.0,0\n90.1,1\n90.14,2\n89.6,4\n88.08,5\n87.82,6\n88.28,8\n88.19,9\n87.11,10\n86.35,11\n", "csv2_example": "Volume,Unnamed: 0,Low\n36875013,0,89.66\n24159683,1,89.66\n25621164,2,88.75\n36599736,3,88.01\n17808877,5,87.24\n18652201,6,87.41\n19484317,7,87.86\n22113049,8,87.6\n23407110,9,87.43\n26061439,11,85.97\n", "csv1_path": "infiagent/merge_test/44_1_0.csv", "csv2_path": "infiagent/merge_test/44_1_1.csv", "instruction": "Show the values of Close, Volume, and Unnamed: 0 for all merged rows in the two tables, where the merge is performed based on the column names with the same name, and only the successfully merged portions are kept.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/44_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/44_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Close\", \"Volume\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Close\", \"Volume\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\npeople_fully_vaccinated_per_hundred,date,source_website,country,total_vaccinations,daily_vaccinations_per_million,people_fully_vaccinated\n,2021-01-10,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/,Albania,0.0,,\n,2021-01-29,https://www.aps.dz/regions/116777-blida-covid-19-trente-vaccines-au-matin-du-1er-jour-de-la-campagne,Algeria,0.0,,\n,2021-01-25,https://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,Andorra,576.0,,\n,2021-02-04,https://www.facebook.com/MinistryofHealthAnguilla/posts/1391067271247845,Anguilla,0.0,,\n,2020-12-29,http://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,Argentina,700.0,,\n,2021-01-10,https://info.gesundheitsministerium.gv.at/opendata/,Austria,,,\n,2021-01-17,https://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,Azerbaijan,0.0,,\n,2020-12-23,https://twitter.com/MOH_Bahrain/status/1362144927535267841,Bahrain,38965.0,,\n,2021-01-26,https://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,Bangladesh,0.0,,\n\nHeader and first few lines of CSV file 2:\npeople_vaccinated,total_vaccinations_per_hundred,country\n0.0,0.0,Albania\n,0.0,Algeria\n576.0,0.75,Andorra\n,0.0,Argentina\n6784.0,,Austria\n38965.0,2.29,Bahrain\n298.0,0.0,Belgium\n0.0,0.0,Bermuda\n,0.0,Bolivia\n\nQuestion: Merge all rows in the two tables, show the value of  date, total_vaccinations , total_vaccinations_per_hundred , people_fully_vaccinated_per_hundred  and country, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "people_fully_vaccinated_per_hundred,date,source_website,country,total_vaccinations,daily_vaccinations_per_million,people_fully_vaccinated\n,2021-01-10,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/,Albania,0.0,,\n,2021-01-29,https://www.aps.dz/regions/116777-blida-covid-19-trente-vaccines-au-matin-du-1er-jour-de-la-campagne,Algeria,0.0,,\n,2021-01-25,https://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,Andorra,576.0,,\n,2021-02-04,https://www.facebook.com/MinistryofHealthAnguilla/posts/1391067271247845,Anguilla,0.0,,\n,2020-12-29,http://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,Argentina,700.0,,\n,2021-01-10,https://info.gesundheitsministerium.gv.at/opendata/,Austria,,,\n,2021-01-17,https://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,Azerbaijan,0.0,,\n,2020-12-23,https://twitter.com/MOH_Bahrain/status/1362144927535267841,Bahrain,38965.0,,\n,2021-01-26,https://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,Bangladesh,0.0,,\n,2020-12-28,https://epistat.wiv-isp.be/covid/,Belgium,298.0,,\n", "csv2_example": "people_vaccinated,total_vaccinations_per_hundred,country\n0.0,0.0,Albania\n,0.0,Algeria\n576.0,0.75,Andorra\n,0.0,Argentina\n6784.0,,Austria\n38965.0,2.29,Bahrain\n298.0,0.0,Belgium\n0.0,0.0,Bermuda\n,0.0,Bolivia\n0.0,0.0,Brazil\n", "csv1_path": "infiagent/merge_test/22_1_0.csv", "csv2_path": "infiagent/merge_test/22_1_1.csv", "instruction": "Merge all rows in the two tables, show the value of  date, total_vaccinations , total_vaccinations_per_hundred , people_fully_vaccinated_per_hundred  and country, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/22_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/22_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"date\", \"total_vaccinations\", \"total_vaccinations_per_hundred\", \"people_fully_vaccinated_per_hundred\", \"country\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"date\", \"total_vaccinations\", \"total_vaccinations_per_hundred\", \"people_fully_vaccinated_per_hundred\", \"country\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 4,NUM ROUNDS,Name,STANDARD TEAM NAME,Unnamed: 5\n,5.0,Gatlin & Ramarao,Mitty GR,\n,,Lahiri & Ponnuswamy,Mitty PL,\n,,Patwa & Aggarwal,Mitty PA,\n,,Shaikh & Singh,Mitty SS,\n,,Cheng & Parulekar,Gunn PC,\n,,Prabhakar & Hamilton,Gunn PH,\n,,Kim & Joshi,Mitty JK,\n,,Kang & Korolik,Mitty KK,\n,,Khanna & Sultan,Clovis North KS,\n\nHeader and first few lines of CSV file 2:\nSchool,Wins,Name,Code,WEIGHTING\nArchbishop Mitty,5,Lahiri & Ponnuswamy,Archbishop Mitty LP,\nArchbishop Mitty,4,Patwa & Aggarwal,Archbishop Mitty PA,\nGunn Sr,4,Arun & Schacter,Gunn Sr AS,\nGunn Sr,4,Cheng & Parulekar,Gunn Sr CP,\nGunn Sr,4,Prabhakar & Hamilton,Gunn Sr PH,\nJames Logan,4,Menotti & Bhasin,James Logan MB,\nArchbishop Mitty,3,Kim & Joshi,Archbishop Mitty KJ,\nArchbishop Mitty,3,Kang & Korolik,Archbishop Mitty KK,\nClovis North,3,Manjal & Perez,Clovis North MP,\n\nQuestion: Merge all rows in the two tables that the length of 'School' is greater than 10 and the value of 'Wins' is not greater than 3, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "Unnamed: 4,NUM ROUNDS,Name,STANDARD TEAM NAME,Unnamed: 5\n,5.0,Gatlin & Ramarao,Mitty GR,\n,,Lahiri & Ponnuswamy,Mitty PL,\n,,Patwa & Aggarwal,Mitty PA,\n,,Shaikh & Singh,Mitty SS,\n,,Cheng & Parulekar,Gunn PC,\n,,Prabhakar & Hamilton,Gunn PH,\n,,Kim & Joshi,Mitty JK,\n,,Kang & Korolik,Mitty KK,\n,,Khanna & Sultan,Clovis North KS,\n,,Kalra & Bhat,Dougherty Valley KB,\n", "csv2_example": "School,Wins,Name,Code,WEIGHTING\nArchbishop Mitty,5,Lahiri & Ponnuswamy,Archbishop Mitty LP,\nArchbishop Mitty,4,Patwa & Aggarwal,Archbishop Mitty PA,\nGunn Sr,4,Arun & Schacter,Gunn Sr AS,\nGunn Sr,4,Cheng & Parulekar,Gunn Sr CP,\nGunn Sr,4,Prabhakar & Hamilton,Gunn Sr PH,\nJames Logan,4,Menotti & Bhasin,James Logan MB,\nArchbishop Mitty,3,Kim & Joshi,Archbishop Mitty KJ,\nArchbishop Mitty,3,Kang & Korolik,Archbishop Mitty KK,\nClovis North,3,Manjal & Perez,Clovis North MP,\nGunn Sr,3,Tomaszewski & Shaotran,Gunn Sr TS,\n", "csv1_path": "infiagent/merge_test/26_3_0.csv", "csv2_path": "infiagent/merge_test/26_3_1.csv", "instruction": "Merge all rows in the two tables that the length of 'School' is greater than 10 and the value of 'Wins' is not greater than 3, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/26_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/26_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['School'].str.len() > 10]\ndf = df[df['Wins'] >= 3]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['School'].str.len() > 10]\ndf = df[df['Wins'] >= 3]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nper_point_diff,per_gop,votes_dem\n15.17%,0.528870018006,93003.0\n5.61%,0.522714148219,4848.0\n55.54%,0.769661636946,1874.0\n50.86%,0.242288874708,3530.0\n13.53%,0.563154864709,3716.0\n41.38%,0.6923969942589999,13197.0\n14.81%,0.5663376397150001,5763.0\n69.36%,0.838712748738,1524.0\n13.66%,0.5643918547060001,3109.0\n\nHeader and first few lines of CSV file 2:\nstate_abbr,votes_dem,total_votes,combined_fips,per_dem,diff\nAK,93003.0,246588.0,2013,0.37715947248,37,410\nAL,5908.0,24661.0,1001,0.239568549532,12,202\nAL,4848.0,10390.0,1005,0.466602502406,583\nAL,2150.0,25384.0,1009,0.0846990230066,20,658\nAL,13197.0,47376.0,1015,0.278558763931,19,606\nAL,5763.0,13778.0,1017,0.418275511685,2,040\nAL,1524.0,10503.0,1019,0.1451013996,7,285\nAL,2909.0,18255.0,1021,0.159353601753,12,159\nAL,3109.0,7268.0,1023,0.427765547606,993\n\nQuestion: Merge all rows in the two tables that the value of 'per_point_diff' is '26.55%', show the value of  per_point_diff and votes_dem, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "per_point_diff,per_gop,votes_dem\n15.17%,0.528870018006,93003.0\n5.61%,0.522714148219,4848.0\n55.54%,0.769661636946,1874.0\n50.86%,0.242288874708,3530.0\n13.53%,0.563154864709,3716.0\n41.38%,0.6923969942589999,13197.0\n14.81%,0.5663376397150001,5763.0\n69.36%,0.838712748738,1524.0\n13.66%,0.5643918547060001,3109.0\n60.80%,0.7958003651860001,1234.0\n", "csv2_example": "state_abbr,votes_dem,total_votes,combined_fips,per_dem,diff\nAK,93003.0,246588.0,2013,0.37715947248,37,410\nAL,5908.0,24661.0,1001,0.239568549532,12,202\nAL,4848.0,10390.0,1005,0.466602502406,583\nAL,2150.0,25384.0,1009,0.0846990230066,20,658\nAL,13197.0,47376.0,1015,0.278558763931,19,606\nAL,5763.0,13778.0,1017,0.418275511685,2,040\nAL,1524.0,10503.0,1019,0.1451013996,7,285\nAL,2909.0,18255.0,1021,0.159353601753,12,159\nAL,3109.0,7268.0,1023,0.427765547606,993\nAL,5712.0,12936.0,1025,0.441558441558,1,397\n", "csv1_path": "infiagent/merge_test/30_0_0.csv", "csv2_path": "infiagent/merge_test/30_0_1.csv", "instruction": "Merge all rows in the two tables that the value of 'per_point_diff' is '26.55%', show the value of  per_point_diff and votes_dem, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/30_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/30_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['per_point_diff'] == '26.55%']\ndf = df[[\"per_point_diff\", \"votes_dem\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['per_point_diff'] == '26.55%']\ndf = df[[\"per_point_diff\", \"votes_dem\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nage\n19\n32\n46\n60\n25\n62\n56\n30\n34\n\nHeader and first few lines of CSV file 2:\nbmi,children,age,smoker\n33.77,1,18,no\n33.0,3,28,no\n28.88,0,32,no\n25.74,0,31,no\n33.44,1,46,no\n27.74,3,37,no\n25.84,0,60,no\n26.22,0,25,no\n26.29,0,62,yes\n\nQuestion: Combine the tables by matching rows with the same column name where 'smoker' does not equal 'no', and fill NULL values with NaN.", "csv1_example": "age\n19\n32\n46\n60\n25\n62\n56\n30\n34\n59\n", "csv2_example": "bmi,children,age,smoker\n33.77,1,18,no\n33.0,3,28,no\n28.88,0,32,no\n25.74,0,31,no\n33.44,1,46,no\n27.74,3,37,no\n25.84,0,60,no\n26.22,0,25,no\n26.29,0,62,yes\n34.4,0,23,no\n", "csv1_path": "infiagent/merge_test/43_1_0.csv", "csv2_path": "infiagent/merge_test/43_1_1.csv", "instruction": "Combine the tables by matching rows with the same column name where 'smoker' does not equal 'no', and fill NULL values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/43_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/43_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['smoker'] == 'no']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['smoker'] != 'no']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\npublishedAt,pos,text,Unnamed: 0,neu,author,description\n2018-08-28 11:04:51,0.102,\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\" the hospital said in a statement Tuesday night. \"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boyâs condition on an anti-vaccination group on Facebook, according to a statement.\n \n\nHeader and first few lines of CSV file 2:\nsource,Unnamed: 0,urlToImage,compound\nabc-news,0,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,0.9746\nabc-news,1,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,0.9869\nabc-news,2,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,0.9875\nabc-news,3,https://s.abcnews.com/images/Nightline/180827_ntl_az_hpMain_16x9_992.jpg,0.9799\nabc-news,4,https://s.abcnews.com/images/Politics/kelli-ward-ap-171025_hpMain_5_16x9_992.jpg,-0.9872\nabc-news,5,https://s.abcnews.com/images/Technology/180827_atm_techbytes_hpMain_16x9_992.jpg,0.9826\nabc-news,6,https://s.abcnews.com/images/Technology/WireAP_86828a77b1c1446e96826da60a57aed8_16x9_992.jpg,-0.9965\nabc-news,8,,0.1603\nabc-news,9,,-0.9837\n\nQuestion: Merge all rows in the two tables that the value of 'compound' is less than 0.7461, merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "publishedAt,pos,text,Unnamed: 0,neu,author,description\n2018-08-28 11:04:51,0.102,\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\" the hospital said in a statement Tuesday night. \"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boyâs condition on an anti-vaccination group on Facebook, according to a statement.\n \nShe allegedly wrote about the case on the \"Proud Parents of Unvaccinated Children - Texas\" Facebook page, which has since been deleted. A concerned parent posted screenshots of the unidentified nurseâs comments on the hospital's Facebook page late Friday.\n \n \n \nâI think itâs easy for us nonvaxxers to make assumptions but most of us have never and will never see one of theses diseases,â the self-identified nurse wrote, according to the screenshots. \"[F]or the first time in my career I saw measles this week. Actually most of my coworkers and the ER docs saw measles for the first time as well. And honestly, it was rough. The kid was super sick. Sick enough to be admitted to the ICU and he looked miserable.\n \nâBy no means have I changed my vax stance, and I never will. But I just wanted to share my experience and how much worse it was than I expected,\" she added.\n \n \n \nThe nurse claimed the patient had recently traveled to a region \"where measles is very common\" and speculated that he may have contracted the disease overseas.\n \nTexas Children's Hospital responded to the parentâs post on Friday, confirming that it was aware of the post.\n \nâThank you for your post. We are aware of this situation and have started a thorough investigation.We take these matters very seriously,â the hospital wrote. It edited the statement later to add: âThe views of this employee do not represent that of the organization.â\n \nIf confirmed, the toddlerâs diagnosis would mark the stateâs eighth measles case this year. More than 100 people have been diagnosed in the U.S. this year, according to the Centers for Disease Control and Prevention.\n \n \n \nThe hospital issued a longer statement on Monday, saying it had reached out to children who may have had contact with the infected toddler.\n \nâA patient treated at Texas Children's Hospital West Campus tested positive for measles. This is a highly-contagious, vaccine-preventable infection. We know vaccination is the best protection against measles,â the hospital told Houston ABC station KTRK Monday. âOur Infection Control and Prevention team immediately identified other children who may have come in contact with this patient to assess their risk and provide clinical recommendations. We have contacted all of those families.â\n \n \n \nIt said the nurse in question was in good standing with the licensing board, but it would not comment on her vaccination status. The hospital says it strongly encourages all staff to obtain the recommended vaccines.\n \nMeasles, also known as rubeola, is a highly contagious virus that spreads through respiratory droplets, especially coughing and sneezing, according to medical officials. Early symptoms include a high fever, runny nose, cough and red eyes, preceded by red spots on the face.\n \n\"Measles is such a concern, because one, it's preventable. We have a vaccination that can prevent it,\" Dr. Umair Shah, executive director of the Harris County, Texas, Health System, told KTRK. \"And two, it's so easily transmittable to someone else. ... Vaccines save lives.â\n \nThis report appeared in the Wednesday, Aug. 29, 2018, episode of ABC News' daily news podcast \"Start Here.\"\n \n \n \n\"Start Here\" is a daily ABC News podcast hosted by Brad Mielke featuring original reporting on stories that are driving the national conversation. Listen for FREE on the ABC News app, Apple Podcasts, TuneIn, Spotify, Stitcher, Google Play Music, iHeartRadio -- or ask Alexa: \"Play 'Start Here.'\"\n \nFollow @StartHereABC on social for exclusive content, show updates and more: Twitter, Facebook, Instagram.,2,0.847,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.\n2018-08-27 11:43:25,0.215,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. Facebook admits to providing data to dozens of tech companies The admission from the tech giant was made in new company documents given to Congress. Now Playing: Facebook admits to providing data to dozens of tech companies Now Playing: Say goodbye to overripe avocados thanks to new natural tech from food startup Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: Pentagon requests $8 billion for Space Force  Now Playing: {{itm.title}},5,0.719,ABC News,The video site is introducing nonskippable ads to all of its channels, which will be controlled by content creators.\n2018-08-27 10:26:19,0.08,\n \nFacebook said Monday that it is banning Myanmar's powerful military chief and 19 other individuals and organizations from its site to prevent the spread of hate and misinformation. \nThe social media giant was heavily criticized for permitting itself to be used to inflame ethnic and religious conflict in the country, particularly against minority Rohingya Muslims. It has been accused of being lax in fighting online misinformation and manipulation in many countries, but Myanmar is one where it has been most closely tied to deadly violence. \nSome 700,000 Rohingya have fled from Myanmar's western state of Rakhine over the past year in response to a brutal counterinsurgency campaign by the military, which has been accused of massive human rights violations. Critics accuse the military of carrying out ethnic cleansing, or even genocide, an allegation denied by the government, which says it was responding to attacks on security forces. \nFacebook said it also targeted pages and accounts that pretended to provide independent news and opinion, while covertly promoting messages of Myanmar's military. It said it was deleting 18 Facebook accounts, one Instagram account and 52 Facebook pages. \nA separate report by investigators working for the U.N.'s top human rights body, released Monday, charged that \"Facebook has been a useful instrument for those seeking to spread hate, in a context where for most users Facebook is the internet.\" \n\"Although improved in recent months, Facebook's response has been slow and ineffective,\" said the report by the Fact-Finding Mission on Myanmar, authorized by the U.N. Human Rights Council. \"The extent to which Facebook posts and messages have led to real-world discrimination and violence must be independently and thoroughly examined.\" \nFour high-ranking officers and two military units targeted by Facebook were also put on a U.S. government blacklist earlier this month for human rights abuses. The sanctions block any property they own within the U.S. and prohibit U.S. citizens from engaging in transactions with them. The U.S. already maintains restrictions on visas, arms sales and assistance to Myanmar's military. In June, the EU imposed similar sanctions on seven senior army and police officers, all of whom are on Facebook's blacklist. \nSix officers on Facebook's list were also named in the U.N. human rights report, which said Myanmar's top leaders should be prosecuted for genocide. Those it recommended as \"priority subjects for investigation and prosecution\" included top commander Senior Gen. Min Aung Hlaing. \nIn a statement, Facebook referred to the U.N. report, which it said \"found evidence that many of these individuals and organizations committed or enabled serious human rights abuses in the country. And we want to prevent them from using our service to further inflame ethnic and religious tensions.\" Discrimination against the Rohingya ran deep and wide even before the spread of Facebook. \nFacebook has been under pressure for several months to take action on the problem, especially after civic and rights groups in Myanmar said in April that it had failed to adequately act against online hate speech that incited violence against the country's Muslim minorities, neglecting to effectively enforce its own rules. \n\"The ethnic violence in Myanmar has been truly horrific,\" Facebook said in its statement. \"While we were too slow to act, we're now making progress â with better technology to identify hate speech, improved reporting tools, and more people to review content.\" \n\"We continue to work to prevent the misuse of Facebook in Myanmar â including through the independent human rights impact assessment we commissioned earlier in the year. This is a huge responsibility given so many people there rely on Facebook for information â more so than in almost any other country given the nascent state of the news media and the recent rapid adoption of mobile phones. It's why we're so determined to do better in the future.\" \nYangon-based political analyst David Mathieson said that Facebook's action, together with the damning U.N. report, force Myanmar's military brass \"into an isolation they're not going to like.\" \n\"They have to find alternative ways to communicate with the Myanmar population, because Facebook really is the internet for many people here. And Facebook just excommunicated the commander in chief from the worldwide web,\" he told The Associated Press.,6,0.767,The Associated Press,He was among 20 individuals and organizations banned for spreading hate.\n2018-08-24 22:18:05,0.125,\nMembers of the country's most influential tech companies met Friday to prepare for misinformation campaigns by foreign countries, including Russia, ahead of the midterm elections in November.\n \nAt least one company confirmed Friday's meeting, held in San Francisco, to ABC News.\n \nThe companies plan to discuss efforts to defend their platforms from the type of meddling that occurred during the 2016 presidential election.\n \nThe meeting was first reported in BuzzFeed.\n \nIt comes after a tumultuous week in the cyber landscape. Microsoft announced Tuesday it took control of half a dozen websites that had been operated by the Russian military intelligence agency that hacked the Democratic National Committee in 2016.\n \nOn Wednesday, Facebook and Twitter suspended or removed hundreds of accounts linked to Russia and Iran that displayed âinauthentic behavior.â\n \n \n \nFridayâs meeting comes on the heels of a meeting that took place in May between the top tech companies and United States intelligence officials. That meeting occurred at Facebookâs headquarters and included Twitter, Snap, Amazon, Apple, Google, Microsoft and Oath, according to the New York Times.\n \n \n \nAccording to reports, the tech companies walked away disappointed because intelligence officials refused to share information.\n \nTech companies, including Facebook and Twitter, have come under heavy fire after federal authorities announced that Russian agents launched a misinformation campaign on social media in the United States during the 2016 election.\n \n \n \nEarlier this month, a number of top U.S. national security leaders gathered in the White House Press Briefing Room and shared the same message: Russia is continuing to interfere in the U.S. political system.\n \n \n \nAt the time, Homeland Security Secretary Kristjen Nielsen said: âFree and fair elections are the cornerstone of our democracy, and it has become clear that they are the target of our adversaries who seek to sow discord and undermine our way of life.â\n \n \n \nIn February, Special Counsel Robert Mueller indicted 12 Russians, all members of a Russian intelligence agency, for trying to hack Democratsâ emails and computer networks.\n \nABC News' Lauren Botchan contributed to this report.\n,7,0.807,Will Carr,The most influential companies are preparing for misinformation campaigns.\n2018-08-24 17:45:08,0.102,\n \nThis week has seen major social media sites step up their policing of online disinformation campaigns. \nGoogle disabled dozens of YouTube channels and other accounts linked to a state-run Iranian broadcaster running a political-influence campaign. \nFacebook removed 652 suspicious pages, groups and accounts linked to Russia and Iran. \nTwitter took similar action shortly thereafter. \nWhat did they have in common? The security firm FireEye. \nBest known for its work on high-profile cyberattacks against companies including Target, JPMorgan Chase and Sony Pictures, FireEye is emerging as a key player in the fight against election interference and disinformation campaigns. \nFounded in 2004, FireEye is based in Silicon Valley and staffed with a roster of former military and law-enforcement cyberexperts. \n\"They've really become the Navy SEALs of cybersecurity, especially for next-generation cybersecurity threats,\" said GBH Insights analyst Dan Ives. \nLee Foster, manager of information operations analysis at FireEye, said his team works within the company's intelligence outfit, which researches not only \"info-ops\" â like the Iran-linked social media activity it recently uncovered â but espionage, financial crime and other forms of vulnerability and exploitation. Specialist teams at FireEye focus on particular areas of cyberthreats, each with their own expertise and language capabilities. \n\"We kind of operate like a private-sector intelligence operation,\" he said. \nFireEye was founded by Ashar Aziz, who developed a system for spotting threats that haven't been tracked before, unlike older companies that sold firewalls or anti-virus programs that block known malware. \nAziz, a former Sun Microsystems engineer, created a system that uses software to simulate a computer network and check programs for suspicious behavior, before allowing them into the network itself. \nFireEye raised its profile in 2014 by acquiring Mandiant, known for expertise in assessing damage and tracing the source of cyberattacks. Mandiant founder Kevin Mandia, a former U.S. Air Force investigator, is now FireEye's CEO. \nWhile businesses are spending more on information security, FireEye itself has spent heavily on research, development, sales and marketing. That has led to struggles to remain profitable, as heavy investments offset revenue growth. \nMandia said that during the three months ended June 30, FireEye's email security found 6 million spear-phishing attacks, a type of hacking, and its security products alerted companies of attempts to breach security 29 million times. That's important, Mandia said, because most of FireEye's products are deployed behind their client's existing firewalls or antivirus software, so everything FireEye catches has already evaded other defenses, he said. \n\"We are the investigators called in when the processes, people, and technology fail to prevent a security breach or incident,\" he said. \"We find the gaps in the security fabric and we find the needle in the haystack.\" \nFireEye Inc.'s second-quarter revenue rose 6 percent to $203 million but it lost $72.9 million, or 38 cents per share. That met Wall Street's expectations, but its shares fell as investors expected more. \nThat's a common problem in the white-hot cybersecurity sector, which includes competitors like Palo Alto Networks, CloudFlare and Check Point. The companies are facing high expectations as the cybersecurity market booms, fueled by heightened cyberattacks and hacking fears. \n\"As the space has become more competitive ... profitability and growth has been a challenge for (FireEye),\" Ives said. \nStill, FireEye's stock jumped 6 percent on Thursday when news broke of its role in uncovering the fake accounts on YouTube, Facebook and Twitter. It was up another 3 percent Friday. \nFireEye shares hit their all-time peak of $95.63 on March 5, 2014, a few months after they went public, but began a long decline after that, hitting an all-time low of $10.40 almost exactly three years later on March 14, 2017. In the past month the stock has traded between $14.38 and $16.69. \nAnd the company's reputation continues to grow. \n\"There are many vendors that play in cybersecurity when you look at some of the very sophisticated threats facing enterprise and governments,\" Ives said. \"FireEye many times gets that first phone call when it comes to assess threat environment for companies.\" \n——— \nAP Technology Writer Barbara Ortutay in New York contributed to this story.,8,0.81,The Associated Press,Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news.\n2018-08-24 12:33:44,0.057,\nPresident Trump on Friday continued his tirade against social media companies who have moved to suspend accounts they have identified as intentionally disseminating fake news, inaccurately claiming \"millions\" of voices are being silenced.\n \n\"Social Media Giants are silencing millions of people,\" Trump said in a tweet. \"Canât do this even if it means we must continue to hear Fake News like CNN, whose ratings have suffered gravely. People have to figure out what is real, and what is not, without censorship!\"\n \n Social Media Giants are silencing millions of people. Canât do this even if it means we must continue to hear Fake News like CNN, whose ratings have suffered gravely. People have to figure out what is real, and what is not, without censorship! \n \nThe president's tweet followed an announcement by Facebook on Tuesday that it had identified and removed 652 accounts and groups it deemed had \"coordinated inauthentic behavior,\" including several it said were linked to Iran and Russia.\n \n \n \nTrump has previously accused social media companies of targeting only conservative voices in their recent crackdowns against accounts like that of conspiracy theorist Alex Jones.\n \nFacebook earlier this month removed content from several of Jones' pages and his organization \"InfoWars,\" saying content promoted by their accounts violated their hate speech policies by \"glorifying violence ... and using dehumanizing language to describe people who are transgender, Muslims and immigrants.\"\n \nTrump said during his campaign rally in West Virginia earlier this week that \"every one of us is sort of like a newspaper.\"\n \n\"You have Twitter, or whatever you have, you have Facebook. But you can't pick one person and say, 'We don't like what he's been saying, he's out,'\" he said. \"So we will live with fake news.â,9,0.792,Alexander Mallin,President Trump is railing against social media giants for taking action against fake news accounts.\n2018-08-23 20:33:08,0.068,\n \nA New York man who was arrested for falsely claiming he was owed half-ownership of Facebook but then fled the country has been captured in Ecuador, authorities said Thursday. \nProsecutors notified a judge presiding over his case in Manhattan federal court that Paul Ceglia was arrested in the morning. They said he'll appear in a court in Quito, Ecuador's capital, within a day. \nRobert Ross Fogg, Ceglia's defense lawyer, said he was surprised at news of his client's capture, but \"mostly relieved that he was located without incident and hope the family has maintained good health.\" \n\"Look forward to his return and resuming our vigorous defense of his case,\" Fogg said in an email. \nNikki Credic-Barrett, a spokeswoman for the U.S. Marshals Service, said the agency had no information on the matter because Ceglia was in the custody of Ecuadorian law enforcement authorities. \nCeglia, 45, was arrested on fraud charges in October 2012. \nWhile under house arrest in Wellsville, New York, in March 2015 he disappeared with his wife and two young sons and the family's dog. Authorities said he sliced off his electronic monitoring device and created a crude contraption to make it seem he was moving around his home. \nProsecutors told U.S. District Judge Vernon Broderick they would update him on the status of extradition attempts. \nCeglia claimed in a lawsuit that he gave Facebook founder Mark Zuckerberg $1,000 in startup money in exchange for 50 percent of the future company in 2003 as part of a software development contract. \nFacebook lawyers said Ceglia and Zuckerberg did have a contract but references to Facebook were slipped in for purposes of the lawsuit. \nThe lawsuit was tossed out by a judge in Buffalo, New York, in 2014. Prosecutors then filed fraud charges after a forensic analysis of Ceglia's computers and Harvard's email archive determined he had altered an unrelated contract and falsified emails to make it seem Zuckerberg had promised him a half-share. \nCeglia maintained he was not guilty before he vanished. Mail and wire fraud charges against him carry a potential maximum sentence of 40 years in prison if he is convicted.,11,0.77,The Associated Press,Authorities say a New York man who was arrested for falsely claiming he was owed half-ownership of Facebook but then fled the country has been captured in Ecuador\n2018-08-23 19:21:31,0.076,\nIn the 2016 presidential election Facebook and other social media platforms were utilized by political campaigns and interest groups to run advertisements with very few rules regulating their disclosure and little public information about their source.\n \nThe lack of knowledge about these advertisements underscored the ease with which political actors, both foreign and domestic, were able to manipulate platforms like Facebook to disseminate misleading and often false information to Facebook users in the United States and around the world.\n \nThe spread of misinformation meant to foment political discord and stir unrest continues to be a concern less than three months out from a consequential midterm election. Just this week Facebook announced it removed \"652 pages, groups and accounts for coordinated inauthentic behavior that originated in Iran and targeted people across multiple internet services in the Middle East, Latin America, UK and US.\"\n \nFacebook also said it \"removed multiple pages, groups and accounts\" linked to \"inauthentic behavior\" on its platforms, including actions that originated in Russia.\n \n \n \nAside from the threat from nefarious actors, Facebook's influence as a medium through which American politicians and campaigns communicate with voters has skyrocketed in the last decade. But exactly how that influence and power is being yielded by politicians, campaigns and outside groups from all sides of the political spectrum still remains somewhat of a mystery due to the lack of rules governing the disclosure of that information.\n \n \n \nIn order to better understand the tactics and strategies of all political actors on Facebook, ABC News is partnering with ProPublica, a nonprofit, award-winning news organization that has built a tool to collect information and data on political ads that are being targeted to users of the social media giant.\n \nProPublica's tool is a simple, free browser plugin on a Firefox or Google Chrome web browser, that Facebook users can opt into and report political advertisements they see on the site. Those advertisements will then be collected by ProPublica in a public database to track and analyze advertisements on Facebook.\n \nABC News and other news organizations will then use that database and information to better understand and report on how political actors are utilizing one of the most powerful communication mediums on the planet.\n \nThus far the tool has gathered almost 60,000 individual advertisements from over 12,500 contributors, according to ProPublica.\n \nProPublica clearly states: \"The tool doesn't collect your personal information,\" and privacy controls ensure that the information that is being collected relates only to the advertisement that appeared on the site.\n \n \n \nEarlier this year Facebook announced steps to make the source of political advertisements on the site more transparent and understandable, but has still not given news organizations or academic institutions full and total access to all of the data behind what goes into purchasing and targeting those ads.\n \nThe site has created its own archive of political advertisements that allows users to obtain \"more information about some of the ads they see and the advertisers who are funding them,\" according to Facebook. But that archive remains an incomplete picture of how political advertisements are tailored to certain users because it does not show who was targeted with the ad.\n \nBy utilizing ProPublica's tool to track advertisements, Facebook users can more easily report political advertising, giving news organizations more data to gather and a greater ability to understand, analyze, fact-check and investigate political communication on the site.\n \nAlready the tool has helped locate political ads that were actually scams and malware, political ads that were ignore federal election rules by not disclosing who funded them, and uncovered a fake Facebook page claiming to belong to Special Counsel Robert Mueller with more than 100,000 followers.\n \nThe midterms are fast approaching, and ProPublica's tool will be a key asset in understanding how politicians, their campaigns, and powerful outside interest groups are using the medium to communicate with voters.\n \n \n,12,0.889,John Verhovek,Amid two recent discoveries of disinformation campaigns on Facebook, ProPublica created a tool to track and analyze political advertisements on the social media platform.\n2018-08-23 10:40:41,0.083,\n \nPolice acting on a complaint by Thailand's ruling junta have charged the leaders of a new political party that strongly opposes military rule with violating the computer crime law, which could result in five-year prison terms. \nFuture Forward party leader Thanathorn Juangroongruangkit and two other senior party members were charged by police this week with violating a section of the law that makes it a crime to transmit false information or information that damages the country's stability, group spokeswoman Pannika Wanich said Thursday. \nShe said the charges involve a June 29 Facebook live video posted on the pages of Thanathorn and the party, but did not describe how it may have violated the law. She said police had issued summonses in the past few days for the three party members to turn themselves in on Friday, but they had busy schedules and would ask for the date to be postponed to the middle of next month. \n\"And since all three have now officially become defendants in the case, our legal team needs some time to prepare,\" she said. \nThe Future Forward party is a new grouping featuring young politicians and is viewed as offering a real alternative to the ruling junta and its allies in upcoming elections. \nThailand's military government has kept a tight lid on dissent since it seized power from an elected government in a 2014 coup. It has promised to hold elections by early next year in a contest its critics charge will be set up to ensure it continues its influence over the country's administration. \nThe junta has already pushed back several promised election dates. Prime Minister and junta leader Prayuth Chan-ocha said Tuesday that the next polls are still scheduled for Feb. 24, though he did not entirely rule out another delay. \nPannika said a junta official had filed a police complaint against Thanathorn and his colleagues and had provided a brief explanation of the charges to local media. \nThe country's biggest newspaper, Thai Rath, said Col. Burin Thongprapai, a legal officer for the junta, stated last month that he filed the charges because in the Facebook live broadcast Thanathorn had made accusations against the junta and twisted facts in a manner that amounted to an attack on Thailand's justice system.,14,0.803,The Associated Press,Police acting on a complaint by Thailand's ruling junta have charged the leaders of a new political party that strongly opposes military rule with violating the computer crime law, which could result in five-year prison terms\n2018-08-23 09:41:21,0.061,\n \nFacebook has pulled one of its own products from Apple's app store because it didn't want to stop tracking what people were doing on their iPhones. Facebook also banned a quiz app from its social network for possible privacy intrusions on about 4 million users. \nThe twin developments come as Facebook is under intense scrutiny over privacy following the Cambridge Analytica scandal earlier this year. Allegations that the political consultancy used personal information harvested from 87 million Facebook accounts have dented Facebook's reputation. \nSince the scandal broke, Facebook has investigated thousands of apps and suspended more than 400 of them over data-sharing concerns. \nThe social media company said late Wednesday that it took action against the myPersonality quiz app, saying that its creators refused an inspection. But even as Facebook did that, it found its own Onavo Protect security app at odds with Apple's tighter rules for applications. \nOnavo Protect is a virtual-private network service aimed at helping users secure their personal information over public Wi-Fi networks. The app also alerts users when other apps use too much data. \nSince acquiring Onavo in 2013, Facebook has used it to track what apps people were using on phones. This surveillance helped Facebook detect trendy services, tipping off the company to startups it might want to buy and areas it might want to work on for upcoming features. \nFacebook said in a statement that it has \"always been clear when people download Onavo about the information that is collected and how it is used.\" \nBut Onavo fell out of compliance with Apple's app-store guidelines after they were tightened two months ago to protect the reservoir of personal information that people keep on their iPhones and iPads. \nApple's revised guidelines require apps to get users' express consent before recording and logging their activity on a device. According to Apple, the new rules also \"made it explicitly clear that apps should not collect information about which other apps are installed on a user's device for the purposes of analytics or advertising/marketing.\" \nFacebook will still be able to deploy Onavo on devices powered by Google's Android software. \nOnavo's ouster from Apple's app store widens the rift between two of the world's most popular companies. \nApple CEO Tim Cook has been outspoken in his belief that Facebook does a shoddy job of protecting its 2.2 billion users' privacy â something that he has framed as \"a fundamental human right.\"  \nCook sharpened his criticism following the Cambridge Analytica scandal. He emphasized that Apple would never be caught in the same situation as Facebook because it doesn't collect information about its customers to sell advertising. Facebook CEO Mark Zuckerberg fired back in a separate interview and called Cook's remarks \"extremely glib.\" Zuckerberg implied that Apple caters primarily to rich people with a line of products that includes the $1,000 iPhone X. \nLate Wednesday, Facebook said it moved to ban the myPersonality app after it found user information was shared with researchers and companies \"with only limited protections in place.\" The company said it would notify the app's users that their data may have been misused. \nIt said myPersonality was \"mainly active\" prior to 2012. Though Facebook has tightened its rules since then, it is only now reviewing those older apps following the Cambridge Analytica scandal. \nThe app was created in 2007 by researcher David Stillwell and allowed users to take a personality questionnaire and get feedback on the results. \n\"There was no misuse of personal data,\" Stillwell said in a statement, adding that \"this ban appears to be purely cosmetic.\" Stillwell said users gave their consent and the app's data was fully anonymized before it was used for academic research. He also rejected Facebook's assertion that he refused to submit to an audit.,15,0.865,The Associated Press,Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news.\n", "csv2_example": "source,Unnamed: 0,urlToImage,compound\nabc-news,0,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,0.9746\nabc-news,1,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,0.9869\nabc-news,2,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,0.9875\nabc-news,3,https://s.abcnews.com/images/Nightline/180827_ntl_az_hpMain_16x9_992.jpg,0.9799\nabc-news,4,https://s.abcnews.com/images/Politics/kelli-ward-ap-171025_hpMain_5_16x9_992.jpg,-0.9872\nabc-news,5,https://s.abcnews.com/images/Technology/180827_atm_techbytes_hpMain_16x9_992.jpg,0.9826\nabc-news,6,https://s.abcnews.com/images/Technology/WireAP_86828a77b1c1446e96826da60a57aed8_16x9_992.jpg,-0.9965\nabc-news,8,,0.1603\nabc-news,9,,-0.9837\nabc-news,10,https://s.abcnews.com/images/US/shawn-richard-christy-ht-001-jpo-180823_hpMain_16x9_992.jpg,-0.9945\n", "csv1_path": "infiagent/merge_test/33_0_0.csv", "csv2_path": "infiagent/merge_test/33_0_1.csv", "instruction": "Merge all rows in the two tables that the value of 'compound' is less than 0.7461, merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/33_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/33_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['compound'] < 0.7461]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['compound'] < 0.7461]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nTime,Open\n09:16:59,317.7\n09:18:59,318.65\n09:19:59,319.3\n09:20:59,319.6\n09:22:59,320.15\n09:23:59,319.65\n09:24:59,319.5\n09:25:59,319.7\n09:26:59,318.35\n\nHeader and first few lines of CSV file 2:\nLow,Volume,Time,Close,High\n316.05,143354,09:15:59,317.7,319.4\n317.7,52695,09:16:59,318.0,318.2\n318.5,44745,09:18:59,319.2,319.4\n319.6,67482,09:20:59,320.25,320.4\n319.95,56590,09:21:59,320.05,320.3\n319.6,52413,09:22:59,319.6,320.15\n319.15,56305,09:23:59,319.4,319.65\n319.45,36525,09:24:59,319.7,320.05\n318.3,50383,09:25:59,318.35,319.7\n\nQuestion: Merge all rows in the two tables that the value of 'Low' is not greater than 316.1, show the value of  Close, Low , Volume  and Time, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "Time,Open\n09:16:59,317.7\n09:18:59,318.65\n09:19:59,319.3\n09:20:59,319.6\n09:22:59,320.15\n09:23:59,319.65\n09:24:59,319.5\n09:25:59,319.7\n09:26:59,318.35\n09:28:59,319.7\n", "csv2_example": "Low,Volume,Time,Close,High\n316.05,143354,09:15:59,317.7,319.4\n317.7,52695,09:16:59,318.0,318.2\n318.5,44745,09:18:59,319.2,319.4\n319.6,67482,09:20:59,320.25,320.4\n319.95,56590,09:21:59,320.05,320.3\n319.6,52413,09:22:59,319.6,320.15\n319.15,56305,09:23:59,319.4,319.65\n319.45,36525,09:24:59,319.7,320.05\n318.3,50383,09:25:59,318.35,319.7\n318.3,134400,09:26:59,319.05,319.15\n", "csv1_path": "infiagent/merge_test/49_1_0.csv", "csv2_path": "infiagent/merge_test/49_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'Low' is not greater than 316.1, show the value of  Close, Low , Volume  and Time, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/49_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/49_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Low'] >= 316.1]\ndf = df[[\"Close\", \"Low\", \"Volume\", \"Time\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Low'] >= 316.1]\ndf = df[[\"Close\", \"Low\", \"Volume\", \"Time\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nAge,Category,Date,Expungible\n15,Crimes Against Person,1991,False\n12,Crimes Against Person,1993,False\n9,Crimes Against Person,1994,False\n9,Crimes Against Person,1996,False\n9,Crimes Against Person,1997,False\n8,Crimes Against Person,1998,False\n9,Crimes Against Person,1999,False\n4,Crimes Against Person,2000,False\n8,Crimes Against Person,2002,False\n\nHeader and first few lines of CSV file 2:\nOffense,Disqualifying_Offense,Count,Date\nAggravated Assault,True,1.0,1991\nAggravated Assault,True,1.0,1992\nAggravated Assault,True,4.0,1993\nAggravated Assault,True,1.0,1994\nAggravated Assault,True,2.0,1995\nAggravated Assault,True,3.0,1996\nAggravated Assault,True,2.0,1997\nAggravated Assault,True,2.0,1998\nAggravated Assault,True,1.0,2000\n\nQuestion: Combine rows from both tables where the value of 'Count' is 1.0 and the value of 'Expungible' is False, using common column values for merging, and replace missing values with NaN.", "csv1_example": "Age,Category,Date,Expungible\n15,Crimes Against Person,1991,False\n12,Crimes Against Person,1993,False\n9,Crimes Against Person,1994,False\n9,Crimes Against Person,1996,False\n9,Crimes Against Person,1997,False\n8,Crimes Against Person,1998,False\n9,Crimes Against Person,1999,False\n4,Crimes Against Person,2000,False\n8,Crimes Against Person,2002,False\n7,Crimes Against Person,2003,False\n", "csv2_example": "Offense,Disqualifying_Offense,Count,Date\nAggravated Assault,True,1.0,1991\nAggravated Assault,True,1.0,1992\nAggravated Assault,True,4.0,1993\nAggravated Assault,True,1.0,1994\nAggravated Assault,True,2.0,1995\nAggravated Assault,True,3.0,1996\nAggravated Assault,True,2.0,1997\nAggravated Assault,True,2.0,1998\nAggravated Assault,True,1.0,2000\nAggravated Assault,True,1.0,2002\n", "csv1_path": "infiagent/merge_test/10_0_0.csv", "csv2_path": "infiagent/merge_test/10_0_1.csv", "instruction": "Combine rows from both tables where the value of 'Count' is 1.0 and the value of 'Expungible' is False, using common column values for merging, and replace missing values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/10_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/10_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Count'] == 1.0]\ndf = df[df['Expungible'] == False]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Count'] == 1.0]\ndf = df[df['Expungible'] == False]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nbathrooms,yr_renovated,Unnamed: 0,sqft_living,view,zipcode\n1.0,0,0,1180,0,98178\n2.25,1991,1,2570,0,98125\n1.0,0,2,770,0,98028\n3.0,0,3,1960,0,98136\n2.0,0,4,1680,0,98074\n4.5,0,5,5420,0,98053\n2.25,0,6,1715,0,98003\n1.5,0,7,1060,0,98198\n1.0,0,8,1780,0,98146\n\nHeader and first few lines of CSV file 2:\nlong,sqft_above,condition,sqft_lot,sqft_basement,bedrooms,Unnamed: 0\n-122.257,1180,3,5650,0,3,0\n-122.319,2170,3,7242,400,3,1\n-122.233,770,3,10000,0,2,2\n-122.045,1680,3,8080,0,3,4\n-122.005,3890,3,101930,1530,4,5\n-122.327,1715,3,6819,0,3,6\n-122.337,1050,3,7470,730,3,8\n-122.031,1890,3,6560,0,3,9\n-122.145,1860,3,9796,1700,3,10\n\nQuestion: Combine the rows from the two tables where the 'Unnamed: 0' value is less than or equal to 10806. Display the values of 'bathrooms', 'zipcode', 'bedrooms', 'yr_renovated', 'sqft_lot', 'condition', and 'sqft_living', while also showing 'Unnamed: 0'. Merge the entries with the same column names by filling in any missing values with NAN.", "csv1_example": "bathrooms,yr_renovated,Unnamed: 0,sqft_living,view,zipcode\n1.0,0,0,1180,0,98178\n2.25,1991,1,2570,0,98125\n1.0,0,2,770,0,98028\n3.0,0,3,1960,0,98136\n2.0,0,4,1680,0,98074\n4.5,0,5,5420,0,98053\n2.25,0,6,1715,0,98003\n1.5,0,7,1060,0,98198\n1.0,0,8,1780,0,98146\n1.0,0,11,1160,0,98115\n", "csv2_example": "long,sqft_above,condition,sqft_lot,sqft_basement,bedrooms,Unnamed: 0\n-122.257,1180,3,5650,0,3,0\n-122.319,2170,3,7242,400,3,1\n-122.233,770,3,10000,0,2,2\n-122.045,1680,3,8080,0,3,4\n-122.005,3890,3,101930,1530,4,5\n-122.327,1715,3,6819,0,3,6\n-122.337,1050,3,7470,730,3,8\n-122.031,1890,3,6560,0,3,9\n-122.145,1860,3,9796,1700,3,10\n-122.292,860,4,6000,300,2,11\n", "csv1_path": "infiagent/merge_test/41_2_0.csv", "csv2_path": "infiagent/merge_test/41_2_1.csv", "instruction": "Combine the rows from the two tables where the 'Unnamed: 0' value is less than or equal to 10806. Display the values of 'bathrooms', 'zipcode', 'bedrooms', 'yr_renovated', 'sqft_lot', 'condition', and 'sqft_living', while also showing 'Unnamed: 0'. Merge the entries with the same column names by filling in any missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/41_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/41_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Unnamed: 0'] >= 10806]\ndf = df[[\"bathrooms\", \"zipcode\", \"bedrooms\", \"yr_renovated\", \"sqft_lot\", \"condition\", \"sqft_living\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Unnamed: 0'] <= 10806]\ndf = df[[\"bathrooms\", \"zipcode\", \"bedrooms\", \"yr_renovated\", \"sqft_lot\", \"condition\", \"sqft_living\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nJDTDB\n2451214.540613426\n2451217.540613426\n2451219.540613426\n2451220.540613426\n2451221.540613426\n2451222.540613426\n2451224.540613426\n2451225.540613426\n2451226.540613426\n\nHeader and first few lines of CSV file 2:\nZ,JDTDB,Calendar Date (TDB)\n22126855.53510034,2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000\n23605803.31485712,2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000\n24343878.22074887,2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000\n25080989.23178605,2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000\n26552228.01875284,2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000\n27286310.90891188,2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000\n28019340.125221,2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000\n28751294.0975184,2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000\n29482151.59801376,2451224.540613426, A.D. 1999-Feb-15 00:58:29.0000\n\nQuestion: Combine rows from two tables and display the values of Calendar Date (TDB) and JDTDB. Merge the data based on the same column names and keep only the successful merges.", "csv1_example": "JDTDB\n2451214.540613426\n2451217.540613426\n2451219.540613426\n2451220.540613426\n2451221.540613426\n2451222.540613426\n2451224.540613426\n2451225.540613426\n2451226.540613426\n2451227.540613426\n", "csv2_example": "Z,JDTDB,Calendar Date (TDB)\n22126855.53510034,2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000\n23605803.31485712,2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000\n24343878.22074887,2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000\n25080989.23178605,2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000\n26552228.01875284,2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000\n27286310.90891188,2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000\n28019340.125221,2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000\n28751294.0975184,2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000\n29482151.59801376,2451224.540613426, A.D. 1999-Feb-15 00:58:29.0000\n30211891.73755629,2451225.540613426, A.D. 1999-Feb-16 00:58:29.0000\n", "csv1_path": "infiagent/merge_test/27_0_0.csv", "csv2_path": "infiagent/merge_test/27_0_1.csv", "instruction": "Combine rows from two tables and display the values of Calendar Date (TDB) and JDTDB. Merge the data based on the same column names and keep only the successful merges.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/27_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/27_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Calendar Date (TDB)\", \"JDTDB\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Calendar Date (TDB)\", \"JDTDB\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nExpungible,Date\nFalse,1991\nFalse,1992\nFalse,1995\nFalse,1996\nFalse,1999\nFalse,2000\nFalse,2001\nFalse,2005\nFalse,2006\n\nHeader and first few lines of CSV file 2:\nDisqualifying_Offense,Age,Count,Date,Offense_Type,Category\nTrue,15,1.0,1991,misdemeanor,Crimes Against Person\nTrue,14,1.0,1992,misdemeanor,Crimes Against Person\nTrue,12,4.0,1993,misdemeanor,Crimes Against Person\nTrue,9,1.0,1994,misdemeanor,Crimes Against Person\nTrue,9,2.0,1995,misdemeanor,Crimes Against Person\nTrue,9,3.0,1996,misdemeanor,Crimes Against Person\nTrue,9,2.0,1997,misdemeanor,Crimes Against Person\nTrue,8,2.0,1998,misdemeanor,Crimes Against Person\nTrue,9,3.0,1999,misdemeanor,Crimes Against Person\n\nQuestion: Merge all rows in the two tables that the length of 'Offense_Type' is less than 11, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "Expungible,Date\nFalse,1991\nFalse,1992\nFalse,1995\nFalse,1996\nFalse,1999\nFalse,2000\nFalse,2001\nFalse,2005\nFalse,2006\nFalse,2007\n", "csv2_example": "Disqualifying_Offense,Age,Count,Date,Offense_Type,Category\nTrue,15,1.0,1991,misdemeanor,Crimes Against Person\nTrue,14,1.0,1992,misdemeanor,Crimes Against Person\nTrue,12,4.0,1993,misdemeanor,Crimes Against Person\nTrue,9,1.0,1994,misdemeanor,Crimes Against Person\nTrue,9,2.0,1995,misdemeanor,Crimes Against Person\nTrue,9,3.0,1996,misdemeanor,Crimes Against Person\nTrue,9,2.0,1997,misdemeanor,Crimes Against Person\nTrue,8,2.0,1998,misdemeanor,Crimes Against Person\nTrue,9,3.0,1999,misdemeanor,Crimes Against Person\nTrue,4,1.0,2000,misdemeanor,Crimes Against Person\n", "csv1_path": "infiagent/merge_test/10_2_0.csv", "csv2_path": "infiagent/merge_test/10_2_1.csv", "instruction": "Merge all rows in the two tables that the length of 'Offense_Type' is less than 11, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/10_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/10_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Offense_Type'].str.len() < 11]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Offense_Type'].str.len() < 11]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ngrade,floors,bedrooms,sqft_lot15,sqft_lot,zipcode,sqft_living15,condition,price,long,yr_renovated,Unnamed: 0,sqft_living\n7,1.0,3,5650,5650,98178,1340,3,221900.0,-122.257,0,0,1180\n7,2.0,3,7639,7242,98125,1690,3,538000.0,-122.319,1991,1,2570\n7,1.0,4,5000,5000,98136,1360,5,604000.0,-122.393,0,3,1960\n8,1.0,3,7503,8080,98074,1800,3,510000.0,-122.045,0,4,1680\n11,1.0,4,101930,101930,98053,4760,3,1225000.0,-122.005,0,5,5420\n7,2.0,3,6819,6819,98003,2238,3,257500.0,-122.327,0,6,1715\n7,1.0,3,9711,9711,98198,1650,3,291850.0,-122.315,0,7,1060\n7,1.0,3,8113,7470,98146,1780,3,229500.0,-122.337,0,8,1780\n7,2.0,3,7570,6560,98038,2390,3,323000.0,-122.031,0,9,1890\n\nHeader and first few lines of CSV file 2:\nsqft_above,Unnamed: 0,lat,waterfront,view,sqft_basement\n1180,0,47.5112,0,0,0\n2170,1,47.721,0,0,400\n770,2,47.7379,0,0,0\n1050,3,47.5208,0,0,910\n1680,4,47.6168,0,0,0\n3890,5,47.6561,0,0,1530\n1715,6,47.3097,0,0,0\n1060,7,47.4095,0,0,0\n1050,8,47.5123,0,0,730\n\nQuestion: Combine the rows in two tables where 'sqft_basement' is greater than 0, displaying sqft_lot15, view, sqft_living15, sqft_basement, sqft_living, zipcode, sqft_above, sqft_lot, floors, yr_renovated, long, waterfront, condition, grade, bedrooms, and Unnamed: 0. Merge entries with the same column name and fill missing values with NaN.", "csv1_example": "grade,floors,bedrooms,sqft_lot15,sqft_lot,zipcode,sqft_living15,condition,price,long,yr_renovated,Unnamed: 0,sqft_living\n7,1.0,3,5650,5650,98178,1340,3,221900.0,-122.257,0,0,1180\n7,2.0,3,7639,7242,98125,1690,3,538000.0,-122.319,1991,1,2570\n7,1.0,4,5000,5000,98136,1360,5,604000.0,-122.393,0,3,1960\n8,1.0,3,7503,8080,98074,1800,3,510000.0,-122.045,0,4,1680\n11,1.0,4,101930,101930,98053,4760,3,1225000.0,-122.005,0,5,5420\n7,2.0,3,6819,6819,98003,2238,3,257500.0,-122.327,0,6,1715\n7,1.0,3,9711,9711,98198,1650,3,291850.0,-122.315,0,7,1060\n7,1.0,3,8113,7470,98146,1780,3,229500.0,-122.337,0,8,1780\n7,2.0,3,7570,6560,98038,2390,3,323000.0,-122.031,0,9,1890\n8,1.0,3,8925,9796,98007,2210,3,662500.0,-122.145,0,10,3560\n", "csv2_example": "sqft_above,Unnamed: 0,lat,waterfront,view,sqft_basement\n1180,0,47.5112,0,0,0\n2170,1,47.721,0,0,400\n770,2,47.7379,0,0,0\n1050,3,47.5208,0,0,910\n1680,4,47.6168,0,0,0\n3890,5,47.6561,0,0,1530\n1715,6,47.3097,0,0,0\n1060,7,47.4095,0,0,0\n1050,8,47.5123,0,0,730\n1890,9,47.3684,0,0,0\n", "csv1_path": "infiagent/merge_test/41_0_0.csv", "csv2_path": "infiagent/merge_test/41_0_1.csv", "instruction": "Combine the rows in two tables where 'sqft_basement' is greater than 0, displaying sqft_lot15, view, sqft_living15, sqft_basement, sqft_living, zipcode, sqft_above, sqft_lot, floors, yr_renovated, long, waterfront, condition, grade, bedrooms, and Unnamed: 0. Merge entries with the same column name and fill missing values with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/41_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/41_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['sqft_basement'] > 0]\ndf = df[[\"sqft_lot15\", \"view\", \"sqft_living15\", \"sqft_basement\", \"sqft_living\", \"zipcode\", \"sqft_above\", \"sqft_lot\", \"floors\", \"yr_renovated\", \"long\", \"waterfront\", \"condition\", \"grade\", \"bedrooms\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['sqft_basement'] > 0]\ndf = df[[\"sqft_lot15\", \"view\", \"sqft_living15\", \"sqft_basement\", \"sqft_living\", \"zipcode\", \"sqft_above\", \"sqft_lot\", \"floors\", \"yr_renovated\", \"long\", \"waterfront\", \"condition\", \"grade\", \"bedrooms\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nclass9,ID,class7,class1,class6\n0.0,1,0.0,0.0,0.0\n0.0,2,0.0,1.0,0.0\n0.0,5,1.0,0.0,0.0\n0.0,10,1.0,0.0,0.0\n0.0,12,0.0,0.0,0.0\n0.0,13,0.0,1.0,0.0\n0.0,14,1.0,0.0,0.0\n0.0,21,1.0,0.0,0.0\n0.0,24,1.0,0.0,0.0\n\nHeader and first few lines of CSV file 2:\nID,class3,class2\n1,0.0,0.0\n2,0.0,0.0\n5,0.0,0.0\n10,0.0,0.0\n13,0.0,0.0\n14,0.0,0.0\n21,0.0,0.0\n29,0.0,0.0\n33,0.0,0.0\n\nQuestion: Join both tables on columns with matching names and replace any values in merged rows where 'class9' equals '0.0' with NAN.", "csv1_example": "class9,ID,class7,class1,class6\n0.0,1,0.0,0.0,0.0\n0.0,2,0.0,1.0,0.0\n0.0,5,1.0,0.0,0.0\n0.0,10,1.0,0.0,0.0\n0.0,12,0.0,0.0,0.0\n0.0,13,0.0,1.0,0.0\n0.0,14,1.0,0.0,0.0\n0.0,21,1.0,0.0,0.0\n0.0,24,1.0,0.0,0.0\n0.0,26,1.0,0.0,0.0\n", "csv2_example": "ID,class3,class2\n1,0.0,0.0\n2,0.0,0.0\n5,0.0,0.0\n10,0.0,0.0\n13,0.0,0.0\n14,0.0,0.0\n21,0.0,0.0\n29,0.0,0.0\n33,0.0,0.0\n37,0.0,0.0\n", "csv1_path": "infiagent/merge_test/48_1_0.csv", "csv2_path": "infiagent/merge_test/48_1_1.csv", "instruction": "Join both tables on columns with matching names and replace any values in merged rows where 'class9' equals '0.0' with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/48_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/48_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['class9'] == 0.0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['class9'] == '0.0']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ngdppercap,year\n779.4453145,1952\n853.10071,1962\n836.1971382,1967\n739.9811058,1972\n786.11336,1977\n978.0114388,1982\n852.3959448,1987\n726.7340548,2002\n974.5803384,2007\n\nHeader and first few lines of CSV file 2:\npop,lifeexp,country,year\n8425333,28.801,afghanistan,1952\n9240934,30.332,afghanistan,1957\n10267083,31.997,afghanistan,1962\n11537966,34.02,afghanistan,1967\n13079460,36.088,afghanistan,1972\n14880372,38.438,afghanistan,1977\n12881816,39.854,afghanistan,1982\n16317921,41.674,afghanistan,1992\n22227415,41.763,afghanistan,1997\n\nQuestion: Join both tables on columns with common names and retain only the successfully merged rows where the value of 'pop' is less than 13867957, and display the columns 'pop' and 'year' from the resulting joined table.", "csv1_example": "gdppercap,year\n779.4453145,1952\n853.10071,1962\n836.1971382,1967\n739.9811058,1972\n786.11336,1977\n978.0114388,1982\n852.3959448,1987\n726.7340548,2002\n974.5803384,2007\n", "csv2_example": "pop,lifeexp,country,year\n8425333,28.801,afghanistan,1952\n9240934,30.332,afghanistan,1957\n10267083,31.997,afghanistan,1962\n11537966,34.02,afghanistan,1967\n13079460,36.088,afghanistan,1972\n14880372,38.438,afghanistan,1977\n12881816,39.854,afghanistan,1982\n16317921,41.674,afghanistan,1992\n22227415,41.763,afghanistan,1997\n25268405,42.129,afghanistan,2002\n", "csv1_path": "infiagent/merge_test/35_0_0.csv", "csv2_path": "infiagent/merge_test/35_0_1.csv", "instruction": "Join both tables on columns with common names and retain only the successfully merged rows where the value of 'pop' is less than 13867957, and display the columns 'pop' and 'year' from the resulting joined table.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/35_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/35_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['pop'] < 13867957]\ndf = df[[\"pop\", \"year\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['pop'] < 13867957]\ndf = df[[\"pop\", \"year\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nname,Unnamed: 0,Skin color\nAbe Sapien,1,blue\nAbin Sur,2,red\nAbomination,3,-\nAbraxas,4,-\nAdam Strange,7,-\nAgent 13,8,-\nAgent Bob,9,-\nAgent Zero,10,-\nAir-Walker,11,-\n\nHeader and first few lines of CSV file 2:\nAlignment,Unnamed: 0,Publisher,Weight,Eye color,Hair color,Race,Gender\ngood,0,Marvel Comics,441.0,yellow,No Hair,Human,Male\ngood,1,Dark Horse Comics,65.0,blue,No Hair,Icthyo Sapien,Male\ngood,2,DC Comics,90.0,blue,No Hair,Ungaran,Male\nbad,3,Marvel Comics,441.0,green,No Hair,Human / Radiation,Male\nbad,4,Marvel Comics,-99.0,blue,Black,Cosmic Entity,Male\nbad,5,Marvel Comics,122.0,blue,No Hair,Human,Male\ngood,6,NBC - Heroes,-99.0,blue,Blond,-,Male\ngood,7,DC Comics,88.0,blue,Blond,Human,Male\ngood,8,Marvel Comics,61.0,blue,Blond,-,Female\n\nQuestion: Combine the tables by matching rows with 'Unnamed: 0' values less than 367, and display Race, Alignment, Eye color, Publisher, Gender, Skin color, Hair color, and Unnamed: 0 columns. Only include successfully merged entries with the same column names.", "csv1_example": "name,Unnamed: 0,Skin color\nAbe Sapien,1,blue\nAbin Sur,2,red\nAbomination,3,-\nAbraxas,4,-\nAdam Strange,7,-\nAgent 13,8,-\nAgent Bob,9,-\nAgent Zero,10,-\nAir-Walker,11,-\nAjax,12,-\n", "csv2_example": "Alignment,Unnamed: 0,Publisher,Weight,Eye color,Hair color,Race,Gender\ngood,0,Marvel Comics,441.0,yellow,No Hair,Human,Male\ngood,1,Dark Horse Comics,65.0,blue,No Hair,Icthyo Sapien,Male\ngood,2,DC Comics,90.0,blue,No Hair,Ungaran,Male\nbad,3,Marvel Comics,441.0,green,No Hair,Human / Radiation,Male\nbad,4,Marvel Comics,-99.0,blue,Black,Cosmic Entity,Male\nbad,5,Marvel Comics,122.0,blue,No Hair,Human,Male\ngood,6,NBC - Heroes,-99.0,blue,Blond,-,Male\ngood,7,DC Comics,88.0,blue,Blond,Human,Male\ngood,8,Marvel Comics,61.0,blue,Blond,-,Female\ngood,9,Marvel Comics,81.0,brown,Brown,Human,Male\n", "csv1_path": "infiagent/merge_test/39_1_0.csv", "csv2_path": "infiagent/merge_test/39_1_1.csv", "instruction": "Combine the tables by matching rows with 'Unnamed: 0' values less than 367, and display Race, Alignment, Eye color, Publisher, Gender, Skin color, Hair color, and Unnamed: 0 columns. Only include successfully merged entries with the same column names.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/39_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/39_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Unnamed: 0'] < 367]\ndf = df[[\"Race\", \"Alignment\", \"Eye color\", \"Publisher\", \"Gender\", \"Skin color\", \"Hair color\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Unnamed: 0'] < 367]\ndf = df[[\"Race\", \"Alignment\", \"Eye color\", \"Publisher\", \"Gender\", \"Skin color\", \"Hair color\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nImpaired,Unnamed: 0\n1,0\n1,1\n1,4\n1,6\n1,8\n1,9\n1,10\n1,11\n1,12\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,Gender,id,gloss,Age,transcript_id\n0,0,3772732,and she asked um where do you wanna go,10,9573\n1,0,3772777,and the kids I guess answered Mcdonalds,10,9573\n2,0,3772812,and so they went to Mcdonalds,10,9573\n3,0,3772852,and Lisa couldn't make up her mind,10,9573\n4,0,3772884,and um she was deciding between a Big_Mac or a happymeal,10,9573\n5,0,3772934,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,10,9573\n6,0,3772996,and the mother ordered a salad,10,9573\n7,0,3773020,and the mother reached down to get the purse,10,9573\n10,0,3773104,um Joe wo woke up one morning,10,9573\n\nQuestion: Merge all rows in the two tables that the value of 'gloss' is 'the end', merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "Impaired,Unnamed: 0\n1,0\n1,1\n1,4\n1,6\n1,8\n1,9\n1,10\n1,11\n1,12\n1,13\n", "csv2_example": "Unnamed: 0,Gender,id,gloss,Age,transcript_id\n0,0,3772732,and she asked um where do you wanna go,10,9573\n1,0,3772777,and the kids I guess answered Mcdonalds,10,9573\n2,0,3772812,and so they went to Mcdonalds,10,9573\n3,0,3772852,and Lisa couldn't make up her mind,10,9573\n4,0,3772884,and um she was deciding between a Big_Mac or a happymeal,10,9573\n5,0,3772934,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,10,9573\n6,0,3772996,and the mother ordered a salad,10,9573\n7,0,3773020,and the mother reached down to get the purse,10,9573\n10,0,3773104,um Joe wo woke up one morning,10,9573\n11,0,3773149,and he w he looked at the clock,10,9573\n", "csv1_path": "infiagent/merge_test/37_3_0.csv", "csv2_path": "infiagent/merge_test/37_3_1.csv", "instruction": "Merge all rows in the two tables that the value of 'gloss' is 'the end', merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/37_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/37_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['gloss'] == 'the end']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['gloss'] == 'the end']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,NumberOfOpenCreditLinesAndLoans,MonthlyIncome,NumberRealEstateLoansOrLines\n1,4,5700.0,0\n2,15,9141.0,4\n4,7,3200.0,2\n5,4,3865.0,0\n6,4,4140.0,0\n7,5,0.0,0\n8,8,3301.0,1\n9,4,,1\n11,8,7400.0,1\n\nHeader and first few lines of CSV file 2:\nNumberOfTime30-59DaysPastDueNotWorse,NumberOfTimes90DaysLate,NumberOfDependents,Unnamed: 0,age,RevolvingUtilizationOfUnsecuredLines\n0,0,2.0,2,57,0.463295269\n0,0,2.0,3,59,0.043275036\n1,0,0.0,4,38,0.280308229\n0,0,1.0,5,27,0.9999999\n0,0,1.0,6,63,0.509791452\n0,0,3.0,7,50,0.587778161\n1,0,1.0,8,79,0.046148938\n0,0,0.0,9,68,0.013527027\n98,98,0.0,10,23,0.9999999\n\nQuestion: Merge all rows in the two tables that the value of 'NumberOfTimes90DaysLate' is '0', show the value of  NumberOfTime30-59DaysPastDueNotWorse, NumberOfOpenCreditLinesAndLoans , NumberRealEstateLoansOrLines , NumberOfTimes90DaysLate , RevolvingUtilizationOfUnsecuredLines  and Unnamed: 0, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "Unnamed: 0,NumberOfOpenCreditLinesAndLoans,MonthlyIncome,NumberRealEstateLoansOrLines\n1,4,5700.0,0\n2,15,9141.0,4\n4,7,3200.0,2\n5,4,3865.0,0\n6,4,4140.0,0\n7,5,0.0,0\n8,8,3301.0,1\n9,4,,1\n11,8,7400.0,1\n12,6,4250.0,2\n", "csv2_example": "NumberOfTime30-59DaysPastDueNotWorse,NumberOfTimes90DaysLate,NumberOfDependents,Unnamed: 0,age,RevolvingUtilizationOfUnsecuredLines\n0,0,2.0,2,57,0.463295269\n0,0,2.0,3,59,0.043275036\n1,0,0.0,4,38,0.280308229\n0,0,1.0,5,27,0.9999999\n0,0,1.0,6,63,0.509791452\n0,0,3.0,7,50,0.587778161\n1,0,1.0,8,79,0.046148938\n0,0,0.0,9,68,0.013527027\n98,98,0.0,10,23,0.9999999\n0,0,1.0,11,37,0.0284849\n", "csv1_path": "infiagent/merge_test/25_1_0.csv", "csv2_path": "infiagent/merge_test/25_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'NumberOfTimes90DaysLate' is '0', show the value of  NumberOfTime30-59DaysPastDueNotWorse, NumberOfOpenCreditLinesAndLoans , NumberRealEstateLoansOrLines , NumberOfTimes90DaysLate , RevolvingUtilizationOfUnsecuredLines  and Unnamed: 0, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/25_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/25_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['NumberOfTimes90DaysLate'] == 0]\ndf = df[[\"NumberOfTime30-59DaysPastDueNotWorse\", \"NumberOfOpenCreditLinesAndLoans\", \"NumberRealEstateLoansOrLines\", \"NumberOfTimes90DaysLate\", \"RevolvingUtilizationOfUnsecuredLines\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['NumberOfTimes90DaysLate'] == '0']\ndf = df[[\"NumberOfTime30-59DaysPastDueNotWorse\", \"NumberOfOpenCreditLinesAndLoans\", \"NumberRealEstateLoansOrLines\", \"NumberOfTimes90DaysLate\", \"RevolvingUtilizationOfUnsecuredLines\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nclass5,ID,class3\n0.0,1,0.0\n0.0,2,0.0\n0.0,5,0.0\n0.0,10,0.0\n0.0,13,0.0\n0.0,14,0.0\n0.0,21,0.0\n0.0,24,0.0\n0.0,26,0.0\n\nHeader and first few lines of CSV file 2:\nclass9,ID,class2,class1\n0.0,1,0.0,0.0\n0.0,2,0.0,1.0\n0.0,5,0.0,0.0\n0.0,10,0.0,0.0\n0.0,13,0.0,1.0\n0.0,14,0.0,0.0\n0.0,21,0.0,0.0\n0.0,24,0.0,0.0\n0.0,26,0.0,0.0\n\nQuestion: Merge all rows in the two tables that the value of 'class3' is not greater than 0.0, show the value of  class5, class1 , class9 , class2  and ID, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "class5,ID,class3\n0.0,1,0.0\n0.0,2,0.0\n0.0,5,0.0\n0.0,10,0.0\n0.0,13,0.0\n0.0,14,0.0\n0.0,21,0.0\n0.0,24,0.0\n0.0,26,0.0\n0.0,29,0.0\n", "csv2_example": "class9,ID,class2,class1\n0.0,1,0.0,0.0\n0.0,2,0.0,1.0\n0.0,5,0.0,0.0\n0.0,10,0.0,0.0\n0.0,13,0.0,1.0\n0.0,14,0.0,0.0\n0.0,21,0.0,0.0\n0.0,24,0.0,0.0\n0.0,26,0.0,0.0\n0.0,29,0.0,0.0\n", "csv1_path": "infiagent/merge_test/48_3_0.csv", "csv2_path": "infiagent/merge_test/48_3_1.csv", "instruction": "Merge all rows in the two tables that the value of 'class3' is not greater than 0.0, show the value of  class5, class1 , class9 , class2  and ID, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/48_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/48_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['class3'] >= 0.0]\ndf = df[[\"class5\", \"class1\", \"class9\", \"class2\", \"ID\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['class3'] >= 0.0]\ndf = df[[\"class5\", \"class1\", \"class9\", \"class2\", \"ID\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsmoker,age,children\nyes,19,0\nno,18,1\nno,28,3\nno,33,0\nno,32,0\nno,31,0\nno,46,1\nno,37,3\nno,60,0\n\nHeader and first few lines of CSV file 2:\nsex,age,bmi\nfemale,19,27.9\nmale,18,33.77\nmale,33,22.705\nmale,32,28.88\nfemale,31,25.74\nfemale,46,33.44\nfemale,37,27.74\nfemale,60,25.84\nmale,25,26.22\n\nQuestion: Merge all rows in the two tables that the length of 'smoker' is less than 2 and the value of 'children' is not less than 1, merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "smoker,age,children\nyes,19,0\nno,18,1\nno,28,3\nno,33,0\nno,32,0\nno,31,0\nno,46,1\nno,37,3\nno,60,0\nno,25,0\n", "csv2_example": "sex,age,bmi\nfemale,19,27.9\nmale,18,33.77\nmale,33,22.705\nmale,32,28.88\nfemale,31,25.74\nfemale,46,33.44\nfemale,37,27.74\nfemale,60,25.84\nmale,25,26.22\nfemale,62,26.29\n", "csv1_path": "infiagent/merge_test/43_0_0.csv", "csv2_path": "infiagent/merge_test/43_0_1.csv", "instruction": "Merge all rows in the two tables that the length of 'smoker' is less than 2 and the value of 'children' is not less than 1, merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/43_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/43_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['smoker'].str.len() < 2]\ndf = df[df['children'] <= 1]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['smoker'].str.len() < 2]\ndf = df[df['children'] <= 1]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nMiles traveled to date:,Date,Cumulative trips (since launch):\n23121175,10/1/2014,13296973\n23181814,10/2/2014,13335259\n23247553,10/3/2014,13374215\n23271807,10/4/2014,13389303\n23320737,10/5/2014,13415550\n23377117,10/6/2014,13451201\n23435857,10/7/2014,13488132\n23496451,10/8/2014,13526002\n23609006,10/10/2014,13597119\n\nHeader and first few lines of CSV file 2:\n7-Day Passes Purchased (midnight to 11:59 pm),Miles traveled today (midnight to 11:59 pm),Annual Member Sign-Ups (midnight to 11:59 pm),Total Annual Memberships Sold,Date,24-Hour Passes Purchased (midnight to 11:59 pm)\n86,60639,113,124959,10/2/2014,602\n107,65739,65,125024,10/3/2014,1276\n90,48930,51,125109,10/5/2014,1470\n99,56380,94,125203,10/6/2014,710\n86,60594,92,125354,10/8/2014,667\n78,54690,57,125464,10/10/2014,905\n34,23278,23,125487,10/11/2014,528\n80,52781,42,125529,10/12/2014,2083\n60,40275,60,125589,10/13/2014,749\n\nQuestion: Merge all rows in the two tables that the value of 'Total Annual Memberships Sold' is less than 126811 and the value of 'Miles traveled today (midnight to 11:59 pm)' is less than 27503, merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "Miles traveled to date:,Date,Cumulative trips (since launch):\n23121175,10/1/2014,13296973\n23181814,10/2/2014,13335259\n23247553,10/3/2014,13374215\n23271807,10/4/2014,13389303\n23320737,10/5/2014,13415550\n23377117,10/6/2014,13451201\n23435857,10/7/2014,13488132\n23496451,10/8/2014,13526002\n23609006,10/10/2014,13597119\n23632284,10/11/2014,13611924\n", "csv2_example": "7-Day Passes Purchased (midnight to 11:59 pm),Miles traveled today (midnight to 11:59 pm),Annual Member Sign-Ups (midnight to 11:59 pm),Total Annual Memberships Sold,Date,24-Hour Passes Purchased (midnight to 11:59 pm)\n86,60639,113,124959,10/2/2014,602\n107,65739,65,125024,10/3/2014,1276\n90,48930,51,125109,10/5/2014,1470\n99,56380,94,125203,10/6/2014,710\n86,60594,92,125354,10/8/2014,667\n78,54690,57,125464,10/10/2014,905\n34,23278,23,125487,10/11/2014,528\n80,52781,42,125529,10/12/2014,2083\n60,40275,60,125589,10/13/2014,749\n101,61691,62,125651,10/14/2014,869\n", "csv1_path": "infiagent/merge_test/1_0_0.csv", "csv2_path": "infiagent/merge_test/1_0_1.csv", "instruction": "Merge all rows in the two tables that the value of 'Total Annual Memberships Sold' is less than 126811 and the value of 'Miles traveled today (midnight to 11:59 pm)' is less than 27503, merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/1_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/1_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Total Annual Memberships Sold'] < 126811]\ndf = df[df['Miles traveled today (midnight to 11:59 pm)'] < 27503]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Total Annual Memberships Sold'] < 126811]\ndf = df[df['Miles traveled today (midnight to 11:59 pm)'] < 27503]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nvotes_dem,diff,per_gop,votes_gop\n93003.0,37,410,0.528870018006,130413.0\n5908.0,12,202,0.734357893029,18110.0\n18409.0,54,371,0.773514719949,72780.0\n4848.0,583,0.522714148219,5431.0\n1874.0,4,859,0.769661636946,6733.0\n2150.0,20,658,0.8985187519700001,22808.0\n3530.0,2,391,0.242288874708,1139.0\n3716.0,1,175,0.563154864709,4891.0\n13197.0,19,606,0.6923969942589999,32803.0\n\nHeader and first few lines of CSV file 2:\nvotes_dem,state_abbr,per_dem,county_name,per_point_diff,total_votes\n93003.0,AK,0.37715947248,Alaska,15.17%,246588.0\n5908.0,AL,0.239568549532,Autauga County,49.48%,24661.0\n4848.0,AL,0.466602502406,Barbour County,5.61%,10390.0\n1874.0,AL,0.214220393233,Bibb County,55.54%,8748.0\n2150.0,AL,0.0846990230066,Blount County,81.38%,25384.0\n3530.0,AL,0.750904062965,Bullock County,50.86%,4701.0\n3716.0,AL,0.427864133564,Butler County,13.53%,8685.0\n5763.0,AL,0.418275511685,Chambers County,14.81%,13778.0\n5712.0,AL,0.441558441558,Clarke County,10.80%,12936.0\n\nQuestion: Merge all rows in the two tables, show the value of  per_point_diff, per_dem , votes_gop , diff , total_votes , state_abbr  and votes_dem, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "votes_dem,diff,per_gop,votes_gop\n93003.0,37,410,0.528870018006,130413.0\n5908.0,12,202,0.734357893029,18110.0\n18409.0,54,371,0.773514719949,72780.0\n4848.0,583,0.522714148219,5431.0\n1874.0,4,859,0.769661636946,6733.0\n2150.0,20,658,0.8985187519700001,22808.0\n3530.0,2,391,0.242288874708,1139.0\n3716.0,1,175,0.563154864709,4891.0\n13197.0,19,606,0.6923969942589999,32803.0\n5763.0,2,040,0.5663376397150001,7803.0\n", "csv2_example": "votes_dem,state_abbr,per_dem,county_name,per_point_diff,total_votes\n93003.0,AK,0.37715947248,Alaska,15.17%,246588.0\n5908.0,AL,0.239568549532,Autauga County,49.48%,24661.0\n4848.0,AL,0.466602502406,Barbour County,5.61%,10390.0\n1874.0,AL,0.214220393233,Bibb County,55.54%,8748.0\n2150.0,AL,0.0846990230066,Blount County,81.38%,25384.0\n3530.0,AL,0.750904062965,Bullock County,50.86%,4701.0\n3716.0,AL,0.427864133564,Butler County,13.53%,8685.0\n5763.0,AL,0.418275511685,Chambers County,14.81%,13778.0\n5712.0,AL,0.441558441558,Clarke County,10.80%,12936.0\n1234.0,AL,0.187766281193,Clay County,60.80%,6572.0\n", "csv1_path": "infiagent/merge_test/30_3_0.csv", "csv2_path": "infiagent/merge_test/30_3_1.csv", "instruction": "Merge all rows in the two tables, show the value of  per_point_diff, per_dem , votes_gop , diff , total_votes , state_abbr  and votes_dem, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/30_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/30_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"per_point_diff\", \"per_dem\", \"votes_gop\", \"diff\", \"total_votes\", \"state_abbr\", \"votes_dem\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"per_point_diff\", \"per_dem\", \"votes_gop\", \"diff\", \"total_votes\", \"state_abbr\", \"votes_dem\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLatitude,AveRooms,MedInc,MedianHouseValue,Longitude\n33.93,3.67616191904048,0.9298,1.0,-118.25\n32.79,4.499388004895961,2.7006,1.663,-117.03\n34.89,6.184375,5.0286,1.58,-120.43\n33.82,3.9095238095238094,4.1292,2.019,-118.27\n41.4,6.997518610421836,2.6062,0.526,-120.12\n34.05,5.8474576271186445,5.222,5.00001,-118.39\n33.97,3.727699530516432,2.2917,1.12,-118.25\n36.47,7.075313807531381,3.2292,2.25,-120.95\n34.05,4.149245388485188,2.7002,0.883,-117.31\n\nHeader and first few lines of CSV file 2:\nAveBedrms,MedInc,AveOccup\n1.1004497751124438,0.9298,3.9940029985007497\n1.06875,5.0286,3.121875\n0.9825834542815676,3.9038,2.816011574632264\n1.051282051282051,7.1754,2.816011574632264\n1.0333333333333334,4.1292,3.75\n1.2779156327543424,2.6062,2.816011574632264\n1.0527306967984935,5.222,2.35969868173258\n0.9859154929577464,2.2917,3.821596244131456\n1.1813880126182963,7.472,2.331230283911672\n\nQuestion: Combine the rows of the two tables where the 'MedianHouseValue' is not less than 1.81, and display the values of Longitude, AveBedrms, AveRooms, and MedInc. Merge entries with the same column name, and fill in any missing values with NAN.", "csv1_example": "Latitude,AveRooms,MedInc,MedianHouseValue,Longitude\n33.93,3.67616191904048,0.9298,1.0,-118.25\n32.79,4.499388004895961,2.7006,1.663,-117.03\n34.89,6.184375,5.0286,1.58,-120.43\n33.82,3.9095238095238094,4.1292,2.019,-118.27\n41.4,6.997518610421836,2.6062,0.526,-120.12\n34.05,5.8474576271186445,5.222,5.00001,-118.39\n33.97,3.727699530516432,2.2917,1.12,-118.25\n36.47,7.075313807531381,3.2292,2.25,-120.95\n34.05,4.149245388485188,2.7002,0.883,-117.31\n37.44,8.84504132231405,15.0001,5.00001,-122.22\n", "csv2_example": "AveBedrms,MedInc,AveOccup\n1.1004497751124438,0.9298,3.9940029985007497\n1.06875,5.0286,3.121875\n0.9825834542815676,3.9038,2.816011574632264\n1.051282051282051,7.1754,2.816011574632264\n1.0333333333333334,4.1292,3.75\n1.2779156327543424,2.6062,2.816011574632264\n1.0527306967984935,5.222,2.35969868173258\n0.9859154929577464,2.2917,3.821596244131456\n1.1813880126182963,7.472,2.331230283911672\n1.179988820570151,2.7002,2.287311347121297\n", "csv1_path": "infiagent/merge_test/45_2_0.csv", "csv2_path": "infiagent/merge_test/45_2_1.csv", "instruction": "Combine the rows of the two tables where the 'MedianHouseValue' is not less than 1.81, and display the values of Longitude, AveBedrms, AveRooms, and MedInc. Merge entries with the same column name, and fill in any missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/45_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/45_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['MedianHouseValue'] <= 1.81]\ndf = df[[\"Longitude\", \"AveBedrms\", \"AveRooms\", \"MedInc\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['MedianHouseValue'] <= 1.81]\ndf = df[[\"Longitude\", \"AveBedrms\", \"AveRooms\", \"MedInc\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nact_arr_time,fleet_number,origin_station,station_code,origin_station_code,station,start_time,sch_dep_time,station_type\n,3088.0,Machine Mist,MAM,MAM,Machine Mist,21/09/15 05:01:00,21/09/15 05:01:00,Passenger\n,3088.0,Roll Test,ROT,ROT,Roll Test,21/09/15 05:50:00,21/09/15 05:50:00,Passenger\n,3088.0,Attempt Pin,ATP,ATP,Attempt Pin,21/09/15 07:20:00,21/09/15 07:20:00,Passenger\n,3063.0,Step Scarecrow Turnback,STSTb,STSTb,Step Scarecrow Turnback,21/09/15 05:06:30,21/09/15 05:06:30,Vehicle\n,3063.0,Children Cast Turnback,CHATb,CHATb,Children Cast Turnback,21/09/15 06:38:00,21/09/15 06:38:00,Vehicle\n,3006.0,Bridge Bottle,BRB,BRB,Bridge Bottle,21/09/15 06:32:00,21/09/15 06:32:00,Passenger\n,3017.0,Crib Team,CRT,CRT,Crib Team,21/09/15 07:32:00,21/09/15 07:32:00,Passenger\n,,Clouds Goose,CLG,CLG,Clouds Goose,21/09/15 23:58:00,21/09/15 23:58:00,Passenger\n,3099.0,Skin Shape,SKH,SKH,Skin Shape,22/09/15 00:20:00,22/09/15 00:20:00,Passenger\n\nHeader and first few lines of CSV file 2:\nsch_arr_time,end_time,destination_station_code,origin_station_code,route_code\n,21/09/15 05:41:00,ROT,MAM,4\n,21/09/15 07:14:00,ATP,ROT,68\n,21/09/15 08:42:00,ROT,ATP,1\n,21/09/15 05:44:00,JAV,STSTb,73\n,21/09/15 06:32:00,CHATb,JAV,20\n,21/09/15 07:20:00,JAV,CHATb,74\n,21/09/15 07:27:00,CRT,BRB,10\n,21/09/15 08:27:00,BRB,CRT,58\n,22/09/15 00:05:30,STSTb,CLG,83\n\nQuestion: Merge all rows in the two tables, show the value of  route_code, act_arr_time , end_time , station , sch_dep_time , sch_arr_time  and origin_station_code, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "act_arr_time,fleet_number,origin_station,station_code,origin_station_code,station,start_time,sch_dep_time,station_type\n,3088.0,Machine Mist,MAM,MAM,Machine Mist,21/09/15 05:01:00,21/09/15 05:01:00,Passenger\n,3088.0,Roll Test,ROT,ROT,Roll Test,21/09/15 05:50:00,21/09/15 05:50:00,Passenger\n,3088.0,Attempt Pin,ATP,ATP,Attempt Pin,21/09/15 07:20:00,21/09/15 07:20:00,Passenger\n,3063.0,Step Scarecrow Turnback,STSTb,STSTb,Step Scarecrow Turnback,21/09/15 05:06:30,21/09/15 05:06:30,Vehicle\n,3063.0,Children Cast Turnback,CHATb,CHATb,Children Cast Turnback,21/09/15 06:38:00,21/09/15 06:38:00,Vehicle\n,3006.0,Bridge Bottle,BRB,BRB,Bridge Bottle,21/09/15 06:32:00,21/09/15 06:32:00,Passenger\n,3017.0,Crib Team,CRT,CRT,Crib Team,21/09/15 07:32:00,21/09/15 07:32:00,Passenger\n,,Clouds Goose,CLG,CLG,Clouds Goose,21/09/15 23:58:00,21/09/15 23:58:00,Passenger\n,3099.0,Skin Shape,SKH,SKH,Skin Shape,22/09/15 00:20:00,22/09/15 00:20:00,Passenger\n,3052.0,Skate Stone Turnback,SKSTb,SKSTb,Skate Stone Turnback,21/09/15 05:49:00,21/09/15 05:49:00,Vehicle\n", "csv2_example": "sch_arr_time,end_time,destination_station_code,origin_station_code,route_code\n,21/09/15 05:41:00,ROT,MAM,4\n,21/09/15 07:14:00,ATP,ROT,68\n,21/09/15 08:42:00,ROT,ATP,1\n,21/09/15 05:44:00,JAV,STSTb,73\n,21/09/15 06:32:00,CHATb,JAV,20\n,21/09/15 07:20:00,JAV,CHATb,74\n,21/09/15 07:27:00,CRT,BRB,10\n,21/09/15 08:27:00,BRB,CRT,58\n,22/09/15 00:05:30,STSTb,CLG,83\n,22/09/15 00:46:00,MAM,SKH,91\n", "csv1_path": "infiagent/merge_test/2_2_0.csv", "csv2_path": "infiagent/merge_test/2_2_1.csv", "instruction": "Merge all rows in the two tables, show the value of  route_code, act_arr_time , end_time , station , sch_dep_time , sch_arr_time  and origin_station_code, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/2_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/2_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"route_code\", \"act_arr_time\", \"end_time\", \"station\", \"sch_dep_time\", \"sch_arr_time\", \"origin_station_code\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"route_code\", \"act_arr_time\", \"end_time\", \"station\", \"sch_dep_time\", \"sch_arr_time\", \"origin_station_code\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\n#featureID,LibraryID\n358.3677167129743_3.65612984126984,\n423.2744890715284_4.29798541001065,\n389.2691196723436_3.383737479270316,\n332.3307817246258_5.3103554720133594,Spectral Match to Benzyltetradecyldimethylammonium from NIST14\n358.28219024838404_4.7314112935323385,\n516.3001954425159_3.6121693593314776,Spectral Match to Taurocholic acid from NIST14\n357.27906873865504_4.755498426870746,Spectral Match to Chenodeoxycholic acid from NIST14\n358.28198237386744_5.002188993710692,\n506.27907974349694_3.788732373472948,\n\nHeader and first few lines of CSV file 2:\n#featureID,row ID,importance.score,_feature_id,standard_indentification_level_1\n358.3677167129743_3.65612984126984,241,0.0670524225212391,358.3677167129743_3.65612984126984,\n423.2744890715284_4.29798541001065,695,0.0405984341742611,423.2744890715284_4.29798541001065,\n304.2993572401259_5.121302585521083,382,0.0341410429343644,304.2993572401259_5.121302585521083,\n332.3307817246258_5.3103554720133594,612,0.0322569080107531,332.3307817246258_5.3103554720133594,\n358.28219024838404_4.7314112935323385,168,0.0272377330395867,358.28219024838404_4.7314112935323385,\n516.3001954425159_3.6121693593314776,822,0.0264573012282939,516.3001954425159_3.6121693593314776,\n326.377650457406_5.358003633720932,109,0.0234222353379512,326.377650457406_5.358003633720932,\n357.27906873865504_4.755498426870746,100,0.0232615688365795,357.27906873865504_4.755498426870746,Chenodeoxycholic acid\n358.28198237386744_5.002188993710692,17,0.0218383013013502,358.28198237386744_5.002188993710692,\n\nQuestion: Merge all rows in the two tables that the length of '#featureID' is greater than 36, show the value of  row ID, standard_indentification_level_1  and #featureID, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "#featureID,LibraryID\n358.3677167129743_3.65612984126984,\n423.2744890715284_4.29798541001065,\n389.2691196723436_3.383737479270316,\n332.3307817246258_5.3103554720133594,Spectral Match to Benzyltetradecyldimethylammonium from NIST14\n358.28219024838404_4.7314112935323385,\n516.3001954425159_3.6121693593314776,Spectral Match to Taurocholic acid from NIST14\n357.27906873865504_4.755498426870746,Spectral Match to Chenodeoxycholic acid from NIST14\n358.28198237386744_5.002188993710692,\n506.27907974349694_3.788732373472948,\n533.3268329649845_3.536782531194295,\n", "csv2_example": "#featureID,row ID,importance.score,_feature_id,standard_indentification_level_1\n358.3677167129743_3.65612984126984,241,0.0670524225212391,358.3677167129743_3.65612984126984,\n423.2744890715284_4.29798541001065,695,0.0405984341742611,423.2744890715284_4.29798541001065,\n304.2993572401259_5.121302585521083,382,0.0341410429343644,304.2993572401259_5.121302585521083,\n332.3307817246258_5.3103554720133594,612,0.0322569080107531,332.3307817246258_5.3103554720133594,\n358.28219024838404_4.7314112935323385,168,0.0272377330395867,358.28219024838404_4.7314112935323385,\n516.3001954425159_3.6121693593314776,822,0.0264573012282939,516.3001954425159_3.6121693593314776,\n326.377650457406_5.358003633720932,109,0.0234222353379512,326.377650457406_5.358003633720932,\n357.27906873865504_4.755498426870746,100,0.0232615688365795,357.27906873865504_4.755498426870746,Chenodeoxycholic acid\n358.28198237386744_5.002188993710692,17,0.0218383013013502,358.28198237386744_5.002188993710692,\n506.27907974349694_3.788732373472948,446,0.0215121109166815,506.27907974349694_3.788732373472948,\n", "csv1_path": "infiagent/merge_test/42_1_0.csv", "csv2_path": "infiagent/merge_test/42_1_1.csv", "instruction": "Merge all rows in the two tables that the length of '#featureID' is greater than 36, show the value of  row ID, standard_indentification_level_1  and #featureID, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/42_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/42_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['#featureID'].str.len() > 36]\ndf = df[[\"row ID\", \"standard_indentification_level_1\", \"#featureID\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['#featureID'].str.len() > 36]\ndf = df[[\"row ID\", \"standard_indentification_level_1\", \"#featureID\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nweathersit,mnth,weekday,holiday,instant,hum,temp,registered\n1,1,6,0,1,0.81,0.24,13\n1,1,6,0,2,0.8,0.22,32\n1,1,6,0,3,0.8,0.22,27\n1,1,6,0,4,0.75,0.24,10\n1,1,6,0,5,0.75,0.24,1\n1,1,6,0,7,0.8,0.22,0\n1,1,6,0,9,0.75,0.24,7\n1,1,6,0,10,0.76,0.32,6\n1,1,6,0,11,0.76,0.38,24\n\nHeader and first few lines of CSV file 2:\ncasual,instant,hr,workingday\n3,1,0,0\n8,2,1,0\n5,3,2,0\n3,4,3,0\n0,5,4,0\n0,6,5,0\n2,7,6,0\n1,8,7,0\n1,9,8,0\n\nQuestion: Merge all rows in the two tables, show the value of  casual and instant, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "weathersit,mnth,weekday,holiday,instant,hum,temp,registered\n1,1,6,0,1,0.81,0.24,13\n1,1,6,0,2,0.8,0.22,32\n1,1,6,0,3,0.8,0.22,27\n1,1,6,0,4,0.75,0.24,10\n1,1,6,0,5,0.75,0.24,1\n1,1,6,0,7,0.8,0.22,0\n1,1,6,0,9,0.75,0.24,7\n1,1,6,0,10,0.76,0.32,6\n1,1,6,0,11,0.76,0.38,24\n2,1,6,0,14,0.72,0.46,47\n", "csv2_example": "casual,instant,hr,workingday\n3,1,0,0\n8,2,1,0\n5,3,2,0\n3,4,3,0\n0,5,4,0\n0,6,5,0\n2,7,6,0\n1,8,7,0\n1,9,8,0\n8,10,9,0\n", "csv1_path": "infiagent/merge_test/16_3_0.csv", "csv2_path": "infiagent/merge_test/16_3_1.csv", "instruction": "Merge all rows in the two tables, show the value of  casual and instant, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/16_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/16_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"casual\", \"instant\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"casual\", \"instant\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnumber_of_doubles,salary_in_thousands_of_dollars,indicator_of_arbitration_in_1991_1992,number_of_stolen_bases,on_base_percentage,indicator_of_free_agent_in_1991_1992,number_of_home_runs,batting_average,number_of_hits\n17,2600,0.0,0,0.335,1,18.0,0.269,111\n28,2313,0.0,3,0.346,0,8.0,0.273,169\n32,2175,0.0,22,0.379,0,26.0,0.291,170\n7,460,0.0,2,0.279,0,3.0,0.228,38\n11,240,0.0,14,0.327,0,1.0,0.25,61\n10,200,0.0,13,0.24,0,10.0,0.203,64\n5,177,0.0,2,0.283,0,0.0,0.262,38\n9,140,0.0,3,0.307,0,6.0,0.222,45\n0,115,0.0,0,0.37,0,0.0,0.261,6\n\nHeader and first few lines of CSV file 2:\nnumber_of_walks,number_of_triples,indicator_of_arbitration_eligibility,salary_in_thousands_of_dollars,number_of_runs,indicator_of_free_agency_eligibility,number_of_runs_batted_in,number_of_strike_outs\n63,1,0.0,2500,54,1.0,73,116.0\n23,7,1.0,2475,59,0.0,50,64.0\n70,5,1.0,2313,87,0.0,58,53.0\n15,1,0.0,600,34,1.0,38,45.0\n11,2,0.0,460,16,0.0,21,32.0\n24,0,0.0,240,40,0.0,18,26.0\n14,1,0.0,200,39,0.0,33,96.0\n5,0,0.0,177,7,0.0,10,18.0\n19,0,0.0,140,21,0.0,22,56.0\n\nQuestion: Combine rows from both tables where the value of 'number_of_walks' is less than 34 and the value of 'indicator_of_free_agent_in_1991_1992' is '0', merging by common column values and preserving only the successfully merged elements.", "csv1_example": "number_of_doubles,salary_in_thousands_of_dollars,indicator_of_arbitration_in_1991_1992,number_of_stolen_bases,on_base_percentage,indicator_of_free_agent_in_1991_1992,number_of_home_runs,batting_average,number_of_hits\n17,2600,0.0,0,0.335,1,18.0,0.269,111\n28,2313,0.0,3,0.346,0,8.0,0.273,169\n32,2175,0.0,22,0.379,0,26.0,0.291,170\n7,460,0.0,2,0.279,0,3.0,0.228,38\n11,240,0.0,14,0.327,0,1.0,0.25,61\n10,200,0.0,13,0.24,0,10.0,0.203,64\n5,177,0.0,2,0.283,0,0.0,0.262,38\n9,140,0.0,3,0.307,0,6.0,0.222,45\n0,115,0.0,0,0.37,0,0.0,0.261,6\n22,1907,0.0,2,0.292,0,13.0,0.225,130\n", "csv2_example": "number_of_walks,number_of_triples,indicator_of_arbitration_eligibility,salary_in_thousands_of_dollars,number_of_runs,indicator_of_free_agency_eligibility,number_of_runs_batted_in,number_of_strike_outs\n63,1,0.0,2500,54,1.0,73,116.0\n23,7,1.0,2475,59,0.0,50,64.0\n70,5,1.0,2313,87,0.0,58,53.0\n15,1,0.0,600,34,1.0,38,45.0\n11,2,0.0,460,16,0.0,21,32.0\n24,0,0.0,240,40,0.0,18,26.0\n14,1,0.0,200,39,0.0,33,96.0\n5,0,0.0,177,7,0.0,10,18.0\n19,0,0.0,140,21,0.0,22,56.0\n2,0,0.0,117,4,0.0,3,1.0\n", "csv1_path": "infiagent/merge_test/14_0_0.csv", "csv2_path": "infiagent/merge_test/14_0_1.csv", "instruction": "Combine rows from both tables where the value of 'number_of_walks' is less than 34 and the value of 'indicator_of_free_agent_in_1991_1992' is '0', merging by common column values and preserving only the successfully merged elements.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/14_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/14_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['number_of_walks'] < 34]\ndf = df[df['indicator_of_free_agent_in_1991_1992'] == 0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['number_of_walks'] < 34]\ndf = df[df['indicator_of_free_agent_in_1991_1992'] == '0']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,Age,Gender,Rating\n1,34,Male,283\n2,82,Female,483\n3,71,Male,514\n6,77,Male,569\n7,37,Female,259\n8,87,Male,512\n9,66,Female,266\n10,41,Female,491\n11,30,Male,589\n\nHeader and first few lines of CSV file 2:\nEducation,Balance,Limit,Student,Cards,Married,Unnamed: 0\n11,333,3606,No,2,Yes,1\n11,580,7075,No,4,No,3\n11,964,9504,No,3,No,4\n16,331,4897,No,2,Yes,5\n12,203,3388,No,2,No,7\n9,872,7114,No,2,No,8\n19,1350,6819,Yes,3,Yes,10\n14,1407,8117,No,4,Yes,11\n16,0,1311,No,3,No,12\n\nQuestion: Combine the data from both tables based on the same column names and display the values of Cards, Limit, Rating, Balance, and Unnamed: 0. Only include the rows where the merge was successful.", "csv1_example": "Unnamed: 0,Age,Gender,Rating\n1,34,Male,283\n2,82,Female,483\n3,71,Male,514\n6,77,Male,569\n7,37,Female,259\n8,87,Male,512\n9,66,Female,266\n10,41,Female,491\n11,30,Male,589\n12,64,Male,138\n", "csv2_example": "Education,Balance,Limit,Student,Cards,Married,Unnamed: 0\n11,333,3606,No,2,Yes,1\n11,580,7075,No,4,No,3\n11,964,9504,No,3,No,4\n16,331,4897,No,2,Yes,5\n12,203,3388,No,2,No,7\n9,872,7114,No,2,No,8\n19,1350,6819,Yes,3,Yes,10\n14,1407,8117,No,4,Yes,11\n16,0,1311,No,3,No,12\n7,204,5308,No,1,Yes,13\n", "csv1_path": "infiagent/merge_test/24_0_0.csv", "csv2_path": "infiagent/merge_test/24_0_1.csv", "instruction": "Combine the data from both tables based on the same column names and display the values of Cards, Limit, Rating, Balance, and Unnamed: 0. Only include the rows where the merge was successful.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/24_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/24_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Cards\", \"Limit\", \"Rating\", \"Balance\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"Cards\", \"Limit\", \"Rating\", \"Balance\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ncountry,source_website,people_vaccinated_per_hundred,daily_vaccinations_raw,daily_vaccinations_per_million,people_fully_vaccinated_per_hundred\nAlbania,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/,0.0,,,\nAndorra,https://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,0.75,,,\nArgentina,http://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,,,,\nAzerbaijan,https://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,0.0,,,\nBahrain,https://twitter.com/MOH_Bahrain/status/1362144927535267841,2.29,,,\nBangladesh,https://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,0.0,,,\nBelgium,https://epistat.wiv-isp.be/covid/,0.0,,,\nBermuda,https://www.gov.bm/articles/covid-19-update-premiers-remarks-16-february-2021,0.0,,,\nBolivia,https://twitter.com/SaludDeportesBo/status/1359831763791581185,,,,\n\nHeader and first few lines of CSV file 2:\nvaccines,people_fully_vaccinated,people_vaccinated,total_vaccinations,country,source_name,daily_vaccinations,total_vaccinations_per_hundred,iso_code\nSputnik V,,,0.0,Algeria,Ministry of Health,,0.0,DZA\nPfizer/BioNTech,,576.0,576.0,Andorra,Government of Andorra,,0.75,AND\nOxford/AstraZeneca,,0.0,0.0,Anguilla,Ministry of Health,,0.0,AIA\nSputnik V,,,700.0,Argentina,Ministry of Health,,0.0,ARG\nOxford/AstraZeneca, Sputnik V,,0.0,0.0,Azerbaijan,Government of Azerbaijan,,0.0,AZE\nPfizer/BioNTech, Sinopharm/Beijing,,38965.0,38965.0,Bahrain,Ministry of Health,,2.29,BHR\nOxford/AstraZeneca,,0.0,0.0,Bangladesh,Directorate General of Health Services,,0.0,BGD\nModerna, Oxford/AstraZeneca, Pfizer/BioNTech,,298.0,298.0,Belgium,Sciensano,,0.0,BEL\nPfizer/BioNTech,,0.0,0.0,Bermuda,Government of Bermuda,,0.0,BMU\n\nQuestion: Combine the tables by aligning rows with the same column names, excluding those where 'vaccines' is 'Pfizer/BioNTech'. Fill any empty cells with NaN.", "csv1_example": "country,source_website,people_vaccinated_per_hundred,daily_vaccinations_raw,daily_vaccinations_per_million,people_fully_vaccinated_per_hundred\nAlbania,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/,0.0,,,\nAndorra,https://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,0.75,,,\nArgentina,http://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,,,,\nAzerbaijan,https://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,0.0,,,\nBahrain,https://twitter.com/MOH_Bahrain/status/1362144927535267841,2.29,,,\nBangladesh,https://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,0.0,,,\nBelgium,https://epistat.wiv-isp.be/covid/,0.0,,,\nBermuda,https://www.gov.bm/articles/covid-19-update-premiers-remarks-16-february-2021,0.0,,,\nBolivia,https://twitter.com/SaludDeportesBo/status/1359831763791581185,,,,\nBrazil,https://coronavirusbra1.github.io/,0.0,,,\n", "csv2_example": "vaccines,people_fully_vaccinated,people_vaccinated,total_vaccinations,country,source_name,daily_vaccinations,total_vaccinations_per_hundred,iso_code\nSputnik V,,,0.0,Algeria,Ministry of Health,,0.0,DZA\nPfizer/BioNTech,,576.0,576.0,Andorra,Government of Andorra,,0.75,AND\nOxford/AstraZeneca,,0.0,0.0,Anguilla,Ministry of Health,,0.0,AIA\nSputnik V,,,700.0,Argentina,Ministry of Health,,0.0,ARG\nOxford/AstraZeneca, Sputnik V,,0.0,0.0,Azerbaijan,Government of Azerbaijan,,0.0,AZE\nPfizer/BioNTech, Sinopharm/Beijing,,38965.0,38965.0,Bahrain,Ministry of Health,,2.29,BHR\nOxford/AstraZeneca,,0.0,0.0,Bangladesh,Directorate General of Health Services,,0.0,BGD\nModerna, Oxford/AstraZeneca, Pfizer/BioNTech,,298.0,298.0,Belgium,Sciensano,,0.0,BEL\nPfizer/BioNTech,,0.0,0.0,Bermuda,Government of Bermuda,,0.0,BMU\nSputnik V,,,0.0,Bolivia,Ministry of Health,,0.0,BOL\n", "csv1_path": "infiagent/merge_test/22_0_0.csv", "csv2_path": "infiagent/merge_test/22_0_1.csv", "instruction": "Combine the tables by aligning rows with the same column names, excluding those where 'vaccines' is 'Pfizer/BioNTech'. Fill any empty cells with NaN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/22_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/22_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['vaccines'] != 'Pfizer/BioNTech']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['vaccines'] != 'Pfizer/BioNTech']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nExcess Readmission Ratio,Number of Discharges,Number of Readmissions,Hospital Name\n1.9095,242,38.0,FROEDTERT MEMORIAL LUTHERAN HOSPITAL\n1.7521,247,33.0,PROVIDENCE HOSPITAL\n1.5836,586,53.0,BEAUFORT COUNTY MEMORIAL HOSPITAL\n1.576,965,95.0,ADVOCATE CHRIST HOSPITAL & MEDICAL CENTER\n1.5308,149,20.0,BRAZOSPORT REGIONAL HEALTH SYSTEM\n1.5189,141,19.0,WESTERN MISSOURI MEDICAL CENTER\n1.5079,390,38.0,SAINT AGNES HOSPITAL\n1.5019,178,24.0,MERCY HOSPITAL JEFFERSON\n1.4953,98,15.0,ONSLOW MEMORIAL HOSPITAL\n\nHeader and first few lines of CSV file 2:\nHospital Name,Predicted Readmission Rate,Provider Number,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,10.8,520177,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,9.0,140208,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,8.2,450072,06/30/2013\nMERCY HOSPITAL JEFFERSON,9.2,260023,06/30/2013\nONSLOW MEMORIAL HOSPITAL,7.9,340042,06/30/2013\nFAUQUIER HOSPITAL,7.4,490023,06/30/2013\nROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,9.0,310024,06/30/2013\nRESTON HOSPITAL CENTER,6.7,490107,06/30/2013\nCAPITAL HEALTH MEDICAL CENTER - HOPEWELL,8.4,310044,06/30/2013\n\nQuestion: Merge all rows in the two tables, show the value of  Number of Readmissions, Excess Readmission Ratio  and Hospital Name, merging by entries with the same column name,  and fill in the blanks with NAN.", "csv1_example": "Excess Readmission Ratio,Number of Discharges,Number of Readmissions,Hospital Name\n1.9095,242,38.0,FROEDTERT MEMORIAL LUTHERAN HOSPITAL\n1.7521,247,33.0,PROVIDENCE HOSPITAL\n1.5836,586,53.0,BEAUFORT COUNTY MEMORIAL HOSPITAL\n1.576,965,95.0,ADVOCATE CHRIST HOSPITAL & MEDICAL CENTER\n1.5308,149,20.0,BRAZOSPORT REGIONAL HEALTH SYSTEM\n1.5189,141,19.0,WESTERN MISSOURI MEDICAL CENTER\n1.5079,390,38.0,SAINT AGNES HOSPITAL\n1.5019,178,24.0,MERCY HOSPITAL JEFFERSON\n1.4953,98,15.0,ONSLOW MEMORIAL HOSPITAL\n1.4844,256,26.0,FAUQUIER HOSPITAL\n", "csv2_example": "Hospital Name,Predicted Readmission Rate,Provider Number,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,10.8,520177,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,9.0,140208,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,8.2,450072,06/30/2013\nMERCY HOSPITAL JEFFERSON,9.2,260023,06/30/2013\nONSLOW MEMORIAL HOSPITAL,7.9,340042,06/30/2013\nFAUQUIER HOSPITAL,7.4,490023,06/30/2013\nROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,9.0,310024,06/30/2013\nRESTON HOSPITAL CENTER,6.7,490107,06/30/2013\nCAPITAL HEALTH MEDICAL CENTER - HOPEWELL,8.4,310044,06/30/2013\nTHOMAS JEFFERSON UNIVERSITY HOSPITAL,7.5,390174,06/30/2013\n", "csv1_path": "infiagent/merge_test/20_0_0.csv", "csv2_path": "infiagent/merge_test/20_0_1.csv", "instruction": "Merge all rows in the two tables, show the value of  Number of Readmissions, Excess Readmission Ratio  and Hospital Name, merging by entries with the same column name,  and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/20_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/20_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Number of Readmissions\", \"Excess Readmission Ratio\", \"Hospital Name\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"Number of Readmissions\", \"Excess Readmission Ratio\", \"Hospital Name\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nbudget_year_end,github-dept-code,Department Name\n9/30/2017,ANM,Animal Services\n9/30/2017,CON,Austin Convention Center\n9/30/2017,ENE,Austin Energy\n9/30/2017,LIB,Austin Public Library\n9/30/2017,WAT,Austin Water\n9/30/2017,AVI,Aviation\n9/30/2017,BLD,Building Services\n9/30/2017,TEC,Communication and Technology Management\n9/30/2017,COM,Communications and Public Information\n\nHeader and first few lines of CSV file 2:\nDepartment Name,dept_group,budget_year_start\nAnimal Services,Community Services,10/1/2016\nAustin Code,Community Services,10/1/2016\nAustin Convention Center,Utility and Other Enterprises,10/1/2016\nAustin Energy,Utility and Other Enterprises,10/1/2016\nAustin Transportation,Infrastructure/Transportation,10/1/2016\nAustin Water,Utility and Other Enterprises,10/1/2016\nAviation,Utility and Other Enterprises,10/1/2016\nBuilding Services,Support Services,10/1/2016\nCommunications and Public Information,Support Services,10/1/2016\n\nQuestion: Merge all rows in the two tables that the value of 'budget_year_end' is not '9/30/2017', show the value of  budget_year_start, github-dept-code , dept_group  and Department Name, merging by entries with the same column name,  keeping only the successfully merged portions.", "csv1_example": "budget_year_end,github-dept-code,Department Name\n9/30/2017,ANM,Animal Services\n9/30/2017,CON,Austin Convention Center\n9/30/2017,ENE,Austin Energy\n9/30/2017,LIB,Austin Public Library\n9/30/2017,WAT,Austin Water\n9/30/2017,AVI,Aviation\n9/30/2017,BLD,Building Services\n9/30/2017,TEC,Communication and Technology Management\n9/30/2017,COM,Communications and Public Information\n9/30/2017,DEV,Development Services\n", "csv2_example": "Department Name,dept_group,budget_year_start\nAnimal Services,Community Services,10/1/2016\nAustin Code,Community Services,10/1/2016\nAustin Convention Center,Utility and Other Enterprises,10/1/2016\nAustin Energy,Utility and Other Enterprises,10/1/2016\nAustin Transportation,Infrastructure/Transportation,10/1/2016\nAustin Water,Utility and Other Enterprises,10/1/2016\nAviation,Utility and Other Enterprises,10/1/2016\nBuilding Services,Support Services,10/1/2016\nCommunications and Public Information,Support Services,10/1/2016\nDevelopment Services,Development,10/1/2016\n", "csv1_path": "infiagent/merge_test/19_2_0.csv", "csv2_path": "infiagent/merge_test/19_2_1.csv", "instruction": "Merge all rows in the two tables that the value of 'budget_year_end' is not '9/30/2017', show the value of  budget_year_start, github-dept-code , dept_group  and Department Name, merging by entries with the same column name,  keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/19_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/19_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['budget_year_end'] != '9/30/2017']\ndf = df[[\"budget_year_start\", \"github-dept-code\", \"dept_group\", \"Department Name\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['budget_year_end'] != '9/30/2017']\ndf = df[[\"budget_year_start\", \"github-dept-code\", \"dept_group\", \"Department Name\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\npositive_diffsel,site\n4.147102073893115,(HA2)121\n3.61560099488037,326\n5.146937744324779,280\n4.420441253098646,9\n5.824385887858723,210\n4.7228763586939415,192\n3.3941409184005606,-12\n3.190966528591367,171\n4.061786995426102,312\n\nHeader and first few lines of CSV file 2:\nmax_diffsel,abs_diffsel,negative_diffsel,site\n1.5787387471316894,9.026365225783264,-4.879263151890148,(HA2)121\n0.9710714351184296,8.418637730396656,-3.271699986071876,280\n1.37896385796,8.058662714496545,-2.2342768266378226,210\n1.263069276940311,8.015976108875662,-3.293099750181721,192\n0.6785731341245361,7.975893014675133,-4.581752096274572,-12\n1.2322928805568882,7.856854881480676,-4.665888352889309,171\n1.01733128501569,7.741099847882175,-2.704191739007377,242\n0.9648881227390486,7.663259982085831,-5.198842431213596,299\n0.6622422558042422,7.656895938536413,-4.453168832872083,(HA2)34\n\nQuestion: Join the two tables on columns with matching names and keep only the merged rows where the value of 'abs_diffsel' is less than 4.51423766225707 and the value of 'max_diffsel' is greater than 0.7027858681526212, while preferring the values from the merged rows over those from either table in case of conflicts.", "csv1_example": "positive_diffsel,site\n4.147102073893115,(HA2)121\n3.61560099488037,326\n5.146937744324779,280\n4.420441253098646,9\n5.824385887858723,210\n4.7228763586939415,192\n3.3941409184005606,-12\n3.190966528591367,171\n4.061786995426102,312\n2.4644175508722346,299\n", "csv2_example": "max_diffsel,abs_diffsel,negative_diffsel,site\n1.5787387471316894,9.026365225783264,-4.879263151890148,(HA2)121\n0.9710714351184296,8.418637730396656,-3.271699986071876,280\n1.37896385796,8.058662714496545,-2.2342768266378226,210\n1.263069276940311,8.015976108875662,-3.293099750181721,192\n0.6785731341245361,7.975893014675133,-4.581752096274572,-12\n1.2322928805568882,7.856854881480676,-4.665888352889309,171\n1.01733128501569,7.741099847882175,-2.704191739007377,242\n0.9648881227390486,7.663259982085831,-5.198842431213596,299\n0.6622422558042422,7.656895938536413,-4.453168832872083,(HA2)34\n1.1218042266443229,7.65451380242451,-4.343193848567695,262\n", "csv1_path": "infiagent/merge_test/34_1_0.csv", "csv2_path": "infiagent/merge_test/34_1_1.csv", "instruction": "Join the two tables on columns with matching names and keep only the merged rows where the value of 'abs_diffsel' is less than 4.51423766225707 and the value of 'max_diffsel' is greater than 0.7027858681526212, while preferring the values from the merged rows over those from either table in case of conflicts.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/34_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/34_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['abs_diffsel'] < 4.51423766225707]\ndf = df[df['max_diffsel'] > 0.7027858681526212]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['abs_diffsel'] < 4.51423766225707]\ndf = df[df['max_diffsel'] > 0.7027858681526212]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nName,STANDARD TEAM NAME\nGatlin & Ramarao,Mitty GR\nLahiri & Ponnuswamy,Mitty PL\nGupta & Chatradhi,Mitty GuCh\nPatwa & Aggarwal,Mitty PA\nArun & Schacter,Gunn AS\nCheng & Parulekar,Gunn PC\nMenotti & Bhasin,James Logan MB\nKim & Joshi,Mitty JK\nKang & Korolik,Mitty KK\n\nHeader and first few lines of CSV file 2:\nSchool,Name,Unnamed: 4\nArchbishop Mitty,Gatlin & Ramarao,\nArchbishop Mitty,Lahiri & Ponnuswamy,\nArchbishop Mitty,Gupta & Chatradhi,\nArchbishop Mitty,Patwa & Aggarwal,\nGunn Sr,Arun & Schacter,\nGunn Sr,Cheng & Parulekar,\nGunn Sr,Prabhakar & Hamilton,\nArchbishop Mitty,Kim & Joshi,\nClovis North,Manjal & Perez,\n\nQuestion: Combine rows from both tables where the length of 'STANDARD TEAM NAME' is less than 9, merging by matching column names and retaining only the successfully merged data.", "csv1_example": "Name,STANDARD TEAM NAME\nGatlin & Ramarao,Mitty GR\nLahiri & Ponnuswamy,Mitty PL\nGupta & Chatradhi,Mitty GuCh\nPatwa & Aggarwal,Mitty PA\nArun & Schacter,Gunn AS\nCheng & Parulekar,Gunn PC\nMenotti & Bhasin,James Logan MB\nKim & Joshi,Mitty JK\nKang & Korolik,Mitty KK\nKalra & Bhat,Dougherty Valley KB\n", "csv2_example": "School,Name,Unnamed: 4\nArchbishop Mitty,Gatlin & Ramarao,\nArchbishop Mitty,Lahiri & Ponnuswamy,\nArchbishop Mitty,Gupta & Chatradhi,\nArchbishop Mitty,Patwa & Aggarwal,\nGunn Sr,Arun & Schacter,\nGunn Sr,Cheng & Parulekar,\nGunn Sr,Prabhakar & Hamilton,\nArchbishop Mitty,Kim & Joshi,\nClovis North,Manjal & Perez,\nDougherty Valley,Kalra & Bhat,\n", "csv1_path": "infiagent/merge_test/26_2_0.csv", "csv2_path": "infiagent/merge_test/26_2_1.csv", "instruction": "Combine rows from both tables where the length of 'STANDARD TEAM NAME' is less than 9, merging by matching column names and retaining only the successfully merged data.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/26_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/26_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['STANDARD TEAM NAME'].str.len() < 9]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['STANDARD TEAM NAME'].str.len() < 9]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nper_dem,combined_fips,state_abbr,per_gop,votes_gop,county_name,votes_dem\n0.37715947248,2013,AK,0.528870018006,130413.0,Alaska,93003.0\n0.239568549532,1001,AL,0.734357893029,18110.0,Autauga County,5908.0\n0.195653098098,1003,AL,0.773514719949,72780.0,Baldwin County,18409.0\n0.466602502406,1005,AL,0.522714148219,5431.0,Barbour County,4848.0\n0.214220393233,1007,AL,0.769661636946,6733.0,Bibb County,1874.0\n0.0846990230066,1009,AL,0.8985187519700001,22808.0,Blount County,2150.0\n0.750904062965,1011,AL,0.242288874708,1139.0,Bullock County,3530.0\n0.427864133564,1013,AL,0.563154864709,4891.0,Butler County,3716.0\n0.278558763931,1015,AL,0.6923969942589999,32803.0,Calhoun County,13197.0\n\nHeader and first few lines of CSV file 2:\nper_point_diff,votes_dem,total_votes\n15.17%,93003.0,246588.0\n57.79%,18409.0,94090.0\n5.61%,4848.0,10390.0\n55.54%,1874.0,8748.0\n50.86%,3530.0,4701.0\n14.81%,5763.0,13778.0\n69.36%,1524.0,10503.0\n66.61%,2909.0,18255.0\n13.66%,3109.0,7268.0\n\nQuestion: Combine the rows from the two tables where the 'per_dem' value is equal to or greater than 0.297546627448, using the same column names, and replace any missing values with NAN.", "csv1_example": "per_dem,combined_fips,state_abbr,per_gop,votes_gop,county_name,votes_dem\n0.37715947248,2013,AK,0.528870018006,130413.0,Alaska,93003.0\n0.239568549532,1001,AL,0.734357893029,18110.0,Autauga County,5908.0\n0.195653098098,1003,AL,0.773514719949,72780.0,Baldwin County,18409.0\n0.466602502406,1005,AL,0.522714148219,5431.0,Barbour County,4848.0\n0.214220393233,1007,AL,0.769661636946,6733.0,Bibb County,1874.0\n0.0846990230066,1009,AL,0.8985187519700001,22808.0,Blount County,2150.0\n0.750904062965,1011,AL,0.242288874708,1139.0,Bullock County,3530.0\n0.427864133564,1013,AL,0.563154864709,4891.0,Butler County,3716.0\n0.278558763931,1015,AL,0.6923969942589999,32803.0,Calhoun County,13197.0\n0.418275511685,1017,AL,0.5663376397150001,7803.0,Chambers County,5763.0\n", "csv2_example": "per_point_diff,votes_dem,total_votes\n15.17%,93003.0,246588.0\n57.79%,18409.0,94090.0\n5.61%,4848.0,10390.0\n55.54%,1874.0,8748.0\n50.86%,3530.0,4701.0\n14.81%,5763.0,13778.0\n69.36%,1524.0,10503.0\n66.61%,2909.0,18255.0\n13.66%,3109.0,7268.0\n10.80%,5712.0,12936.0\n", "csv1_path": "infiagent/merge_test/30_2_0.csv", "csv2_path": "infiagent/merge_test/30_2_1.csv", "instruction": "Combine the rows from the two tables where the 'per_dem' value is equal to or greater than 0.297546627448, using the same column names, and replace any missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/30_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/30_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['per_dem'] <= 0.297546627448]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['per_dem'] >= 0.297546627448]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nmax_sust_wind,Unnamed: 0,year\n43.4488,0,2017\n26.06928,3,2017\n43.4488,4,2017\n52.13856,5,2017\n73.86296,6,2017\n95.58736,7,2017\n112.96688,8,2017\n156.41568,9,2017\n134.69128,10,2017\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,damage_imputed,areas_affected\n0,0,None\n1,0,Guyana, Venezuela, Trinidad and Tobago, Windward Islands\n2,0,Honduras, Belize, Cayman Islands, Yucatán Peninsula, Cuba, Southern United States, Eastern United States\n3,0,None\n4,0,Windward Islands, Barbados, Trinidad and Tobago\n5,0,Florida\n7,0,Bermuda, East Coast of the United States, Atlantic Canada\n8,0,Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucatán Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\n9,0,Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\n\nQuestion: Merge all rows in the two tables that the value of 'areas_affected' is not 'None', show the value of  year, areas_affected  and Unnamed: 0, merging by entries with the same column name, and fill in the blanks with NAN.", "csv1_example": "max_sust_wind,Unnamed: 0,year\n43.4488,0,2017\n26.06928,3,2017\n43.4488,4,2017\n52.13856,5,2017\n73.86296,6,2017\n95.58736,7,2017\n112.96688,8,2017\n156.41568,9,2017\n134.69128,10,2017\n91.24248,11,2017\n", "csv2_example": "Unnamed: 0,damage_imputed,areas_affected\n0,0,None\n1,0,Guyana, Venezuela, Trinidad and Tobago, Windward Islands\n2,0,Honduras, Belize, Cayman Islands, Yucatán Peninsula, Cuba, Southern United States, Eastern United States\n3,0,None\n4,0,Windward Islands, Barbados, Trinidad and Tobago\n5,0,Florida\n7,0,Bermuda, East Coast of the United States, Atlantic Canada\n8,0,Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucatán Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\n9,0,Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\n10,0,Leeward Islands, East Coast of the United States\n", "csv1_path": "infiagent/merge_test/21_0_0.csv", "csv2_path": "infiagent/merge_test/21_0_1.csv", "instruction": "Merge all rows in the two tables that the value of 'areas_affected' is not 'None', show the value of  year, areas_affected  and Unnamed: 0, merging by entries with the same column name, and fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/21_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/21_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['areas_affected'] != 'None']\ndf = df[[\"year\", \"areas_affected\", \"Unnamed: 0\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['areas_affected'] != 'None']\ndf = df[[\"year\", \"areas_affected\", \"Unnamed: 0\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNo. of cases_median,No. of deaths_max,Year,No. of deaths_median,Country\n630308,510.0,2017,298,Afghanistan\n0,,2017,0,Algeria\n4615605,16600.0,2017,13316,Angola\n0,,2017,0,Argentina\n0,,2017,0,Armenia\n0,,2017,0,Azerbaijan\n32924,130.0,2017,76,Bangladesh\n7,,2017,0,Belize\n4111699,8920.0,2017,7328,Benin\n\nHeader and first few lines of CSV file 2:\nNo. of cases_min,WHO Region,No. of cases,No. of deaths_min,Country,No. of cases_max\n495000.0,Eastern Mediterranean,630308[495000-801000],110.0,Afghanistan,801000.0\n,Africa,0,,Algeria,\n3106000.0,Africa,4615605[3106000-6661000],9970.0,Angola,6661000.0\n,Europe,0,,Armenia,\n,Europe,0,,Azerbaijan,\n30000.0,South-East Asia,32924[30000-36000],3.0,Bangladesh,36000.0\n,Americas,7,,Belize,\n2774000.0,Africa,4111699[2774000-6552000],5740.0,Benin,6552000.0\n,South-East Asia,11,,Bhutan,\n\nQuestion: Combine the data from both tables, preserving the original row structure, and display the following columns: No. of cases, No. of deaths_max, No. of deaths_median, Year, No. of deaths_min, No. of cases_median, and Country. Merge the tables based on identical column names, and only include the successfully merged rows.", "csv1_example": "No. of cases_median,No. of deaths_max,Year,No. of deaths_median,Country\n630308,510.0,2017,298,Afghanistan\n0,,2017,0,Algeria\n4615605,16600.0,2017,13316,Angola\n0,,2017,0,Argentina\n0,,2017,0,Armenia\n0,,2017,0,Azerbaijan\n32924,130.0,2017,76,Bangladesh\n7,,2017,0,Belize\n4111699,8920.0,2017,7328,Benin\n11,,2017,0,Bhutan\n", "csv2_example": "No. of cases_min,WHO Region,No. of cases,No. of deaths_min,Country,No. of cases_max\n495000.0,Eastern Mediterranean,630308[495000-801000],110.0,Afghanistan,801000.0\n,Africa,0,,Algeria,\n3106000.0,Africa,4615605[3106000-6661000],9970.0,Angola,6661000.0\n,Europe,0,,Armenia,\n,Europe,0,,Azerbaijan,\n30000.0,South-East Asia,32924[30000-36000],3.0,Bangladesh,36000.0\n,Americas,7,,Belize,\n2774000.0,Africa,4111699[2774000-6552000],5740.0,Benin,6552000.0\n,South-East Asia,11,,Bhutan,\n4900.0,Americas,6512[4900-8300],0.0,Bolivia (Plurinational State of),8300.0\n", "csv1_path": "infiagent/merge_test/32_0_0.csv", "csv2_path": "infiagent/merge_test/32_0_1.csv", "instruction": "Combine the data from both tables, preserving the original row structure, and display the following columns: No. of cases, No. of deaths_max, No. of deaths_median, Year, No. of deaths_min, No. of cases_median, and Country. Merge the tables based on identical column names, and only include the successfully merged rows.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/32_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/32_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"No. of cases\", \"No. of deaths_max\", \"No. of deaths_median\", \"Year\", \"No. of deaths_min\", \"No. of cases_median\", \"Country\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[[\"No. of cases\", \"No. of deaths_max\", \"No. of deaths_median\", \"Year\", \"No. of deaths_min\", \"No. of cases_median\", \"Country\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nper_dem,votes_gop,votes_dem,combined_fips\n0.239568549532,18110.0,5908.0,1001\n0.466602502406,5431.0,4848.0,1005\n0.214220393233,6733.0,1874.0,1007\n0.0846990230066,22808.0,2150.0,1009\n0.750904062965,1139.0,3530.0,1011\n0.427864133564,4891.0,3716.0,1013\n0.278558763931,32803.0,13197.0,1015\n0.418275511685,7803.0,5763.0,1017\n0.1451013996,8809.0,1524.0,1019\n\nHeader and first few lines of CSV file 2:\nper_gop,per_point_diff,total_votes,state_abbr,diff,votes_dem\n0.528870018006,15.17%,246588.0,AK,37,410,93003.0\n0.734357893029,49.48%,24661.0,AL,12,202,5908.0\n0.773514719949,57.79%,94090.0,AL,54,371,18409.0\n0.522714148219,5.61%,10390.0,AL,583,4848.0\n0.769661636946,55.54%,8748.0,AL,4,859,1874.0\n0.8985187519700001,81.38%,25384.0,AL,20,658,2150.0\n0.242288874708,50.86%,4701.0,AL,2,391,3530.0\n0.563154864709,13.53%,8685.0,AL,1,175,3716.0\n0.6923969942589999,41.38%,47376.0,AL,19,606,13197.0\n\nQuestion: Merge all rows in the two tables that the value of 'state_abbr' is 'TX', merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "per_dem,votes_gop,votes_dem,combined_fips\n0.239568549532,18110.0,5908.0,1001\n0.466602502406,5431.0,4848.0,1005\n0.214220393233,6733.0,1874.0,1007\n0.0846990230066,22808.0,2150.0,1009\n0.750904062965,1139.0,3530.0,1011\n0.427864133564,4891.0,3716.0,1013\n0.278558763931,32803.0,13197.0,1015\n0.418275511685,7803.0,5763.0,1017\n0.1451013996,8809.0,1524.0,1019\n0.159353601753,15068.0,2909.0,1021\n", "csv2_example": "per_gop,per_point_diff,total_votes,state_abbr,diff,votes_dem\n0.528870018006,15.17%,246588.0,AK,37,410,93003.0\n0.734357893029,49.48%,24661.0,AL,12,202,5908.0\n0.773514719949,57.79%,94090.0,AL,54,371,18409.0\n0.522714148219,5.61%,10390.0,AL,583,4848.0\n0.769661636946,55.54%,8748.0,AL,4,859,1874.0\n0.8985187519700001,81.38%,25384.0,AL,20,658,2150.0\n0.242288874708,50.86%,4701.0,AL,2,391,3530.0\n0.563154864709,13.53%,8685.0,AL,1,175,3716.0\n0.6923969942589999,41.38%,47376.0,AL,19,606,13197.0\n0.5663376397150001,14.81%,13778.0,AL,2,040,5763.0\n", "csv1_path": "infiagent/merge_test/30_1_0.csv", "csv2_path": "infiagent/merge_test/30_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'state_abbr' is 'TX', merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/30_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/30_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['state_abbr'] == 'TX']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['state_abbr'] == 'TX']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nJOBTITLE,NAME\nAUDITOR TRAINEE,AKROFI,BERNARD\nAIDE BLUE CHIP,Aaron,Keairah T\nAIDE BLUE CHIP,Aaron,Keontae E\nFacilities/Office Services II,Aaron,Patricia G\nASSISTANT STATE'S ATTORNEY,Aaron,Petra L\nEPIDEMIOLOGIST,Abaineh,Yohannes T\nEMT FIREFIGHTER,Abdal-Rahim,Naim A\nPOLICE SERGEANT,Abdi,Ezekiel W\nAIDE BLUE CHIP,Abdul Wajid,Amani B\n\nHeader and first few lines of CSV file 2:\nNAME,DEPTID\nAKROFI,BERNARD,A24002\nAaron,Patricia G,A03031\nAaron,Petra L,A29005\nAbaineh,Yohannes T,A65026\nAbbey,Emmanuel,A40001\nAbdi,Ezekiel W,A99123\nAbdul Adl,Attrice A,A38410\nAbdul Saboor,Jamillah,A75054\nAbdul Wajid,Amani B,W02215\n\nQuestion: Join the two tables by matching rows with the same column name, and display the values of DEPTID and NAME columns. If there are unmatched rows, fill in the blanks with NAN.", "csv1_example": "JOBTITLE,NAME\nAUDITOR TRAINEE,AKROFI,BERNARD\nAIDE BLUE CHIP,Aaron,Keairah T\nAIDE BLUE CHIP,Aaron,Keontae E\nFacilities/Office Services II,Aaron,Patricia G\nASSISTANT STATE'S ATTORNEY,Aaron,Petra L\nEPIDEMIOLOGIST,Abaineh,Yohannes T\nEMT FIREFIGHTER,Abdal-Rahim,Naim A\nPOLICE SERGEANT,Abdi,Ezekiel W\nAIDE BLUE CHIP,Abdul Wajid,Amani B\nSOCIAL SERVICES COORDINATOR,Abdul-Jabbar,Bushra A\n", "csv2_example": "NAME,DEPTID\nAKROFI,BERNARD,A24002\nAaron,Patricia G,A03031\nAaron,Petra L,A29005\nAbaineh,Yohannes T,A65026\nAbbey,Emmanuel,A40001\nAbdi,Ezekiel W,A99123\nAbdul Adl,Attrice A,A38410\nAbdul Saboor,Jamillah,A75054\nAbdul Wajid,Amani B,W02215\nAbdul-Jabbar,Bushra A,A65201\n", "csv1_path": "infiagent/merge_test/12_1_0.csv", "csv2_path": "infiagent/merge_test/12_1_1.csv", "instruction": "Join the two tables by matching rows with the same column name, and display the values of DEPTID and NAME columns. If there are unmatched rows, fill in the blanks with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/12_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/12_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"DEPTID\", \"NAME\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[[\"DEPTID\", \"NAME\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nneg,compound,title,url,Unnamed: 0\n0.067,0.9746,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,0\n0.062,0.9869,WATCH: In the heat of primary day, young Arizonans encourage faith in the political process,https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,1\n0.051,0.9875,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,2\n0.068,0.9799,WATCH: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running,https://abcnews.go.com/Nightline/video/arizona-senate-candidates-joe-arpaio-kelli-ward-running-57440153,3\n0.101,-0.9872,'Political correctness is like a cancer': AZ GOP candidate defends comments,https://abcnews.go.com/Politics/political-correctness-cancer-arizona-republican-defends-controversial-social/story?id=57439818,4\n0.066,0.9826,WATCH: YouTube plans to allow fewer ad skips,https://abcnews.go.com/Technology/video/youtube-plans-fewer-ad-skips-57422058,5\n0.152,-0.9965,Myanmar military chief among those banned by Facebook,https://abcnews.go.com/Technology/wireStory/myanmar-military-chief-banned-facebook-57421062,6\n0.068,0.9538,Tech companies to meet to protect their platforms against foreign meddling,https://abcnews.go.com/Business/ahead-midterm-elections-tech-companies-meet-protect-platforms/story?id=57388549,7\n0.089,0.1603,FireEye: Tech firms' secret weapon against disinformation,https://abcnews.go.com/Technology/wireStory/fireeye-tech-firms-secret-weapon-disinformation-57383874,8\n\nHeader and first few lines of CSV file 2:\nurlToImage,Unnamed: 0,neu,author,pos,source\nhttps://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,0,0.733,ABC News,0.2,abc-news\nhttps://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,1,0.735,ABC News,0.204,abc-news\nhttps://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,2,0.847,Karma Allen,0.102,abc-news\nhttps://s.abcnews.com/images/Nightline/180827_ntl_az_hpMain_16x9_992.jpg,3,0.762,ABC News,0.169,abc-news\nhttps://s.abcnews.com/images/Politics/kelli-ward-ap-171025_hpMain_5_16x9_992.jpg,4,0.851,Meghan Keneally,0.048,abc-news\nhttps://s.abcnews.com/images/Technology/180827_atm_techbytes_hpMain_16x9_992.jpg,5,0.719,ABC News,0.215,abc-news\nhttps://s.abcnews.com/images/US/facebook-users-rtr-jc-180329_hpMain_4_16x9_992.jpg,7,0.807,Will Carr,0.125,abc-news\n,9,0.792,Alexander Mallin,0.057,abc-news\nhttps://s.abcnews.com/images/US/shawn-richard-christy-ht-001-jpo-180823_hpMain_16x9_992.jpg,10,0.848,Luke Barr,0.029,abc-news\n\nQuestion: Merge all rows in the two tables that the value of 'source' is not 'reuters', merging by entries with the same column name, keeping only the successfully merged portions.", "csv1_example": "neg,compound,title,url,Unnamed: 0\n0.067,0.9746,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,0\n0.062,0.9869,WATCH: In the heat of primary day, young Arizonans encourage faith in the political process,https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,1\n0.051,0.9875,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,2\n0.068,0.9799,WATCH: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running,https://abcnews.go.com/Nightline/video/arizona-senate-candidates-joe-arpaio-kelli-ward-running-57440153,3\n0.101,-0.9872,'Political correctness is like a cancer': AZ GOP candidate defends comments,https://abcnews.go.com/Politics/political-correctness-cancer-arizona-republican-defends-controversial-social/story?id=57439818,4\n0.066,0.9826,WATCH: YouTube plans to allow fewer ad skips,https://abcnews.go.com/Technology/video/youtube-plans-fewer-ad-skips-57422058,5\n0.152,-0.9965,Myanmar military chief among those banned by Facebook,https://abcnews.go.com/Technology/wireStory/myanmar-military-chief-banned-facebook-57421062,6\n0.068,0.9538,Tech companies to meet to protect their platforms against foreign meddling,https://abcnews.go.com/Business/ahead-midterm-elections-tech-companies-meet-protect-platforms/story?id=57388549,7\n0.089,0.1603,FireEye: Tech firms' secret weapon against disinformation,https://abcnews.go.com/Technology/wireStory/fireeye-tech-firms-secret-weapon-disinformation-57383874,8\n0.151,-0.9837,President Trump inaccurately claims social media companies 'silencing millions',https://abcnews.go.com/Politics/president-trump-inaccurately-claims-social-media-companies-silencing/story?id=57376514,9\n", "csv2_example": "urlToImage,Unnamed: 0,neu,author,pos,source\nhttps://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,0,0.733,ABC News,0.2,abc-news\nhttps://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,1,0.735,ABC News,0.204,abc-news\nhttps://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,2,0.847,Karma Allen,0.102,abc-news\nhttps://s.abcnews.com/images/Nightline/180827_ntl_az_hpMain_16x9_992.jpg,3,0.762,ABC News,0.169,abc-news\nhttps://s.abcnews.com/images/Politics/kelli-ward-ap-171025_hpMain_5_16x9_992.jpg,4,0.851,Meghan Keneally,0.048,abc-news\nhttps://s.abcnews.com/images/Technology/180827_atm_techbytes_hpMain_16x9_992.jpg,5,0.719,ABC News,0.215,abc-news\nhttps://s.abcnews.com/images/US/facebook-users-rtr-jc-180329_hpMain_4_16x9_992.jpg,7,0.807,Will Carr,0.125,abc-news\n,9,0.792,Alexander Mallin,0.057,abc-news\nhttps://s.abcnews.com/images/US/shawn-richard-christy-ht-001-jpo-180823_hpMain_16x9_992.jpg,10,0.848,Luke Barr,0.029,abc-news\n,11,0.77,The Associated Press,0.068,abc-news\n", "csv1_path": "infiagent/merge_test/33_1_0.csv", "csv2_path": "infiagent/merge_test/33_1_1.csv", "instruction": "Merge all rows in the two tables that the value of 'source' is not 'reuters', merging by entries with the same column name, keeping only the successfully merged portions.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/33_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/33_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['source'] != 'reuters']\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['source'] != 'reuters']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nage,bmi\n19,27.9\n18,33.77\n28,33.0\n33,22.705\n32,28.88\n31,25.74\n46,33.44\n37,27.74\n60,25.84\n\nHeader and first few lines of CSV file 2:\nregion,age,charges,smoker,sex\nsouthwest,19,16884.924,yes,female\nsoutheast,28,4449.462,no,male\nnorthwest,33,21984.47061,no,male\nnorthwest,32,3866.8552,no,male\nsoutheast,31,3756.6216,no,female\nsoutheast,46,8240.5896,no,female\nnortheast,25,2721.3208,no,male\nsoutheast,62,27808.7251,yes,female\nsouthwest,23,1826.843,no,male\n\nQuestion: Combine the rows from the two tables where the 'bmi' value is more than 29.59 and the 'age' value is over 41, using the same column names, and retaining only the parts that have been successfully merged.", "csv1_example": "age,bmi\n19,27.9\n18,33.77\n28,33.0\n33,22.705\n32,28.88\n31,25.74\n46,33.44\n37,27.74\n60,25.84\n25,26.22\n", "csv2_example": "region,age,charges,smoker,sex\nsouthwest,19,16884.924,yes,female\nsoutheast,28,4449.462,no,male\nnorthwest,33,21984.47061,no,male\nnorthwest,32,3866.8552,no,male\nsoutheast,31,3756.6216,no,female\nsoutheast,46,8240.5896,no,female\nnortheast,25,2721.3208,no,male\nsoutheast,62,27808.7251,yes,female\nsouthwest,23,1826.843,no,male\nsoutheast,56,11090.7178,no,female\n", "csv1_path": "infiagent/merge_test/43_3_0.csv", "csv2_path": "infiagent/merge_test/43_3_1.csv", "instruction": "Combine the rows from the two tables where the 'bmi' value is more than 29.59 and the 'age' value is over 41, using the same column names, and retaining only the parts that have been successfully merged.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/43_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/43_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['bmi'] > 29.59]\ndf = df[df['age'] > 41]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['bmi'] > 29.59]\ndf = df[df['age'] > 41]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nHospital Name,Measure Name,Provider Number\nBEAUFORT COUNTY MEMORIAL HOSPITAL,READM-30-HIP-KNEE-HRRP,420067\nWESTERN MISSOURI MEDICAL CENTER,READM-30-HIP-KNEE-HRRP,260097\nSAINT AGNES HOSPITAL,READM-30-HIP-KNEE-HRRP,210011\nMERCY HOSPITAL JEFFERSON,READM-30-HIP-KNEE-HRRP,260023\nROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,READM-30-HIP-KNEE-HRRP,310024\nMERCY ST ANNE HOSPITAL,READM-30-HIP-KNEE-HRRP,360262\nRESTON HOSPITAL CENTER,READM-30-HIP-KNEE-HRRP,490107\nCAPITAL HEALTH MEDICAL CENTER - HOPEWELL,READM-30-HIP-KNEE-HRRP,310044\nTHOMAS JEFFERSON UNIVERSITY HOSPITAL,READM-30-HIP-KNEE-HRRP,390174\n\nHeader and first few lines of CSV file 2:\nFootnote,Start Date,Hospital Name,Expected Readmission Rate,End Date,Number of Discharges\n,07/01/2010,FROEDTERT MEMORIAL LUTHERAN HOSPITAL,5.6,06/30/2013,242\n,07/01/2010,PROVIDENCE HOSPITAL,5.3,06/30/2013,247\n,07/01/2010,BEAUFORT COUNTY MEMORIAL HOSPITAL,4.8,06/30/2013,586\n,07/01/2010,WESTERN MISSOURI MEDICAL CENTER,5.3,06/30/2013,141\n,07/01/2010,SAINT AGNES HOSPITAL,5.2,06/30/2013,390\n,07/01/2010,MERCY HOSPITAL JEFFERSON,6.1,06/30/2013,178\n,07/01/2010,ONSLOW MEMORIAL HOSPITAL,5.3,06/30/2013,98\n,07/01/2010,FAUQUIER HOSPITAL,5.0,06/30/2013,256\n,07/01/2010,ROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,6.1,06/30/2013,121\n\nQuestion: Combine rows from both tables where the length of 'Start Date' exceeds 10 characters and 'Expected Readmission Rate' is less than 17.9, merging by columns with identical names, and retaining only the successfully merged data.", "csv1_example": "Hospital Name,Measure Name,Provider Number\nBEAUFORT COUNTY MEMORIAL HOSPITAL,READM-30-HIP-KNEE-HRRP,420067\nWESTERN MISSOURI MEDICAL CENTER,READM-30-HIP-KNEE-HRRP,260097\nSAINT AGNES HOSPITAL,READM-30-HIP-KNEE-HRRP,210011\nMERCY HOSPITAL JEFFERSON,READM-30-HIP-KNEE-HRRP,260023\nROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,READM-30-HIP-KNEE-HRRP,310024\nMERCY ST ANNE HOSPITAL,READM-30-HIP-KNEE-HRRP,360262\nRESTON HOSPITAL CENTER,READM-30-HIP-KNEE-HRRP,490107\nCAPITAL HEALTH MEDICAL CENTER - HOPEWELL,READM-30-HIP-KNEE-HRRP,310044\nTHOMAS JEFFERSON UNIVERSITY HOSPITAL,READM-30-HIP-KNEE-HRRP,390174\nHOUSTON MEDICAL CENTER,READM-30-HIP-KNEE-HRRP,110069\n", "csv2_example": "Footnote,Start Date,Hospital Name,Expected Readmission Rate,End Date,Number of Discharges\n,07/01/2010,FROEDTERT MEMORIAL LUTHERAN HOSPITAL,5.6,06/30/2013,242\n,07/01/2010,PROVIDENCE HOSPITAL,5.3,06/30/2013,247\n,07/01/2010,BEAUFORT COUNTY MEMORIAL HOSPITAL,4.8,06/30/2013,586\n,07/01/2010,WESTERN MISSOURI MEDICAL CENTER,5.3,06/30/2013,141\n,07/01/2010,SAINT AGNES HOSPITAL,5.2,06/30/2013,390\n,07/01/2010,MERCY HOSPITAL JEFFERSON,6.1,06/30/2013,178\n,07/01/2010,ONSLOW MEMORIAL HOSPITAL,5.3,06/30/2013,98\n,07/01/2010,FAUQUIER HOSPITAL,5.0,06/30/2013,256\n,07/01/2010,ROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,6.1,06/30/2013,121\n,07/01/2010,RESTON HOSPITAL CENTER,4.6,06/30/2013,780\n", "csv1_path": "infiagent/merge_test/20_3_0.csv", "csv2_path": "infiagent/merge_test/20_3_1.csv", "instruction": "Combine rows from both tables where the length of 'Start Date' exceeds 10 characters and 'Expected Readmission Rate' is less than 17.9, merging by columns with identical names, and retaining only the successfully merged data.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/20_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/20_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Start Date'].str.len() > 10]\ndf = df[df['Expected Readmission Rate'] < 17.9]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['Start Date'].str.len() > 10]\ndf = df[df['Expected Readmission Rate'] < 17.9]\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nSource,lng_org\n1,51.089356\n2,51.13283300000001\n3,51.10351\n4,51.102301\n5,51.085218\n6,51.139829\n7,51.138691\n8,51.141671\n9,51.143921\n\nHeader and first few lines of CSV file 2:\nlat_org,Source,Target,Type\n16.960160000000002,0,1.0,Directed\n17.001061,1,0.0,Directed\n17.065654000000002,2,1.0,Directed\n17.116356,4,2.0,Directed\n17.048482999999994,5,1.0,Directed\n17.032705,6,2.0,Directed\n17.030970999999994,10,1.0,Directed\n17.057259,11,0.0,Directed\n16.971785,13,0.0,Directed\n\nQuestion: Join both tables on columns with matching names and merge all rows where the value of 'Type' is 'Directed'. Display the values of lat_org, Type, Target, and Source, filling in any missing values with NAN.", "csv1_example": "Source,lng_org\n1,51.089356\n2,51.13283300000001\n3,51.10351\n4,51.102301\n5,51.085218\n6,51.139829\n7,51.138691\n8,51.141671\n9,51.143921\n10,51.074422\n", "csv2_example": "lat_org,Source,Target,Type\n16.960160000000002,0,1.0,Directed\n17.001061,1,0.0,Directed\n17.065654000000002,2,1.0,Directed\n17.116356,4,2.0,Directed\n17.048482999999994,5,1.0,Directed\n17.032705,6,2.0,Directed\n17.030970999999994,10,1.0,Directed\n17.057259,11,0.0,Directed\n16.971785,13,0.0,Directed\n16.95721,14,0.0,Directed\n", "csv1_path": "infiagent/merge_test/5_0_0.csv", "csv2_path": "infiagent/merge_test/5_0_1.csv", "instruction": "Join both tables on columns with matching names and merge all rows where the value of 'Type' is 'Directed'. Display the values of lat_org, Type, Target, and Source, filling in any missing values with NAN.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/5_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/5_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Type'] == 'Directed']\ndf = df[[\"lat_org\", \"Type\", \"Target\", \"Source\"]]\n\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\ndf = df[df['Type'] == 'Directed']\ndf = df[[\"lat_org\", \"Type\", \"Target\", \"Source\"]]\n\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNumberRealEstateLoansOrLines,Unnamed: 0,NumberOfDependents,NumberOfOpenCreditLinesAndLoans,SeriousDlqin2yrs\n0,1,0.0,4,\n4,2,2.0,15,\n1,3,2.0,12,\n2,4,0.0,7,\n0,5,1.0,4,\n0,6,1.0,4,\n0,7,3.0,5,\n1,8,1.0,8,\n1,9,0.0,4,\n\nHeader and first few lines of CSV file 2:\nDebtRatio,MonthlyIncome,Unnamed: 0,NumberOfTimes90DaysLate,NumberOfTime60-89DaysPastDueNotWorse,RevolvingUtilizationOfUnsecuredLines\n0.177512717,5700.0,1,0,0,0.88551908\n0.527236928,9141.0,2,0,0,0.463295269\n0.687647522,5083.0,3,0,0,0.043275036\n0.925960637,3200.0,4,0,0,0.280308229\n0.019917227,3865.0,5,0,0,0.9999999\n0.342429365,4140.0,6,0,0,0.509791452\n1048.0,0.0,7,0,0,0.587778161\n0.3691702,3301.0,8,0,0,0.046148938\n2024.0,,9,0,0,0.013527027\n\nQuestion: Combine the rows from the two tables where 'NumberRealEstateLoansOrLines' is greater than 1 and 'NumberOfTimes90DaysLate' is '0', and merge them based on the column name. Only include the successful merged sections.", "csv1_example": "NumberRealEstateLoansOrLines,Unnamed: 0,NumberOfDependents,NumberOfOpenCreditLinesAndLoans,SeriousDlqin2yrs\n0,1,0.0,4,\n4,2,2.0,15,\n1,3,2.0,12,\n2,4,0.0,7,\n0,5,1.0,4,\n0,6,1.0,4,\n0,7,3.0,5,\n1,8,1.0,8,\n1,9,0.0,4,\n0,10,0.0,0,\n", "csv2_example": "DebtRatio,MonthlyIncome,Unnamed: 0,NumberOfTimes90DaysLate,NumberOfTime60-89DaysPastDueNotWorse,RevolvingUtilizationOfUnsecuredLines\n0.177512717,5700.0,1,0,0,0.88551908\n0.527236928,9141.0,2,0,0,0.463295269\n0.687647522,5083.0,3,0,0,0.043275036\n0.925960637,3200.0,4,0,0,0.280308229\n0.019917227,3865.0,5,0,0,0.9999999\n0.342429365,4140.0,6,0,0,0.509791452\n1048.0,0.0,7,0,0,0.587778161\n0.3691702,3301.0,8,0,0,0.046148938\n2024.0,,9,0,0,0.013527027\n0.0,0.0,10,98,98,0.9999999\n", "csv1_path": "infiagent/merge_test/25_3_0.csv", "csv2_path": "infiagent/merge_test/25_3_1.csv", "instruction": "Combine the rows from the two tables where 'NumberRealEstateLoansOrLines' is greater than 1 and 'NumberOfTimes90DaysLate' is '0', and merge them based on the column name. Only include the successful merged sections.", "instruction_type": "Merge-Screening", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/25_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/25_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['NumberRealEstateLoansOrLines'] > 1]\ndf = df[df['NumberOfTimes90DaysLate'] == 0]\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\ndf = df[df['NumberRealEstateLoansOrLines'] > 1]\ndf = df[df['NumberOfTimes90DaysLate'] == '0']\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,Open,Low,High\n2014-01-03,82.929,77.2,79.1\n2014-01-07,81.64800000000001,76.85,77.99\n2014-01-08,80.8185,76.96,77.94\n2014-01-09,82.0155,76.48,78.12\n2014-01-10,80.97600000000001,75.87,77.26\n2014-01-13,79.485,75.7,77.5\n2014-01-14,80.7345,76.81,78.1\n2014-01-15,83.0235,78.81,80.03\n2014-01-16,83.23349999999999,78.81,79.55\n\nHeader and first few lines of CSV file 2:\nDate,Close\n2014-01-02,79.02\n2014-01-03,77.28\n2014-01-06,77.7\n2014-01-07,77.15\n2014-01-08,77.64\n2014-01-09,76.65\n2014-01-13,76.53\n2014-01-14,78.06\n2014-01-16,79.18\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Date,Open,Low,High\n2014-01-03,82.929,77.2,79.1\n2014-01-07,81.64800000000001,76.85,77.99\n2014-01-08,80.8185,76.96,77.94\n2014-01-09,82.0155,76.48,78.12\n2014-01-10,80.97600000000001,75.87,77.26\n2014-01-13,79.485,75.7,77.5\n2014-01-14,80.7345,76.81,78.1\n2014-01-15,83.0235,78.81,80.03\n2014-01-16,83.23349999999999,78.81,79.55\n2014-01-17,82.71900000000001,77.13,78.87\n", "csv2_example": "Date,Close\n2014-01-02,79.02\n2014-01-03,77.28\n2014-01-06,77.7\n2014-01-07,77.15\n2014-01-08,77.64\n2014-01-09,76.65\n2014-01-13,76.53\n2014-01-14,78.06\n2014-01-16,79.18\n2014-01-17,77.24\n", "csv1_path": "infiagent/merge_test/29_2_0.csv", "csv2_path": "infiagent/merge_test/29_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/29_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/29_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nHouseAge,MedInc\n36.0,0.9298\n17.0,2.7006\n30.0,5.0286\n21.0,3.9038\n52.0,7.1754\n28.0,4.1292\n33.0,2.6062\n42.0,5.222\n37.0,2.2917\n\nHeader and first few lines of CSV file 2:\nMedInc,Longitude,Latitude\n0.9298,-118.25,33.93\n2.7006,-117.03,32.79\n3.9038,-122.02,37.36\n7.1754,-122.28,37.9\n4.1292,-118.27,33.82\n2.6062,-120.12,41.4\n5.222,-118.39,34.05\n2.2917,-118.25,33.97\n3.2292,-120.95,36.47\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "HouseAge,MedInc\n36.0,0.9298\n17.0,2.7006\n30.0,5.0286\n21.0,3.9038\n52.0,7.1754\n28.0,4.1292\n33.0,2.6062\n42.0,5.222\n37.0,2.2917\n10.0,7.472\n", "csv2_example": "MedInc,Longitude,Latitude\n0.9298,-118.25,33.93\n2.7006,-117.03,32.79\n3.9038,-122.02,37.36\n7.1754,-122.28,37.9\n4.1292,-118.27,33.82\n2.6062,-120.12,41.4\n5.222,-118.39,34.05\n2.2917,-118.25,33.97\n3.2292,-120.95,36.47\n7.472,-117.25,32.99\n", "csv1_path": "infiagent/merge_test/45_0_0.csv", "csv2_path": "infiagent/merge_test/45_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/45_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/45_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntimestamp,contempt,index,neutral,name\n00h-00m-00s,0.001,0,0.981,alckmin\n00h-00m-00s,0.001,12845,0.996,boulos\n00h-00m-00s,0.01,19125,0.942,ciro\n00h-00m-00s,0.002,25592,0.626,marina\n00h-00m-00s,0.003,32061,0.884,meirelles\n00h-00m-00s,0.003,6397,0.852,alvaro\n00h-00m-01s,0.005,25593,0.563,marina\n00h-00m-01s,0.016,19126,0.939,ciro\n00h-00m-01s,0.0,6398,0.303,alvaro\n\nHeader and first few lines of CSV file 2:\nsurprise,happiness,anger,index,blur,fear\n0.0,0.0,0.0,0,0.0,0.0\n0.0,0.001,0.001,12845,0.05,0.0\n0.0,0.0,0.003,19125,0.3,0.0\n0.001,0.0,0.111,32061,0.0,0.0\n0.0,0.145,0.0,6397,0.31,0.0\n0.0,0.432,0.0,25593,0.0,0.0\n0.0,0.0,0.002,19126,0.18,0.0\n0.001,0.694,0.0,6398,0.89,0.0\n0.0,0.0,0.0,12846,0.07,0.0\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "timestamp,contempt,index,neutral,name\n00h-00m-00s,0.001,0,0.981,alckmin\n00h-00m-00s,0.001,12845,0.996,boulos\n00h-00m-00s,0.01,19125,0.942,ciro\n00h-00m-00s,0.002,25592,0.626,marina\n00h-00m-00s,0.003,32061,0.884,meirelles\n00h-00m-00s,0.003,6397,0.852,alvaro\n00h-00m-01s,0.005,25593,0.563,marina\n00h-00m-01s,0.016,19126,0.939,ciro\n00h-00m-01s,0.0,6398,0.303,alvaro\n00h-00m-01s,0.0,12846,0.998,boulos\n", "csv2_example": "surprise,happiness,anger,index,blur,fear\n0.0,0.0,0.0,0,0.0,0.0\n0.0,0.001,0.001,12845,0.05,0.0\n0.0,0.0,0.003,19125,0.3,0.0\n0.001,0.0,0.111,32061,0.0,0.0\n0.0,0.145,0.0,6397,0.31,0.0\n0.0,0.432,0.0,25593,0.0,0.0\n0.0,0.0,0.002,19126,0.18,0.0\n0.001,0.694,0.0,6398,0.89,0.0\n0.0,0.0,0.0,12846,0.07,0.0\n0.0,0.0,0.0,1,0.0,0.0\n", "csv1_path": "infiagent/merge_test/9_3_0.csv", "csv2_path": "infiagent/merge_test/9_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/9_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/9_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nINDICATOR,LOCATION,TIME\nEDUEXP,AUS,2012\nEDUEXP,AUT,2012\nEDUEXP,BEL,2012\nEDUEXP,CAN,2012\nEDUEXP,CZE,2012\nEDUEXP,FIN,2012\nEDUEXP,FRA,2012\nEDUEXP,DEU,2012\nEDUEXP,GRC,2012\n\nHeader and first few lines of CSV file 2:\nLOCATION,Flag Codes,SUBJECT\nAUS,,TRY\nAUT,,TRY\nBEL,,TRY\nCAN,,TRY\nCZE,,TRY\nDNK,M,TRY\nFIN,,TRY\nHUN,,TRY\nISL,,TRY\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "INDICATOR,LOCATION,TIME\nEDUEXP,AUS,2012\nEDUEXP,AUT,2012\nEDUEXP,BEL,2012\nEDUEXP,CAN,2012\nEDUEXP,CZE,2012\nEDUEXP,FIN,2012\nEDUEXP,FRA,2012\nEDUEXP,DEU,2012\nEDUEXP,GRC,2012\nEDUEXP,HUN,2012\n", "csv2_example": "LOCATION,Flag Codes,SUBJECT\nAUS,,TRY\nAUT,,TRY\nBEL,,TRY\nCAN,,TRY\nCZE,,TRY\nDNK,M,TRY\nFIN,,TRY\nHUN,,TRY\nISL,,TRY\nIRL,,TRY\n", "csv1_path": "infiagent/merge_test/46_1_0.csv", "csv2_path": "infiagent/merge_test/46_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/46_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/46_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nEX2,EQ8,EQ3,EX6,EX5,Unnamed: 0,EQ2,EX8,EX4\n0.0426168252660709,-0.0219164236380736,-0.0209965026800568,-0.0272364598603708,-0.110649838313963,1,-0.208033710010569,0.200716505553366,-0.0730019227052621\n0.0263969887842081,-0.252665055370363,-0.0419930053601136,-0.0095903027677361,-0.098188303125206,2,0.0303187465307899,0.241603201129052,-0.094665094890157\n0.0257609167653115,-0.171574287909491,-0.0681647023627196,-0.0333742536317219,-0.0780994489847098,3,0.0990492914551702,0.307269712205153,-0.114399354483246\n0.0337118170015188,-0.144022212478769,-0.0678689769728596,-0.0061377937713511,-0.0642413624385921,4,0.203353027030218,0.323376592280424,-0.0908072697065456\n0.0149476924440696,-0.319666693349616,-0.0344520079186847,-0.0118919754319929,-0.0482347353426887,5,0.54078497298146,0.209389440978512,-0.0310109793605687\n\nHeader and first few lines of CSV file 2:\nNZ,EQ6,EQ4,EX1,EQ7,Unnamed: 0\n0.362830325973427,0.0508805382290177,-0.0642619047779573,0.0410182786002027,-0.0192616021801591,1\n0.3381555792905,0.0769627933528486,-0.0162345864702208,0.0300434883392789,-0.0200465944831218,2\n0.0146529160116964,0.0821742091932279,-0.085908020071585,0.0237329839392477,-0.0115054095747666,3\n-0.36930248144376,0.0588109536382906,-0.0987604010271765,0.0128953785565855,-0.025916462300798,4\n-0.383436113240686,0.0137460533760731,-0.040586466175552,0.0026065126869694,-0.0260687742401788,5\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "EX2,EQ8,EQ3,EX6,EX5,Unnamed: 0,EQ2,EX8,EX4\n0.0426168252660709,-0.0219164236380736,-0.0209965026800568,-0.0272364598603708,-0.110649838313963,1,-0.208033710010569,0.200716505553366,-0.0730019227052621\n0.0263969887842081,-0.252665055370363,-0.0419930053601136,-0.0095903027677361,-0.098188303125206,2,0.0303187465307899,0.241603201129052,-0.094665094890157\n0.0257609167653115,-0.171574287909491,-0.0681647023627196,-0.0333742536317219,-0.0780994489847098,3,0.0990492914551702,0.307269712205153,-0.114399354483246\n0.0337118170015188,-0.144022212478769,-0.0678689769728596,-0.0061377937713511,-0.0642413624385921,4,0.203353027030218,0.323376592280424,-0.0908072697065456\n0.0149476924440696,-0.319666693349616,-0.0344520079186847,-0.0118919754319929,-0.0482347353426887,5,0.54078497298146,0.209389440978512,-0.0310109793605687\n0.0181280525385525,-0.316848867453293,0.0076888601363588,-0.0210986660890196,-0.052854097524728,6,0.474591056252882,0.039647704800665,-0.0051932262087076\n0.03721021310545,0.0284913507294957,0.0344520079186847,-0.0149608723176685,-0.06606762190591,7,0.262601414175049,-0.0582325664259767,0.0\n0.0222625206613803,0.317788142752067,0.0360784975629145,-0.06943379203841,-0.0526392434697494,8,0.27117763331324,-0.073100455726226,-0.0847237761477737\n0.002226252066138,0.159363709025421,0.0220315415445666,-0.0444990048422959,-0.0060159135393999,9,-0.0264534083276613,-0.0179653662378013,-0.117366912316793\n0.0073148282173106,-0.207579841029183,0.0156734456625776,0.0337578657424314,0.0553249191569815,10,-0.548757233025412,0.0291162832129883,-0.0501517273869484\n", "csv2_example": "NZ,EQ6,EQ4,EX1,EQ7,Unnamed: 0\n0.362830325973427,0.0508805382290177,-0.0642619047779573,0.0410182786002027,-0.0192616021801591,1\n0.3381555792905,0.0769627933528486,-0.0162345864702208,0.0300434883392789,-0.0200465944831218,2\n0.0146529160116964,0.0821742091932279,-0.085908020071585,0.0237329839392477,-0.0115054095747666,3\n-0.36930248144376,0.0588109536382906,-0.0987604010271765,0.0128953785565855,-0.025916462300798,4\n-0.383436113240686,0.0137460533760731,-0.040586466175552,0.0026065126869694,-0.0260687742401788,5\n-0.154171177528628,-0.0231114963355954,-0.055468170439921,-0.0071336136696004,-0.0049911450904791,6\n-0.208110086071965,-0.020795311517649,-0.071026315807216,-0.0111119751391853,-0.0295016510277619,7\n-0.313322994728527,0.0169182195397822,-0.0033822055146293,-0.007270798547862,-0.0303335085428418,8\n-0.0604205326815528,0.05772838899512,-0.0121759398526656,-0.0109747902609238,-0.0260570579371495,9\n-0.11687746493838,0.0803111909700971,-0.0493802005135883,-0.0219495805218476,-0.0709187822363157,10\n", "csv1_path": "infiagent/merge_test/31_0_0.csv", "csv2_path": "infiagent/merge_test/31_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/31_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/31_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nR_VALUE,SHRGT45,TIME,MEANGBH,MEANGBZ,TOTUSJZ,ABSNJZH,MEANJZD,TRUE_TIME,TOTUSJH\n0.0,0.061,11.6,31.21,92.809,3141588000000.0,14.092,0.08746085,2014.03.23_20:24:00_TAI,143.341\n0.0,0.0,11.8,31.535,89.779,3745627000000.0,18.216,0.15138598,2014.03.23_20:36:00_TAI,173.704\n0.0,0.016,12.0,30.425,89.566,3790352000000.0,18.001,0.139126,2014.03.23_20:48:00_TAI,174.009\n0.0,0.048,12.2,30.44,89.499,3604093000000.0,19.141,0.2345193,2014.03.23_21:00:00_TAI,164.412\n0.0,0.046,12.4,29.875,87.454,3622492000000.0,22.204,0.26665705,2014.03.23_21:12:00_TAI,163.141\n0.0,0.045,12.8,29.165,86.595,3534322000000.0,18.042,0.18933275,2014.03.23_21:36:00_TAI,157.135\n2.07,0.026,13.2,27.636,85.236,3761321000000.0,12.963,0.00457878,2014.03.23_22:00:00_TAI,162.369\n0.0,0.0,13.8,27.23,87.682,3761963000000.0,19.912,0.18886125,2014.03.23_22:36:00_TAI,163.987\n0.0,0.053,14.0,26.903,87.929,3606528000000.0,19.032,0.18899454,2014.03.23_22:48:00_TAI,157.176\n\nHeader and first few lines of CSV file 2:\nTRUE_TIME,AREA_ACR,MEANSHR,MEANJZH,MEANGBT\n2014.03.23_20:24:00_TAI,69.26413,18.695,0.00286312,93.013\n2014.03.23_20:36:00_TAI,83.896141,18.172,0.00309745,89.953\n2014.03.23_20:48:00_TAI,86.314224,18.322,0.00293124,89.552\n2014.03.23_21:12:00_TAI,84.621979,17.850,0.00341966,87.089\n2014.03.23_21:36:00_TAI,93.034233,17.608,0.00270168,86.248\n2014.03.23_21:48:00_TAI,96.688416,17.502,0.00314102,84.835\n2014.03.23_22:00:00_TAI,96.260063,17.662,0.00170337,84.526\n2014.03.23_22:12:00_TAI,97.720245,17.638,0.00244001,84.563\n2014.03.23_22:24:00_TAI,97.474724,17.485,0.00241245,86.069\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "R_VALUE,SHRGT45,TIME,MEANGBH,MEANGBZ,TOTUSJZ,ABSNJZH,MEANJZD,TRUE_TIME,TOTUSJH\n0.0,0.061,11.6,31.21,92.809,3141588000000.0,14.092,0.08746085,2014.03.23_20:24:00_TAI,143.341\n0.0,0.0,11.8,31.535,89.779,3745627000000.0,18.216,0.15138598,2014.03.23_20:36:00_TAI,173.704\n0.0,0.016,12.0,30.425,89.566,3790352000000.0,18.001,0.139126,2014.03.23_20:48:00_TAI,174.009\n0.0,0.048,12.2,30.44,89.499,3604093000000.0,19.141,0.2345193,2014.03.23_21:00:00_TAI,164.412\n0.0,0.046,12.4,29.875,87.454,3622492000000.0,22.204,0.26665705,2014.03.23_21:12:00_TAI,163.141\n0.0,0.045,12.8,29.165,86.595,3534322000000.0,18.042,0.18933275,2014.03.23_21:36:00_TAI,157.135\n2.07,0.026,13.2,27.636,85.236,3761321000000.0,12.963,0.00457878,2014.03.23_22:00:00_TAI,162.369\n0.0,0.0,13.8,27.23,87.682,3761963000000.0,19.912,0.18886125,2014.03.23_22:36:00_TAI,163.987\n0.0,0.053,14.0,26.903,87.929,3606528000000.0,19.032,0.18899454,2014.03.23_22:48:00_TAI,157.176\n0.0,0.026,14.2,27.225,85.809,3815674000000.0,23.008,0.30007368,2014.03.23_23:00:00_TAI,162.814\n", "csv2_example": "TRUE_TIME,AREA_ACR,MEANSHR,MEANJZH,MEANGBT\n2014.03.23_20:24:00_TAI,69.26413,18.695,0.00286312,93.013\n2014.03.23_20:36:00_TAI,83.896141,18.172,0.00309745,89.953\n2014.03.23_20:48:00_TAI,86.314224,18.322,0.00293124,89.552\n2014.03.23_21:12:00_TAI,84.621979,17.850,0.00341966,87.089\n2014.03.23_21:36:00_TAI,93.034233,17.608,0.00270168,86.248\n2014.03.23_21:48:00_TAI,96.688416,17.502,0.00314102,84.835\n2014.03.23_22:00:00_TAI,96.260063,17.662,0.00170337,84.526\n2014.03.23_22:12:00_TAI,97.720245,17.638,0.00244001,84.563\n2014.03.23_22:24:00_TAI,97.474724,17.485,0.00241245,86.069\n2014.03.23_22:36:00_TAI,99.95433,17.464,0.00262174,86.868\n", "csv1_path": "infiagent/merge_test/6_2_0.csv", "csv2_path": "infiagent/merge_test/6_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/6_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/6_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,pos,title\n0,0.2,WATCH: Instagram rolls out new features aimed at improving security\n1,0.204,WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\n\nHeader and first few lines of CSV file 2:\nurlToImage,neu,description,Unnamed: 0,text\nhttps://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,0.733,Updates to the app include an \"about this account\" feature for users with large followings, and people can now request to be verified, too.,0,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"GMA\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\nhttps://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,0.735,Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.,1,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Unnamed: 0,pos,title\n0,0.2,WATCH: Instagram rolls out new features aimed at improving security\n1,0.204,WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\n2,0.102,Nurse under investigation after posting on anti-vaccination page\n3,0.169,WATCH: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running\n4,0.048,'Political correctness is like a cancer': AZ GOP candidate defends comments\n5,0.215,WATCH: YouTube plans to allow fewer ad skips\n6,0.08,Myanmar military chief among those banned by Facebook\n8,0.102,FireEye: Tech firms' secret weapon against disinformation\n10,0.029,Man who threatened to kill Trump considered armed and dangerous: Law enforcement\n11,0.068,Wannabe Facebook owner who fled arrest is caught in Ecuador\n", "csv2_example": "urlToImage,neu,description,Unnamed: 0,text\nhttps://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,0.733,Updates to the app include an \"about this account\" feature for users with large followings, and people can now request to be verified, too.,0,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"GMA\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\nhttps://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,0.735,Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.,1,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\nhttps://s.abcnews.com/images/Technology/180827_atm_techbytes_hpMain_16x9_992.jpg,0.719,The video site is introducing nonskippable ads to all of its channels, which will be controlled by content creators.,5,Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. Facebook admits to providing data to dozens of tech companies The admission from the tech giant was made in new company documents given to Congress. Now Playing: Facebook admits to providing data to dozens of tech companies Now Playing: Say goodbye to overripe avocados thanks to new natural tech from food startup Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: Pentagon requests $8 billion for Space Force  Now Playing: {{itm.title}}\nhttps://s.abcnews.com/images/Technology/WireAP_86828a77b1c1446e96826da60a57aed8_16x9_992.jpg,0.767,He was among 20 individuals and organizations banned for spreading hate.,6,\n \nFacebook said Monday that it is banning Myanmar's powerful military chief and 19 other individuals and organizations from its site to prevent the spread of hate and misinformation. \nThe social media giant was heavily criticized for permitting itself to be used to inflame ethnic and religious conflict in the country, particularly against minority Rohingya Muslims. It has been accused of being lax in fighting online misinformation and manipulation in many countries, but Myanmar is one where it has been most closely tied to deadly violence. \nSome 700,000 Rohingya have fled from Myanmar's western state of Rakhine over the past year in response to a brutal counterinsurgency campaign by the military, which has been accused of massive human rights violations. Critics accuse the military of carrying out ethnic cleansing, or even genocide, an allegation denied by the government, which says it was responding to attacks on security forces. \nFacebook said it also targeted pages and accounts that pretended to provide independent news and opinion, while covertly promoting messages of Myanmar's military. It said it was deleting 18 Facebook accounts, one Instagram account and 52 Facebook pages. \nA separate report by investigators working for the U.N.'s top human rights body, released Monday, charged that \"Facebook has been a useful instrument for those seeking to spread hate, in a context where for most users Facebook is the internet.\" \n\"Although improved in recent months, Facebook's response has been slow and ineffective,\" said the report by the Fact-Finding Mission on Myanmar, authorized by the U.N. Human Rights Council. \"The extent to which Facebook posts and messages have led to real-world discrimination and violence must be independently and thoroughly examined.\" \nFour high-ranking officers and two military units targeted by Facebook were also put on a U.S. government blacklist earlier this month for human rights abuses. The sanctions block any property they own within the U.S. and prohibit U.S. citizens from engaging in transactions with them. The U.S. already maintains restrictions on visas, arms sales and assistance to Myanmar's military. In June, the EU imposed similar sanctions on seven senior army and police officers, all of whom are on Facebook's blacklist. \nSix officers on Facebook's list were also named in the U.N. human rights report, which said Myanmar's top leaders should be prosecuted for genocide. Those it recommended as \"priority subjects for investigation and prosecution\" included top commander Senior Gen. Min Aung Hlaing. \nIn a statement, Facebook referred to the U.N. report, which it said \"found evidence that many of these individuals and organizations committed or enabled serious human rights abuses in the country. And we want to prevent them from using our service to further inflame ethnic and religious tensions.\" Discrimination against the Rohingya ran deep and wide even before the spread of Facebook. \nFacebook has been under pressure for several months to take action on the problem, especially after civic and rights groups in Myanmar said in April that it had failed to adequately act against online hate speech that incited violence against the country's Muslim minorities, neglecting to effectively enforce its own rules. \n\"The ethnic violence in Myanmar has been truly horrific,\" Facebook said in its statement. \"While we were too slow to act, we're now making progress â with better technology to identify hate speech, improved reporting tools, and more people to review content.\" \n\"We continue to work to prevent the misuse of Facebook in Myanmar â including through the independent human rights impact assessment we commissioned earlier in the year. This is a huge responsibility given so many people there rely on Facebook for information â more so than in almost any other country given the nascent state of the news media and the recent rapid adoption of mobile phones. It's why we're so determined to do better in the future.\" \nYangon-based political analyst David Mathieson said that Facebook's action, together with the damning U.N. report, force Myanmar's military brass \"into an isolation they're not going to like.\" \n\"They have to find alternative ways to communicate with the Myanmar population, because Facebook really is the internet for many people here. And Facebook just excommunicated the commander in chief from the worldwide web,\" he told The Associated Press.\n,0.81,Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news.,8,\n \nThis week has seen major social media sites step up their policing of online disinformation campaigns. \nGoogle disabled dozens of YouTube channels and other accounts linked to a state-run Iranian broadcaster running a political-influence campaign. \nFacebook removed 652 suspicious pages, groups and accounts linked to Russia and Iran. \nTwitter took similar action shortly thereafter. \nWhat did they have in common? The security firm FireEye. \nBest known for its work on high-profile cyberattacks against companies including Target, JPMorgan Chase and Sony Pictures, FireEye is emerging as a key player in the fight against election interference and disinformation campaigns. \nFounded in 2004, FireEye is based in Silicon Valley and staffed with a roster of former military and law-enforcement cyberexperts. \n\"They've really become the Navy SEALs of cybersecurity, especially for next-generation cybersecurity threats,\" said GBH Insights analyst Dan Ives. \nLee Foster, manager of information operations analysis at FireEye, said his team works within the company's intelligence outfit, which researches not only \"info-ops\" â like the Iran-linked social media activity it recently uncovered â but espionage, financial crime and other forms of vulnerability and exploitation. Specialist teams at FireEye focus on particular areas of cyberthreats, each with their own expertise and language capabilities. \n\"We kind of operate like a private-sector intelligence operation,\" he said. \nFireEye was founded by Ashar Aziz, who developed a system for spotting threats that haven't been tracked before, unlike older companies that sold firewalls or anti-virus programs that block known malware. \nAziz, a former Sun Microsystems engineer, created a system that uses software to simulate a computer network and check programs for suspicious behavior, before allowing them into the network itself. \nFireEye raised its profile in 2014 by acquiring Mandiant, known for expertise in assessing damage and tracing the source of cyberattacks. Mandiant founder Kevin Mandia, a former U.S. Air Force investigator, is now FireEye's CEO. \nWhile businesses are spending more on information security, FireEye itself has spent heavily on research, development, sales and marketing. That has led to struggles to remain profitable, as heavy investments offset revenue growth. \nMandia said that during the three months ended June 30, FireEye's email security found 6 million spear-phishing attacks, a type of hacking, and its security products alerted companies of attempts to breach security 29 million times. That's important, Mandia said, because most of FireEye's products are deployed behind their client's existing firewalls or antivirus software, so everything FireEye catches has already evaded other defenses, he said. \n\"We are the investigators called in when the processes, people, and technology fail to prevent a security breach or incident,\" he said. \"We find the gaps in the security fabric and we find the needle in the haystack.\" \nFireEye Inc.'s second-quarter revenue rose 6 percent to $203 million but it lost $72.9 million, or 38 cents per share. That met Wall Street's expectations, but its shares fell as investors expected more. \nThat's a common problem in the white-hot cybersecurity sector, which includes competitors like Palo Alto Networks, CloudFlare and Check Point. The companies are facing high expectations as the cybersecurity market booms, fueled by heightened cyberattacks and hacking fears. \n\"As the space has become more competitive ... profitability and growth has been a challenge for (FireEye),\" Ives said. \nStill, FireEye's stock jumped 6 percent on Thursday when news broke of its role in uncovering the fake accounts on YouTube, Facebook and Twitter. It was up another 3 percent Friday. \nFireEye shares hit their all-time peak of $95.63 on March 5, 2014, a few months after they went public, but began a long decline after that, hitting an all-time low of $10.40 almost exactly three years later on March 14, 2017. In the past month the stock has traded between $14.38 and $16.69. \nAnd the company's reputation continues to grow. \n\"There are many vendors that play in cybersecurity when you look at some of the very sophisticated threats facing enterprise and governments,\" Ives said. \"FireEye many times gets that first phone call when it comes to assess threat environment for companies.\" \n——— \nAP Technology Writer Barbara Ortutay in New York contributed to this story.\nhttps://s.abcnews.com/images/US/shawn-richard-christy-ht-001-jpo-180823_hpMain_16x9_992.jpg,0.848,Shawn Richard Christy threatened to kill president trump and other elected officials.,10,\nA Pennsylvania man who threatened to kill President Donald Trump back in June is still on the run and was last seen in Cumberland, Maryland, earlier this week in a stolen truck, the U.S. Marshals Service said.\n \nA warrant was issued June 19 for 27-year-old Shawn Richard Christy and he remains wanted by federal authorities for threatening other elected officials as well.\n \nAuthorities have not been able to locate Christy since the warrant was issued. He drew federal authorities attention with his remarks about the president on Facebook and his previous behavior.\n \nAccording to court documents, Christy, in a threatening post on Facebook to Northampton County, Pennsylvania District Attorney John Morganelli, wrote: âKeep it up Morganelli, I promise Iâll put a bullet in your head as soon as I put one in the head of President Donald J. Trump.â\n \nSupervisory Deputy U.S. Marshal Robert Clark told ABC News the stolen red pickup truck was recovered Tuesday, in Tamaqua, Pennsylvania, near Christy's hometown of McAdoo.\n \nChristy is considered by the U.S. Marshals, FBI and U.S. Secret Service to be armed and dangerous.\n \nAuthorities say Christy has a distinct lisp and a barbed wire cross tattoo on his upper right arm. He also, according to authorities, considers himself a survivalist.\n \nChristy also said that he was going to use \"lethal force\" on any law enforcement attempting to detain him. He made the comments between June 3-12.\n \nThe reward for any information is now up to $20,000 and the three agencies are asking for the public's help with any information leading to Christy.\n \nHe is also wanted in Pennsylvania for burglary, probation violation and failure to appear for an aggravated assault case.\n \n\"Persons having information should contact the U.S. Marshals at 1-877-Wanted-2 (1-877-926-8332) or the FBI at 215-418-4000,\" a release from the Marshals said.\n \n\"Individuals should not attempt to arrest Christy themselves,\" the release said.\n \n \n \nThis is not the only time Christy has made threats against a public official.\n \nIn an interview with Newsweek, Shawn Christy's father, Craig, said that his son explained to him how he hid from law enforcement and noted that he has various guns on his possession that he had taken from the home.\n \nLast year, according to the Associated Press he swung a stick at the McAdoo mayor over a dispute about snow plowing.\n \nHe's also had problems with former Alaska governor and vice presidential candidate Sarah Palin, who filed a restraining order against then 18-year-old Christy in 2010.\n \nA year later he pleaded guilty to making harassing phone calls to Palin, her family members and lawyers.\n \n\"This is one of the stalkers who has tormented my family for years. Threatening my kids and my parents, following my daughters, moving to Alaska, then following Bristol to Texas to more aggressively physically invade and intimidate... itâs been a hellish ride with these stalkers... as weâve informed the FBI and law enforcement for years,\" Palin said in a Facebook post August 20.\n \n\"Finally, perhaps, in this particular case with one of the stalkers - Shawn Christy (having recently threatened the President) justice may prevail.\"\n \nABC News' Jack Date contributed to this report.\n,0.77,Authorities say a New York man who was arrested for falsely claiming he was owed half-ownership of Facebook but then fled the country has been captured in Ecuador,11,\n \nA New York man who was arrested for falsely claiming he was owed half-ownership of Facebook but then fled the country has been captured in Ecuador, authorities said Thursday. \nProsecutors notified a judge presiding over his case in Manhattan federal court that Paul Ceglia was arrested in the morning. They said he'll appear in a court in Quito, Ecuador's capital, within a day. \nRobert Ross Fogg, Ceglia's defense lawyer, said he was surprised at news of his client's capture, but \"mostly relieved that he was located without incident and hope the family has maintained good health.\" \n\"Look forward to his return and resuming our vigorous defense of his case,\" Fogg said in an email. \nNikki Credic-Barrett, a spokeswoman for the U.S. Marshals Service, said the agency had no information on the matter because Ceglia was in the custody of Ecuadorian law enforcement authorities. \nCeglia, 45, was arrested on fraud charges in October 2012. \nWhile under house arrest in Wellsville, New York, in March 2015 he disappeared with his wife and two young sons and the family's dog. Authorities said he sliced off his electronic monitoring device and created a crude contraption to make it seem he was moving around his home. \nProsecutors told U.S. District Judge Vernon Broderick they would update him on the status of extradition attempts. \nCeglia claimed in a lawsuit that he gave Facebook founder Mark Zuckerberg $1,000 in startup money in exchange for 50 percent of the future company in 2003 as part of a software development contract. \nFacebook lawyers said Ceglia and Zuckerberg did have a contract but references to Facebook were slipped in for purposes of the lawsuit. \nThe lawsuit was tossed out by a judge in Buffalo, New York, in 2014. Prosecutors then filed fraud charges after a forensic analysis of Ceglia's computers and Harvard's email archive determined he had altered an unrelated contract and falsified emails to make it seem Zuckerberg had promised him a half-share. \nCeglia maintained he was not guilty before he vanished. Mail and wire fraud charges against him carry a potential maximum sentence of 40 years in prison if he is convicted.\nhttps://s.abcnews.com/images/Politics/facebook-headquarters-01-as-gty-180822_hpMain_16x9_992.jpg,0.889,Amid two recent discoveries of disinformation campaigns on Facebook, ProPublica created a tool to track and analyze political advertisements on the social media platform.,12,\nIn the 2016 presidential election Facebook and other social media platforms were utilized by political campaigns and interest groups to run advertisements with very few rules regulating their disclosure and little public information about their source.\n \nThe lack of knowledge about these advertisements underscored the ease with which political actors, both foreign and domestic, were able to manipulate platforms like Facebook to disseminate misleading and often false information to Facebook users in the United States and around the world.\n \nThe spread of misinformation meant to foment political discord and stir unrest continues to be a concern less than three months out from a consequential midterm election. Just this week Facebook announced it removed \"652 pages, groups and accounts for coordinated inauthentic behavior that originated in Iran and targeted people across multiple internet services in the Middle East, Latin America, UK and US.\"\n \nFacebook also said it \"removed multiple pages, groups and accounts\" linked to \"inauthentic behavior\" on its platforms, including actions that originated in Russia.\n \n \n \nAside from the threat from nefarious actors, Facebook's influence as a medium through which American politicians and campaigns communicate with voters has skyrocketed in the last decade. But exactly how that influence and power is being yielded by politicians, campaigns and outside groups from all sides of the political spectrum still remains somewhat of a mystery due to the lack of rules governing the disclosure of that information.\n \n \n \nIn order to better understand the tactics and strategies of all political actors on Facebook, ABC News is partnering with ProPublica, a nonprofit, award-winning news organization that has built a tool to collect information and data on political ads that are being targeted to users of the social media giant.\n \nProPublica's tool is a simple, free browser plugin on a Firefox or Google Chrome web browser, that Facebook users can opt into and report political advertisements they see on the site. Those advertisements will then be collected by ProPublica in a public database to track and analyze advertisements on Facebook.\n \nABC News and other news organizations will then use that database and information to better understand and report on how political actors are utilizing one of the most powerful communication mediums on the planet.\n \nThus far the tool has gathered almost 60,000 individual advertisements from over 12,500 contributors, according to ProPublica.\n \nProPublica clearly states: \"The tool doesn't collect your personal information,\" and privacy controls ensure that the information that is being collected relates only to the advertisement that appeared on the site.\n \n \n \nEarlier this year Facebook announced steps to make the source of political advertisements on the site more transparent and understandable, but has still not given news organizations or academic institutions full and total access to all of the data behind what goes into purchasing and targeting those ads.\n \nThe site has created its own archive of political advertisements that allows users to obtain \"more information about some of the ads they see and the advertisers who are funding them,\" according to Facebook. But that archive remains an incomplete picture of how political advertisements are tailored to certain users because it does not show who was targeted with the ad.\n \nBy utilizing ProPublica's tool to track advertisements, Facebook users can more easily report political advertising, giving news organizations more data to gather and a greater ability to understand, analyze, fact-check and investigate political communication on the site.\n \nAlready the tool has helped locate political ads that were actually scams and malware, political ads that were ignore federal election rules by not disclosing who funded them, and uncovered a fake Facebook page claiming to belong to Special Counsel Robert Mueller with more than 100,000 followers.\n \nThe midterms are fast approaching, and ProPublica's tool will be a key asset in understanding how politicians, their campaigns, and powerful outside interest groups are using the medium to communicate with voters.\n \n \n\n,0.803,Police acting on a complaint by Thailand's ruling junta have charged the leaders of a new political party that strongly opposes military rule with violating the computer crime law, which could result in five-year prison terms,14,\n \nPolice acting on a complaint by Thailand's ruling junta have charged the leaders of a new political party that strongly opposes military rule with violating the computer crime law, which could result in five-year prison terms. \nFuture Forward party leader Thanathorn Juangroongruangkit and two other senior party members were charged by police this week with violating a section of the law that makes it a crime to transmit false information or information that damages the country's stability, group spokeswoman Pannika Wanich said Thursday. \nShe said the charges involve a June 29 Facebook live video posted on the pages of Thanathorn and the party, but did not describe how it may have violated the law. She said police had issued summonses in the past few days for the three party members to turn themselves in on Friday, but they had busy schedules and would ask for the date to be postponed to the middle of next month. \n\"And since all three have now officially become defendants in the case, our legal team needs some time to prepare,\" she said. \nThe Future Forward party is a new grouping featuring young politicians and is viewed as offering a real alternative to the ruling junta and its allies in upcoming elections. \nThailand's military government has kept a tight lid on dissent since it seized power from an elected government in a 2014 coup. It has promised to hold elections by early next year in a contest its critics charge will be set up to ensure it continues its influence over the country's administration. \nThe junta has already pushed back several promised election dates. Prime Minister and junta leader Prayuth Chan-ocha said Tuesday that the next polls are still scheduled for Feb. 24, though he did not entirely rule out another delay. \nPannika said a junta official had filed a police complaint against Thanathorn and his colleagues and had provided a brief explanation of the charges to local media. \nThe country's biggest newspaper, Thai Rath, said Col. Burin Thongprapai, a legal officer for the junta, stated last month that he filed the charges because in the Facebook live broadcast Thanathorn had made accusations against the junta and twisted facts in a manner that amounted to an attack on Thailand's justice system.\n,0.865,Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news.,15,\n \nFacebook has pulled one of its own products from Apple's app store because it didn't want to stop tracking what people were doing on their iPhones. Facebook also banned a quiz app from its social network for possible privacy intrusions on about 4 million users. \nThe twin developments come as Facebook is under intense scrutiny over privacy following the Cambridge Analytica scandal earlier this year. Allegations that the political consultancy used personal information harvested from 87 million Facebook accounts have dented Facebook's reputation. \nSince the scandal broke, Facebook has investigated thousands of apps and suspended more than 400 of them over data-sharing concerns. \nThe social media company said late Wednesday that it took action against the myPersonality quiz app, saying that its creators refused an inspection. But even as Facebook did that, it found its own Onavo Protect security app at odds with Apple's tighter rules for applications. \nOnavo Protect is a virtual-private network service aimed at helping users secure their personal information over public Wi-Fi networks. The app also alerts users when other apps use too much data. \nSince acquiring Onavo in 2013, Facebook has used it to track what apps people were using on phones. This surveillance helped Facebook detect trendy services, tipping off the company to startups it might want to buy and areas it might want to work on for upcoming features. \nFacebook said in a statement that it has \"always been clear when people download Onavo about the information that is collected and how it is used.\" \nBut Onavo fell out of compliance with Apple's app-store guidelines after they were tightened two months ago to protect the reservoir of personal information that people keep on their iPhones and iPads. \nApple's revised guidelines require apps to get users' express consent before recording and logging their activity on a device. According to Apple, the new rules also \"made it explicitly clear that apps should not collect information about which other apps are installed on a user's device for the purposes of analytics or advertising/marketing.\" \nFacebook will still be able to deploy Onavo on devices powered by Google's Android software. \nOnavo's ouster from Apple's app store widens the rift between two of the world's most popular companies. \nApple CEO Tim Cook has been outspoken in his belief that Facebook does a shoddy job of protecting its 2.2 billion users' privacy â something that he has framed as \"a fundamental human right.\"  \nCook sharpened his criticism following the Cambridge Analytica scandal. He emphasized that Apple would never be caught in the same situation as Facebook because it doesn't collect information about its customers to sell advertising. Facebook CEO Mark Zuckerberg fired back in a separate interview and called Cook's remarks \"extremely glib.\" Zuckerberg implied that Apple caters primarily to rich people with a line of products that includes the $1,000 iPhone X. \nLate Wednesday, Facebook said it moved to ban the myPersonality app after it found user information was shared with researchers and companies \"with only limited protections in place.\" The company said it would notify the app's users that their data may have been misused. \nIt said myPersonality was \"mainly active\" prior to 2012. Though Facebook has tightened its rules since then, it is only now reviewing those older apps following the Cambridge Analytica scandal. \nThe app was created in 2007 by researcher David Stillwell and allowed users to take a personality questionnaire and get feedback on the results. \n\"There was no misuse of personal data,\" Stillwell said in a statement, adding that \"this ban appears to be purely cosmetic.\" Stillwell said users gave their consent and the app's data was fully anonymized before it was used for academic research. He also rejected Facebook's assertion that he refused to submit to an audit.\n", "csv1_path": "infiagent/merge_test/33_3_0.csv", "csv2_path": "infiagent/merge_test/33_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/33_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/33_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLength,Shucked weight,Viscera weight\n0.455,0.2245,0.101\n0.35,0.0995,0.0485\n0.53,0.2565,0.1415\n0.44,0.2155,0.114\n0.33,0.0895,0.0395\n0.425,0.141,0.0775\n0.545,0.294,0.1495\n0.475,0.2165,0.1125\n0.55,0.3145,0.151\n\nHeader and first few lines of CSV file 2:\nShell weight,Diameter,Length\n0.15,0.365,0.455\n0.07,0.265,0.35\n0.21,0.42,0.53\n0.055,0.255,0.33\n0.12,0.3,0.425\n0.26,0.425,0.545\n0.32,0.44,0.55\n0.135,0.35,0.43\n0.19,0.38,0.49\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Length,Shucked weight,Viscera weight\n0.455,0.2245,0.101\n0.35,0.0995,0.0485\n0.53,0.2565,0.1415\n0.44,0.2155,0.114\n0.33,0.0895,0.0395\n0.425,0.141,0.0775\n0.545,0.294,0.1495\n0.475,0.2165,0.1125\n0.55,0.3145,0.151\n0.43,0.1675,0.081\n", "csv2_example": "Shell weight,Diameter,Length\n0.15,0.365,0.455\n0.07,0.265,0.35\n0.21,0.42,0.53\n0.055,0.255,0.33\n0.12,0.3,0.425\n0.26,0.425,0.545\n0.32,0.44,0.55\n0.135,0.35,0.43\n0.19,0.38,0.49\n0.185,0.355,0.47\n", "csv1_path": "infiagent/merge_test/8_1_0.csv", "csv2_path": "infiagent/merge_test/8_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/8_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/8_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nRings,Viscera weight,Length,Height,Sex\n7,0.0485,0.35,0.09,M\n8,0.0775,0.425,0.095,I\n16,0.1495,0.545,0.125,F\n9,0.1125,0.475,0.125,M\n19,0.151,0.55,0.15,F\n14,0.1475,0.525,0.14,F\n10,0.081,0.43,0.11,M\n11,0.095,0.49,0.135,M\n10,0.171,0.535,0.145,F\n\nHeader and first few lines of CSV file 2:\nShucked weight,Length,Whole weight\n0.2245,0.455,0.514\n0.2565,0.53,0.677\n0.2155,0.44,0.516\n0.0895,0.33,0.205\n0.141,0.425,0.3515\n0.294,0.545,0.768\n0.2165,0.475,0.5095\n0.3145,0.55,0.8945\n0.194,0.525,0.6065\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Rings,Viscera weight,Length,Height,Sex\n7,0.0485,0.35,0.09,M\n8,0.0775,0.425,0.095,I\n16,0.1495,0.545,0.125,F\n9,0.1125,0.475,0.125,M\n19,0.151,0.55,0.15,F\n14,0.1475,0.525,0.14,F\n10,0.081,0.43,0.11,M\n11,0.095,0.49,0.135,M\n10,0.171,0.535,0.145,F\n10,0.0805,0.47,0.1,F\n", "csv2_example": "Shucked weight,Length,Whole weight\n0.2245,0.455,0.514\n0.2565,0.53,0.677\n0.2155,0.44,0.516\n0.0895,0.33,0.205\n0.141,0.425,0.3515\n0.294,0.545,0.768\n0.2165,0.475,0.5095\n0.3145,0.55,0.8945\n0.194,0.525,0.6065\n0.1675,0.43,0.406\n", "csv1_path": "infiagent/merge_test/8_0_0.csv", "csv2_path": "infiagent/merge_test/8_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/8_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/8_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsch_arr_time,origin_station_code,fleet_number,end_time,station_type,destination_station_code\n,MAM,3088.0,21/09/15 05:41:00,Passenger,ROT\n,ROT,3088.0,21/09/15 07:14:00,Passenger,ATP\n,STSTb,3063.0,21/09/15 05:44:00,Vehicle,JAV\n,JAV,3063.0,21/09/15 06:32:00,Passenger,CHATb\n,CHATb,3063.0,21/09/15 07:20:00,Vehicle,JAV\n,BRB,3006.0,21/09/15 07:27:00,Passenger,CRT\n,CRT,3017.0,21/09/15 08:27:00,Passenger,BRB\n,SKH,3099.0,22/09/15 00:46:00,Passenger,MAM\n,SKSTb,3052.0,21/09/15 06:18:30,Vehicle,SPT\n\nHeader and first few lines of CSV file 2:\nact_dep_time,destination_station,sch_dep_time,start_time,origin_station,act_arr_time,station,origin_station_code\n21/09/15 04:59:19,Roll Test,21/09/15 05:01:00,21/09/15 05:01:00,Machine Mist,,Machine Mist,MAM\n21/09/15 05:50:38,Attempt Pin,21/09/15 05:50:00,21/09/15 05:50:00,Roll Test,,Roll Test,ROT\n21/09/15 07:21:38,Roll Test,21/09/15 07:20:00,21/09/15 07:20:00,Attempt Pin,,Attempt Pin,ATP\n21/09/15 05:00:01,Jail Vest,21/09/15 05:06:30,21/09/15 05:06:30,Step Scarecrow Turnback,,Step Scarecrow Turnback,STSTb\n21/09/15 06:36:58,Jail Vest,21/09/15 06:38:00,21/09/15 06:38:00,Children Cast Turnback,,Children Cast Turnback,CHATb\n21/09/15 06:40:32,Crib Team,21/09/15 06:32:00,21/09/15 06:32:00,Bridge Bottle,,Bridge Bottle,BRB\n21/09/15 07:38:27,Bridge Bottle,21/09/15 07:32:00,21/09/15 07:32:00,Crib Team,,Crib Team,CRT\n,Step Scarecrow Turnback,21/09/15 23:58:00,21/09/15 23:58:00,Clouds Goose,,Clouds Goose,CLG\n22/09/15 00:21:21,Machine Mist,22/09/15 00:20:00,22/09/15 00:20:00,Skin Shape,,Skin Shape,SKH\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo a successful merging process.", "csv1_example": "sch_arr_time,origin_station_code,fleet_number,end_time,station_type,destination_station_code\n,MAM,3088.0,21/09/15 05:41:00,Passenger,ROT\n,ROT,3088.0,21/09/15 07:14:00,Passenger,ATP\n,STSTb,3063.0,21/09/15 05:44:00,Vehicle,JAV\n,JAV,3063.0,21/09/15 06:32:00,Passenger,CHATb\n,CHATb,3063.0,21/09/15 07:20:00,Vehicle,JAV\n,BRB,3006.0,21/09/15 07:27:00,Passenger,CRT\n,CRT,3017.0,21/09/15 08:27:00,Passenger,BRB\n,SKH,3099.0,22/09/15 00:46:00,Passenger,MAM\n,SKSTb,3052.0,21/09/15 06:18:30,Vehicle,SPT\n,SQFTb,3052.0,21/09/15 07:17:00,Vehicle,SUTTb\n", "csv2_example": "act_dep_time,destination_station,sch_dep_time,start_time,origin_station,act_arr_time,station,origin_station_code\n21/09/15 04:59:19,Roll Test,21/09/15 05:01:00,21/09/15 05:01:00,Machine Mist,,Machine Mist,MAM\n21/09/15 05:50:38,Attempt Pin,21/09/15 05:50:00,21/09/15 05:50:00,Roll Test,,Roll Test,ROT\n21/09/15 07:21:38,Roll Test,21/09/15 07:20:00,21/09/15 07:20:00,Attempt Pin,,Attempt Pin,ATP\n21/09/15 05:00:01,Jail Vest,21/09/15 05:06:30,21/09/15 05:06:30,Step Scarecrow Turnback,,Step Scarecrow Turnback,STSTb\n21/09/15 06:36:58,Jail Vest,21/09/15 06:38:00,21/09/15 06:38:00,Children Cast Turnback,,Children Cast Turnback,CHATb\n21/09/15 06:40:32,Crib Team,21/09/15 06:32:00,21/09/15 06:32:00,Bridge Bottle,,Bridge Bottle,BRB\n21/09/15 07:38:27,Bridge Bottle,21/09/15 07:32:00,21/09/15 07:32:00,Crib Team,,Crib Team,CRT\n,Step Scarecrow Turnback,21/09/15 23:58:00,21/09/15 23:58:00,Clouds Goose,,Clouds Goose,CLG\n22/09/15 00:21:21,Machine Mist,22/09/15 00:20:00,22/09/15 00:20:00,Skin Shape,,Skin Shape,SKH\n,Spiders Toothbrush,21/09/15 05:49:00,21/09/15 05:49:00,Skate Stone Turnback,,Skate Stone Turnback,SKSTb\n", "csv1_path": "infiagent/merge_test/2_3_0.csv", "csv2_path": "infiagent/merge_test/2_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo a successful merging process.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/2_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/2_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsqft_basement,view,waterfront,price,zipcode,Unnamed: 0,lat,floors\n0,0,0,221900.0,98178,0,47.5112,1.0\n0,0,0,180000.0,98028,2,47.7379,1.0\n910,0,0,604000.0,98136,3,47.5208,1.0\n0,0,0,510000.0,98074,4,47.6168,1.0\n0,0,0,291850.0,98198,7,47.4095,1.0\n0,0,0,323000.0,98038,9,47.3684,2.0\n1700,0,0,662500.0,98007,10,47.6007,1.0\n0,0,0,310000.0,98028,12,47.7558,1.5\n0,0,0,400000.0,98074,13,47.6127,1.0\n\nHeader and first few lines of CSV file 2:\ngrade,sqft_living15,sqft_lot,bedrooms,sqft_living,yr_renovated,condition,long,bathrooms,sqft_lot15,Unnamed: 0,sqft_above\n7,1340,5650,3,1180,0,3,-122.257,1.0,5650,0,1180\n7,1690,7242,3,2570,1991,3,-122.319,2.25,7639,1,2170\n6,2720,10000,2,770,0,3,-122.233,1.0,8062,2,770\n8,1800,8080,3,1680,0,3,-122.045,2.0,7503,4,1680\n11,4760,101930,4,5420,0,3,-122.005,4.5,101930,5,3890\n7,2238,6819,3,1715,0,3,-122.327,2.25,6819,6,1715\n7,1650,9711,3,1060,0,3,-122.315,1.5,9711,7,1060\n7,1780,7470,3,1780,0,3,-122.337,1.0,8113,8,1050\n7,1330,6000,2,1160,0,4,-122.292,1.0,6000,11,860\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "sqft_basement,view,waterfront,price,zipcode,Unnamed: 0,lat,floors\n0,0,0,221900.0,98178,0,47.5112,1.0\n0,0,0,180000.0,98028,2,47.7379,1.0\n910,0,0,604000.0,98136,3,47.5208,1.0\n0,0,0,510000.0,98074,4,47.6168,1.0\n0,0,0,291850.0,98198,7,47.4095,1.0\n0,0,0,323000.0,98038,9,47.3684,2.0\n1700,0,0,662500.0,98007,10,47.6007,1.0\n0,0,0,310000.0,98028,12,47.7558,1.5\n0,0,0,400000.0,98074,13,47.6127,1.0\n0,0,0,530000.0,98107,14,47.67,1.5\n", "csv2_example": "grade,sqft_living15,sqft_lot,bedrooms,sqft_living,yr_renovated,condition,long,bathrooms,sqft_lot15,Unnamed: 0,sqft_above\n7,1340,5650,3,1180,0,3,-122.257,1.0,5650,0,1180\n7,1690,7242,3,2570,1991,3,-122.319,2.25,7639,1,2170\n6,2720,10000,2,770,0,3,-122.233,1.0,8062,2,770\n8,1800,8080,3,1680,0,3,-122.045,2.0,7503,4,1680\n11,4760,101930,4,5420,0,3,-122.005,4.5,101930,5,3890\n7,2238,6819,3,1715,0,3,-122.327,2.25,6819,6,1715\n7,1650,9711,3,1060,0,3,-122.315,1.5,9711,7,1060\n7,1780,7470,3,1780,0,3,-122.337,1.0,8113,8,1050\n7,1330,6000,2,1160,0,4,-122.292,1.0,6000,11,860\n7,1780,19901,3,1430,0,4,-122.229,1.0,12697,12,1430\n", "csv1_path": "infiagent/merge_test/41_3_0.csv", "csv2_path": "infiagent/merge_test/41_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/41_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/41_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nwage,smllcity,goodhlth,union,bigcity\n4.28,1,1,0,0\n7.96,1,1,0,0\n11.57,0,1,0,1\n11.42,1,1,0,0\n3.91,0,0,0,1\n8.76,0,1,0,1\n7.69,1,1,1,0\n5.0,0,1,0,0\n3.89,1,1,0,0\n\nHeader and first few lines of CSV file 2:\nmarried,Slooks,educ,expersq,wage,black\n1,4,14,900,5.73,0\n1,0,12,784,4.28,0\n0,0,10,1225,7.96,0\n1,3,16,1444,11.57,0\n1,0,16,729,11.42,0\n1,0,12,400,3.91,0\n1,0,16,144,8.76,0\n0,0,16,25,7.69,0\n0,0,16,25,5.0,0\n\nQuestion: Combine the two tables and insert NAN values in the empty spaces.", "csv1_example": "wage,smllcity,goodhlth,union,bigcity\n4.28,1,1,0,0\n7.96,1,1,0,0\n11.57,0,1,0,1\n11.42,1,1,0,0\n3.91,0,0,0,1\n8.76,0,1,0,1\n7.69,1,1,1,0\n5.0,0,1,0,0\n3.89,1,1,0,0\n3.45,0,1,0,1\n", "csv2_example": "married,Slooks,educ,expersq,wage,black\n1,4,14,900,5.73,0\n1,0,12,784,4.28,0\n0,0,10,1225,7.96,0\n1,3,16,1444,11.57,0\n1,0,16,729,11.42,0\n1,0,12,400,3.91,0\n1,0,16,144,8.76,0\n0,0,16,25,7.69,0\n0,0,16,25,5.0,0\n0,0,12,144,3.89,0\n", "csv1_path": "infiagent/merge_test/15_3_0.csv", "csv2_path": "infiagent/merge_test/15_3_1.csv", "instruction": "Combine the two tables and insert NAN values in the empty spaces.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/15_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/15_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nHigh,Market Cap,Date,Volume\n116.01,744,652,000,Sep 16, 2017,5,683,580\n113.75,652,107,000,Sep 15, 2017,8,539,660\n117.38,775,543,000,Sep 14, 2017,6,367,800\n123.7,822,282,000,Sep 13, 2017,6,315,510\n130.91,835,911,000,Sep 12, 2017,6,861,080\n128.2,812,292,000,Sep 11, 2017,7,083,720\n131.51,854,520,000,Sep 09, 2017,6,225,680\n139.78,914,556,000,Sep 08, 2017,7,051,050\n142.16,863,537,000,Sep 06, 2017,8,049,390\n\nHeader and first few lines of CSV file 2:\nOpen,Close,Date\n109.75,106.84,Sep 17, 2017\n111.11,109.85,Sep 16, 2017\n97.42,111.22,Sep 15, 2017\n115.97,96.71,Sep 14, 2017\n123.14,115.97,Sep 13, 2017\n125.29,123.5,Sep 12, 2017\n129.7,122.92,Sep 10, 2017\n128.51,130.05,Sep 09, 2017\n137.68,128.29,Sep 08, 2017\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "High,Market Cap,Date,Volume\n116.01,744,652,000,Sep 16, 2017,5,683,580\n113.75,652,107,000,Sep 15, 2017,8,539,660\n117.38,775,543,000,Sep 14, 2017,6,367,800\n123.7,822,282,000,Sep 13, 2017,6,315,510\n130.91,835,911,000,Sep 12, 2017,6,861,080\n128.2,812,292,000,Sep 11, 2017,7,083,720\n131.51,854,520,000,Sep 09, 2017,6,225,680\n139.78,914,556,000,Sep 08, 2017,7,051,050\n142.16,863,537,000,Sep 06, 2017,8,049,390\n131.57,756,793,000,Sep 05, 2017,8,537,600\n", "csv2_example": "Open,Close,Date\n109.75,106.84,Sep 17, 2017\n111.11,109.85,Sep 16, 2017\n97.42,111.22,Sep 15, 2017\n115.97,96.71,Sep 14, 2017\n123.14,115.97,Sep 13, 2017\n125.29,123.5,Sep 12, 2017\n129.7,122.92,Sep 10, 2017\n128.51,130.05,Sep 09, 2017\n137.68,128.29,Sep 08, 2017\n135.88,137.61,Sep 07, 2017\n", "csv1_path": "infiagent/merge_test/17_3_0.csv", "csv2_path": "infiagent/merge_test/17_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/17_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/17_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nclass1,class5,class6,ID\n0.0,0.0,0.0,1\n1.0,0.0,0.0,2\n0.0,0.0,0.0,5\n0.0,0.0,0.0,10\n0.0,0.0,0.0,12\n1.0,0.0,0.0,13\n0.0,0.0,0.0,14\n0.0,0.0,0.0,21\n0.0,0.0,0.0,24\n\nHeader and first few lines of CSV file 2:\nclass8,class2,ID,class3,class4,class9\n0.0,0.0,1,0.0,1.0,0.0\n0.0,0.0,2,0.0,0.0,0.0\n0.0,0.0,5,0.0,0.0,0.0\n0.0,0.0,10,0.0,0.0,0.0\n0.0,0.0,13,0.0,0.0,0.0\n0.0,0.0,14,0.0,0.0,0.0\n0.0,0.0,21,0.0,0.0,0.0\n0.0,0.0,24,0.0,0.0,0.0\n0.0,0.0,26,0.0,0.0,0.0\n\nQuestion: Combine the two tables and retain only the rows that have been successfully integrated.", "csv1_example": "class1,class5,class6,ID\n0.0,0.0,0.0,1\n1.0,0.0,0.0,2\n0.0,0.0,0.0,5\n0.0,0.0,0.0,10\n0.0,0.0,0.0,12\n1.0,0.0,0.0,13\n0.0,0.0,0.0,14\n0.0,0.0,0.0,21\n0.0,0.0,0.0,24\n0.0,0.0,0.0,26\n", "csv2_example": "class8,class2,ID,class3,class4,class9\n0.0,0.0,1,0.0,1.0,0.0\n0.0,0.0,2,0.0,0.0,0.0\n0.0,0.0,5,0.0,0.0,0.0\n0.0,0.0,10,0.0,0.0,0.0\n0.0,0.0,13,0.0,0.0,0.0\n0.0,0.0,14,0.0,0.0,0.0\n0.0,0.0,21,0.0,0.0,0.0\n0.0,0.0,24,0.0,0.0,0.0\n0.0,0.0,26,0.0,0.0,0.0\n0.0,0.0,33,0.0,0.0,0.0\n", "csv1_path": "infiagent/merge_test/48_0_0.csv", "csv2_path": "infiagent/merge_test/48_0_1.csv", "instruction": "Combine the two tables and retain only the rows that have been successfully integrated.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/48_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/48_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntable,carat,depth\n55.0,0.23,61.5\n58.0,0.29,62.4\n58.0,0.31,63.3\n55.0,0.26,61.9\n61.0,0.22,65.1\n55.0,0.3,64.0\n62.0,0.2,60.2\n55.0,0.33,61.8\n60.0,0.25,63.3\n\nHeader and first few lines of CSV file 2:\ncarat,y,z,color\n0.23,3.98,2.43,E\n0.21,3.84,2.31,E\n0.29,4.23,2.63,I\n0.31,4.35,2.75,J\n0.26,4.11,2.53,H\n0.22,3.78,2.49,E\n0.3,4.28,2.73,J\n0.2,3.75,2.27,E\n0.32,4.42,2.68,E\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "table,carat,depth\n55.0,0.23,61.5\n58.0,0.29,62.4\n58.0,0.31,63.3\n55.0,0.26,61.9\n61.0,0.22,65.1\n55.0,0.3,64.0\n62.0,0.2,60.2\n55.0,0.33,61.8\n60.0,0.25,63.3\n59.0,0.42,61.5\n", "csv2_example": "carat,y,z,color\n0.23,3.98,2.43,E\n0.21,3.84,2.31,E\n0.29,4.23,2.63,I\n0.31,4.35,2.75,J\n0.26,4.11,2.53,H\n0.22,3.78,2.49,E\n0.3,4.28,2.73,J\n0.2,3.75,2.27,E\n0.32,4.42,2.68,E\n0.33,4.51,2.78,I\n", "csv1_path": "infiagent/merge_test/28_0_0.csv", "csv2_path": "infiagent/merge_test/28_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/28_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/28_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntimestamp,num. busy overflows,avg. wait time,num. calls answered\nApr 13  2017 12:00:00 AM,0,00:00:00,0\nApr 13  2017 12:15:00 AM,0,00:00:00,0\nApr 13  2017 12:30:00 AM,0,00:00:00,0\nApr 13  2017 12:45:00 AM,0,00:00:00,0\nApr 13  2017 1:00:00 AM,0,00:00:00,0\nApr 13  2017 1:15:00 AM,0,00:00:00,0\nApr 13  2017 1:30:00 AM,0,00:00:00,0\nApr 13  2017 1:45:00 AM,0,00:00:00,0\nApr 13  2017 2:00:00 AM,0,00:00:00,0\n\nHeader and first few lines of CSV file 2:\nnum. calls abandoned,avg. num. agents staffed,avg. abandonment time,timestamp\n0,4,00:00:00,Apr 13  2017 12:00:00 AM\n0,4,00:00:00,Apr 13  2017 12:15:00 AM\n0,4,00:00:00,Apr 13  2017 12:30:00 AM\n0,4,00:00:00,Apr 13  2017 1:00:00 AM\n0,4,00:00:00,Apr 13  2017 1:30:00 AM\n0,4,00:00:00,Apr 13  2017 1:45:00 AM\n0,4,00:00:00,Apr 13  2017 2:30:00 AM\n0,4,00:00:00,Apr 13  2017 2:45:00 AM\n0,4,00:00:00,Apr 13  2017 3:00:00 AM\n\nQuestion: Combine two tables, retaining only the rows that have been successfully combined.", "csv1_example": "timestamp,num. busy overflows,avg. wait time,num. calls answered\nApr 13  2017 12:00:00 AM,0,00:00:00,0\nApr 13  2017 12:15:00 AM,0,00:00:00,0\nApr 13  2017 12:30:00 AM,0,00:00:00,0\nApr 13  2017 12:45:00 AM,0,00:00:00,0\nApr 13  2017 1:00:00 AM,0,00:00:00,0\nApr 13  2017 1:15:00 AM,0,00:00:00,0\nApr 13  2017 1:30:00 AM,0,00:00:00,0\nApr 13  2017 1:45:00 AM,0,00:00:00,0\nApr 13  2017 2:00:00 AM,0,00:00:00,0\nApr 13  2017 2:15:00 AM,0,00:00:00,0\n", "csv2_example": "num. calls abandoned,avg. num. agents staffed,avg. abandonment time,timestamp\n0,4,00:00:00,Apr 13  2017 12:00:00 AM\n0,4,00:00:00,Apr 13  2017 12:15:00 AM\n0,4,00:00:00,Apr 13  2017 12:30:00 AM\n0,4,00:00:00,Apr 13  2017 1:00:00 AM\n0,4,00:00:00,Apr 13  2017 1:30:00 AM\n0,4,00:00:00,Apr 13  2017 1:45:00 AM\n0,4,00:00:00,Apr 13  2017 2:30:00 AM\n0,4,00:00:00,Apr 13  2017 2:45:00 AM\n0,4,00:00:00,Apr 13  2017 3:00:00 AM\n0,4,00:00:00,Apr 13  2017 3:30:00 AM\n", "csv1_path": "infiagent/merge_test/4_1_0.csv", "csv2_path": "infiagent/merge_test/4_1_1.csv", "instruction": "Combine two tables, retaining only the rows that have been successfully combined.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/4_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/4_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNUM ROUNDS,Name\n5.0,Gatlin & Ramarao\n,Lahiri & Ponnuswamy\n,Gupta & Chatradhi\n,Patwa & Aggarwal\n,Shaikh & Singh\n,Arun & Schacter\n,Cheng & Parulekar\n,Prabhakar & Hamilton\n,Menotti & Bhasin\n\nHeader and first few lines of CSV file 2:\nWEIGHTING,STANDARD TEAM NAME,Name\n0.8,Mitty GR,Gatlin & Ramarao\n,Mitty GuCh,Gupta & Chatradhi\n,Mitty PA,Patwa & Aggarwal\n,Mitty SS,Shaikh & Singh\n,Gunn AS,Arun & Schacter\n,Gunn PC,Cheng & Parulekar\n,James Logan MB,Menotti & Bhasin\n,Mitty KK,Kang & Korolik\n,Clovis North KS,Khanna & Sultan\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "NUM ROUNDS,Name\n5.0,Gatlin & Ramarao\n,Lahiri & Ponnuswamy\n,Gupta & Chatradhi\n,Patwa & Aggarwal\n,Shaikh & Singh\n,Arun & Schacter\n,Cheng & Parulekar\n,Prabhakar & Hamilton\n,Menotti & Bhasin\n,Kim & Joshi\n", "csv2_example": "WEIGHTING,STANDARD TEAM NAME,Name\n0.8,Mitty GR,Gatlin & Ramarao\n,Mitty GuCh,Gupta & Chatradhi\n,Mitty PA,Patwa & Aggarwal\n,Mitty SS,Shaikh & Singh\n,Gunn AS,Arun & Schacter\n,Gunn PC,Cheng & Parulekar\n,James Logan MB,Menotti & Bhasin\n,Mitty KK,Kang & Korolik\n,Clovis North KS,Khanna & Sultan\n,Dougherty Valley KB,Kalra & Bhat\n", "csv1_path": "infiagent/merge_test/26_0_0.csv", "csv2_path": "infiagent/merge_test/26_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/26_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/26_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\norigin,mpg,horsepower\n1,18.0,130.0\n1,15.0,165.0\n1,16.0,150.0\n1,14.0,220.0\n3,24.0,95.0\n1,22.0,95.0\n1,21.0,85.0\n3,27.0,88.0\n2,26.0,46.0\n\nHeader and first few lines of CSV file 2:\nweight,displacement,modelyear,cylinders,mpg\n3504.0,307.0,70,8,18.0\n3693.0,350.0,70,8,15.0\n3433.0,304.0,70,8,16.0\n3449.0,302.0,70,8,17.0\n4354.0,454.0,70,8,14.0\n2372.0,113.0,70,4,24.0\n2833.0,198.0,70,6,22.0\n2587.0,200.0,70,6,21.0\n2130.0,97.0,70,4,27.0\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "origin,mpg,horsepower\n1,18.0,130.0\n1,15.0,165.0\n1,16.0,150.0\n1,14.0,220.0\n3,24.0,95.0\n1,22.0,95.0\n1,21.0,85.0\n3,27.0,88.0\n2,26.0,46.0\n1,10.0,215.0\n", "csv2_example": "weight,displacement,modelyear,cylinders,mpg\n3504.0,307.0,70,8,18.0\n3693.0,350.0,70,8,15.0\n3433.0,304.0,70,8,16.0\n3449.0,302.0,70,8,17.0\n4354.0,454.0,70,8,14.0\n2372.0,113.0,70,4,24.0\n2833.0,198.0,70,6,22.0\n2587.0,200.0,70,6,21.0\n2130.0,97.0,70,4,27.0\n2672.0,110.0,70,4,25.0\n", "csv1_path": "infiagent/merge_test/11_0_0.csv", "csv2_path": "infiagent/merge_test/11_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/11_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/11_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nrevolving_utilization_of_unsecured_lines,monthly_income,debt_ratio,number_of_time30-59_days_past_due_not_worse,serious_dlqin2yrs\n0.957151019,2600.0,0.121876201,0,0\n0.65818014,3042.0,0.085113375,1,0\n0.233809776,3300.0,0.036049682,0,0\n0.9072394,63588.0,0.024925695,1,0\n0.213178682,3500.0,0.375606969,0,0\n0.7544636479999999,3500.0,0.209940017,0,0\n0.116950644,,46.0,0,0\n0.189169052,23684.0,0.606290901,0,0\n0.01879812,6501.0,0.53152876,0,0\n\nHeader and first few lines of CSV file 2:\nage,number_of_times90_days_late,number_of_open_credit_lines_and_loans,revolving_utilization_of_unsecured_lines,number_real_estate_loans_or_lines\n45,0,13,0.7661266089999998,6\n40,0,4,0.957151019,0\n49,0,7,0.9072394,1\n57,0,8,0.305682465,3\n39,0,8,0.7544636479999999,0\n57,0,9,0.189169052,4\n30,0,5,0.644225962,0\n46,0,13,0.010351857,2\n76,0,6,0.0196565809999999,1\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "revolving_utilization_of_unsecured_lines,monthly_income,debt_ratio,number_of_time30-59_days_past_due_not_worse,serious_dlqin2yrs\n0.957151019,2600.0,0.121876201,0,0\n0.65818014,3042.0,0.085113375,1,0\n0.233809776,3300.0,0.036049682,0,0\n0.9072394,63588.0,0.024925695,1,0\n0.213178682,3500.0,0.375606969,0,0\n0.7544636479999999,3500.0,0.209940017,0,0\n0.116950644,,46.0,0,0\n0.189169052,23684.0,0.606290901,0,0\n0.01879812,6501.0,0.53152876,0,0\n0.010351857,12454.0,0.298354075,0,0\n", "csv2_example": "age,number_of_times90_days_late,number_of_open_credit_lines_and_loans,revolving_utilization_of_unsecured_lines,number_real_estate_loans_or_lines\n45,0,13,0.7661266089999998,6\n40,0,4,0.957151019,0\n49,0,7,0.9072394,1\n57,0,8,0.305682465,3\n39,0,8,0.7544636479999999,0\n57,0,9,0.189169052,4\n30,0,5,0.644225962,0\n46,0,13,0.010351857,2\n76,0,6,0.0196565809999999,1\n64,0,7,0.548458062,1\n", "csv1_path": "infiagent/merge_test/23_2_0.csv", "csv2_path": "infiagent/merge_test/23_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/23_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/23_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nJDTDB\n2451214.540613426\n2451215.540613426\n2451218.540613426\n2451219.540613426\n2451220.540613426\n2451222.540613426\n2451224.540613426\n2451225.540613426\n2451226.540613426\n\nHeader and first few lines of CSV file 2:\nJDTDB,X,Unnamed: 5,Calendar Date (TDB)\n2451214.540613426,-288747920.4014895,, A.D. 1999-Feb-05 00:58:29.0000\n2451216.540613426,-290133649.6015852,, A.D. 1999-Feb-07 00:58:29.0000\n2451217.540613426,-290809152.5744454,, A.D. 1999-Feb-08 00:58:29.0000\n2451218.540613426,-291473141.8477991,, A.D. 1999-Feb-09 00:58:29.0000\n2451219.540613426,-292125663.2967085,, A.D. 1999-Feb-10 00:58:29.0000\n2451220.540613426,-292766763.0494693,, A.D. 1999-Feb-11 00:58:29.0000\n2451221.540613426,-293396487.4748839,, A.D. 1999-Feb-12 00:58:29.0000\n2451222.540613426,-294014883.1697997,, A.D. 1999-Feb-13 00:58:29.0000\n2451223.540613426,-294621996.946905,, A.D. 1999-Feb-14 00:58:29.0000\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "JDTDB\n2451214.540613426\n2451215.540613426\n2451218.540613426\n2451219.540613426\n2451220.540613426\n2451222.540613426\n2451224.540613426\n2451225.540613426\n2451226.540613426\n2451227.540613426\n", "csv2_example": "JDTDB,X,Unnamed: 5,Calendar Date (TDB)\n2451214.540613426,-288747920.4014895,, A.D. 1999-Feb-05 00:58:29.0000\n2451216.540613426,-290133649.6015852,, A.D. 1999-Feb-07 00:58:29.0000\n2451217.540613426,-290809152.5744454,, A.D. 1999-Feb-08 00:58:29.0000\n2451218.540613426,-291473141.8477991,, A.D. 1999-Feb-09 00:58:29.0000\n2451219.540613426,-292125663.2967085,, A.D. 1999-Feb-10 00:58:29.0000\n2451220.540613426,-292766763.0494693,, A.D. 1999-Feb-11 00:58:29.0000\n2451221.540613426,-293396487.4748839,, A.D. 1999-Feb-12 00:58:29.0000\n2451222.540613426,-294014883.1697997,, A.D. 1999-Feb-13 00:58:29.0000\n2451223.540613426,-294621996.946905,, A.D. 1999-Feb-14 00:58:29.0000\n2451224.540613426,-295217875.8227791,, A.D. 1999-Feb-15 00:58:29.0000\n", "csv1_path": "infiagent/merge_test/27_3_0.csv", "csv2_path": "infiagent/merge_test/27_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/27_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/27_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ngdpPercap_2002,gdpPercap_1952,country\n726.7340548,779.4453145,Afghanistan\n896.2260153,368.4692856,Cambodia\n30209.01516,3054.421209,Hong Kong China\n1746.769454,546.5657493,India\n2873.91287,749.6816546,Indonesia\n9240.761975,3035.326002,Iran\n4390.717312,4129.766056,Iraq\n21905.59514,4086.522128,Israel\n3844.917194,1546.907807,Jordan\n\nHeader and first few lines of CSV file 2:\ngdpPercap_1992,country,gdpPercap_1967,gdpPercap_1957,gdpPercap_1962,gdpPercap_1982,gdpPercap_1977\n649.3413952,Afghanistan,836.1971382,820.8530296,853.10071,978.0114388,786.11336\n19035.57917,Bahrain,14804.6727,11635.79945,12753.27514,19211.14731,19340.10196\n837.8101643,Bangladesh,721.1860862,661.6374577,686.3415538,676.9818656,659.8772322\n682.3031755,Cambodia,523.4323142,434.0383364,496.9136476,624.4754784,524.9721832\n1655.784158,China,612.7056934,575.9870009,487.6740183,962.4213805,741.2374699\n24757.60301,Hong Kong China,6197.962814,3629.076457,4692.648272,14560.53051,11186.14125\n1164.406809,India,700.7706107,590.061996,658.3471509,855.7235377,813.337323\n2383.140898,Indonesia,762.4317721,858.9002707,849.2897701,1516.872988,1382.702056\n7235.653188,Iran,5906.731805,3290.257643,4187.329802,7608.334602,11888.59508\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "gdpPercap_2002,gdpPercap_1952,country\n726.7340548,779.4453145,Afghanistan\n896.2260153,368.4692856,Cambodia\n30209.01516,3054.421209,Hong Kong China\n1746.769454,546.5657493,India\n2873.91287,749.6816546,Indonesia\n9240.761975,3035.326002,Iran\n4390.717312,4129.766056,Iraq\n21905.59514,4086.522128,Israel\n3844.917194,1546.907807,Jordan\n1646.758151,1088.277758,Korea Dem. Rep.\n", "csv2_example": "gdpPercap_1992,country,gdpPercap_1967,gdpPercap_1957,gdpPercap_1962,gdpPercap_1982,gdpPercap_1977\n649.3413952,Afghanistan,836.1971382,820.8530296,853.10071,978.0114388,786.11336\n19035.57917,Bahrain,14804.6727,11635.79945,12753.27514,19211.14731,19340.10196\n837.8101643,Bangladesh,721.1860862,661.6374577,686.3415538,676.9818656,659.8772322\n682.3031755,Cambodia,523.4323142,434.0383364,496.9136476,624.4754784,524.9721832\n1655.784158,China,612.7056934,575.9870009,487.6740183,962.4213805,741.2374699\n24757.60301,Hong Kong China,6197.962814,3629.076457,4692.648272,14560.53051,11186.14125\n1164.406809,India,700.7706107,590.061996,658.3471509,855.7235377,813.337323\n2383.140898,Indonesia,762.4317721,858.9002707,849.2897701,1516.872988,1382.702056\n7235.653188,Iran,5906.731805,3290.257643,4187.329802,7608.334602,11888.59508\n3745.640687,Iraq,8931.459811,6229.333562,8341.737815,14517.90711,14688.23507\n", "csv1_path": "infiagent/merge_test/36_2_0.csv", "csv2_path": "infiagent/merge_test/36_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/36_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/36_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhotel_type,hotel_id\nHotel,75737\nHotel,93401\nHotel,224217\nHotel,488793\nHotel,1028569\nHotel,1383001\nHotel,1733337\nHotel,1776857\nHotel,2643161\n\nHeader and first few lines of CSV file 2:\nbubble_score,brand_name,city_name,parent_brand_name,hotel_name,hotel_id\n40.0,Night Hotel,New York City,Wyndham Hotel Group,Night Theater District,75737\n35.0,None,New York City,,Heritage Hotel New York City,93401\n35.0,Clarion,New York City,Choice Hotels International, Inc.,Clarion Hotel Park Avenue,224217\n40.0,Ascend Collection,New York City,Choice Hotels International, Inc.,Solita Soho Hotel,488793\n45.0,The Leading Hotels of the World,New York City,The Leading Hotels of the World, Ltd,Greenwich Hotel,1028569\n40.0,Comfort Inn,New York City,Choice Hotels International, Inc.,Comfort Inn Manhattan Bridge,1383001\n45.0,,New York City,,Sutton Court Hotel Residences,1733337\n45.0,Langham,New York City,Langham Hospitality Group,The Langham New York Fifth Avenue,1776857\n45.0,,New York City,,The NoMad Hotel New York,2643161\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "hotel_type,hotel_id\nHotel,75737\nHotel,93401\nHotel,224217\nHotel,488793\nHotel,1028569\nHotel,1383001\nHotel,1733337\nHotel,1776857\nHotel,2643161\nHotel,7148761\n", "csv2_example": "bubble_score,brand_name,city_name,parent_brand_name,hotel_name,hotel_id\n40.0,Night Hotel,New York City,Wyndham Hotel Group,Night Theater District,75737\n35.0,None,New York City,,Heritage Hotel New York City,93401\n35.0,Clarion,New York City,Choice Hotels International, Inc.,Clarion Hotel Park Avenue,224217\n40.0,Ascend Collection,New York City,Choice Hotels International, Inc.,Solita Soho Hotel,488793\n45.0,The Leading Hotels of the World,New York City,The Leading Hotels of the World, Ltd,Greenwich Hotel,1028569\n40.0,Comfort Inn,New York City,Choice Hotels International, Inc.,Comfort Inn Manhattan Bridge,1383001\n45.0,,New York City,,Sutton Court Hotel Residences,1733337\n45.0,Langham,New York City,Langham Hospitality Group,The Langham New York Fifth Avenue,1776857\n45.0,,New York City,,The NoMad Hotel New York,2643161\n45.0,None,New York City,,Cassa Hotel Times Square,7148761\n", "csv1_path": "infiagent/merge_test/40_0_0.csv", "csv2_path": "infiagent/merge_test/40_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/40_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/40_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDEPTID,ANNUAL_RT,DESCR,NAME\nA24002,37407.0,COMP-Audits (002),AKROFI,BERNARD\nW02278,11310.0,Youth Summer  (278),Aaron,Keairah T\nW02200,11310.0,Youth Summer  (200),Aaron,Keontae E\nA03031,51862.0,OED-Employment Dev (031),Aaron,Patricia G\nA29005,64000.0,States Attorneys Office (005),Aaron,Petra L\nA65026,57900.0,HLTH-Health Department (026),Abaineh,Yohannes T\nA40001,52000.0,M-R Info Technology (001),Abbey,Emmanuel\nA64120,40650.0,Fire Department (120),Abdal-Rahim,Naim A\nA99123,68847.0,Police Department (123),Abdi,Ezekiel W\n\nHeader and first few lines of CSV file 2:\nNAME,HIRE_DT,JOBTITLE\nAKROFI,BERNARD,02/04/2013,AUDITOR TRAINEE\nAaron,Keairah T,06/19/2013,AIDE BLUE CHIP\nAaron,Keontae E,06/10/2013,AIDE BLUE CHIP\nAaron,Patricia G,10/24/1979,Facilities/Office Services II\nAaron,Petra L,09/25/2006,ASSISTANT STATE'S ATTORNEY\nAbaineh,Yohannes T,07/23/2009,EPIDEMIOLOGIST\nAbbey,Emmanuel,05/01/2013,CONTRACT SERV SPEC II\nAbdal-Rahim,Naim A,03/30/2011,EMT FIREFIGHTER\nAbdi,Ezekiel W,06/14/2007,POLICE SERGEANT\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "DEPTID,ANNUAL_RT,DESCR,NAME\nA24002,37407.0,COMP-Audits (002),AKROFI,BERNARD\nW02278,11310.0,Youth Summer  (278),Aaron,Keairah T\nW02200,11310.0,Youth Summer  (200),Aaron,Keontae E\nA03031,51862.0,OED-Employment Dev (031),Aaron,Patricia G\nA29005,64000.0,States Attorneys Office (005),Aaron,Petra L\nA65026,57900.0,HLTH-Health Department (026),Abaineh,Yohannes T\nA40001,52000.0,M-R Info Technology (001),Abbey,Emmanuel\nA64120,40650.0,Fire Department (120),Abdal-Rahim,Naim A\nA99123,68847.0,Police Department (123),Abdi,Ezekiel W\nA38410,41194.0,Sheriff's Office (410),Abdul Adl,Attrice A\n", "csv2_example": "NAME,HIRE_DT,JOBTITLE\nAKROFI,BERNARD,02/04/2013,AUDITOR TRAINEE\nAaron,Keairah T,06/19/2013,AIDE BLUE CHIP\nAaron,Keontae E,06/10/2013,AIDE BLUE CHIP\nAaron,Patricia G,10/24/1979,Facilities/Office Services II\nAaron,Petra L,09/25/2006,ASSISTANT STATE'S ATTORNEY\nAbaineh,Yohannes T,07/23/2009,EPIDEMIOLOGIST\nAbbey,Emmanuel,05/01/2013,CONTRACT SERV SPEC II\nAbdal-Rahim,Naim A,03/30/2011,EMT FIREFIGHTER\nAbdi,Ezekiel W,06/14/2007,POLICE SERGEANT\nAbdul Adl,Attrice A,09/02/1999,RADIO DISPATCHER SHERIFF\n", "csv1_path": "infiagent/merge_test/12_3_0.csv", "csv2_path": "infiagent/merge_test/12_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/12_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/12_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntemp,registered,holiday,instant,dteday,weekday\n0.24,13,0,1,2011-01-01,6\n0.22,32,0,2,2011-01-01,6\n0.24,10,0,4,2011-01-01,6\n0.24,1,0,5,2011-01-01,6\n0.24,1,0,6,2011-01-01,6\n0.2,2,0,8,2011-01-01,6\n0.38,24,0,11,2011-01-01,6\n0.36,30,0,12,2011-01-01,6\n0.42,55,0,13,2011-01-01,6\n\nHeader and first few lines of CSV file 2:\nyr,hum,instant,workingday,season,windspeed,mnth,hr,atemp\n0,0.81,1,0,1,0.0,1,0,0.2879\n0,0.8,2,0,1,0.0,1,1,0.2727\n0,0.75,5,0,1,0.0,1,4,0.2879\n0,0.75,6,0,1,0.0896,1,5,0.2576\n0,0.8,7,0,1,0.0,1,6,0.2727\n0,0.86,8,0,1,0.0,1,7,0.2576\n0,0.76,10,0,1,0.0,1,9,0.3485\n0,0.76,11,0,1,0.2537,1,10,0.3939\n0,0.77,13,0,1,0.2836,1,12,0.4242\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "temp,registered,holiday,instant,dteday,weekday\n0.24,13,0,1,2011-01-01,6\n0.22,32,0,2,2011-01-01,6\n0.24,10,0,4,2011-01-01,6\n0.24,1,0,5,2011-01-01,6\n0.24,1,0,6,2011-01-01,6\n0.2,2,0,8,2011-01-01,6\n0.38,24,0,11,2011-01-01,6\n0.36,30,0,12,2011-01-01,6\n0.42,55,0,13,2011-01-01,6\n0.46,47,0,14,2011-01-01,6\n", "csv2_example": "yr,hum,instant,workingday,season,windspeed,mnth,hr,atemp\n0,0.81,1,0,1,0.0,1,0,0.2879\n0,0.8,2,0,1,0.0,1,1,0.2727\n0,0.75,5,0,1,0.0,1,4,0.2879\n0,0.75,6,0,1,0.0896,1,5,0.2576\n0,0.8,7,0,1,0.0,1,6,0.2727\n0,0.86,8,0,1,0.0,1,7,0.2576\n0,0.76,10,0,1,0.0,1,9,0.3485\n0,0.76,11,0,1,0.2537,1,10,0.3939\n0,0.77,13,0,1,0.2836,1,12,0.4242\n0,0.72,14,0,1,0.2985,1,13,0.4545\n", "csv1_path": "infiagent/merge_test/16_2_0.csv", "csv2_path": "infiagent/merge_test/16_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/16_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/16_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nHigh Price,Average Price,Close Price,No. of Trades,Symbol,Prev Close,Date,Total Traded Quantity\n584.0,578.09,578.55,21649,GODREJIND,564.6,15-May-2017,797171\n594.0,588.74,588.6,8567,GODREJIND,584.8,17-May-2017,504155\n588.85,580.9,574.6,7144,GODREJIND,588.6,18-May-2017,223583\n585.8,577.31,578.0,4969,GODREJIND,574.6,19-May-2017,245436\n586.75,576.1,565.95,9762,GODREJIND,578.0,22-May-2017,460156\n581.55,571.72,561.3,20749,GODREJIND,565.95,23-May-2017,1085338\n585.0,578.12,583.9,12426,GODREJIND,569.95,25-May-2017,597536\n612.9,599.27,610.2,11183,GODREJIND,585.25,30-May-2017,541003\n618.75,611.53,604.55,13900,GODREJIND,610.2,31-May-2017,693740\n\nHeader and first few lines of CSV file 2:\nDate,Open Price,% Dly Qt to Traded Qty,Low Price,Last Price,Turnover\n15-May-2017,581.0,45.28,568.5,578.9,460836225.3\n16-May-2017,581.45,42.05,572.25,583.8,291930164.6\n17-May-2017,583.0,51.9,576.85,584.9,296814880.85\n18-May-2017,582.0,44.63,571.2,572.25,129878624.25\n19-May-2017,581.0,27.72,567.55,579.85,141692454.15\n24-May-2017,560.75,34.63,551.8,569.0,411258378.25\n25-May-2017,570.4,50.37,569.75,584.5,345447321.45\n26-May-2017,584.0,34.64,577.35,594.8,249816977.85\n30-May-2017,587.4,26.24,580.7,611.9,324208618.85\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "High Price,Average Price,Close Price,No. of Trades,Symbol,Prev Close,Date,Total Traded Quantity\n584.0,578.09,578.55,21649,GODREJIND,564.6,15-May-2017,797171\n594.0,588.74,588.6,8567,GODREJIND,584.8,17-May-2017,504155\n588.85,580.9,574.6,7144,GODREJIND,588.6,18-May-2017,223583\n585.8,577.31,578.0,4969,GODREJIND,574.6,19-May-2017,245436\n586.75,576.1,565.95,9762,GODREJIND,578.0,22-May-2017,460156\n581.55,571.72,561.3,20749,GODREJIND,565.95,23-May-2017,1085338\n585.0,578.12,583.9,12426,GODREJIND,569.95,25-May-2017,597536\n612.9,599.27,610.2,11183,GODREJIND,585.25,30-May-2017,541003\n618.75,611.53,604.55,13900,GODREJIND,610.2,31-May-2017,693740\n621.0,615.0,611.9,7143,GODREJIND,615.3,02-Jun-2017,253132\n", "csv2_example": "Date,Open Price,% Dly Qt to Traded Qty,Low Price,Last Price,Turnover\n15-May-2017,581.0,45.28,568.5,578.9,460836225.3\n16-May-2017,581.45,42.05,572.25,583.8,291930164.6\n17-May-2017,583.0,51.9,576.85,584.9,296814880.85\n18-May-2017,582.0,44.63,571.2,572.25,129878624.25\n19-May-2017,581.0,27.72,567.55,579.85,141692454.15\n24-May-2017,560.75,34.63,551.8,569.0,411258378.25\n25-May-2017,570.4,50.37,569.75,584.5,345447321.45\n26-May-2017,584.0,34.64,577.35,594.8,249816977.85\n30-May-2017,587.4,26.24,580.7,611.9,324208618.85\n31-May-2017,612.0,30.82,601.55,603.0,424245557.6\n", "csv1_path": "infiagent/merge_test/38_3_0.csv", "csv2_path": "infiagent/merge_test/38_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/38_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/38_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nMedianHouseValue,MedInc\n1.663,2.7006\n1.58,5.0286\n2.438,3.9038\n3.629,7.1754\n2.019,4.1292\n0.526,2.6062\n5.00001,5.222\n1.12,2.2917\n2.25,3.2292\n\nHeader and first few lines of CSV file 2:\nMedInc,AveOccup,AveBedrms,HouseAge,Longitude,Latitude,Population\n0.9298,3.9940029985007497,1.1004497751124438,36.0,-118.25,33.93,2664.0\n2.7006,2.038555691554468,1.039779681762546,17.0,-117.03,32.79,3331.0\n3.9038,2.816011574632264,0.9825834542815676,21.0,-122.02,37.36,1486.0\n7.1754,2.816011574632264,1.051282051282051,52.0,-122.28,37.9,779.0\n4.1292,3.75,1.0333333333333334,28.0,-118.27,33.82,1575.0\n2.6062,2.816011574632264,1.2779156327543424,33.0,-120.12,41.4,976.0\n5.222,2.35969868173258,1.0527306967984935,42.0,-118.39,34.05,1253.0\n2.2917,3.821596244131456,0.9859154929577464,37.0,-118.25,33.97,814.0\n3.2292,2.816011574632264,1.2594142259414225,52.0,-120.95,36.47,618.0\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "MedianHouseValue,MedInc\n1.663,2.7006\n1.58,5.0286\n2.438,3.9038\n3.629,7.1754\n2.019,4.1292\n0.526,2.6062\n5.00001,5.222\n1.12,2.2917\n2.25,3.2292\n4.399,7.472\n", "csv2_example": "MedInc,AveOccup,AveBedrms,HouseAge,Longitude,Latitude,Population\n0.9298,3.9940029985007497,1.1004497751124438,36.0,-118.25,33.93,2664.0\n2.7006,2.038555691554468,1.039779681762546,17.0,-117.03,32.79,3331.0\n3.9038,2.816011574632264,0.9825834542815676,21.0,-122.02,37.36,1486.0\n7.1754,2.816011574632264,1.051282051282051,52.0,-122.28,37.9,779.0\n4.1292,3.75,1.0333333333333334,28.0,-118.27,33.82,1575.0\n2.6062,2.816011574632264,1.2779156327543424,33.0,-120.12,41.4,976.0\n5.222,2.35969868173258,1.0527306967984935,42.0,-118.39,34.05,1253.0\n2.2917,3.821596244131456,0.9859154929577464,37.0,-118.25,33.97,814.0\n3.2292,2.816011574632264,1.2594142259414225,52.0,-120.95,36.47,618.0\n7.472,2.331230283911672,1.1813880126182963,10.0,-117.25,32.99,1478.0\n", "csv1_path": "infiagent/merge_test/45_3_0.csv", "csv2_path": "infiagent/merge_test/45_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/45_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/45_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nlat_org,lat_dest,Source\n16.960160000000002,17.001061,0\n17.001061,16.960160000000002,1\n17.065654000000002,17.001061,2\n17.116356,17.065654000000002,4\n17.048482999999994,17.001061,5\n17.032705,17.065654000000002,6\n17.026285,17.001061,8\n16.889872,17.033555,9\n17.057259,16.960160000000002,11\n\nHeader and first few lines of CSV file 2:\nlng_dest,Source,Weight,Target\n51.089356,0,10,1.0\n51.048332,1,4,0.0\n51.13283300000001,3,2,2.0\n51.13283300000001,4,1,2.0\n51.13283300000001,6,5,2.0\n51.13283300000001,7,30,2.0\n51.089356,8,1,1.0\n51.116737,9,1,15.0\n51.089356,10,13,1.0\n\nQuestion: Combine the contents of two tables, retaining only the rows that have been successfully integrated.", "csv1_example": "lat_org,lat_dest,Source\n16.960160000000002,17.001061,0\n17.001061,16.960160000000002,1\n17.065654000000002,17.001061,2\n17.116356,17.065654000000002,4\n17.048482999999994,17.001061,5\n17.032705,17.065654000000002,6\n17.026285,17.001061,8\n16.889872,17.033555,9\n17.057259,16.960160000000002,11\n16.971785,16.960160000000002,13\n", "csv2_example": "lng_dest,Source,Weight,Target\n51.089356,0,10,1.0\n51.048332,1,4,0.0\n51.13283300000001,3,2,2.0\n51.13283300000001,4,1,2.0\n51.13283300000001,6,5,2.0\n51.13283300000001,7,30,2.0\n51.089356,8,1,1.0\n51.116737,9,1,15.0\n51.089356,10,13,1.0\n51.048332,13,111,0.0\n", "csv1_path": "infiagent/merge_test/5_2_0.csv", "csv2_path": "infiagent/merge_test/5_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that have been successfully integrated.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/5_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/5_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ndteday,hum,cnt,yr,workingday,instant,holiday,mnth,windspeed,atemp\n2011-01-01,0.81,16,0,0,1,0,1,0.0,0.2879\n2011-01-01,0.8,40,0,0,2,0,1,0.0,0.2727\n2011-01-01,0.8,32,0,0,3,0,1,0.0,0.2727\n2011-01-01,0.75,13,0,0,4,0,1,0.0,0.2879\n2011-01-01,0.75,1,0,0,5,0,1,0.0,0.2879\n2011-01-01,0.75,1,0,0,6,0,1,0.0896,0.2576\n2011-01-01,0.8,2,0,0,7,0,1,0.0,0.2727\n2011-01-01,0.86,3,0,0,8,0,1,0.0,0.2576\n2011-01-01,0.75,8,0,0,9,0,1,0.0,0.2879\n\nHeader and first few lines of CSV file 2:\ninstant,season,weekday,temp,weathersit,registered\n1,1,6,0.24,1,13\n2,1,6,0.22,1,32\n3,1,6,0.22,1,27\n4,1,6,0.24,1,10\n5,1,6,0.24,1,1\n6,1,6,0.24,2,1\n7,1,6,0.22,1,0\n8,1,6,0.2,1,2\n9,1,6,0.24,1,7\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "dteday,hum,cnt,yr,workingday,instant,holiday,mnth,windspeed,atemp\n2011-01-01,0.81,16,0,0,1,0,1,0.0,0.2879\n2011-01-01,0.8,40,0,0,2,0,1,0.0,0.2727\n2011-01-01,0.8,32,0,0,3,0,1,0.0,0.2727\n2011-01-01,0.75,13,0,0,4,0,1,0.0,0.2879\n2011-01-01,0.75,1,0,0,5,0,1,0.0,0.2879\n2011-01-01,0.75,1,0,0,6,0,1,0.0896,0.2576\n2011-01-01,0.8,2,0,0,7,0,1,0.0,0.2727\n2011-01-01,0.86,3,0,0,8,0,1,0.0,0.2576\n2011-01-01,0.75,8,0,0,9,0,1,0.0,0.2879\n2011-01-01,0.76,14,0,0,10,0,1,0.0,0.3485\n", "csv2_example": "instant,season,weekday,temp,weathersit,registered\n1,1,6,0.24,1,13\n2,1,6,0.22,1,32\n3,1,6,0.22,1,27\n4,1,6,0.24,1,10\n5,1,6,0.24,1,1\n6,1,6,0.24,2,1\n7,1,6,0.22,1,0\n8,1,6,0.2,1,2\n9,1,6,0.24,1,7\n10,1,6,0.32,1,6\n", "csv1_path": "infiagent/merge_test/16_0_0.csv", "csv2_path": "infiagent/merge_test/16_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/16_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/16_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nName,WEIGHTING\nGatlin & Ramarao,0.8\nLahiri & Ponnuswamy,\nGupta & Chatradhi,\nPatwa & Aggarwal,\nShaikh & Singh,\nArun & Schacter,\nCheng & Parulekar,\nMenotti & Bhasin,\nKim & Joshi,\n\nHeader and first few lines of CSV file 2:\nNUM ROUNDS,Name,STANDARD TEAM NAME\n5.0,Gatlin & Ramarao,Mitty GR\n,Lahiri & Ponnuswamy,Mitty PL\n,Gupta & Chatradhi,Mitty GuCh\n,Shaikh & Singh,Mitty SS\n,Arun & Schacter,Gunn AS\n,Cheng & Parulekar,Gunn PC\n,Prabhakar & Hamilton,Gunn PH\n,Menotti & Bhasin,James Logan MB\n,Kim & Joshi,Mitty JK\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Name,WEIGHTING\nGatlin & Ramarao,0.8\nLahiri & Ponnuswamy,\nGupta & Chatradhi,\nPatwa & Aggarwal,\nShaikh & Singh,\nArun & Schacter,\nCheng & Parulekar,\nMenotti & Bhasin,\nKim & Joshi,\nKang & Korolik,\n", "csv2_example": "NUM ROUNDS,Name,STANDARD TEAM NAME\n5.0,Gatlin & Ramarao,Mitty GR\n,Lahiri & Ponnuswamy,Mitty PL\n,Gupta & Chatradhi,Mitty GuCh\n,Shaikh & Singh,Mitty SS\n,Arun & Schacter,Gunn AS\n,Cheng & Parulekar,Gunn PC\n,Prabhakar & Hamilton,Gunn PH\n,Menotti & Bhasin,James Logan MB\n,Kim & Joshi,Mitty JK\n,Kang & Korolik,Mitty KK\n", "csv1_path": "infiagent/merge_test/26_1_0.csv", "csv2_path": "infiagent/merge_test/26_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/26_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/26_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\neducation,hour-per-week,age\n Bachelors,13,50\n HS-grad,40,38\n 11th,40,53\n Bachelors,40,28\n 9th,16,49\n HS-grad,45,52\n Masters,50,31\n Bachelors,40,42\n Bachelors,40,30\n\nHeader and first few lines of CSV file 2:\nmarital-status,native-country,income,education-num,race,age,final-weight,occupation,relationship,capital-loos\n Never-married, United-States, <=50K,13, White,39,77516, Adm-clerical, Not-in-family,0\n Divorced, United-States, <=50K,9, White,38,215646, Handlers-cleaners, Not-in-family,0\n Married-civ-spouse, United-States, <=50K,7, Black,53,234721, Handlers-cleaners, Husband,0\n Married-civ-spouse, Cuba, <=50K,13, Black,28,338409, Prof-specialty, Wife,0\n Married-civ-spouse, United-States, <=50K,14, White,37,284582, Exec-managerial, Wife,0\n Married-spouse-absent, Jamaica, <=50K,5, Black,49,160187, Other-service, Not-in-family,0\n Never-married, United-States, >50K,14, White,31,45781, Prof-specialty, Not-in-family,0\n Married-civ-spouse, India, >50K,13, Asian-Pac-Islander,30,141297, Prof-specialty, Husband,0\n Never-married, United-States, <=50K,13, White,23,122272, Adm-clerical, Own-child,0\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "education,hour-per-week,age\n Bachelors,13,50\n HS-grad,40,38\n 11th,40,53\n Bachelors,40,28\n 9th,16,49\n HS-grad,45,52\n Masters,50,31\n Bachelors,40,42\n Bachelors,40,30\n Bachelors,30,23\n", "csv2_example": "marital-status,native-country,income,education-num,race,age,final-weight,occupation,relationship,capital-loos\n Never-married, United-States, <=50K,13, White,39,77516, Adm-clerical, Not-in-family,0\n Divorced, United-States, <=50K,9, White,38,215646, Handlers-cleaners, Not-in-family,0\n Married-civ-spouse, United-States, <=50K,7, Black,53,234721, Handlers-cleaners, Husband,0\n Married-civ-spouse, Cuba, <=50K,13, Black,28,338409, Prof-specialty, Wife,0\n Married-civ-spouse, United-States, <=50K,14, White,37,284582, Exec-managerial, Wife,0\n Married-spouse-absent, Jamaica, <=50K,5, Black,49,160187, Other-service, Not-in-family,0\n Never-married, United-States, >50K,14, White,31,45781, Prof-specialty, Not-in-family,0\n Married-civ-spouse, India, >50K,13, Asian-Pac-Islander,30,141297, Prof-specialty, Husband,0\n Never-married, United-States, <=50K,13, White,23,122272, Adm-clerical, Own-child,0\n Never-married, United-States, <=50K,12, Black,32,205019, Sales, Not-in-family,0\n", "csv1_path": "infiagent/merge_test/18_0_0.csv", "csv2_path": "infiagent/merge_test/18_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/18_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/18_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nmin_diffsel,site\n-1.004167098795603,(HA2)121\n-1.2184218611180495,326\n-1.0182673783732994,280\n-0.8471518556126452,9\n-1.027758871160202,192\n-1.2624452625748657,-12\n-1.9128310938041404,171\n-1.663800562787797,(HA2)102\n-1.4217237593217844,312\n\nHeader and first few lines of CSV file 2:\nabs_diffsel,site\n9.026365225783264,(HA2)121\n9.002764774505879,326\n8.418637730396656,280\n8.185717407618215,9\n8.058662714496545,210\n8.015976108875662,192\n7.975893014675133,-12\n7.856854881480676,171\n7.846329400937815,(HA2)102\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "min_diffsel,site\n-1.004167098795603,(HA2)121\n-1.2184218611180495,326\n-1.0182673783732994,280\n-0.8471518556126452,9\n-1.027758871160202,192\n-1.2624452625748657,-12\n-1.9128310938041404,171\n-1.663800562787797,(HA2)102\n-1.4217237593217844,312\n-0.6621019482592285,242\n", "csv2_example": "abs_diffsel,site\n9.026365225783264,(HA2)121\n9.002764774505879,326\n8.418637730396656,280\n8.185717407618215,9\n8.058662714496545,210\n8.015976108875662,192\n7.975893014675133,-12\n7.856854881480676,171\n7.846329400937815,(HA2)102\n7.823952542822076,312\n", "csv1_path": "infiagent/merge_test/34_2_0.csv", "csv2_path": "infiagent/merge_test/34_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/34_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/34_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDATE TIME\n01/01/2015 00:00\n01/01/2015 01:00\n01/01/2015 02:00\n01/01/2015 05:00\n01/01/2015 06:00\n01/01/2015 08:00\n01/01/2015 09:00\n01/01/2015 10:00\n01/01/2015 12:00\n\nHeader and first few lines of CSV file 2:\nVIS,RELHUM,WINDSPEED,DATE TIME\n,,2.72,01/01/2015 00:00\n,,3.89,01/01/2015 01:00\n,,4.86,01/01/2015 02:00\n,,4.47,01/01/2015 03:00\n,,4.08,01/01/2015 04:00\n,,5.64,01/01/2015 05:00\n,,9.14,01/01/2015 09:00\n,,7.19,01/01/2015 10:00\n,,7.19,01/01/2015 12:00\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "DATE TIME\n01/01/2015 00:00\n01/01/2015 01:00\n01/01/2015 02:00\n01/01/2015 05:00\n01/01/2015 06:00\n01/01/2015 08:00\n01/01/2015 09:00\n01/01/2015 10:00\n01/01/2015 12:00\n01/01/2015 14:00\n", "csv2_example": "VIS,RELHUM,WINDSPEED,DATE TIME\n,,2.72,01/01/2015 00:00\n,,3.89,01/01/2015 01:00\n,,4.86,01/01/2015 02:00\n,,4.47,01/01/2015 03:00\n,,4.08,01/01/2015 04:00\n,,5.64,01/01/2015 05:00\n,,9.14,01/01/2015 09:00\n,,7.19,01/01/2015 10:00\n,,7.19,01/01/2015 12:00\n,,10.3,01/01/2015 13:00\n", "csv1_path": "infiagent/merge_test/13_0_0.csv", "csv2_path": "infiagent/merge_test/13_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/13_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/13_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDeliverable Qty,Last Price,Series,Date,High Price,Turnover\n360927,578.9,EQ,15-May-2017,584.0,460836225.3\n210364,583.8,EQ,16-May-2017,589.0,291930164.6\n261667,584.9,EQ,17-May-2017,594.0,296814880.85\n99785,572.25,EQ,18-May-2017,588.85,129878624.25\n68041,579.85,EQ,19-May-2017,585.8,141692454.15\n131406,567.0,EQ,22-May-2017,586.75,265093680.35\n185396,558.8,EQ,23-May-2017,581.55,620507087.55\n251584,569.0,EQ,24-May-2017,573.6,411258378.25\n300989,584.5,EQ,25-May-2017,585.0,345447321.45\n\nHeader and first few lines of CSV file 2:\nSymbol,% Dly Qt to Traded Qty,Open Price,Close Price,No. of Trades,Date,Prev Close\nGODREJIND,45.28,581.0,578.55,21649,15-May-2017,564.6\nGODREJIND,42.05,581.45,584.8,17204,16-May-2017,578.55\nGODREJIND,44.63,582.0,574.6,7144,18-May-2017,588.6\nGODREJIND,27.72,581.0,578.0,4969,19-May-2017,574.6\nGODREJIND,28.56,584.45,565.95,9762,22-May-2017,578.0\nGODREJIND,17.08,576.0,561.3,20749,23-May-2017,565.95\nGODREJIND,34.63,560.75,569.95,13113,24-May-2017,561.3\nGODREJIND,50.37,570.4,583.9,12426,25-May-2017,569.95\nGODREJIND,32.9,592.0,585.25,5018,29-May-2017,593.5\n\nQuestion: Combine the two tables and insert NAN values in the empty slots.", "csv1_example": "Deliverable Qty,Last Price,Series,Date,High Price,Turnover\n360927,578.9,EQ,15-May-2017,584.0,460836225.3\n210364,583.8,EQ,16-May-2017,589.0,291930164.6\n261667,584.9,EQ,17-May-2017,594.0,296814880.85\n99785,572.25,EQ,18-May-2017,588.85,129878624.25\n68041,579.85,EQ,19-May-2017,585.8,141692454.15\n131406,567.0,EQ,22-May-2017,586.75,265093680.35\n185396,558.8,EQ,23-May-2017,581.55,620507087.55\n251584,569.0,EQ,24-May-2017,573.6,411258378.25\n300989,584.5,EQ,25-May-2017,585.0,345447321.45\n141935,611.9,EQ,30-May-2017,612.9,324208618.85\n", "csv2_example": "Symbol,% Dly Qt to Traded Qty,Open Price,Close Price,No. of Trades,Date,Prev Close\nGODREJIND,45.28,581.0,578.55,21649,15-May-2017,564.6\nGODREJIND,42.05,581.45,584.8,17204,16-May-2017,578.55\nGODREJIND,44.63,582.0,574.6,7144,18-May-2017,588.6\nGODREJIND,27.72,581.0,578.0,4969,19-May-2017,574.6\nGODREJIND,28.56,584.45,565.95,9762,22-May-2017,578.0\nGODREJIND,17.08,576.0,561.3,20749,23-May-2017,565.95\nGODREJIND,34.63,560.75,569.95,13113,24-May-2017,561.3\nGODREJIND,50.37,570.4,583.9,12426,25-May-2017,569.95\nGODREJIND,32.9,592.0,585.25,5018,29-May-2017,593.5\nGODREJIND,30.82,612.0,604.55,13900,31-May-2017,610.2\n", "csv1_path": "infiagent/merge_test/38_2_0.csv", "csv2_path": "infiagent/merge_test/38_2_1.csv", "instruction": "Combine the two tables and insert NAN values in the empty slots.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/38_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/38_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nz,carat\n2.43,0.23\n2.31,0.21\n2.63,0.29\n2.75,0.31\n2.48,0.24\n2.53,0.26\n2.49,0.22\n2.27,0.2\n2.68,0.32\n\nHeader and first few lines of CSV file 2:\nclarity,carat,price,depth,x\nSI2,0.23,326,61.5,3.95\nSI1,0.21,326,59.8,3.89\nVS2,0.29,334,62.4,4.2\nSI2,0.31,335,63.3,4.34\nVVS2,0.24,336,62.8,3.94\nSI1,0.26,337,61.9,4.07\nVS2,0.22,337,65.1,3.87\nSI1,0.3,339,64.0,4.25\nSI2,0.2,345,60.2,3.79\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "z,carat\n2.43,0.23\n2.31,0.21\n2.63,0.29\n2.75,0.31\n2.48,0.24\n2.53,0.26\n2.49,0.22\n2.27,0.2\n2.68,0.32\n2.54,0.25\n", "csv2_example": "clarity,carat,price,depth,x\nSI2,0.23,326,61.5,3.95\nSI1,0.21,326,59.8,3.89\nVS2,0.29,334,62.4,4.2\nSI2,0.31,335,63.3,4.34\nVVS2,0.24,336,62.8,3.94\nSI1,0.26,337,61.9,4.07\nVS2,0.22,337,65.1,3.87\nSI1,0.3,339,64.0,4.25\nSI2,0.2,345,60.2,3.79\nI1,0.32,345,60.9,4.38\n", "csv1_path": "infiagent/merge_test/28_1_0.csv", "csv2_path": "infiagent/merge_test/28_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/28_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/28_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate\n2014-01-03\n2014-01-06\n2014-01-07\n2014-01-08\n2014-01-10\n2014-01-13\n2014-01-14\n2014-01-15\n2014-01-16\n\nHeader and first few lines of CSV file 2:\nHigh,Date,Volume,Close\n79.58,2014-01-02,58791957,79.02\n79.1,2014-01-03,98303870,77.28\n77.99,2014-01-07,79432766,77.15\n77.94,2014-01-08,64686685,77.64\n78.12,2014-01-09,69905199,76.65\n77.26,2014-01-10,76320664,76.13\n77.5,2014-01-13,94860843,76.53\n80.03,2014-01-15,98472619,79.62\n79.55,2014-01-16,57471330,79.18\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Date\n2014-01-03\n2014-01-06\n2014-01-07\n2014-01-08\n2014-01-10\n2014-01-13\n2014-01-14\n2014-01-15\n2014-01-16\n2014-01-17\n", "csv2_example": "High,Date,Volume,Close\n79.58,2014-01-02,58791957,79.02\n79.1,2014-01-03,98303870,77.28\n77.99,2014-01-07,79432766,77.15\n77.94,2014-01-08,64686685,77.64\n78.12,2014-01-09,69905199,76.65\n77.26,2014-01-10,76320664,76.13\n77.5,2014-01-13,94860843,76.53\n80.03,2014-01-15,98472619,79.62\n79.55,2014-01-16,57471330,79.18\n78.87,2014-01-17,108426689,77.24\n", "csv1_path": "infiagent/merge_test/29_1_0.csv", "csv2_path": "infiagent/merge_test/29_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/29_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/29_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nWeight,Publisher,Alignment,Unnamed: 0,Eye color\n441.0,Marvel Comics,good,0,yellow\n65.0,Dark Horse Comics,good,1,blue\n90.0,DC Comics,good,2,blue\n441.0,Marvel Comics,bad,3,green\n122.0,Marvel Comics,bad,5,blue\n-99.0,NBC - Heroes,good,6,blue\n88.0,DC Comics,good,7,blue\n61.0,Marvel Comics,good,8,blue\n81.0,Marvel Comics,good,9,brown\n\nHeader and first few lines of CSV file 2:\nHeight,name,Unnamed: 0\n191.0,Abe Sapien,1\n185.0,Abin Sur,2\n203.0,Abomination,3\n-99.0,Abraxas,4\n193.0,Absorbing Man,5\n-99.0,Adam Monroe,6\n185.0,Adam Strange,7\n173.0,Agent 13,8\n178.0,Agent Bob,9\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Weight,Publisher,Alignment,Unnamed: 0,Eye color\n441.0,Marvel Comics,good,0,yellow\n65.0,Dark Horse Comics,good,1,blue\n90.0,DC Comics,good,2,blue\n441.0,Marvel Comics,bad,3,green\n122.0,Marvel Comics,bad,5,blue\n-99.0,NBC - Heroes,good,6,blue\n88.0,DC Comics,good,7,blue\n61.0,Marvel Comics,good,8,blue\n81.0,Marvel Comics,good,9,brown\n104.0,Marvel Comics,good,10,-\n", "csv2_example": "Height,name,Unnamed: 0\n191.0,Abe Sapien,1\n185.0,Abin Sur,2\n203.0,Abomination,3\n-99.0,Abraxas,4\n193.0,Absorbing Man,5\n-99.0,Adam Monroe,6\n185.0,Adam Strange,7\n173.0,Agent 13,8\n178.0,Agent Bob,9\n191.0,Agent Zero,10\n", "csv1_path": "infiagent/merge_test/39_3_0.csv", "csv2_path": "infiagent/merge_test/39_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/39_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/39_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,name\n0,ARLENE\n1,BRET\n2,CINDY\n3,FOUR\n4,DON\n5,EMILY\n6,FRANKLIN\n7,GERT\n8,HARVEY\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,min_p,damage_imputed,dates_active,areas_affected,damage_USD\n0,990.0,0,April 19 – 21,None,0.0\n1,1007.0,0,June 19 – 20,Guyana, Venezuela, Trinidad and Tobago, Windward Islands,3000000.0\n2,991.0,0,June 20 – 23,Honduras, Belize, Cayman Islands, Yucatán Peninsula, Cuba, Southern United States, Eastern United States,25000000.0\n3,1009.0,0,July 5 – 7,None,0.0\n4,1005.0,0,July 17 – 18,Windward Islands, Barbados, Trinidad and Tobago,0.0\n5,1001.0,0,July 30 – August 1,Florida,10000000.0\n6,981.0,0,August 7 – 10,Nicaragua, Honduras, Guatemala, Belize, Yucatán Peninsula, Central Mexico,15000000.0\n7,962.0,0,August 12 – 17,Bermuda, East Coast of the United States, Atlantic Canada,0.0\n8,937.0,0,August 17 – September 1,Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucatán Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States,125000000000.0\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Unnamed: 0,name\n0,ARLENE\n1,BRET\n2,CINDY\n3,FOUR\n4,DON\n5,EMILY\n6,FRANKLIN\n7,GERT\n8,HARVEY\n10,JOSE\n", "csv2_example": "Unnamed: 0,min_p,damage_imputed,dates_active,areas_affected,damage_USD\n0,990.0,0,April 19 – 21,None,0.0\n1,1007.0,0,June 19 – 20,Guyana, Venezuela, Trinidad and Tobago, Windward Islands,3000000.0\n2,991.0,0,June 20 – 23,Honduras, Belize, Cayman Islands, Yucatán Peninsula, Cuba, Southern United States, Eastern United States,25000000.0\n3,1009.0,0,July 5 – 7,None,0.0\n4,1005.0,0,July 17 – 18,Windward Islands, Barbados, Trinidad and Tobago,0.0\n5,1001.0,0,July 30 – August 1,Florida,10000000.0\n6,981.0,0,August 7 – 10,Nicaragua, Honduras, Guatemala, Belize, Yucatán Peninsula, Central Mexico,15000000.0\n7,962.0,0,August 12 – 17,Bermuda, East Coast of the United States, Atlantic Canada,0.0\n8,937.0,0,August 17 – September 1,Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucatán Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States,125000000000.0\n9,914.0,0,August 30 – September 12,Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States,64760000000.00001\n", "csv1_path": "infiagent/merge_test/21_1_0.csv", "csv2_path": "infiagent/merge_test/21_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/21_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/21_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLow Price,High Price,No. of Trades,Date,Turnover\n568.5,584.0,21649,15-May-2017,460836225.3\n572.25,589.0,17204,16-May-2017,291930164.6\n576.85,594.0,8567,17-May-2017,296814880.85\n571.2,588.85,7144,18-May-2017,129878624.25\n567.55,585.8,4969,19-May-2017,141692454.15\n562.35,586.75,9762,22-May-2017,265093680.35\n555.55,581.55,20749,23-May-2017,620507087.55\n551.8,573.6,13113,24-May-2017,411258378.25\n569.75,585.0,12426,25-May-2017,345447321.45\n\nHeader and first few lines of CSV file 2:\nSymbol,Last Price,Prev Close,Date,Close Price,% Dly Qt to Traded Qty\nGODREJIND,578.9,564.6,15-May-2017,578.55,45.28\nGODREJIND,583.8,578.55,16-May-2017,584.8,42.05\nGODREJIND,584.9,584.8,17-May-2017,588.6,51.9\nGODREJIND,567.0,578.0,22-May-2017,565.95,28.56\nGODREJIND,558.8,565.95,23-May-2017,561.3,17.08\nGODREJIND,569.0,561.3,24-May-2017,569.95,34.63\nGODREJIND,584.5,569.95,25-May-2017,583.9,50.37\nGODREJIND,594.8,583.9,26-May-2017,593.5,34.64\nGODREJIND,585.9,593.5,29-May-2017,585.25,32.9\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Low Price,High Price,No. of Trades,Date,Turnover\n568.5,584.0,21649,15-May-2017,460836225.3\n572.25,589.0,17204,16-May-2017,291930164.6\n576.85,594.0,8567,17-May-2017,296814880.85\n571.2,588.85,7144,18-May-2017,129878624.25\n567.55,585.8,4969,19-May-2017,141692454.15\n562.35,586.75,9762,22-May-2017,265093680.35\n555.55,581.55,20749,23-May-2017,620507087.55\n551.8,573.6,13113,24-May-2017,411258378.25\n569.75,585.0,12426,25-May-2017,345447321.45\n577.35,596.65,7144,26-May-2017,249816977.85\n", "csv2_example": "Symbol,Last Price,Prev Close,Date,Close Price,% Dly Qt to Traded Qty\nGODREJIND,578.9,564.6,15-May-2017,578.55,45.28\nGODREJIND,583.8,578.55,16-May-2017,584.8,42.05\nGODREJIND,584.9,584.8,17-May-2017,588.6,51.9\nGODREJIND,567.0,578.0,22-May-2017,565.95,28.56\nGODREJIND,558.8,565.95,23-May-2017,561.3,17.08\nGODREJIND,569.0,561.3,24-May-2017,569.95,34.63\nGODREJIND,584.5,569.95,25-May-2017,583.9,50.37\nGODREJIND,594.8,583.9,26-May-2017,593.5,34.64\nGODREJIND,585.9,593.5,29-May-2017,585.25,32.9\nGODREJIND,611.9,585.25,30-May-2017,610.2,26.24\n", "csv1_path": "infiagent/merge_test/38_1_0.csv", "csv2_path": "infiagent/merge_test/38_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/38_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/38_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\npart_of_speech,Unnamed: 0,Impaired\ncoord pro0sub v adv0int mod pro0per v v,0,1\ncoord det0art n v n0prop,1,1\ncoord co pro0sub v prep n0prop,2,1\ncoord n0prop mod v prep det0poss n,3,1\ncoord pro0sub aux part prep det0art n0prop coord det0art,4,1\ncoord n0prop v det0art adj n n coord adj n,5,1\ncoord det0art n v det0art n,6,1\ncoord det0art n v adv inf v det0art n,7,1\ncoord pro0per cop adv,8,1\n\nHeader and first few lines of CSV file 2:\nid,Unnamed: 0,Gender\n3772732,0,0\n3772777,1,0\n3772812,2,0\n3772852,3,0\n3772884,4,0\n3772934,5,0\n3772996,6,0\n3773020,7,0\n3773061,8,0\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "part_of_speech,Unnamed: 0,Impaired\ncoord pro0sub v adv0int mod pro0per v v,0,1\ncoord det0art n v n0prop,1,1\ncoord co pro0sub v prep n0prop,2,1\ncoord n0prop mod v prep det0poss n,3,1\ncoord pro0sub aux part prep det0art n0prop coord det0art,4,1\ncoord n0prop v det0art adj n n coord adj n,5,1\ncoord det0art n v det0art n,6,1\ncoord det0art n v adv inf v det0art n,7,1\ncoord pro0per cop adv,8,1\ncoord co pro0dem pro0per,9,1\n", "csv2_example": "id,Unnamed: 0,Gender\n3772732,0,0\n3772777,1,0\n3772812,2,0\n3772852,3,0\n3772884,4,0\n3772934,5,0\n3772996,6,0\n3773020,7,0\n3773061,8,0\n3773088,9,0\n", "csv1_path": "infiagent/merge_test/37_0_0.csv", "csv2_path": "infiagent/merge_test/37_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/37_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/37_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nEX8,EX2,Unnamed: 0,EX1,EQ7,EQ6,EQ8,EQ3\n0.200716505553366,0.0426168252660709,1,0.0410182786002027,-0.0192616021801591,0.0508805382290177,-0.0219164236380736,-0.0209965026800568\n0.241603201129052,0.0263969887842081,2,0.0300434883392789,-0.0200465944831218,0.0769627933528486,-0.252665055370363,-0.0419930053601136\n0.307269712205153,0.0257609167653115,3,0.0237329839392477,-0.0115054095747666,0.0821742091932279,-0.171574287909491,-0.0681647023627196\n0.323376592280424,0.0337118170015188,4,0.0128953785565855,-0.025916462300798,0.0588109536382906,-0.144022212478769,-0.0678689769728596\n\nHeader and first few lines of CSV file 2:\nEX6,EQ5,EX3,EX7,Unnamed: 0,EQ1,EQ4,NZ,EQ2,EX4\n-0.0272364598603708,0.134687464025311,-0.0080529851935652,-0.0563269342288962,1,0.0008177611295828,-0.0642619047779573,0.362830325973427,-0.208033710010569,-0.0730019227052621\n-0.0095903027677361,0.0869856538496802,-0.0019217351030098,-0.0850724761803679,2,0.0338007933560907,-0.0162345864702208,0.3381555792905,0.0303187465307899,-0.094665094890157\n-0.0333742536317219,0.116167937721831,-0.0111643658365336,-0.111310618833892,3,0.0166278096348511,-0.085908020071585,0.0146529160116964,0.0990492914551702,-0.114399354483246\n-0.0061377937713511,0.11336194888797,-0.0183022390762845,-0.103877970914975,4,0.0046339797343027,-0.0987604010271765,-0.36930248144376,0.203353027030218,-0.0908072697065456\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "EX8,EX2,Unnamed: 0,EX1,EQ7,EQ6,EQ8,EQ3\n0.200716505553366,0.0426168252660709,1,0.0410182786002027,-0.0192616021801591,0.0508805382290177,-0.0219164236380736,-0.0209965026800568\n0.241603201129052,0.0263969887842081,2,0.0300434883392789,-0.0200465944831218,0.0769627933528486,-0.252665055370363,-0.0419930053601136\n0.307269712205153,0.0257609167653115,3,0.0237329839392477,-0.0115054095747666,0.0821742091932279,-0.171574287909491,-0.0681647023627196\n0.323376592280424,0.0337118170015188,4,0.0128953785565855,-0.025916462300798,0.0588109536382906,-0.144022212478769,-0.0678689769728596\n0.209389440978512,0.0149476924440696,5,0.0026065126869694,-0.0260687742401788,0.0137460533760731,-0.319666693349616,-0.0344520079186847\n0.039647704800665,0.0181280525385525,6,-0.0071336136696004,-0.0049911450904791,-0.0231114963355954,-0.316848867453293,0.0076888601363588\n-0.0582325664259767,0.03721021310545,7,-0.0111119751391853,-0.0295016510277619,-0.020795311517649,0.0284913507294957,0.0344520079186847\n-0.073100455726226,0.0222625206613803,8,-0.007270798547862,-0.0303335085428418,0.0169182195397822,0.317788142752067,0.0360784975629145\n-0.0179653662378013,0.002226252066138,9,-0.0109747902609238,-0.0260570579371495,0.05772838899512,0.159363709025421,0.0220315415445666\n0.0557545848759351,-0.0015901800472414,11,-0.0360796229827869,-0.0457755959354511,0.0807391816429785,-0.28616587435999,0.0122726036791881\n", "csv2_example": "EX6,EQ5,EX3,EX7,Unnamed: 0,EQ1,EQ4,NZ,EQ2,EX4\n-0.0272364598603708,0.134687464025311,-0.0080529851935652,-0.0563269342288962,1,0.0008177611295828,-0.0642619047779573,0.362830325973427,-0.208033710010569,-0.0730019227052621\n-0.0095903027677361,0.0869856538496802,-0.0019217351030098,-0.0850724761803679,2,0.0338007933560907,-0.0162345864702208,0.3381555792905,0.0303187465307899,-0.094665094890157\n-0.0333742536317219,0.116167937721831,-0.0111643658365336,-0.111310618833892,3,0.0166278096348511,-0.085908020071585,0.0146529160116964,0.0990492914551702,-0.114399354483246\n-0.0061377937713511,0.11336194888797,-0.0183022390762845,-0.103877970914975,4,0.0046339797343027,-0.0987604010271765,-0.36930248144376,0.203353027030218,-0.0908072697065456\n-0.0118919754319929,0.0488242057091753,-0.0130861009395434,-0.0825650768824202,5,-0.0365266637880335,-0.040586466175552,-0.383436113240686,0.54078497298146,-0.0310109793605687\n-0.0210986660890196,-0.0202031196037967,-0.0049416045505968,-0.0665356313705403,6,-0.0250780079738737,-0.055468170439921,-0.154171177528628,0.474591056252882,-0.0051932262087076\n-0.0149608723176685,-0.104382784619616,-0.0015556903214841,-0.0462077870621788,7,-0.0264409431898451,-0.071026315807216,-0.208110086071965,0.262601414175049,0.0\n-0.06943379203841,-0.140860639459805,-0.0111643658365336,-0.0204173942832883,8,-0.0109034817277712,-0.0033822055146293,-0.313322994728527,0.27117763331324,-0.0847237761477737\n-0.0444990048422959,-0.202031196037967,-0.0291920713266738,0.0227456936313826,9,0.0128115910301312,-0.0121759398526656,-0.0604205326815528,-0.0264534083276613,-0.117366912316793\n0.0337578657424314,-0.317076738226253,-0.04438292975999,0.066356531420687,10,-0.0169003966780454,-0.0493802005135883,-0.11687746493838,-0.548757233025412,-0.0501517273869484\n", "csv1_path": "infiagent/merge_test/31_1_0.csv", "csv2_path": "infiagent/merge_test/31_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/31_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/31_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nJDTDB,Y,Unnamed: 5,Z\n2451216.540613426,21313000.30706115,,23605803.31485712\n2451219.540613426,16139392.39760209,,25817113.37101007\n2451220.540613426,14413497.43427704,,26552228.01875284\n2451221.540613426,12687042.01291503,,27286310.90891188\n2451222.540613426,10960096.28765337,,28019340.125221\n2451226.540613426,4048784.251037874,,30940493.96190543\n2451227.540613426,2320410.227655286,,31667938.04800605\n2451228.540613426,591949.4600703884,,32394204.1002703\n2451229.540613426,-1136533.625792298,,33119272.54686797\n\nHeader and first few lines of CSV file 2:\nJDTDB,Calendar Date (TDB)\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000\n2451224.540613426, A.D. 1999-Feb-15 00:58:29.0000\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "JDTDB,Y,Unnamed: 5,Z\n2451216.540613426,21313000.30706115,,23605803.31485712\n2451219.540613426,16139392.39760209,,25817113.37101007\n2451220.540613426,14413497.43427704,,26552228.01875284\n2451221.540613426,12687042.01291503,,27286310.90891188\n2451222.540613426,10960096.28765337,,28019340.125221\n2451226.540613426,4048784.251037874,,30940493.96190543\n2451227.540613426,2320410.227655286,,31667938.04800605\n2451228.540613426,591949.4600703884,,32394204.1002703\n2451229.540613426,-1136533.625792298,,33119272.54686797\n2451230.540613426,-2864975.396086989,,33843124.13602761\n", "csv2_example": "JDTDB,Calendar Date (TDB)\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000\n2451224.540613426, A.D. 1999-Feb-15 00:58:29.0000\n2451225.540613426, A.D. 1999-Feb-16 00:58:29.0000\n", "csv1_path": "infiagent/merge_test/27_1_0.csv", "csv2_path": "infiagent/merge_test/27_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/27_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/27_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDepartment Name,dept_group\nAustin Code,Community Services\nAustin Convention Center,Utility and Other Enterprises\nAustin Resource Recovery,Utility and Other Enterprises\nAustin Transportation,Infrastructure/Transportation\nAustin Water,Utility and Other Enterprises\nAviation,Utility and Other Enterprises\nBuilding Services,Support Services\nCommunication and Technology Management,Internal Services\nDevelopment Services,Development\n\nHeader and first few lines of CSV file 2:\ncoa_dept_id,Department Name,github-dept-code\n92,Animal Services,ANM\n16,Austin Code,COD\n88,Austin Convention Center,CON\n11,Austin Energy,ENE\n85,Austin Public Library,LIB\n15,Austin Resource Recovery,RES\n24,Austin Transportation,TRA\n22,Austin Water,WAT\n81,Aviation,AVI\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Department Name,dept_group\nAustin Code,Community Services\nAustin Convention Center,Utility and Other Enterprises\nAustin Resource Recovery,Utility and Other Enterprises\nAustin Transportation,Infrastructure/Transportation\nAustin Water,Utility and Other Enterprises\nAviation,Utility and Other Enterprises\nBuilding Services,Support Services\nCommunication and Technology Management,Internal Services\nDevelopment Services,Development\nEconomic Development,Development\n", "csv2_example": "coa_dept_id,Department Name,github-dept-code\n92,Animal Services,ANM\n16,Austin Code,COD\n88,Austin Convention Center,CON\n11,Austin Energy,ENE\n85,Austin Public Library,LIB\n15,Austin Resource Recovery,RES\n24,Austin Transportation,TRA\n22,Austin Water,WAT\n81,Aviation,AVI\n75,Building Services,BLD\n", "csv1_path": "infiagent/merge_test/19_0_0.csv", "csv2_path": "infiagent/merge_test/19_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/19_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/19_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nANNUAL_RT,DEPTID,NAME\n37407.0,A24002,AKROFI,BERNARD\n11310.0,W02278,Aaron,Keairah T\n51862.0,A03031,Aaron,Patricia G\n64000.0,A29005,Aaron,Petra L\n57900.0,A65026,Abaineh,Yohannes T\n52000.0,A40001,Abbey,Emmanuel\n40650.0,A64120,Abdal-Rahim,Naim A\n68847.0,A99123,Abdi,Ezekiel W\n41194.0,A38410,Abdul Adl,Attrice A\n\nHeader and first few lines of CSV file 2:\nGross,HIRE_DT,NAME,DESCR\n14387.3,02/04/2013,AKROFI,BERNARD,COMP-Audits (002)\n,06/19/2013,Aaron,Keairah T,Youth Summer  (278)\n,06/10/2013,Aaron,Keontae E,Youth Summer  (200)\n51771.01,10/24/1979,Aaron,Patricia G,OED-Employment Dev (031)\n63909.03,09/25/2006,Aaron,Petra L,States Attorneys Office (005)\n57428.85,07/23/2009,Abaineh,Yohannes T,HLTH-Health Department (026)\n44159.59,03/30/2011,Abdal-Rahim,Naim A,Fire Department (120)\n66496.24,06/14/2007,Abdi,Ezekiel W,Police Department (123)\n32186.22,07/27/2009,Abdul Saboor,Jamillah,Enoch Pratt Free Library (054)\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "ANNUAL_RT,DEPTID,NAME\n37407.0,A24002,AKROFI,BERNARD\n11310.0,W02278,Aaron,Keairah T\n51862.0,A03031,Aaron,Patricia G\n64000.0,A29005,Aaron,Petra L\n57900.0,A65026,Abaineh,Yohannes T\n52000.0,A40001,Abbey,Emmanuel\n40650.0,A64120,Abdal-Rahim,Naim A\n68847.0,A99123,Abdi,Ezekiel W\n41194.0,A38410,Abdul Adl,Attrice A\n31741.0,A75054,Abdul Saboor,Jamillah\n", "csv2_example": "Gross,HIRE_DT,NAME,DESCR\n14387.3,02/04/2013,AKROFI,BERNARD,COMP-Audits (002)\n,06/19/2013,Aaron,Keairah T,Youth Summer  (278)\n,06/10/2013,Aaron,Keontae E,Youth Summer  (200)\n51771.01,10/24/1979,Aaron,Patricia G,OED-Employment Dev (031)\n63909.03,09/25/2006,Aaron,Petra L,States Attorneys Office (005)\n57428.85,07/23/2009,Abaineh,Yohannes T,HLTH-Health Department (026)\n44159.59,03/30/2011,Abdal-Rahim,Naim A,Fire Department (120)\n66496.24,06/14/2007,Abdi,Ezekiel W,Police Department (123)\n32186.22,07/27/2009,Abdul Saboor,Jamillah,Enoch Pratt Free Library (054)\n,06/13/2013,Abdul Wajid,Amani B,Youth Summer  (215)\n", "csv1_path": "infiagent/merge_test/12_0_0.csv", "csv2_path": "infiagent/merge_test/12_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/12_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/12_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nFreedom,Country,Family,Standard Error,Generosity\n0.66973,Norway,1.33095,0.0388,0.34699\n0.63297,Canada,1.32261,0.03553,0.45811\n0.61576,Netherlands,1.28017,0.02799,0.4761\n0.6598,Sweden,1.28907,0.03157,0.36262\n0.63938,New Zealand,1.31967,0.03371,0.47501\n0.65124,Australia,1.30923,0.04083,0.43562\n0.41319,Israel,1.22393,0.0347,0.33172\n0.63376,Costa Rica,1.23788,0.04454,0.25497\n0.62433,Austria,1.29704,0.03751,0.33088\n\nHeader and first few lines of CSV file 2:\nDystopia Residual,Health (Life Expectancy),Region,Country,Happiness Rank\n2.51738,0.94143,Western Europe,Switzerland,1\n2.49204,0.87464,Western Europe,Denmark,3\n2.46531,0.88521,Western Europe,Norway,4\n2.45176,0.90563,North America,Canada,5\n2.61955,0.88911,Western Europe,Finland,6\n2.4657,0.89284,Western Europe,Netherlands,7\n2.26425,0.90837,Australia and New Zealand,New Zealand,9\n2.26646,0.93156,Australia and New Zealand,Australia,10\n3.08854,0.91387,Middle East and Northern Africa,Israel,11\n\nQuestion: Combine the two tables and insert NAN values in the empty spaces.", "csv1_example": "Freedom,Country,Family,Standard Error,Generosity\n0.66973,Norway,1.33095,0.0388,0.34699\n0.63297,Canada,1.32261,0.03553,0.45811\n0.61576,Netherlands,1.28017,0.02799,0.4761\n0.6598,Sweden,1.28907,0.03157,0.36262\n0.63938,New Zealand,1.31967,0.03371,0.47501\n0.65124,Australia,1.30923,0.04083,0.43562\n0.41319,Israel,1.22393,0.0347,0.33172\n0.63376,Costa Rica,1.23788,0.04454,0.25497\n0.62433,Austria,1.29704,0.03751,0.33088\n0.48181,Mexico,0.91451,0.04176,0.14074\n", "csv2_example": "Dystopia Residual,Health (Life Expectancy),Region,Country,Happiness Rank\n2.51738,0.94143,Western Europe,Switzerland,1\n2.49204,0.87464,Western Europe,Denmark,3\n2.46531,0.88521,Western Europe,Norway,4\n2.45176,0.90563,North America,Canada,5\n2.61955,0.88911,Western Europe,Finland,6\n2.4657,0.89284,Western Europe,Netherlands,7\n2.26425,0.90837,Australia and New Zealand,New Zealand,9\n2.26646,0.93156,Australia and New Zealand,Australia,10\n3.08854,0.91387,Middle East and Northern Africa,Israel,11\n3.17728,0.86027,Latin America and Caribbean,Costa Rica,12\n", "csv1_path": "infiagent/merge_test/3_2_0.csv", "csv2_path": "infiagent/merge_test/3_2_1.csv", "instruction": "Combine the two tables and insert NAN values in the empty spaces.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/3_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/3_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNo. of cases,Country\n630308[495000-801000],Afghanistan\n0,Algeria\n0,Argentina\n0,Armenia\n0,Azerbaijan\n32924[30000-36000],Bangladesh\n7,Belize\n4111699[2774000-6552000],Benin\n6512[4900-8300],Bolivia (Plurinational State of)\n\nHeader and first few lines of CSV file 2:\nNo. of cases_max,Year,Country,WHO Region,No. of deaths_median\n,2017,Algeria,Africa,0\n,2017,Argentina,Americas,0\n,2017,Armenia,Europe,0\n,2017,Azerbaijan,Europe,0\n,2017,Belize,Americas,0\n,2017,Bhutan,South-East Asia,0\n8300.0,2017,Bolivia (Plurinational State of),Americas,2\n4200.0,2017,Botswana,Africa,7\n11330000.0,2017,Burkina Faso,Africa,27791\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "No. of cases,Country\n630308[495000-801000],Afghanistan\n0,Algeria\n0,Argentina\n0,Armenia\n0,Azerbaijan\n32924[30000-36000],Bangladesh\n7,Belize\n4111699[2774000-6552000],Benin\n6512[4900-8300],Bolivia (Plurinational State of)\n217928[196000-236000],Brazil\n", "csv2_example": "No. of cases_max,Year,Country,WHO Region,No. of deaths_median\n,2017,Algeria,Africa,0\n,2017,Argentina,Americas,0\n,2017,Armenia,Europe,0\n,2017,Azerbaijan,Europe,0\n,2017,Belize,Americas,0\n,2017,Bhutan,South-East Asia,0\n8300.0,2017,Bolivia (Plurinational State of),Americas,2\n4200.0,2017,Botswana,Africa,7\n11330000.0,2017,Burkina Faso,Africa,27791\n3401000.0,2017,Burundi,Africa,5253\n", "csv1_path": "infiagent/merge_test/32_3_0.csv", "csv2_path": "infiagent/merge_test/32_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/32_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/32_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nWINDSPEED,DATE TIME\n2.72,01/01/2015 00:00\n3.89,01/01/2015 01:00\n4.86,01/01/2015 02:00\n4.47,01/01/2015 03:00\n4.08,01/01/2015 04:00\n5.64,01/01/2015 05:00\n8.55,01/01/2015 06:00\n7.58,01/01/2015 07:00\n6.61,01/01/2015 08:00\n\nHeader and first few lines of CSV file 2:\nBARO,RELHUM,DATE TIME,GUSTS,DIR,AT,VIS\n1023.0,,01/01/2015 00:00,5.25,288,27.7,\n1022.7,,01/01/2015 01:00,7.0,273,26.8,\n1022.1,,01/01/2015 02:00,6.41,268,27.0,\n1020.9,,01/01/2015 04:00,7.19,283,25.9,\n1020.5,,01/01/2015 05:00,9.33,271,25.5,\n1019.5,,01/01/2015 06:00,11.47,260,26.1,\n1019.1,,01/01/2015 07:00,10.5,264,25.7,\n1018.9,,01/01/2015 08:00,10.3,267,25.2,\n1018.5,,01/01/2015 09:00,13.41,274,25.2,\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "WINDSPEED,DATE TIME\n2.72,01/01/2015 00:00\n3.89,01/01/2015 01:00\n4.86,01/01/2015 02:00\n4.47,01/01/2015 03:00\n4.08,01/01/2015 04:00\n5.64,01/01/2015 05:00\n8.55,01/01/2015 06:00\n7.58,01/01/2015 07:00\n6.61,01/01/2015 08:00\n9.14,01/01/2015 09:00\n", "csv2_example": "BARO,RELHUM,DATE TIME,GUSTS,DIR,AT,VIS\n1023.0,,01/01/2015 00:00,5.25,288,27.7,\n1022.7,,01/01/2015 01:00,7.0,273,26.8,\n1022.1,,01/01/2015 02:00,6.41,268,27.0,\n1020.9,,01/01/2015 04:00,7.19,283,25.9,\n1020.5,,01/01/2015 05:00,9.33,271,25.5,\n1019.5,,01/01/2015 06:00,11.47,260,26.1,\n1019.1,,01/01/2015 07:00,10.5,264,25.7,\n1018.9,,01/01/2015 08:00,10.3,267,25.2,\n1018.5,,01/01/2015 09:00,13.41,274,25.2,\n1019.0,,01/01/2015 10:00,10.11,267,25.5,\n", "csv1_path": "infiagent/merge_test/13_1_0.csv", "csv2_path": "infiagent/merge_test/13_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/13_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/13_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nSource,lat_dest,Target,lng_dest\n0,17.001061,1.0,51.089356\n1,16.960160000000002,0.0,51.048332\n2,17.001061,1.0,51.089356\n3,17.065654000000002,2.0,51.13283300000001\n4,17.065654000000002,2.0,51.13283300000001\n5,17.001061,1.0,51.089356\n6,17.065654000000002,2.0,51.13283300000001\n7,17.065654000000002,2.0,51.13283300000001\n8,17.001061,1.0,51.089356\n\nHeader and first few lines of CSV file 2:\nSource,Weight\n0,10\n1,4\n2,1\n3,2\n4,1\n5,6\n6,5\n7,30\n8,1\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Source,lat_dest,Target,lng_dest\n0,17.001061,1.0,51.089356\n1,16.960160000000002,0.0,51.048332\n2,17.001061,1.0,51.089356\n3,17.065654000000002,2.0,51.13283300000001\n4,17.065654000000002,2.0,51.13283300000001\n5,17.001061,1.0,51.089356\n6,17.065654000000002,2.0,51.13283300000001\n7,17.065654000000002,2.0,51.13283300000001\n8,17.001061,1.0,51.089356\n9,17.033555,15.0,51.116737\n", "csv2_example": "Source,Weight\n0,10\n1,4\n2,1\n3,2\n4,1\n5,6\n6,5\n7,30\n8,1\n10,13\n", "csv1_path": "infiagent/merge_test/5_1_0.csv", "csv2_path": "infiagent/merge_test/5_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/5_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/5_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nrevolving_utilization_of_unsecured_lines,number_of_time30-59_days_past_due_not_worse\n0.7661266089999998,2\n0.957151019,0\n0.65818014,1\n0.233809776,0\n0.9072394,1\n0.213178682,0\n0.305682465,0\n0.7544636479999999,0\n0.116950644,0\n\nHeader and first few lines of CSV file 2:\nnumber_of_time60-89_days_past_due_not_worse,number_of_dependents,number_real_estate_loans_or_lines,number_of_open_credit_lines_and_loans,revolving_utilization_of_unsecured_lines\n0,2.0,6,13,0.7661266089999998\n0,1.0,0,4,0.957151019\n0,0.0,0,2,0.65818014\n0,0.0,0,5,0.233809776\n0,0.0,1,7,0.9072394\n0,1.0,1,3,0.213178682\n0,0.0,3,8,0.305682465\n0,0.0,0,8,0.7544636479999999\n0,0.0,0,2,0.116950644\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "revolving_utilization_of_unsecured_lines,number_of_time30-59_days_past_due_not_worse\n0.7661266089999998,2\n0.957151019,0\n0.65818014,1\n0.233809776,0\n0.9072394,1\n0.213178682,0\n0.305682465,0\n0.7544636479999999,0\n0.116950644,0\n0.189169052,0\n", "csv2_example": "number_of_time60-89_days_past_due_not_worse,number_of_dependents,number_real_estate_loans_or_lines,number_of_open_credit_lines_and_loans,revolving_utilization_of_unsecured_lines\n0,2.0,6,13,0.7661266089999998\n0,1.0,0,4,0.957151019\n0,0.0,0,2,0.65818014\n0,0.0,0,5,0.233809776\n0,0.0,1,7,0.9072394\n0,1.0,1,3,0.213178682\n0,0.0,3,8,0.305682465\n0,0.0,0,8,0.7544636479999999\n0,0.0,0,2,0.116950644\n0,2.0,4,9,0.189169052\n", "csv1_path": "infiagent/merge_test/23_0_0.csv", "csv2_path": "infiagent/merge_test/23_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/23_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/23_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ngdpPercap_1957,gdpPercap_1972,gdpPercap_1952,country,gdpPercap_1977\n820.8530296,739.9811058,779.4453145,Afghanistan,786.11336\n11635.79945,18268.65839,9867.084765,Bahrain,19340.10196\n661.6374577,630.2336265,684.2441716,Bangladesh,659.8772322\n434.0383364,421.6240257,368.4692856,Cambodia,524.9721832\n575.9870009,676.9000921,400.4486107,China,741.2374699\n3629.076457,8315.928145,3054.421209,Hong Kong China,11186.14125\n590.061996,724.032527,546.5657493,India,813.337323\n858.9002707,1111.107907,749.6816546,Indonesia,1382.702056\n3290.257643,9613.818607,3035.326002,Iran,11888.59508\n\nHeader and first few lines of CSV file 2:\ncountry,gdpPercap_1987,gdpPercap_1982,gdpPercap_1997,gdpPercap_1992\nAfghanistan,852.3959448,978.0114388,635.341351,649.3413952\nBangladesh,751.9794035,676.9818656,972.7700352,837.8101643\nCambodia,683.8955732,624.4754784,734.28517,682.3031755\nChina,1378.904018,962.4213805,2289.234136,1655.784158\nHong Kong China,20038.47269,14560.53051,28377.63219,24757.60301\nIndonesia,1748.356961,1516.872988,3119.335603,2383.140898\nIran,6642.881371,7608.334602,8263.590301,7235.653188\nIsrael,17122.47986,15367.0292,20896.60924,18051.52254\nJapan,22375.94189,19384.10571,28816.58499,26824.89511\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "gdpPercap_1957,gdpPercap_1972,gdpPercap_1952,country,gdpPercap_1977\n820.8530296,739.9811058,779.4453145,Afghanistan,786.11336\n11635.79945,18268.65839,9867.084765,Bahrain,19340.10196\n661.6374577,630.2336265,684.2441716,Bangladesh,659.8772322\n434.0383364,421.6240257,368.4692856,Cambodia,524.9721832\n575.9870009,676.9000921,400.4486107,China,741.2374699\n3629.076457,8315.928145,3054.421209,Hong Kong China,11186.14125\n590.061996,724.032527,546.5657493,India,813.337323\n858.9002707,1111.107907,749.6816546,Indonesia,1382.702056\n3290.257643,9613.818607,3035.326002,Iran,11888.59508\n6229.333562,9576.037596,4129.766056,Iraq,14688.23507\n", "csv2_example": "country,gdpPercap_1987,gdpPercap_1982,gdpPercap_1997,gdpPercap_1992\nAfghanistan,852.3959448,978.0114388,635.341351,649.3413952\nBangladesh,751.9794035,676.9818656,972.7700352,837.8101643\nCambodia,683.8955732,624.4754784,734.28517,682.3031755\nChina,1378.904018,962.4213805,2289.234136,1655.784158\nHong Kong China,20038.47269,14560.53051,28377.63219,24757.60301\nIndonesia,1748.356961,1516.872988,3119.335603,2383.140898\nIran,6642.881371,7608.334602,8263.590301,7235.653188\nIsrael,17122.47986,15367.0292,20896.60924,18051.52254\nJapan,22375.94189,19384.10571,28816.58499,26824.89511\nJordan,4448.679912,4161.415959,3645.379572,3431.593647\n", "csv1_path": "infiagent/merge_test/36_0_0.csv", "csv2_path": "infiagent/merge_test/36_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/36_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/36_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhotel_name,hotel_id\nNight Theater District,75737\nHeritage Hotel New York City,93401\nClarion Hotel Park Avenue,224217\nSolita Soho Hotel,488793\nGreenwich Hotel,1028569\nComfort Inn Manhattan Bridge,1383001\nSutton Court Hotel Residences,1733337\nThe Langham New York Fifth Avenue,1776857\nThe NoMad Hotel New York,2643161\n\nHeader and first few lines of CSV file 2:\nreview_count,hotel_id,brand_name,star_rating,city_name,parent_brand_name\n2291,75737,Night Hotel,4.0,New York City,Wyndham Hotel Group\n582,1028569,The Leading Hotels of the World,5.0,New York City,The Leading Hotels of the World, Ltd\n578,1383001,Comfort Inn,2.0,New York City,Choice Hotels International, Inc.\n72,1733337,,3.5,New York City,\n2608,1776857,Langham,5.0,New York City,Langham Hospitality Group\n964,2643161,,5.0,New York City,\n1601,7148761,None,4.0,New York City,\n2,7379289,None,,Brooklyn,\n0,15646553,,,Jersey City,\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "hotel_name,hotel_id\nNight Theater District,75737\nHeritage Hotel New York City,93401\nClarion Hotel Park Avenue,224217\nSolita Soho Hotel,488793\nGreenwich Hotel,1028569\nComfort Inn Manhattan Bridge,1383001\nSutton Court Hotel Residences,1733337\nThe Langham New York Fifth Avenue,1776857\nThe NoMad Hotel New York,2643161\nBrooklyn Hostel - Utica Avenue,7379289\n", "csv2_example": "review_count,hotel_id,brand_name,star_rating,city_name,parent_brand_name\n2291,75737,Night Hotel,4.0,New York City,Wyndham Hotel Group\n582,1028569,The Leading Hotels of the World,5.0,New York City,The Leading Hotels of the World, Ltd\n578,1383001,Comfort Inn,2.0,New York City,Choice Hotels International, Inc.\n72,1733337,,3.5,New York City,\n2608,1776857,Langham,5.0,New York City,Langham Hospitality Group\n964,2643161,,5.0,New York City,\n1601,7148761,None,4.0,New York City,\n2,7379289,None,,Brooklyn,\n0,15646553,,,Jersey City,\n111,260396,,1.5,New York City,\n", "csv1_path": "infiagent/merge_test/40_3_0.csv", "csv2_path": "infiagent/merge_test/40_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/40_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/40_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLatitude,AveOccup,MedInc\n33.93,3.9940029985007497,0.9298\n32.79,2.038555691554468,2.7006\n34.89,3.121875,5.0286\n37.36,2.816011574632264,3.9038\n37.9,2.816011574632264,7.1754\n33.82,3.75,4.1292\n41.4,2.816011574632264,2.6062\n34.05,2.35969868173258,5.222\n33.97,3.821596244131456,2.2917\n\nHeader and first few lines of CSV file 2:\nMedInc,Population,Longitude\n0.9298,2664.0,-118.25\n2.7006,3331.0,-117.03\n5.0286,999.0,-120.43\n3.9038,1486.0,-122.02\n7.1754,779.0,-122.28\n4.1292,1575.0,-118.27\n5.222,1253.0,-118.39\n2.2917,814.0,-118.25\n3.2292,618.0,-120.95\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Latitude,AveOccup,MedInc\n33.93,3.9940029985007497,0.9298\n32.79,2.038555691554468,2.7006\n34.89,3.121875,5.0286\n37.36,2.816011574632264,3.9038\n37.9,2.816011574632264,7.1754\n33.82,3.75,4.1292\n41.4,2.816011574632264,2.6062\n34.05,2.35969868173258,5.222\n33.97,3.821596244131456,2.2917\n36.47,2.816011574632264,3.2292\n", "csv2_example": "MedInc,Population,Longitude\n0.9298,2664.0,-118.25\n2.7006,3331.0,-117.03\n5.0286,999.0,-120.43\n3.9038,1486.0,-122.02\n7.1754,779.0,-122.28\n4.1292,1575.0,-118.27\n5.222,1253.0,-118.39\n2.2917,814.0,-118.25\n3.2292,618.0,-120.95\n7.472,1478.0,-117.25\n", "csv1_path": "infiagent/merge_test/45_1_0.csv", "csv2_path": "infiagent/merge_test/45_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/45_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/45_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,Volume\n0,36875013\n1,24159683\n2,25621164\n3,36599736\n4,24271531\n5,17808877\n6,18652201\n7,19484317\n8,22113049\n\nHeader and first few lines of CSV file 2:\nHigh,Unnamed: 0,Open,Close\n90.61,0,90.14,90.0\n90.67,1,89.8,90.1\n90.28,2,89.08,90.14\n90.79,3,90.1,88.35\n89.78,4,88.67,89.6\n88.13,5,88.13,88.08\n88.73,7,88.65,88.22\n88.58,8,88.2,88.28\n88.41,9,87.66,88.19\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Unnamed: 0,Volume\n0,36875013\n1,24159683\n2,25621164\n3,36599736\n4,24271531\n5,17808877\n6,18652201\n7,19484317\n8,22113049\n9,23407110\n", "csv2_example": "High,Unnamed: 0,Open,Close\n90.61,0,90.14,90.0\n90.67,1,89.8,90.1\n90.28,2,89.08,90.14\n90.79,3,90.1,88.35\n89.78,4,88.67,89.6\n88.13,5,88.13,88.08\n88.73,7,88.65,88.22\n88.58,8,88.2,88.28\n88.41,9,87.66,88.19\n87.66,10,86.59,87.11\n", "csv1_path": "infiagent/merge_test/44_2_0.csv", "csv2_path": "infiagent/merge_test/44_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/44_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/44_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLength,Viscera weight,Height,Sex\n0.455,0.101,0.095,M\n0.35,0.0485,0.09,M\n0.44,0.114,0.125,M\n0.33,0.0395,0.08,I\n0.425,0.0775,0.095,I\n0.545,0.1495,0.125,F\n0.475,0.1125,0.125,M\n0.55,0.151,0.15,F\n0.525,0.1475,0.14,F\n\nHeader and first few lines of CSV file 2:\nLength,Diameter\n0.455,0.365\n0.44,0.365\n0.33,0.255\n0.545,0.425\n0.475,0.37\n0.55,0.44\n0.525,0.38\n0.43,0.35\n0.49,0.38\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Length,Viscera weight,Height,Sex\n0.455,0.101,0.095,M\n0.35,0.0485,0.09,M\n0.44,0.114,0.125,M\n0.33,0.0395,0.08,I\n0.425,0.0775,0.095,I\n0.545,0.1495,0.125,F\n0.475,0.1125,0.125,M\n0.55,0.151,0.15,F\n0.525,0.1475,0.14,F\n0.43,0.081,0.11,M\n", "csv2_example": "Length,Diameter\n0.455,0.365\n0.44,0.365\n0.33,0.255\n0.545,0.425\n0.475,0.37\n0.55,0.44\n0.525,0.38\n0.43,0.35\n0.49,0.38\n0.47,0.355\n", "csv1_path": "infiagent/merge_test/8_3_0.csv", "csv2_path": "infiagent/merge_test/8_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/8_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/8_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntranscript_id,Gender,Unnamed: 0,id\n9573,0,0,3772732\n9573,0,1,3772777\n9573,0,3,3772852\n9573,0,4,3772884\n9573,0,5,3772934\n9573,0,7,3773020\n9573,0,8,3773061\n9573,0,9,3773088\n9573,0,10,3773104\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,Age\n0,10\n2,10\n3,10\n4,10\n5,10\n6,10\n7,10\n8,10\n9,10\n\nQuestion: Combine the contents of two tables, retaining only the records that have been successfully combined.", "csv1_example": "transcript_id,Gender,Unnamed: 0,id\n9573,0,0,3772732\n9573,0,1,3772777\n9573,0,3,3772852\n9573,0,4,3772884\n9573,0,5,3772934\n9573,0,7,3773020\n9573,0,8,3773061\n9573,0,9,3773088\n9573,0,10,3773104\n9573,0,11,3773149\n", "csv2_example": "Unnamed: 0,Age\n0,10\n2,10\n3,10\n4,10\n5,10\n6,10\n7,10\n8,10\n9,10\n10,10\n", "csv1_path": "infiagent/merge_test/37_2_0.csv", "csv2_path": "infiagent/merge_test/37_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the records that have been successfully combined.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/37_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/37_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,NumberOfTime30-59DaysPastDueNotWorse,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age\n1,0,,0.88551908,43\n2,0,,0.463295269,57\n3,0,,0.043275036,59\n5,0,,0.9999999,27\n6,0,,0.509791452,63\n7,0,,0.587778161,50\n8,1,,0.046148938,79\n9,0,,0.013527027,68\n10,98,,0.9999999,23\n\nHeader and first few lines of CSV file 2:\nUnnamed: 0,MonthlyIncome,NumberOfOpenCreditLinesAndLoans\n2,9141.0,15\n3,5083.0,12\n6,4140.0,4\n7,0.0,5\n9,,4\n12,4250.0,6\n15,6822.0,14\n16,7133.0,8\n17,5900.0,10\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Unnamed: 0,NumberOfTime30-59DaysPastDueNotWorse,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age\n1,0,,0.88551908,43\n2,0,,0.463295269,57\n3,0,,0.043275036,59\n5,0,,0.9999999,27\n6,0,,0.509791452,63\n7,0,,0.587778161,50\n8,1,,0.046148938,79\n9,0,,0.013527027,68\n10,98,,0.9999999,23\n12,0,,0.085076597,52\n", "csv2_example": "Unnamed: 0,MonthlyIncome,NumberOfOpenCreditLinesAndLoans\n2,9141.0,15\n3,5083.0,12\n6,4140.0,4\n7,0.0,5\n9,,4\n12,4250.0,6\n15,6822.0,14\n16,7133.0,8\n17,5900.0,10\n19,4500.0,4\n", "csv1_path": "infiagent/merge_test/25_2_0.csv", "csv2_path": "infiagent/merge_test/25_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/25_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/25_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nVISITORDESCRIPTION,NEUTRALDESCRIPTION,SCORE,EVENTNUM,EVENTMSGTYPE,PERIOD,EVENTMSGACTIONTYPE,HOMEDESCRIPTION\n,,,1,10,1,0,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio\nPeeler Lost Ball Turnover (P1.T1),,,3,5,1,2,\nNesterovic BLOCK (1 BLK),,,4,2,1,5,MISS Ilgauskas  Layup\nGarnett Lost Ball Turnover (P1.T2),,,8,5,1,2,Boozer STEAL (1 STL)\n,,0 - 2,9,1,1,5,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST)\n,,,10,6,1,2,Palacio S.FOUL (P1.T1)\nHudson Free Throw 1 of 2 (1 PTS),,1 - 2,11,3,1,11,\nHudson Free Throw 2 of 2 (2 PTS),,2 - 2,12,3,1,12,\n,,2 - 4,13,1,1,5,Boozer 1' Layup (2 PTS) (Davis 1 AST)\n\nHeader and first few lines of CSV file 2:\nWCTIMESTRING,PCTIMESTRING,SCOREMARGIN,EVENTNUM\n7:13 PM,12:00,,0\n7:14 PM,12:00,,1\n7:16 PM,11:43,,3\n7:16 PM,11:22,,4\n7:16 PM,11:20,,5\n7:16 PM,11:16,,6\n7:16 PM,11:14,,7\n7:16 PM,10:57,,8\n7:17 PM,10:38,,10\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "VISITORDESCRIPTION,NEUTRALDESCRIPTION,SCORE,EVENTNUM,EVENTMSGTYPE,PERIOD,EVENTMSGACTIONTYPE,HOMEDESCRIPTION\n,,,1,10,1,0,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio\nPeeler Lost Ball Turnover (P1.T1),,,3,5,1,2,\nNesterovic BLOCK (1 BLK),,,4,2,1,5,MISS Ilgauskas  Layup\nGarnett Lost Ball Turnover (P1.T2),,,8,5,1,2,Boozer STEAL (1 STL)\n,,0 - 2,9,1,1,5,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST)\n,,,10,6,1,2,Palacio S.FOUL (P1.T1)\nHudson Free Throw 1 of 2 (1 PTS),,1 - 2,11,3,1,11,\nHudson Free Throw 2 of 2 (2 PTS),,2 - 2,12,3,1,12,\n,,2 - 4,13,1,1,5,Boozer 1' Layup (2 PTS) (Davis 1 AST)\nSzczerbiak S.FOUL (P1.T1),,,14,6,1,2,\n", "csv2_example": "WCTIMESTRING,PCTIMESTRING,SCOREMARGIN,EVENTNUM\n7:13 PM,12:00,,0\n7:14 PM,12:00,,1\n7:16 PM,11:43,,3\n7:16 PM,11:22,,4\n7:16 PM,11:20,,5\n7:16 PM,11:16,,6\n7:16 PM,11:14,,7\n7:16 PM,10:57,,8\n7:17 PM,10:38,,10\n7:17 PM,10:38,1,11\n", "csv1_path": "infiagent/merge_test/7_1_0.csv", "csv2_path": "infiagent/merge_test/7_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/7_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/7_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nfear,sadness,name,index\n0.0,0.018,alckmin,0\n0.0,0.046,ciro,19125\n0.0,0.243,marina,25592\n0.0,0.0,meirelles,32061\n0.0,0.0,alvaro,6397\n0.0,0.043,ciro,19126\n0.0,0.001,alvaro,6398\n0.0,0.001,boulos,12846\n0.0,0.022,alckmin,1\n\nHeader and first few lines of CSV file 2:\nneutral,timestamp,blur,index,happiness,surprise\n0.981,00h-00m-00s,0.0,0,0.0,0.0\n0.996,00h-00m-00s,0.05,12845,0.001,0.0\n0.942,00h-00m-00s,0.3,19125,0.0,0.0\n0.626,00h-00m-00s,0.0,25592,0.126,0.0\n0.884,00h-00m-00s,0.0,32061,0.0,0.001\n0.852,00h-00m-00s,0.31,6397,0.145,0.0\n0.563,00h-00m-01s,0.0,25593,0.432,0.0\n0.939,00h-00m-01s,0.18,19126,0.0,0.0\n0.303,00h-00m-01s,0.89,6398,0.694,0.001\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "fear,sadness,name,index\n0.0,0.018,alckmin,0\n0.0,0.046,ciro,19125\n0.0,0.243,marina,25592\n0.0,0.0,meirelles,32061\n0.0,0.0,alvaro,6397\n0.0,0.043,ciro,19126\n0.0,0.001,alvaro,6398\n0.0,0.001,boulos,12846\n0.0,0.022,alckmin,1\n0.0,0.112,meirelles,32062\n", "csv2_example": "neutral,timestamp,blur,index,happiness,surprise\n0.981,00h-00m-00s,0.0,0,0.0,0.0\n0.996,00h-00m-00s,0.05,12845,0.001,0.0\n0.942,00h-00m-00s,0.3,19125,0.0,0.0\n0.626,00h-00m-00s,0.0,25592,0.126,0.0\n0.884,00h-00m-00s,0.0,32061,0.0,0.001\n0.852,00h-00m-00s,0.31,6397,0.145,0.0\n0.563,00h-00m-01s,0.0,25593,0.432,0.0\n0.939,00h-00m-01s,0.18,19126,0.0,0.0\n0.303,00h-00m-01s,0.89,6398,0.694,0.001\n0.977,00h-00m-01s,0.0,1,0.0,0.0\n", "csv1_path": "infiagent/merge_test/9_0_0.csv", "csv2_path": "infiagent/merge_test/9_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/9_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/9_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLOCATION,SUBJECT,Value,Flag Codes\nBEL,TRY,1.4,\nCAN,TRY,2.5,\nCZE,TRY,1.4,\nDNK,TRY,,M\nFRA,TRY,1.4,\nHUN,TRY,1.2,\nIRL,TRY,1.4,\nITA,TRY,0.9,\nJPN,TRY,1.5,\n\nHeader and first few lines of CSV file 2:\nLOCATION,MEASURE,TIME\nAUS,PC_GDP,2012\nAUT,PC_GDP,2012\nBEL,PC_GDP,2012\nDNK,PC_GDP,2012\nFRA,PC_GDP,2012\nGRC,PC_GDP,2012\nISL,PC_GDP,2012\nIRL,PC_GDP,2012\nITA,PC_GDP,2012\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "LOCATION,SUBJECT,Value,Flag Codes\nBEL,TRY,1.4,\nCAN,TRY,2.5,\nCZE,TRY,1.4,\nDNK,TRY,,M\nFRA,TRY,1.4,\nHUN,TRY,1.2,\nIRL,TRY,1.4,\nITA,TRY,0.9,\nJPN,TRY,1.5,\nKOR,TRY,2.3,\n", "csv2_example": "LOCATION,MEASURE,TIME\nAUS,PC_GDP,2012\nAUT,PC_GDP,2012\nBEL,PC_GDP,2012\nDNK,PC_GDP,2012\nFRA,PC_GDP,2012\nGRC,PC_GDP,2012\nISL,PC_GDP,2012\nIRL,PC_GDP,2012\nITA,PC_GDP,2012\nJPN,PC_GDP,2012\n", "csv1_path": "infiagent/merge_test/46_0_0.csv", "csv2_path": "infiagent/merge_test/46_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/46_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/46_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nLOCATION,Flag Codes,SUBJECT\nAUS,,TRY\nAUT,,TRY\nCAN,,TRY\nCZE,,TRY\nFIN,,TRY\nFRA,,TRY\nGRC,M,TRY\nHUN,,TRY\nISL,,TRY\n\nHeader and first few lines of CSV file 2:\nFREQUENCY,LOCATION,TIME,INDICATOR\nA,AUS,2012,EDUEXP\nA,AUT,2012,EDUEXP\nA,BEL,2012,EDUEXP\nA,CAN,2012,EDUEXP\nA,CZE,2012,EDUEXP\nA,DNK,2012,EDUEXP\nA,FIN,2012,EDUEXP\nA,DEU,2012,EDUEXP\nA,GRC,2012,EDUEXP\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "LOCATION,Flag Codes,SUBJECT\nAUS,,TRY\nAUT,,TRY\nCAN,,TRY\nCZE,,TRY\nFIN,,TRY\nFRA,,TRY\nGRC,M,TRY\nHUN,,TRY\nISL,,TRY\nITA,,TRY\n", "csv2_example": "FREQUENCY,LOCATION,TIME,INDICATOR\nA,AUS,2012,EDUEXP\nA,AUT,2012,EDUEXP\nA,BEL,2012,EDUEXP\nA,CAN,2012,EDUEXP\nA,CZE,2012,EDUEXP\nA,DNK,2012,EDUEXP\nA,FIN,2012,EDUEXP\nA,DEU,2012,EDUEXP\nA,GRC,2012,EDUEXP\nA,HUN,2012,EDUEXP\n", "csv1_path": "infiagent/merge_test/46_3_0.csv", "csv2_path": "infiagent/merge_test/46_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/46_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/46_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,24-Hour Passes Purchased (midnight to 11:59 pm),Miles traveled today (midnight to 11:59 pm)\n10/2/2014,602,60639\n10/3/2014,1276,65739\n10/4/2014,617,24254\n10/5/2014,1470,48930\n10/7/2014,593,58740\n10/9/2014,709,57865\n10/10/2014,905,54690\n10/11/2014,528,23278\n10/12/2014,2083,52781\n\nHeader and first few lines of CSV file 2:\n7-Day Passes Purchased (midnight to 11:59 pm),Annual Member Sign-Ups (midnight to 11:59 pm),Miles traveled to date:,Cumulative trips (since launch):,Date\n48,112,23121175,13296973,10/1/2014\n86,113,23181814,13335259,10/2/2014\n107,65,23247553,13374215,10/3/2014\n26,34,23271807,13389303,10/4/2014\n99,94,23377117,13451201,10/6/2014\n81,59,23435857,13488132,10/7/2014\n86,92,23496451,13526002,10/8/2014\n71,53,23554316,13562429,10/9/2014\n78,57,23609006,13597119,10/10/2014\n\nQuestion: Combine the two tables and insert NAN in the empty spaces.", "csv1_example": "Date,24-Hour Passes Purchased (midnight to 11:59 pm),Miles traveled today (midnight to 11:59 pm)\n10/2/2014,602,60639\n10/3/2014,1276,65739\n10/4/2014,617,24254\n10/5/2014,1470,48930\n10/7/2014,593,58740\n10/9/2014,709,57865\n10/10/2014,905,54690\n10/11/2014,528,23278\n10/12/2014,2083,52781\n10/13/2014,749,40275\n", "csv2_example": "7-Day Passes Purchased (midnight to 11:59 pm),Annual Member Sign-Ups (midnight to 11:59 pm),Miles traveled to date:,Cumulative trips (since launch):,Date\n48,112,23121175,13296973,10/1/2014\n86,113,23181814,13335259,10/2/2014\n107,65,23247553,13374215,10/3/2014\n26,34,23271807,13389303,10/4/2014\n99,94,23377117,13451201,10/6/2014\n81,59,23435857,13488132,10/7/2014\n86,92,23496451,13526002,10/8/2014\n71,53,23554316,13562429,10/9/2014\n78,57,23609006,13597119,10/10/2014\n34,23,23632284,13611924,10/11/2014\n", "csv1_path": "infiagent/merge_test/1_2_0.csv", "csv2_path": "infiagent/merge_test/1_2_1.csv", "instruction": "Combine the two tables and insert NAN in the empty spaces.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/1_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/1_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nAge,Unnamed: 0,gloss\n10,0,and she asked um where do you wanna go\n10,1,and the kids I guess answered Mcdonalds\n10,2,and so they went to Mcdonalds\n10,3,and Lisa couldn't make up her mind\n10,5,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries\n10,6,and the mother ordered a salad\n10,7,and the mother reached down to get the purse\n10,8,and um sh it wasn't there\n10,10,um Joe wo woke up one morning\n\nHeader and first few lines of CSV file 2:\ntranscript_id,Unnamed: 0,Impaired\n9573,1,1\n9573,2,1\n9573,3,1\n9573,4,1\n9573,5,1\n9573,7,1\n9573,8,1\n9573,9,1\n9573,10,1\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Age,Unnamed: 0,gloss\n10,0,and she asked um where do you wanna go\n10,1,and the kids I guess answered Mcdonalds\n10,2,and so they went to Mcdonalds\n10,3,and Lisa couldn't make up her mind\n10,5,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries\n10,6,and the mother ordered a salad\n10,7,and the mother reached down to get the purse\n10,8,and um sh it wasn't there\n10,10,um Joe wo woke up one morning\n10,11,and he w he looked at the clock\n", "csv2_example": "transcript_id,Unnamed: 0,Impaired\n9573,1,1\n9573,2,1\n9573,3,1\n9573,4,1\n9573,5,1\n9573,7,1\n9573,8,1\n9573,9,1\n9573,10,1\n9573,13,1\n", "csv1_path": "infiagent/merge_test/37_1_0.csv", "csv2_path": "infiagent/merge_test/37_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/37_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/37_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nlooks,service,union,wage\n3,0,0,4.28\n4,0,0,7.96\n3,1,0,11.57\n3,0,0,11.42\n3,0,0,3.91\n3,0,0,8.76\n4,0,1,7.69\n3,0,0,5.0\n4,0,0,3.45\n\nHeader and first few lines of CSV file 2:\nSlooks,married,wage,south,female,belavg,expersq,goodhlth\n4,1,5.73,0,1,0,900,1\n0,1,4.28,1,1,0,784,1\n0,0,7.96,0,1,0,1225,1\n3,1,11.57,0,0,0,1444,1\n0,1,11.42,0,0,0,729,1\n0,1,3.91,0,1,0,400,0\n0,1,8.76,0,0,0,144,1\n0,0,7.69,0,0,0,25,1\n0,0,5.0,0,1,0,25,1\n\nQuestion: Combine two tables and retain only the rows that have been successfully combined.", "csv1_example": "looks,service,union,wage\n3,0,0,4.28\n4,0,0,7.96\n3,1,0,11.57\n3,0,0,11.42\n3,0,0,3.91\n3,0,0,8.76\n4,0,1,7.69\n3,0,0,5.0\n4,0,0,3.45\n4,0,0,4.03\n", "csv2_example": "Slooks,married,wage,south,female,belavg,expersq,goodhlth\n4,1,5.73,0,1,0,900,1\n0,1,4.28,1,1,0,784,1\n0,0,7.96,0,1,0,1225,1\n3,1,11.57,0,0,0,1444,1\n0,1,11.42,0,0,0,729,1\n0,1,3.91,0,1,0,400,0\n0,1,8.76,0,0,0,144,1\n0,0,7.69,0,0,0,25,1\n0,0,5.0,0,1,0,25,1\n0,0,3.89,0,1,0,144,1\n", "csv1_path": "infiagent/merge_test/15_2_0.csv", "csv2_path": "infiagent/merge_test/15_2_1.csv", "instruction": "Combine two tables and retain only the rows that have been successfully combined.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/15_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/15_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nrow ID,#featureID\n241,358.3677167129743_3.65612984126984\n695,423.2744890715284_4.29798541001065\n382,304.2993572401259_5.121302585521083\n300,389.2691196723436_3.383737479270316\n612,332.3307817246258_5.3103554720133594\n168,358.28219024838404_4.7314112935323385\n822,516.3001954425159_3.6121693593314776\n109,326.377650457406_5.358003633720932\n100,357.27906873865504_4.755498426870746\n\nHeader and first few lines of CSV file 2:\n_feature_id,importance.score,row retention time,LibraryID,#featureID\n358.3677167129743_3.65612984126984,0.0670524225212391,3.656129841,,358.3677167129743_3.65612984126984\n423.2744890715284_4.29798541001065,0.0405984341742611,4.29798541,,423.2744890715284_4.29798541001065\n304.2993572401259_5.121302585521083,0.0341410429343644,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,304.2993572401259_5.121302585521083\n389.2691196723436_3.383737479270316,0.03252068176745,3.383737479,,389.2691196723436_3.383737479270316\n332.3307817246258_5.3103554720133594,0.0322569080107531,5.310355472,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,332.3307817246258_5.3103554720133594\n358.28219024838404_4.7314112935323385,0.0272377330395867,4.731411294,,358.28219024838404_4.7314112935323385\n516.3001954425159_3.6121693593314776,0.0264573012282939,3.612169359,Spectral Match to Taurocholic acid from NIST14,516.3001954425159_3.6121693593314776\n326.377650457406_5.358003633720932,0.0234222353379512,5.358003634,,326.377650457406_5.358003633720932\n357.27906873865504_4.755498426870746,0.0232615688365795,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,357.27906873865504_4.755498426870746\n\nQuestion: Combine the two tables and insert NAN values in the empty cells.", "csv1_example": "row ID,#featureID\n241,358.3677167129743_3.65612984126984\n695,423.2744890715284_4.29798541001065\n382,304.2993572401259_5.121302585521083\n300,389.2691196723436_3.383737479270316\n612,332.3307817246258_5.3103554720133594\n168,358.28219024838404_4.7314112935323385\n822,516.3001954425159_3.6121693593314776\n109,326.377650457406_5.358003633720932\n100,357.27906873865504_4.755498426870746\n17,358.28198237386744_5.002188993710692\n", "csv2_example": "_feature_id,importance.score,row retention time,LibraryID,#featureID\n358.3677167129743_3.65612984126984,0.0670524225212391,3.656129841,,358.3677167129743_3.65612984126984\n423.2744890715284_4.29798541001065,0.0405984341742611,4.29798541,,423.2744890715284_4.29798541001065\n304.2993572401259_5.121302585521083,0.0341410429343644,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,304.2993572401259_5.121302585521083\n389.2691196723436_3.383737479270316,0.03252068176745,3.383737479,,389.2691196723436_3.383737479270316\n332.3307817246258_5.3103554720133594,0.0322569080107531,5.310355472,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,332.3307817246258_5.3103554720133594\n358.28219024838404_4.7314112935323385,0.0272377330395867,4.731411294,,358.28219024838404_4.7314112935323385\n516.3001954425159_3.6121693593314776,0.0264573012282939,3.612169359,Spectral Match to Taurocholic acid from NIST14,516.3001954425159_3.6121693593314776\n326.377650457406_5.358003633720932,0.0234222353379512,5.358003634,,326.377650457406_5.358003633720932\n357.27906873865504_4.755498426870746,0.0232615688365795,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,357.27906873865504_4.755498426870746\n358.28198237386744_5.002188993710692,0.0218383013013502,5.002188994,,358.28198237386744_5.002188993710692\n", "csv1_path": "infiagent/merge_test/42_3_0.csv", "csv2_path": "infiagent/merge_test/42_3_1.csv", "instruction": "Combine the two tables and insert NAN values in the empty cells.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/42_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/42_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nBARO,GUSTS,DATE TIME,RELHUM\n1021.4,7.19,01/01/2015 03:00,\n1020.9,7.19,01/01/2015 04:00,\n1020.5,9.33,01/01/2015 05:00,\n1019.5,11.47,01/01/2015 06:00,\n1019.1,10.5,01/01/2015 07:00,\n1018.9,10.3,01/01/2015 08:00,\n1018.5,13.41,01/01/2015 09:00,\n1019.0,10.11,01/01/2015 10:00,\n1019.6,13.61,01/01/2015 11:00,\n\nHeader and first few lines of CSV file 2:\nDIR,DATE TIME,WINDSPEED,VIS\n288,01/01/2015 00:00,2.72,\n273,01/01/2015 01:00,3.89,\n268,01/01/2015 02:00,4.86,\n294,01/01/2015 03:00,4.47,\n283,01/01/2015 04:00,4.08,\n271,01/01/2015 05:00,5.64,\n260,01/01/2015 06:00,8.55,\n264,01/01/2015 07:00,7.58,\n267,01/01/2015 08:00,6.61,\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "BARO,GUSTS,DATE TIME,RELHUM\n1021.4,7.19,01/01/2015 03:00,\n1020.9,7.19,01/01/2015 04:00,\n1020.5,9.33,01/01/2015 05:00,\n1019.5,11.47,01/01/2015 06:00,\n1019.1,10.5,01/01/2015 07:00,\n1018.9,10.3,01/01/2015 08:00,\n1018.5,13.41,01/01/2015 09:00,\n1019.0,10.11,01/01/2015 10:00,\n1019.6,13.61,01/01/2015 11:00,\n1019.4,11.66,01/01/2015 12:00,\n", "csv2_example": "DIR,DATE TIME,WINDSPEED,VIS\n288,01/01/2015 00:00,2.72,\n273,01/01/2015 01:00,3.89,\n268,01/01/2015 02:00,4.86,\n294,01/01/2015 03:00,4.47,\n283,01/01/2015 04:00,4.08,\n271,01/01/2015 05:00,5.64,\n260,01/01/2015 06:00,8.55,\n264,01/01/2015 07:00,7.58,\n267,01/01/2015 08:00,6.61,\n274,01/01/2015 09:00,9.14,\n", "csv1_path": "infiagent/merge_test/13_2_0.csv", "csv2_path": "infiagent/merge_test/13_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/13_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/13_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnumber_of_hits,on_base_percentage,salary_in_thousands_of_dollars\n111,0.335,2600\n115,0.337,2500\n128,0.292,2475\n169,0.346,2313\n86,0.37,600\n38,0.279,460\n61,0.327,240\n64,0.24,200\n38,0.283,177\n\nHeader and first few lines of CSV file 2:\nsalary_in_thousands_of_dollars,number_of_doubles,indicator_of_free_agency_eligibility,number_of_home_runs,number_of_walks,indicator_of_arbitration_eligibility,batting_average\n3300,21,1.0,31.0,22,0.0,0.272\n2600,17,1.0,18.0,39,0.0,0.269\n2500,15,1.0,17.0,63,0.0,0.249\n2475,22,0.0,12.0,23,1.0,0.26\n2313,28,0.0,8.0,70,1.0,0.273\n2175,32,1.0,26.0,87,0.0,0.291\n600,14,1.0,14.0,15,0.0,0.258\n460,7,0.0,3.0,11,0.0,0.228\n240,11,0.0,1.0,24,0.0,0.25\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "number_of_hits,on_base_percentage,salary_in_thousands_of_dollars\n111,0.335,2600\n115,0.337,2500\n128,0.292,2475\n169,0.346,2313\n86,0.37,600\n38,0.279,460\n61,0.327,240\n64,0.24,200\n38,0.283,177\n45,0.307,140\n", "csv2_example": "salary_in_thousands_of_dollars,number_of_doubles,indicator_of_free_agency_eligibility,number_of_home_runs,number_of_walks,indicator_of_arbitration_eligibility,batting_average\n3300,21,1.0,31.0,22,0.0,0.272\n2600,17,1.0,18.0,39,0.0,0.269\n2500,15,1.0,17.0,63,0.0,0.249\n2475,22,0.0,12.0,23,1.0,0.26\n2313,28,0.0,8.0,70,1.0,0.273\n2175,32,1.0,26.0,87,0.0,0.291\n600,14,1.0,14.0,15,0.0,0.258\n460,7,0.0,3.0,11,0.0,0.228\n240,11,0.0,1.0,24,0.0,0.25\n200,10,0.0,10.0,14,0.0,0.203\n", "csv1_path": "infiagent/merge_test/14_2_0.csv", "csv2_path": "infiagent/merge_test/14_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/14_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/14_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nEVENTNUM,HOMEDESCRIPTION,EVENTMSGACTIONTYPE,SCOREMARGIN,VISITORDESCRIPTION,WCTIMESTRING,EVENTMSGTYPE\n0,,0,,,7:13 PM,12\n1,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,0,,,7:14 PM,10\n2,Ilgauskas Bad Pass Turnover (P1.T1),1,,Peeler STEAL (1 STL),7:15 PM,5\n3,,2,,Peeler Lost Ball Turnover (P1.T1),7:16 PM,5\n4,MISS Ilgauskas  Layup,5,,Nesterovic BLOCK (1 BLK),7:16 PM,2\n5,Davis REBOUND (Off:1 Def:0),0,,,7:16 PM,4\n7,,0,,Garnett REBOUND (Off:0 Def:1),7:16 PM,4\n8,Boozer STEAL (1 STL),2,,Garnett Lost Ball Turnover (P1.T2),7:16 PM,5\n9,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),5,2,,7:16 PM,1\n\nHeader and first few lines of CSV file 2:\nPCTIMESTRING,PERIOD,GAME_ID,EVENTNUM\n12:00,1,20200722,0\n12:00,1,20200722,1\n11:43,1,20200722,3\n11:22,1,20200722,4\n11:20,1,20200722,5\n11:16,1,20200722,6\n11:14,1,20200722,7\n10:57,1,20200722,8\n10:52,1,20200722,9\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "EVENTNUM,HOMEDESCRIPTION,EVENTMSGACTIONTYPE,SCOREMARGIN,VISITORDESCRIPTION,WCTIMESTRING,EVENTMSGTYPE\n0,,0,,,7:13 PM,12\n1,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,0,,,7:14 PM,10\n2,Ilgauskas Bad Pass Turnover (P1.T1),1,,Peeler STEAL (1 STL),7:15 PM,5\n3,,2,,Peeler Lost Ball Turnover (P1.T1),7:16 PM,5\n4,MISS Ilgauskas  Layup,5,,Nesterovic BLOCK (1 BLK),7:16 PM,2\n5,Davis REBOUND (Off:1 Def:0),0,,,7:16 PM,4\n7,,0,,Garnett REBOUND (Off:0 Def:1),7:16 PM,4\n8,Boozer STEAL (1 STL),2,,Garnett Lost Ball Turnover (P1.T2),7:16 PM,5\n9,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),5,2,,7:16 PM,1\n10,Palacio S.FOUL (P1.T1),2,,,7:17 PM,6\n", "csv2_example": "PCTIMESTRING,PERIOD,GAME_ID,EVENTNUM\n12:00,1,20200722,0\n12:00,1,20200722,1\n11:43,1,20200722,3\n11:22,1,20200722,4\n11:20,1,20200722,5\n11:16,1,20200722,6\n11:14,1,20200722,7\n10:57,1,20200722,8\n10:52,1,20200722,9\n10:38,1,20200722,10\n", "csv1_path": "infiagent/merge_test/7_2_0.csv", "csv2_path": "infiagent/merge_test/7_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/7_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/7_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nVolume,Time,Open\n143354,09:15:59,319.25\n52695,09:16:59,317.7\n47179,09:17:59,318.0\n44745,09:18:59,318.65\n57892,09:19:59,319.3\n67482,09:20:59,319.6\n56590,09:21:59,320.25\n52413,09:22:59,320.15\n56305,09:23:59,319.65\n\nHeader and first few lines of CSV file 2:\nTime,Low,Close\n09:18:59,318.5,319.2\n09:19:59,319.2,319.65\n09:20:59,319.6,320.25\n09:21:59,319.95,320.05\n09:23:59,319.15,319.4\n09:24:59,319.45,319.7\n09:25:59,318.3,318.35\n09:26:59,318.3,319.05\n09:27:59,318.95,319.65\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Volume,Time,Open\n143354,09:15:59,319.25\n52695,09:16:59,317.7\n47179,09:17:59,318.0\n44745,09:18:59,318.65\n57892,09:19:59,319.3\n67482,09:20:59,319.6\n56590,09:21:59,320.25\n52413,09:22:59,320.15\n56305,09:23:59,319.65\n36525,09:24:59,319.5\n", "csv2_example": "Time,Low,Close\n09:18:59,318.5,319.2\n09:19:59,319.2,319.65\n09:20:59,319.6,320.25\n09:21:59,319.95,320.05\n09:23:59,319.15,319.4\n09:24:59,319.45,319.7\n09:25:59,318.3,318.35\n09:26:59,318.3,319.05\n09:27:59,318.95,319.65\n09:28:59,319.5,319.5\n", "csv1_path": "infiagent/merge_test/49_3_0.csv", "csv2_path": "infiagent/merge_test/49_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/49_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/49_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ncontinent,year\nasia,1962\nasia,1967\nasia,1972\nasia,1977\nasia,1982\nasia,1987\nasia,1992\nasia,1997\nasia,2007\n\nHeader and first few lines of CSV file 2:\nyear,lifeexp,country\n1962,31.997,afghanistan\n1967,34.02,afghanistan\n1982,39.854,afghanistan\n1987,40.822,afghanistan\n1992,41.674,afghanistan\n1997,41.763,afghanistan\n2002,42.129,afghanistan\n2007,43.828,afghanistan\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "continent,year\nasia,1962\nasia,1967\nasia,1972\nasia,1977\nasia,1982\nasia,1987\nasia,1992\nasia,1997\nasia,2007\n", "csv2_example": "year,lifeexp,country\n1962,31.997,afghanistan\n1967,34.02,afghanistan\n1982,39.854,afghanistan\n1987,40.822,afghanistan\n1992,41.674,afghanistan\n1997,41.763,afghanistan\n2002,42.129,afghanistan\n2007,43.828,afghanistan\n", "csv1_path": "infiagent/merge_test/35_1_0.csv", "csv2_path": "infiagent/merge_test/35_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/35_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/35_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nJDTDB\n2451214.540613426\n2451216.540613426\n2451217.540613426\n2451218.540613426\n2451219.540613426\n2451221.540613426\n2451222.540613426\n2451223.540613426\n2451224.540613426\n\nHeader and first few lines of CSV file 2:\nX,Unnamed: 5,Calendar Date (TDB),JDTDB\n-288747920.4014895,, A.D. 1999-Feb-05 00:58:29.0000,2451214.540613426\n-289446587.3203819,, A.D. 1999-Feb-06 00:58:29.0000,2451215.540613426\n-290133649.6015852,, A.D. 1999-Feb-07 00:58:29.0000,2451216.540613426\n-292125663.2967085,, A.D. 1999-Feb-10 00:58:29.0000,2451219.540613426\n-292766763.0494693,, A.D. 1999-Feb-11 00:58:29.0000,2451220.540613426\n-293396487.4748839,, A.D. 1999-Feb-12 00:58:29.0000,2451221.540613426\n-294621996.946905,, A.D. 1999-Feb-14 00:58:29.0000,2451223.540613426\n-295217875.8227791,, A.D. 1999-Feb-15 00:58:29.0000,2451224.540613426\n-296376117.8866599,, A.D. 1999-Feb-17 00:58:29.0000,2451226.540613426\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "JDTDB\n2451214.540613426\n2451216.540613426\n2451217.540613426\n2451218.540613426\n2451219.540613426\n2451221.540613426\n2451222.540613426\n2451223.540613426\n2451224.540613426\n2451226.540613426\n", "csv2_example": "X,Unnamed: 5,Calendar Date (TDB),JDTDB\n-288747920.4014895,, A.D. 1999-Feb-05 00:58:29.0000,2451214.540613426\n-289446587.3203819,, A.D. 1999-Feb-06 00:58:29.0000,2451215.540613426\n-290133649.6015852,, A.D. 1999-Feb-07 00:58:29.0000,2451216.540613426\n-292125663.2967085,, A.D. 1999-Feb-10 00:58:29.0000,2451219.540613426\n-292766763.0494693,, A.D. 1999-Feb-11 00:58:29.0000,2451220.540613426\n-293396487.4748839,, A.D. 1999-Feb-12 00:58:29.0000,2451221.540613426\n-294621996.946905,, A.D. 1999-Feb-14 00:58:29.0000,2451223.540613426\n-295217875.8227791,, A.D. 1999-Feb-15 00:58:29.0000,2451224.540613426\n-296376117.8866599,, A.D. 1999-Feb-17 00:58:29.0000,2451226.540613426\n-296938576.0232202,, A.D. 1999-Feb-18 00:58:29.0000,2451227.540613426\n", "csv1_path": "infiagent/merge_test/27_2_0.csv", "csv2_path": "infiagent/merge_test/27_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/27_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/27_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,Miles traveled to date:\n10/3/2014,23247553\n10/6/2014,23377117\n10/7/2014,23435857\n10/8/2014,23496451\n10/10/2014,23609006\n10/11/2014,23632284\n10/12/2014,23685064\n10/14/2014,23787030\n10/15/2014,23836349\n\nHeader and first few lines of CSV file 2:\n24-Hour Passes Purchased (midnight to 11:59 pm),Annual Member Sign-Ups (midnight to 11:59 pm),Miles traveled today (midnight to 11:59 pm),Date,7-Day Passes Purchased (midnight to 11:59 pm),Total Annual Memberships Sold\n330,112,44612,10/1/2014,48,124846\n602,113,60639,10/2/2014,86,124959\n1276,65,65739,10/3/2014,107,125024\n617,34,24254,10/4/2014,26,125058\n1470,51,48930,10/5/2014,90,125109\n710,94,56380,10/6/2014,99,125203\n593,59,58740,10/7/2014,81,125262\n667,92,60594,10/8/2014,86,125354\n709,53,57865,10/9/2014,71,125407\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Date,Miles traveled to date:\n10/3/2014,23247553\n10/6/2014,23377117\n10/7/2014,23435857\n10/8/2014,23496451\n10/10/2014,23609006\n10/11/2014,23632284\n10/12/2014,23685064\n10/14/2014,23787030\n10/15/2014,23836349\n10/16/2014,23884157\n", "csv2_example": "24-Hour Passes Purchased (midnight to 11:59 pm),Annual Member Sign-Ups (midnight to 11:59 pm),Miles traveled today (midnight to 11:59 pm),Date,7-Day Passes Purchased (midnight to 11:59 pm),Total Annual Memberships Sold\n330,112,44612,10/1/2014,48,124846\n602,113,60639,10/2/2014,86,124959\n1276,65,65739,10/3/2014,107,125024\n617,34,24254,10/4/2014,26,125058\n1470,51,48930,10/5/2014,90,125109\n710,94,56380,10/6/2014,99,125203\n593,59,58740,10/7/2014,81,125262\n667,92,60594,10/8/2014,86,125354\n709,53,57865,10/9/2014,71,125407\n905,57,54690,10/10/2014,78,125464\n", "csv1_path": "infiagent/merge_test/1_1_0.csv", "csv2_path": "infiagent/merge_test/1_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/1_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/1_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nMEANGBT,TRUE_TIME,MEANGBH,MEANJZH,TOTUSJH\n93.013,2014.03.23_20:24:00_TAI,31.21,0.00286312,143.341\n89.355,2014.03.23_21:00:00_TAI,30.44,0.00307095,164.412\n87.089,2014.03.23_21:12:00_TAI,29.875,0.00341966,163.141\n84.835,2014.03.23_21:48:00_TAI,27.8,0.00314102,166.084\n84.526,2014.03.23_22:00:00_TAI,27.636,0.00170337,162.369\n84.563,2014.03.23_22:12:00_TAI,27.331,0.00244001,161.954\n86.069,2014.03.23_22:24:00_TAI,27.117,0.00241245,161.277\n\nHeader and first few lines of CSV file 2:\nMEANJZD,TIME,TOTUSJZ,SAVNCPP,TRUE_TIME,SHRGT45,MEANGAM,MEANSHR,MEANGBZ,ABSNJZH,AREA_ACR,R_VALUE\n0.15138598,11.8,3745627000000.0,465108600000.0,2014.03.23_20:36:00_TAI,0.0,21.74,18.172,89.779,18.216,83.896141,0.0\n0.2345193,12.2,3604093000000.0,763678300000.0,2014.03.23_21:00:00_TAI,0.048,21.654,18.134,89.499,19.141,87.762978,0.0\n0.18933275,12.8,3534322000000.0,660597300000.0,2014.03.23_21:36:00_TAI,0.045,21.737,17.608,86.595,18.042,93.034233,0.0\n0.00457878,13.2,3761321000000.0,18206300000.0,2014.03.23_22:00:00_TAI,0.026,22.237,17.662,85.236,12.963,96.260063,2.07\n0.17288232,13.4,3733276000000.0,684728800000.0,2014.03.23_22:12:00_TAI,0.013,22.024,17.638,85.487,18.495,97.720245,0.0\n0.17619905,13.6,3714417000000.0,698529300000.0,2014.03.23_22:24:00_TAI,0.013,22.004,17.485,87.034,18.303,97.474724,0.0\n0.18886125,13.8,3761963000000.0,749637700000.0,2014.03.23_22:36:00_TAI,0.0,21.947,17.464,87.682,19.912,99.95433,0.0\n\nQuestion: Combine the two tables and retain only the records that have been successfully matched.", "csv1_example": "MEANGBT,TRUE_TIME,MEANGBH,MEANJZH,TOTUSJH\n93.013,2014.03.23_20:24:00_TAI,31.21,0.00286312,143.341\n89.355,2014.03.23_21:00:00_TAI,30.44,0.00307095,164.412\n87.089,2014.03.23_21:12:00_TAI,29.875,0.00341966,163.141\n84.835,2014.03.23_21:48:00_TAI,27.8,0.00314102,166.084\n84.526,2014.03.23_22:00:00_TAI,27.636,0.00170337,162.369\n84.563,2014.03.23_22:12:00_TAI,27.331,0.00244001,161.954\n86.069,2014.03.23_22:24:00_TAI,27.117,0.00241245,161.277\n86.868,2014.03.23_22:36:00_TAI,27.23,0.00262174,163.987\n86.978,2014.03.23_22:48:00_TAI,26.903,0.00254172,157.176\n87.246,2014.03.23_23:12:00_TAI,27.544,0.00271696,171.132\n", "csv2_example": "MEANJZD,TIME,TOTUSJZ,SAVNCPP,TRUE_TIME,SHRGT45,MEANGAM,MEANSHR,MEANGBZ,ABSNJZH,AREA_ACR,R_VALUE\n0.15138598,11.8,3745627000000.0,465108600000.0,2014.03.23_20:36:00_TAI,0.0,21.74,18.172,89.779,18.216,83.896141,0.0\n0.2345193,12.2,3604093000000.0,763678300000.0,2014.03.23_21:00:00_TAI,0.048,21.654,18.134,89.499,19.141,87.762978,0.0\n0.18933275,12.8,3534322000000.0,660597300000.0,2014.03.23_21:36:00_TAI,0.045,21.737,17.608,86.595,18.042,93.034233,0.0\n0.00457878,13.2,3761321000000.0,18206300000.0,2014.03.23_22:00:00_TAI,0.026,22.237,17.662,85.236,12.963,96.260063,2.07\n0.17288232,13.4,3733276000000.0,684728800000.0,2014.03.23_22:12:00_TAI,0.013,22.024,17.638,85.487,18.495,97.720245,0.0\n0.17619905,13.6,3714417000000.0,698529300000.0,2014.03.23_22:24:00_TAI,0.013,22.004,17.485,87.034,18.303,97.474724,0.0\n0.18886125,13.8,3761963000000.0,749637700000.0,2014.03.23_22:36:00_TAI,0.0,21.947,17.464,87.682,19.912,99.95433,0.0\n0.18899454,14.0,3606528000000.0,752054200000.0,2014.03.23_22:48:00_TAI,0.053,21.622,17.358,87.929,19.032,101.623764,0.0\n0.30007368,14.2,3815674000000.0,1206821000000.0,2014.03.23_23:00:00_TAI,0.026,22.089,17.460,85.809,23.008,102.225227,0.0\n0.19985864,14.6,4081217000000.0,831824400000.0,2014.03.23_23:24:00_TAI,0.025,21.922,17.419,89.03,21.861,113.149673,0.0\n", "csv1_path": "infiagent/merge_test/6_1_0.csv", "csv2_path": "infiagent/merge_test/6_1_1.csv", "instruction": "Combine the two tables and retain only the records that have been successfully matched.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/6_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/6_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nlooks,black,exper,lwage,south,goodhlth,service,abvavg,Nservice,educ,belavg,wage\n4,0,30,1.745715,0,1,1,1,0,14,0,5.73\n4,0,35,2.074429,0,1,0,1,1,10,0,7.96\n3,0,38,2.448416,0,1,1,0,0,16,0,11.57\n3,0,27,2.435366,0,1,0,0,1,16,0,11.42\n3,0,20,1.363537,0,0,0,0,1,12,0,3.91\n3,0,12,2.170196,0,1,0,0,1,16,0,8.76\n4,0,5,2.039921,0,1,0,1,1,16,0,7.69\n4,0,3,1.238374,0,1,0,1,1,12,0,3.45\n4,0,6,1.393766,0,1,0,1,1,16,0,4.03\n\nHeader and first few lines of CSV file 2:\nfemale,Slooks,expersq,union,smllcity,wage\n1,4,900,0,1,5.73\n1,0,784,0,1,4.28\n0,3,1444,0,0,11.57\n0,0,144,0,0,8.76\n0,0,25,1,1,7.69\n0,0,36,0,1,4.03\n0,3,64,0,1,3.0\n0,0,144,0,0,7.99\n0,0,289,0,1,6.01\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "looks,black,exper,lwage,south,goodhlth,service,abvavg,Nservice,educ,belavg,wage\n4,0,30,1.745715,0,1,1,1,0,14,0,5.73\n4,0,35,2.074429,0,1,0,1,1,10,0,7.96\n3,0,38,2.448416,0,1,1,0,0,16,0,11.57\n3,0,27,2.435366,0,1,0,0,1,16,0,11.42\n3,0,20,1.363537,0,0,0,0,1,12,0,3.91\n3,0,12,2.170196,0,1,0,0,1,16,0,8.76\n4,0,5,2.039921,0,1,0,1,1,16,0,7.69\n4,0,3,1.238374,0,1,0,1,1,12,0,3.45\n4,0,6,1.393766,0,1,0,1,1,16,0,4.03\n2,0,19,1.637053,0,1,1,0,0,17,1,5.14\n", "csv2_example": "female,Slooks,expersq,union,smllcity,wage\n1,4,900,0,1,5.73\n1,0,784,0,1,4.28\n0,3,1444,0,0,11.57\n0,0,144,0,0,8.76\n0,0,25,1,1,7.69\n0,0,36,0,1,4.03\n0,3,64,0,1,3.0\n0,0,144,0,0,7.99\n0,0,289,0,1,6.01\n0,0,49,0,0,5.16\n", "csv1_path": "infiagent/merge_test/15_1_0.csv", "csv2_path": "infiagent/merge_test/15_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/15_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/15_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNEUTRALDESCRIPTION,EVENTNUM,PERIOD\n,0,1\n,1,1\n,2,1\n,3,1\n,4,1\n,6,1\n,7,1\n,8,1\n,9,1\n\nHeader and first few lines of CSV file 2:\nVISITORDESCRIPTION,EVENTMSGTYPE,GAME_ID,PCTIMESTRING,SCOREMARGIN,EVENTNUM,SCORE,WCTIMESTRING,EVENTMSGACTIONTYPE\n,12,20200722,12:00,,0,,7:13 PM,0\n,10,20200722,12:00,,1,,7:14 PM,0\nPeeler STEAL (1 STL),5,20200722,11:46,,2,,7:15 PM,1\nPeeler Lost Ball Turnover (P1.T1),5,20200722,11:43,,3,,7:16 PM,2\nNesterovic BLOCK (1 BLK),2,20200722,11:22,,4,,7:16 PM,5\n,4,20200722,11:20,,5,,7:16 PM,0\nGarnett Lost Ball Turnover (P1.T2),5,20200722,10:57,,8,,7:16 PM,2\n,1,20200722,10:52,2,9,0 - 2,7:16 PM,5\n,6,20200722,10:38,,10,,7:17 PM,2\n\nQuestion: Combine the two tables and insert NAN values into the empty spaces.", "csv1_example": "NEUTRALDESCRIPTION,EVENTNUM,PERIOD\n,0,1\n,1,1\n,2,1\n,3,1\n,4,1\n,6,1\n,7,1\n,8,1\n,9,1\n,10,1\n", "csv2_example": "VISITORDESCRIPTION,EVENTMSGTYPE,GAME_ID,PCTIMESTRING,SCOREMARGIN,EVENTNUM,SCORE,WCTIMESTRING,EVENTMSGACTIONTYPE\n,12,20200722,12:00,,0,,7:13 PM,0\n,10,20200722,12:00,,1,,7:14 PM,0\nPeeler STEAL (1 STL),5,20200722,11:46,,2,,7:15 PM,1\nPeeler Lost Ball Turnover (P1.T1),5,20200722,11:43,,3,,7:16 PM,2\nNesterovic BLOCK (1 BLK),2,20200722,11:22,,4,,7:16 PM,5\n,4,20200722,11:20,,5,,7:16 PM,0\nGarnett Lost Ball Turnover (P1.T2),5,20200722,10:57,,8,,7:16 PM,2\n,1,20200722,10:52,2,9,0 - 2,7:16 PM,5\n,6,20200722,10:38,,10,,7:17 PM,2\nHudson Free Throw 1 of 2 (1 PTS),3,20200722,10:38,1,11,1 - 2,7:17 PM,11\n", "csv1_path": "infiagent/merge_test/7_3_0.csv", "csv2_path": "infiagent/merge_test/7_3_1.csv", "instruction": "Combine the two tables and insert NAN values into the empty spaces.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/7_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/7_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nAREA_ACR,TRUE_TIME,MEANJZH,SAVNCPP\n69.26413,2014.03.23_20:24:00_TAI,0.00286312,224887400000.0\n83.896141,2014.03.23_20:36:00_TAI,0.00309745,465108600000.0\n87.762978,2014.03.23_21:00:00_TAI,0.00307095,763678300000.0\n84.621979,2014.03.23_21:12:00_TAI,0.00341966,904570900000.0\n93.034233,2014.03.23_21:36:00_TAI,0.00270168,660597300000.0\n96.260063,2014.03.23_22:00:00_TAI,0.00170337,18206300000.0\n97.720245,2014.03.23_22:12:00_TAI,0.00244001,684728800000.0\n99.95433,2014.03.23_22:36:00_TAI,0.00262174,749637700000.0\n\nHeader and first few lines of CSV file 2:\nTRUE_TIME,MEANGBZ,MEANJZD,SHRGT45,R_VALUE,MEANSHR,TOTPOT,TOTUSJH,USFLUX,ABSNJZH\n2014.03.23_20:24:00_TAI,92.809,0.08746085,0.061,0.0,18.695,7.747525e+21,143.341,3.246502e+21,14.092\n2014.03.23_20:36:00_TAI,89.779,0.15138598,0.0,0.0,18.172,9.025444e+21,173.704,3.90834e+21,18.216\n2014.03.23_21:00:00_TAI,89.499,0.2345193,0.048,0.0,18.134,9.107749e+21,164.412,4.096817e+21,19.141\n2014.03.23_21:12:00_TAI,87.454,0.26665705,0.046,0.0,17.850,8.903345e+21,163.141,4.197154e+21,22.204\n2014.03.23_21:36:00_TAI,86.595,0.18933275,0.045,0.0,17.608,8.694996e+21,157.135,4.271472e+21,18.042\n2014.03.23_21:48:00_TAI,85.451,0.27554792,0.014,0.0,17.502,9.148247e+21,166.084,4.596958e+21,22.942\n2014.03.23_22:12:00_TAI,85.487,0.17288232,0.013,0.0,17.638,8.97838e+21,161.954,4.66428e+21,18.495\n2014.03.23_22:24:00_TAI,87.034,0.17619905,0.013,0.0,17.485,8.876519e+21,161.277,4.671479e+21,18.303\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "AREA_ACR,TRUE_TIME,MEANJZH,SAVNCPP\n69.26413,2014.03.23_20:24:00_TAI,0.00286312,224887400000.0\n83.896141,2014.03.23_20:36:00_TAI,0.00309745,465108600000.0\n87.762978,2014.03.23_21:00:00_TAI,0.00307095,763678300000.0\n84.621979,2014.03.23_21:12:00_TAI,0.00341966,904570900000.0\n93.034233,2014.03.23_21:36:00_TAI,0.00270168,660597300000.0\n96.260063,2014.03.23_22:00:00_TAI,0.00170337,18206300000.0\n97.720245,2014.03.23_22:12:00_TAI,0.00244001,684728800000.0\n99.95433,2014.03.23_22:36:00_TAI,0.00262174,749637700000.0\n101.623764,2014.03.23_22:48:00_TAI,0.00254172,752054200000.0\n102.225227,2014.03.23_23:00:00_TAI,0.00298963,1206821000000.0\n", "csv2_example": "TRUE_TIME,MEANGBZ,MEANJZD,SHRGT45,R_VALUE,MEANSHR,TOTPOT,TOTUSJH,USFLUX,ABSNJZH\n2014.03.23_20:24:00_TAI,92.809,0.08746085,0.061,0.0,18.695,7.747525e+21,143.341,3.246502e+21,14.092\n2014.03.23_20:36:00_TAI,89.779,0.15138598,0.0,0.0,18.172,9.025444e+21,173.704,3.90834e+21,18.216\n2014.03.23_21:00:00_TAI,89.499,0.2345193,0.048,0.0,18.134,9.107749e+21,164.412,4.096817e+21,19.141\n2014.03.23_21:12:00_TAI,87.454,0.26665705,0.046,0.0,17.850,8.903345e+21,163.141,4.197154e+21,22.204\n2014.03.23_21:36:00_TAI,86.595,0.18933275,0.045,0.0,17.608,8.694996e+21,157.135,4.271472e+21,18.042\n2014.03.23_21:48:00_TAI,85.451,0.27554792,0.014,0.0,17.502,9.148247e+21,166.084,4.596958e+21,22.942\n2014.03.23_22:12:00_TAI,85.487,0.17288232,0.013,0.0,17.638,8.97838e+21,161.954,4.66428e+21,18.495\n2014.03.23_22:24:00_TAI,87.034,0.17619905,0.013,0.0,17.485,8.876519e+21,161.277,4.671479e+21,18.303\n2014.03.23_22:36:00_TAI,87.682,0.18886125,0.0,0.0,17.464,8.916165e+21,163.987,4.678468e+21,19.912\n2014.03.23_22:48:00_TAI,87.929,0.18899454,0.053,0.0,17.358,8.895343e+21,157.176,4.649883e+21,19.032\n", "csv1_path": "infiagent/merge_test/6_0_0.csv", "csv2_path": "infiagent/merge_test/6_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/6_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/6_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate\nSep 17, 2017\nSep 16, 2017\nSep 15, 2017\nSep 14, 2017\nSep 13, 2017\nSep 12, 2017\nSep 11, 2017\nSep 09, 2017\nSep 08, 2017\n\nHeader and first few lines of CSV file 2:\nClose,Low,Date,Open\n109.85,105.02,Sep 16, 2017,111.11\n111.22,89.36,Sep 15, 2017,97.42\n96.71,96.71,Sep 14, 2017,115.97\n115.97,112.6,Sep 13, 2017,123.14\n125.7,120.76,Sep 11, 2017,121.88\n122.92,118.94,Sep 10, 2017,129.7\n130.05,125.21,Sep 09, 2017,128.51\n128.29,123.23,Sep 08, 2017,137.68\n137.61,132.52,Sep 07, 2017,135.88\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Date\nSep 17, 2017\nSep 16, 2017\nSep 15, 2017\nSep 14, 2017\nSep 13, 2017\nSep 12, 2017\nSep 11, 2017\nSep 09, 2017\nSep 08, 2017\nSep 07, 2017\n", "csv2_example": "Close,Low,Date,Open\n109.85,105.02,Sep 16, 2017,111.11\n111.22,89.36,Sep 15, 2017,97.42\n96.71,96.71,Sep 14, 2017,115.97\n115.97,112.6,Sep 13, 2017,123.14\n125.7,120.76,Sep 11, 2017,121.88\n122.92,118.94,Sep 10, 2017,129.7\n130.05,125.21,Sep 09, 2017,128.51\n128.29,123.23,Sep 08, 2017,137.68\n137.61,132.52,Sep 07, 2017,135.88\n131.33,128.0,Sep 02, 2017,141.11\n", "csv1_path": "infiagent/merge_test/17_2_0.csv", "csv2_path": "infiagent/merge_test/17_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/17_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/17_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,Category\n1991,Crimes Against Person\n1993,Crimes Against Person\n1994,Crimes Against Person\n1996,Crimes Against Person\n1997,Crimes Against Person\n1998,Crimes Against Person\n1999,Crimes Against Person\n2000,Crimes Against Person\n2001,Crimes Against Person\n\nHeader and first few lines of CSV file 2:\nOffense_Type,Date,Disqualifying_Offense,Age,Expungible,Count\nmisdemeanor,1992,True,14,False,1.0\nmisdemeanor,1994,True,9,False,1.0\nmisdemeanor,1996,True,9,False,3.0\nmisdemeanor,1998,True,8,False,2.0\nmisdemeanor,1999,True,9,False,3.0\nmisdemeanor,2001,True,7,False,1.0\nmisdemeanor,2002,True,8,False,1.0\nmisdemeanor,2003,True,7,False,1.0\nmisdemeanor,2004,True,8,False,1.0\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Date,Category\n1991,Crimes Against Person\n1993,Crimes Against Person\n1994,Crimes Against Person\n1996,Crimes Against Person\n1997,Crimes Against Person\n1998,Crimes Against Person\n1999,Crimes Against Person\n2000,Crimes Against Person\n2001,Crimes Against Person\n2003,Crimes Against Person\n", "csv2_example": "Offense_Type,Date,Disqualifying_Offense,Age,Expungible,Count\nmisdemeanor,1992,True,14,False,1.0\nmisdemeanor,1994,True,9,False,1.0\nmisdemeanor,1996,True,9,False,3.0\nmisdemeanor,1998,True,8,False,2.0\nmisdemeanor,1999,True,9,False,3.0\nmisdemeanor,2001,True,7,False,1.0\nmisdemeanor,2002,True,8,False,1.0\nmisdemeanor,2003,True,7,False,1.0\nmisdemeanor,2004,True,8,False,1.0\nmisdemeanor,2005,True,6,False,1.0\n", "csv1_path": "infiagent/merge_test/10_3_0.csv", "csv2_path": "infiagent/merge_test/10_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/10_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/10_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nacceleration,modelyear,weight,origin,mpg\n12.0,70,3504.0,1,18.0\n11.5,70,3693.0,1,15.0\n12.0,70,3433.0,1,16.0\n10.5,70,3449.0,1,17.0\n9.0,70,4354.0,1,14.0\n15.0,70,2372.0,3,24.0\n16.0,70,2587.0,1,21.0\n20.5,70,1835.0,2,26.0\n17.5,70,2672.0,2,25.0\n\nHeader and first few lines of CSV file 2:\nmpg,cylinders,horsepower\n18.0,8,130.0\n15.0,8,165.0\n17.0,8,140.0\n14.0,8,220.0\n24.0,4,95.0\n22.0,6,95.0\n21.0,6,85.0\n26.0,4,46.0\n10.0,8,215.0\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "acceleration,modelyear,weight,origin,mpg\n12.0,70,3504.0,1,18.0\n11.5,70,3693.0,1,15.0\n12.0,70,3433.0,1,16.0\n10.5,70,3449.0,1,17.0\n9.0,70,4354.0,1,14.0\n15.0,70,2372.0,3,24.0\n16.0,70,2587.0,1,21.0\n20.5,70,1835.0,2,26.0\n17.5,70,2672.0,2,25.0\n14.0,70,4615.0,1,10.0\n", "csv2_example": "mpg,cylinders,horsepower\n18.0,8,130.0\n15.0,8,165.0\n17.0,8,140.0\n14.0,8,220.0\n24.0,4,95.0\n22.0,6,95.0\n21.0,6,85.0\n26.0,4,46.0\n10.0,8,215.0\n9.0,8,193.0\n", "csv1_path": "infiagent/merge_test/11_2_0.csv", "csv2_path": "infiagent/merge_test/11_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/11_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/11_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\n#featureID,importance.score\n358.3677167129743_3.65612984126984,0.0670524225212391\n423.2744890715284_4.29798541001065,0.0405984341742611\n304.2993572401259_5.121302585521083,0.0341410429343644\n389.2691196723436_3.383737479270316,0.03252068176745\n332.3307817246258_5.3103554720133594,0.0322569080107531\n358.28219024838404_4.7314112935323385,0.0272377330395867\n516.3001954425159_3.6121693593314776,0.0264573012282939\n326.377650457406_5.358003633720932,0.0234222353379512\n357.27906873865504_4.755498426870746,0.0232615688365795\n\nHeader and first few lines of CSV file 2:\nstandard_indentification_level_1,_feature_id,LibraryID,#featureID\n,358.3677167129743_3.65612984126984,,358.3677167129743_3.65612984126984\n,423.2744890715284_4.29798541001065,,423.2744890715284_4.29798541001065\n,304.2993572401259_5.121302585521083,Spectral Match to Benzyldodecyldimethylammonium from NIST14,304.2993572401259_5.121302585521083\n,389.2691196723436_3.383737479270316,,389.2691196723436_3.383737479270316\n,332.3307817246258_5.3103554720133594,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,332.3307817246258_5.3103554720133594\n,516.3001954425159_3.6121693593314776,Spectral Match to Taurocholic acid from NIST14,516.3001954425159_3.6121693593314776\n,326.377650457406_5.358003633720932,,326.377650457406_5.358003633720932\nChenodeoxycholic acid,357.27906873865504_4.755498426870746,Spectral Match to Chenodeoxycholic acid from NIST14,357.27906873865504_4.755498426870746\n,358.28198237386744_5.002188993710692,,358.28198237386744_5.002188993710692\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "#featureID,importance.score\n358.3677167129743_3.65612984126984,0.0670524225212391\n423.2744890715284_4.29798541001065,0.0405984341742611\n304.2993572401259_5.121302585521083,0.0341410429343644\n389.2691196723436_3.383737479270316,0.03252068176745\n332.3307817246258_5.3103554720133594,0.0322569080107531\n358.28219024838404_4.7314112935323385,0.0272377330395867\n516.3001954425159_3.6121693593314776,0.0264573012282939\n326.377650457406_5.358003633720932,0.0234222353379512\n357.27906873865504_4.755498426870746,0.0232615688365795\n358.28198237386744_5.002188993710692,0.0218383013013502\n", "csv2_example": "standard_indentification_level_1,_feature_id,LibraryID,#featureID\n,358.3677167129743_3.65612984126984,,358.3677167129743_3.65612984126984\n,423.2744890715284_4.29798541001065,,423.2744890715284_4.29798541001065\n,304.2993572401259_5.121302585521083,Spectral Match to Benzyldodecyldimethylammonium from NIST14,304.2993572401259_5.121302585521083\n,389.2691196723436_3.383737479270316,,389.2691196723436_3.383737479270316\n,332.3307817246258_5.3103554720133594,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,332.3307817246258_5.3103554720133594\n,516.3001954425159_3.6121693593314776,Spectral Match to Taurocholic acid from NIST14,516.3001954425159_3.6121693593314776\n,326.377650457406_5.358003633720932,,326.377650457406_5.358003633720932\nChenodeoxycholic acid,357.27906873865504_4.755498426870746,Spectral Match to Chenodeoxycholic acid from NIST14,357.27906873865504_4.755498426870746\n,358.28198237386744_5.002188993710692,,358.28198237386744_5.002188993710692\n,506.27907974349694_3.788732373472948,,506.27907974349694_3.788732373472948\n", "csv1_path": "infiagent/merge_test/42_0_0.csv", "csv2_path": "infiagent/merge_test/42_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/42_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/42_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nRace,Unnamed: 0,Alignment,Hair color,Weight,Height,name\nHuman,0,good,No Hair,441.0,203.0,A-Bomb\nIcthyo Sapien,1,good,No Hair,65.0,191.0,Abe Sapien\nUngaran,2,good,No Hair,90.0,185.0,Abin Sur\nHuman / Radiation,3,bad,No Hair,441.0,203.0,Abomination\nHuman,5,bad,No Hair,122.0,193.0,Absorbing Man\nHuman,7,good,Blond,88.0,185.0,Adam Strange\n-,8,good,Blond,61.0,173.0,Agent 13\nHuman,9,good,Brown,81.0,178.0,Agent Bob\n-,11,bad,White,108.0,188.0,Air-Walker\n\nHeader and first few lines of CSV file 2:\nPublisher,Unnamed: 0,Skin color\nMarvel Comics,0,-\nDark Horse Comics,1,blue\nDC Comics,2,red\nMarvel Comics,3,-\nMarvel Comics,4,-\nMarvel Comics,5,-\nNBC - Heroes,6,-\nDC Comics,7,-\nMarvel Comics,8,-\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Race,Unnamed: 0,Alignment,Hair color,Weight,Height,name\nHuman,0,good,No Hair,441.0,203.0,A-Bomb\nIcthyo Sapien,1,good,No Hair,65.0,191.0,Abe Sapien\nUngaran,2,good,No Hair,90.0,185.0,Abin Sur\nHuman / Radiation,3,bad,No Hair,441.0,203.0,Abomination\nHuman,5,bad,No Hair,122.0,193.0,Absorbing Man\nHuman,7,good,Blond,88.0,185.0,Adam Strange\n-,8,good,Blond,61.0,173.0,Agent 13\nHuman,9,good,Brown,81.0,178.0,Agent Bob\n-,11,bad,White,108.0,188.0,Air-Walker\nCyborg,12,bad,Black,90.0,193.0,Ajax\n", "csv2_example": "Publisher,Unnamed: 0,Skin color\nMarvel Comics,0,-\nDark Horse Comics,1,blue\nDC Comics,2,red\nMarvel Comics,3,-\nMarvel Comics,4,-\nMarvel Comics,5,-\nNBC - Heroes,6,-\nDC Comics,7,-\nMarvel Comics,8,-\nMarvel Comics,9,-\n", "csv1_path": "infiagent/merge_test/39_0_0.csv", "csv2_path": "infiagent/merge_test/39_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/39_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/39_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nimportance.score,row m/z,_feature_id,#featureID\n0.0670524225212391,358.3677167,358.3677167129743_3.65612984126984,358.3677167129743_3.65612984126984\n0.0405984341742611,423.2744891,423.2744890715284_4.29798541001065,423.2744890715284_4.29798541001065\n0.0341410429343644,304.2993572,304.2993572401259_5.121302585521083,304.2993572401259_5.121302585521083\n0.03252068176745,389.2691197,389.2691196723436_3.383737479270316,389.2691196723436_3.383737479270316\n0.0272377330395867,358.2821902,358.28219024838404_4.7314112935323385,358.28219024838404_4.7314112935323385\n0.0264573012282939,516.3001954,516.3001954425159_3.6121693593314776,516.3001954425159_3.6121693593314776\n0.0218383013013502,358.28198239999995,358.28198237386744_5.002188993710692,358.28198237386744_5.002188993710692\n0.020768117594364,407.2793261000001,407.2793260908103_3.6924033502968627,407.2793260908103_3.6924033502968627\n0.0204139428354309,533.326833,533.3268329649845_3.536782531194295,533.3268329649845_3.536782531194295\n\nHeader and first few lines of CSV file 2:\nstandard_indentification_level_1,#featureID\n,358.3677167129743_3.65612984126984\n,423.2744890715284_4.29798541001065\n,304.2993572401259_5.121302585521083\n,389.2691196723436_3.383737479270316\n,332.3307817246258_5.3103554720133594\n,358.28219024838404_4.7314112935323385\n,516.3001954425159_3.6121693593314776\n,326.377650457406_5.358003633720932\nChenodeoxycholic acid,357.27906873865504_4.755498426870746\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "importance.score,row m/z,_feature_id,#featureID\n0.0670524225212391,358.3677167,358.3677167129743_3.65612984126984,358.3677167129743_3.65612984126984\n0.0405984341742611,423.2744891,423.2744890715284_4.29798541001065,423.2744890715284_4.29798541001065\n0.0341410429343644,304.2993572,304.2993572401259_5.121302585521083,304.2993572401259_5.121302585521083\n0.03252068176745,389.2691197,389.2691196723436_3.383737479270316,389.2691196723436_3.383737479270316\n0.0272377330395867,358.2821902,358.28219024838404_4.7314112935323385,358.28219024838404_4.7314112935323385\n0.0264573012282939,516.3001954,516.3001954425159_3.6121693593314776,516.3001954425159_3.6121693593314776\n0.0218383013013502,358.28198239999995,358.28198237386744_5.002188993710692,358.28198237386744_5.002188993710692\n0.020768117594364,407.2793261000001,407.2793260908103_3.6924033502968627,407.2793260908103_3.6924033502968627\n0.0204139428354309,533.326833,533.3268329649845_3.536782531194295,533.3268329649845_3.536782531194295\n0.0178239633349741,506.2791124,506.27911240204685_3.7126306003584237,506.27911240204685_3.7126306003584237\n", "csv2_example": "standard_indentification_level_1,#featureID\n,358.3677167129743_3.65612984126984\n,423.2744890715284_4.29798541001065\n,304.2993572401259_5.121302585521083\n,389.2691196723436_3.383737479270316\n,332.3307817246258_5.3103554720133594\n,358.28219024838404_4.7314112935323385\n,516.3001954425159_3.6121693593314776\n,326.377650457406_5.358003633720932\nChenodeoxycholic acid,357.27906873865504_4.755498426870746\n,358.28198237386744_5.002188993710692\n", "csv1_path": "infiagent/merge_test/42_2_0.csv", "csv2_path": "infiagent/merge_test/42_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/42_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/42_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDepartment Name\nAnimal Services\nAustin Code\nAustin Energy\nAustin Public Library\nAustin Resource Recovery\nAustin Transportation\nAustin Water\nAviation\nBuilding Services\n\nHeader and first few lines of CSV file 2:\nbudget_year_start,Department Name,github-dept-code\n10/1/2016,Austin Convention Center,CON\n10/1/2016,Austin Energy,ENE\n10/1/2016,Austin Public Library,LIB\n10/1/2016,Austin Resource Recovery,RES\n10/1/2016,Austin Transportation,TRA\n10/1/2016,Austin Water,WAT\n10/1/2016,Aviation,AVI\n10/1/2016,Building Services,BLD\n10/1/2016,Communication and Technology Management,TEC\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Department Name\nAnimal Services\nAustin Code\nAustin Energy\nAustin Public Library\nAustin Resource Recovery\nAustin Transportation\nAustin Water\nAviation\nBuilding Services\nCommunication and Technology Management\n", "csv2_example": "budget_year_start,Department Name,github-dept-code\n10/1/2016,Austin Convention Center,CON\n10/1/2016,Austin Energy,ENE\n10/1/2016,Austin Public Library,LIB\n10/1/2016,Austin Resource Recovery,RES\n10/1/2016,Austin Transportation,TRA\n10/1/2016,Austin Water,WAT\n10/1/2016,Aviation,AVI\n10/1/2016,Building Services,BLD\n10/1/2016,Communication and Technology Management,TEC\n10/1/2016,Communications and Public Information,COM\n", "csv1_path": "infiagent/merge_test/19_3_0.csv", "csv2_path": "infiagent/merge_test/19_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/19_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/19_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ncylinders,mpg,horsepower,weight,origin\n8,18.0,130.0,3504.0,1\n8,15.0,165.0,3693.0,1\n8,16.0,150.0,3433.0,1\n8,17.0,140.0,3449.0,1\n8,14.0,220.0,4354.0,1\n4,24.0,95.0,2372.0,3\n6,22.0,95.0,2833.0,1\n6,21.0,85.0,2587.0,1\n4,27.0,88.0,2130.0,3\n\nHeader and first few lines of CSV file 2:\ndisplacement,mpg,acceleration\n307.0,18.0,12.0\n350.0,15.0,11.5\n304.0,16.0,12.0\n302.0,17.0,10.5\n454.0,14.0,9.0\n113.0,24.0,15.0\n97.0,27.0,14.5\n97.0,26.0,20.5\n110.0,25.0,17.5\n\nQuestion: Combine the two tables and insert NAN values in the empty spaces.", "csv1_example": "cylinders,mpg,horsepower,weight,origin\n8,18.0,130.0,3504.0,1\n8,15.0,165.0,3693.0,1\n8,16.0,150.0,3433.0,1\n8,17.0,140.0,3449.0,1\n8,14.0,220.0,4354.0,1\n4,24.0,95.0,2372.0,3\n6,22.0,95.0,2833.0,1\n6,21.0,85.0,2587.0,1\n4,27.0,88.0,2130.0,3\n4,26.0,46.0,1835.0,2\n", "csv2_example": "displacement,mpg,acceleration\n307.0,18.0,12.0\n350.0,15.0,11.5\n304.0,16.0,12.0\n302.0,17.0,10.5\n454.0,14.0,9.0\n113.0,24.0,15.0\n97.0,27.0,14.5\n97.0,26.0,20.5\n110.0,25.0,17.5\n360.0,10.0,14.0\n", "csv1_path": "infiagent/merge_test/11_1_0.csv", "csv2_path": "infiagent/merge_test/11_1_1.csv", "instruction": "Combine the two tables and insert NAN values in the empty spaces.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/11_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/11_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nnum. busy overflows,avg. abandonment time,timestamp,num. calls transferred\n0,00:00:00,Apr 13  2017 12:00:00 AM,0\n0,00:00:00,Apr 13  2017 12:15:00 AM,0\n0,00:00:00,Apr 13  2017 12:30:00 AM,0\n0,00:00:00,Apr 13  2017 12:45:00 AM,0\n0,00:00:00,Apr 13  2017 1:30:00 AM,0\n0,00:00:00,Apr 13  2017 1:45:00 AM,0\n0,00:00:00,Apr 13  2017 2:00:00 AM,0\n0,00:00:00,Apr 13  2017 2:30:00 AM,0\n0,00:00:00,Apr 13  2017 2:45:00 AM,0\n\nHeader and first few lines of CSV file 2:\navg. num. agents talking,num. calls abandoned,timestamp,avg. wait time\n0.0,0,Apr 13  2017 12:00:00 AM,00:00:00\n0.0,0,Apr 13  2017 12:30:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:00:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:15:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:30:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:45:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:00:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:15:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:30:00 AM,00:00:00\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "num. busy overflows,avg. abandonment time,timestamp,num. calls transferred\n0,00:00:00,Apr 13  2017 12:00:00 AM,0\n0,00:00:00,Apr 13  2017 12:15:00 AM,0\n0,00:00:00,Apr 13  2017 12:30:00 AM,0\n0,00:00:00,Apr 13  2017 12:45:00 AM,0\n0,00:00:00,Apr 13  2017 1:30:00 AM,0\n0,00:00:00,Apr 13  2017 1:45:00 AM,0\n0,00:00:00,Apr 13  2017 2:00:00 AM,0\n0,00:00:00,Apr 13  2017 2:30:00 AM,0\n0,00:00:00,Apr 13  2017 2:45:00 AM,0\n0,00:00:00,Apr 13  2017 3:00:00 AM,0\n", "csv2_example": "avg. num. agents talking,num. calls abandoned,timestamp,avg. wait time\n0.0,0,Apr 13  2017 12:00:00 AM,00:00:00\n0.0,0,Apr 13  2017 12:30:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:00:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:15:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:30:00 AM,00:00:00\n0.0,0,Apr 13  2017 1:45:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:00:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:15:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:30:00 AM,00:00:00\n0.0,0,Apr 13  2017 2:45:00 AM,00:00:00\n", "csv1_path": "infiagent/merge_test/4_0_0.csv", "csv2_path": "infiagent/merge_test/4_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/4_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/4_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ndestination_station_code,fleet_number,origin_station_code,start_time,destination_station,sch_dep_time,station,act_dep_time,route_code\nROT,3088.0,MAM,21/09/15 05:01:00,Roll Test,21/09/15 05:01:00,Machine Mist,21/09/15 04:59:19,4\nATP,3088.0,ROT,21/09/15 05:50:00,Attempt Pin,21/09/15 05:50:00,Roll Test,21/09/15 05:50:38,68\nROT,3088.0,ATP,21/09/15 07:20:00,Roll Test,21/09/15 07:20:00,Attempt Pin,21/09/15 07:21:38,1\nJAV,3063.0,STSTb,21/09/15 05:06:30,Jail Vest,21/09/15 05:06:30,Step Scarecrow Turnback,21/09/15 05:00:01,73\nCHATb,3063.0,JAV,21/09/15 05:49:00,Children Cast Turnback,21/09/15 05:49:00,Jail Vest,21/09/15 05:48:54,20\nJAV,3063.0,CHATb,21/09/15 06:38:00,Jail Vest,21/09/15 06:38:00,Children Cast Turnback,21/09/15 06:36:58,74\nBRB,3017.0,CRT,21/09/15 07:32:00,Bridge Bottle,21/09/15 07:32:00,Crib Team,21/09/15 07:38:27,58\nSTSTb,,CLG,21/09/15 23:58:00,Step Scarecrow Turnback,21/09/15 23:58:00,Clouds Goose,,83\nMAM,3099.0,SKH,22/09/15 00:20:00,Machine Mist,22/09/15 00:20:00,Skin Shape,22/09/15 00:21:21,91\n\nHeader and first few lines of CSV file 2:\nstation_code,origin_station,station_type,act_arr_time,platform,origin_station_code\nMAM,Machine Mist,Passenger,,Outbound,MAM\nROT,Roll Test,Passenger,,Platform A,ROT\nATP,Attempt Pin,Passenger,,Platform A,ATP\nSTSTb,Step Scarecrow Turnback,Vehicle,,Platform 3,STSTb\nJAV,Jail Vest,Passenger,,Platform B,JAV\nCHATb,Children Cast Turnback,Vehicle,,Platform 1,CHATb\nBRB,Bridge Bottle,Passenger,,Platform A,BRB\nCRT,Crib Team,Passenger,,Platform B,CRT\nCLG,Clouds Goose,Passenger,,Outbound,CLG\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "destination_station_code,fleet_number,origin_station_code,start_time,destination_station,sch_dep_time,station,act_dep_time,route_code\nROT,3088.0,MAM,21/09/15 05:01:00,Roll Test,21/09/15 05:01:00,Machine Mist,21/09/15 04:59:19,4\nATP,3088.0,ROT,21/09/15 05:50:00,Attempt Pin,21/09/15 05:50:00,Roll Test,21/09/15 05:50:38,68\nROT,3088.0,ATP,21/09/15 07:20:00,Roll Test,21/09/15 07:20:00,Attempt Pin,21/09/15 07:21:38,1\nJAV,3063.0,STSTb,21/09/15 05:06:30,Jail Vest,21/09/15 05:06:30,Step Scarecrow Turnback,21/09/15 05:00:01,73\nCHATb,3063.0,JAV,21/09/15 05:49:00,Children Cast Turnback,21/09/15 05:49:00,Jail Vest,21/09/15 05:48:54,20\nJAV,3063.0,CHATb,21/09/15 06:38:00,Jail Vest,21/09/15 06:38:00,Children Cast Turnback,21/09/15 06:36:58,74\nBRB,3017.0,CRT,21/09/15 07:32:00,Bridge Bottle,21/09/15 07:32:00,Crib Team,21/09/15 07:38:27,58\nSTSTb,,CLG,21/09/15 23:58:00,Step Scarecrow Turnback,21/09/15 23:58:00,Clouds Goose,,83\nMAM,3099.0,SKH,22/09/15 00:20:00,Machine Mist,22/09/15 00:20:00,Skin Shape,22/09/15 00:21:21,91\nSPT,3052.0,SKSTb,21/09/15 05:49:00,Spiders Toothbrush,21/09/15 05:49:00,Skate Stone Turnback,,7\n", "csv2_example": "station_code,origin_station,station_type,act_arr_time,platform,origin_station_code\nMAM,Machine Mist,Passenger,,Outbound,MAM\nROT,Roll Test,Passenger,,Platform A,ROT\nATP,Attempt Pin,Passenger,,Platform A,ATP\nSTSTb,Step Scarecrow Turnback,Vehicle,,Platform 3,STSTb\nJAV,Jail Vest,Passenger,,Platform B,JAV\nCHATb,Children Cast Turnback,Vehicle,,Platform 1,CHATb\nBRB,Bridge Bottle,Passenger,,Platform A,BRB\nCRT,Crib Team,Passenger,,Platform B,CRT\nCLG,Clouds Goose,Passenger,,Outbound,CLG\nSKH,Skin Shape,Passenger,,Inbound Siding,SKH\n", "csv1_path": "infiagent/merge_test/2_1_0.csv", "csv2_path": "infiagent/merge_test/2_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/2_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/2_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,Age\n1,34\n3,71\n5,68\n6,77\n8,87\n9,66\n10,41\n11,30\n13,57\n\nHeader and first few lines of CSV file 2:\nRating,Balance,Unnamed: 0,Married,Education,Gender,Income\n681,964,4,No,11,Female,148.924\n357,331,5,Yes,16,Male,55.882\n569,1151,6,No,10,Male,80.18\n259,203,7,No,12,Female,20.996\n512,872,8,No,9,Male,71.408\n266,279,9,No,13,Female,15.125\n491,1350,10,Yes,19,Female,71.061\n589,1407,11,Yes,14,Male,63.095\n138,0,12,No,16,Male,15.045\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Unnamed: 0,Age\n1,34\n3,71\n5,68\n6,77\n8,87\n9,66\n10,41\n11,30\n13,57\n14,49\n", "csv2_example": "Rating,Balance,Unnamed: 0,Married,Education,Gender,Income\n681,964,4,No,11,Female,148.924\n357,331,5,Yes,16,Male,55.882\n569,1151,6,No,10,Male,80.18\n259,203,7,No,12,Female,20.996\n512,872,8,No,9,Male,71.408\n266,279,9,No,13,Female,15.125\n491,1350,10,Yes,19,Female,71.061\n589,1407,11,Yes,14,Male,63.095\n138,0,12,No,16,Male,15.045\n394,204,13,Yes,7,Female,80.616\n", "csv1_path": "infiagent/merge_test/24_3_0.csv", "csv2_path": "infiagent/merge_test/24_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/24_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/24_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nArt and Performance,Public Administration,Math and Statistics,Communications and Journalism,Year,Foreign Languages,Education,English\n59.9,65.5,39.0,35.5,1971,73.9,74.14920369,64.55648516\n60.4,62.6,40.2,36.6,1972,74.6,73.55451996,63.6642632\n60.2,64.3,40.9,38.4,1973,74.9,73.50181443,62.94150212\n61.9,66.1,41.8,40.5,1974,75.3,73.33681143,62.41341209\n60.9,63.0,40.7,41.5,1975,75.0,72.80185448,61.64720641\n61.3,65.6,41.5,44.3,1976,74.4,72.16652471,62.14819377\n62.0,69.3,41.1,46.9,1977,74.3,72.45639481,62.72306675\n63.2,73.3,42.3,52.3,1979,74.2,73.82114234,65.08838972\n63.3,74.7,43.2,56.4,1981,73.9,75.84512345,65.83832154\n\nHeader and first few lines of CSV file 2:\nBiology,Physical Sciences,Year,Business,Computer Science\n29.08836297,13.8,1970,9.064438975,13.6\n29.39440285,14.9,1971,9.503186594,13.6\n29.81022105,14.8,1972,10.5589621,14.9\n31.14791477,16.5,1973,12.80460152,16.4\n34.44990213,19.1,1975,19.68624931,19.8\n36.07287146,20.0,1976,23.4300375,23.9\n38.33138629,21.3,1977,27.16342715,25.7\n40.11249564,22.5,1978,30.52751868,28.1\n42.06555109,23.7,1979,33.62163381,30.2\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Art and Performance,Public Administration,Math and Statistics,Communications and Journalism,Year,Foreign Languages,Education,English\n59.9,65.5,39.0,35.5,1971,73.9,74.14920369,64.55648516\n60.4,62.6,40.2,36.6,1972,74.6,73.55451996,63.6642632\n60.2,64.3,40.9,38.4,1973,74.9,73.50181443,62.94150212\n61.9,66.1,41.8,40.5,1974,75.3,73.33681143,62.41341209\n60.9,63.0,40.7,41.5,1975,75.0,72.80185448,61.64720641\n61.3,65.6,41.5,44.3,1976,74.4,72.16652471,62.14819377\n62.0,69.3,41.1,46.9,1977,74.3,72.45639481,62.72306675\n63.2,73.3,42.3,52.3,1979,74.2,73.82114234,65.08838972\n63.3,74.7,43.2,56.4,1981,73.9,75.84512345,65.83832154\n63.1,76.8,44.0,58.0,1982,72.7,75.84364914,65.84735212\n", "csv2_example": "Biology,Physical Sciences,Year,Business,Computer Science\n29.08836297,13.8,1970,9.064438975,13.6\n29.39440285,14.9,1971,9.503186594,13.6\n29.81022105,14.8,1972,10.5589621,14.9\n31.14791477,16.5,1973,12.80460152,16.4\n34.44990213,19.1,1975,19.68624931,19.8\n36.07287146,20.0,1976,23.4300375,23.9\n38.33138629,21.3,1977,27.16342715,25.7\n40.11249564,22.5,1978,30.52751868,28.1\n42.06555109,23.7,1979,33.62163381,30.2\n43.99925716,24.6,1980,36.76572529,32.5\n", "csv1_path": "infiagent/merge_test/47_0_0.csv", "csv2_path": "infiagent/merge_test/47_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/47_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/47_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nTIME,R_VALUE,MEANGBZ,TRUE_TIME\n11.6,0.0,92.809,2014.03.23_20:24:00_TAI\n11.8,0.0,89.779,2014.03.23_20:36:00_TAI\n12.0,0.0,89.566,2014.03.23_20:48:00_TAI\n12.2,0.0,89.499,2014.03.23_21:00:00_TAI\n13.0,0.0,85.451,2014.03.23_21:48:00_TAI\n13.2,2.07,85.236,2014.03.23_22:00:00_TAI\n13.4,0.0,85.487,2014.03.23_22:12:00_TAI\n13.8,0.0,87.682,2014.03.23_22:36:00_TAI\n\nHeader and first few lines of CSV file 2:\nMEANJZD,TOTPOT,MEANGBT,TOTUSJZ,TRUE_TIME,SHRGT45,TOTUSJH,MEANSHR,USFLUX,MEANPOT,MEANGAM\n0.08746085,7.747525e+21,93.013,3141588000000.0,2014.03.23_20:24:00_TAI,0.061,143.341,18.695,3.246502e+21,1185.247,21.786\n0.139126,9.235995e+21,89.552,3790352000000.0,2014.03.23_20:48:00_TAI,0.016,174.009,18.322,4.041844e+21,1132.3,21.797\n0.2345193,9.107749e+21,89.355,3604093000000.0,2014.03.23_21:00:00_TAI,0.048,164.412,18.134,4.096817e+21,1100.275,21.654\n0.26665705,8.903345e+21,87.089,3622492000000.0,2014.03.23_21:12:00_TAI,0.046,163.141,17.850,4.197154e+21,1032.512,21.732\n0.18933275,8.694996e+21,86.248,3534322000000.0,2014.03.23_21:36:00_TAI,0.045,157.135,17.608,4.271472e+21,980.416,21.737\n0.27554792,9.148247e+21,84.835,3809157000000.0,2014.03.23_21:48:00_TAI,0.014,166.084,17.502,4.596958e+21,943.1147,21.888\n0.00457878,9.159047e+21,84.526,3761321000000.0,2014.03.23_22:00:00_TAI,0.026,162.369,17.662,4.684366e+21,906.2604,22.237\n0.17288232,8.97838e+21,84.563,3733276000000.0,2014.03.23_22:12:00_TAI,0.013,161.954,17.638,4.66428e+21,891.9001,22.024\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "TIME,R_VALUE,MEANGBZ,TRUE_TIME\n11.6,0.0,92.809,2014.03.23_20:24:00_TAI\n11.8,0.0,89.779,2014.03.23_20:36:00_TAI\n12.0,0.0,89.566,2014.03.23_20:48:00_TAI\n12.2,0.0,89.499,2014.03.23_21:00:00_TAI\n13.0,0.0,85.451,2014.03.23_21:48:00_TAI\n13.2,2.07,85.236,2014.03.23_22:00:00_TAI\n13.4,0.0,85.487,2014.03.23_22:12:00_TAI\n13.8,0.0,87.682,2014.03.23_22:36:00_TAI\n14.0,0.0,87.929,2014.03.23_22:48:00_TAI\n14.2,0.0,85.809,2014.03.23_23:00:00_TAI\n", "csv2_example": "MEANJZD,TOTPOT,MEANGBT,TOTUSJZ,TRUE_TIME,SHRGT45,TOTUSJH,MEANSHR,USFLUX,MEANPOT,MEANGAM\n0.08746085,7.747525e+21,93.013,3141588000000.0,2014.03.23_20:24:00_TAI,0.061,143.341,18.695,3.246502e+21,1185.247,21.786\n0.139126,9.235995e+21,89.552,3790352000000.0,2014.03.23_20:48:00_TAI,0.016,174.009,18.322,4.041844e+21,1132.3,21.797\n0.2345193,9.107749e+21,89.355,3604093000000.0,2014.03.23_21:00:00_TAI,0.048,164.412,18.134,4.096817e+21,1100.275,21.654\n0.26665705,8.903345e+21,87.089,3622492000000.0,2014.03.23_21:12:00_TAI,0.046,163.141,17.850,4.197154e+21,1032.512,21.732\n0.18933275,8.694996e+21,86.248,3534322000000.0,2014.03.23_21:36:00_TAI,0.045,157.135,17.608,4.271472e+21,980.416,21.737\n0.27554792,9.148247e+21,84.835,3809157000000.0,2014.03.23_21:48:00_TAI,0.014,166.084,17.502,4.596958e+21,943.1147,21.888\n0.00457878,9.159047e+21,84.526,3761321000000.0,2014.03.23_22:00:00_TAI,0.026,162.369,17.662,4.684366e+21,906.2604,22.237\n0.17288232,8.97838e+21,84.563,3733276000000.0,2014.03.23_22:12:00_TAI,0.013,161.954,17.638,4.66428e+21,891.9001,22.024\n0.17619905,8.876519e+21,86.069,3714417000000.0,2014.03.23_22:24:00_TAI,0.013,161.277,17.485,4.671479e+21,880.9678,22.004\n0.18899454,8.895343e+21,86.978,3606528000000.0,2014.03.23_22:48:00_TAI,0.053,157.176,17.358,4.649883e+21,894.3887,21.622\n", "csv1_path": "infiagent/merge_test/6_3_0.csv", "csv2_path": "infiagent/merge_test/6_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/6_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/6_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsadness,fear,contempt,index,surprise,name,timestamp\n0.018,0.0,0.001,0,0.0,alckmin,00h-00m-00s\n0.002,0.0,0.001,12845,0.0,boulos,00h-00m-00s\n0.046,0.0,0.01,19125,0.0,ciro,00h-00m-00s\n0.243,0.0,0.002,25592,0.0,marina,00h-00m-00s\n0.0,0.0,0.003,6397,0.0,alvaro,00h-00m-00s\n0.043,0.0,0.016,19126,0.0,ciro,00h-00m-01s\n0.001,0.0,0.0,6398,0.001,alvaro,00h-00m-01s\n0.001,0.0,0.0,12846,0.0,boulos,00h-00m-01s\n0.022,0.0,0.0,1,0.0,alckmin,00h-00m-01s\n\nHeader and first few lines of CSV file 2:\nneutral,anger,index,happiness\n0.996,0.001,12845,0.001\n0.626,0.0,25592,0.126\n0.563,0.0,25593,0.432\n0.939,0.002,19126,0.0\n0.303,0.0,6398,0.694\n0.998,0.0,12846,0.0\n0.977,0.0,1,0.0\n0.998,0.0,12847,0.0\n0.961,0.001,2,0.0\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "sadness,fear,contempt,index,surprise,name,timestamp\n0.018,0.0,0.001,0,0.0,alckmin,00h-00m-00s\n0.002,0.0,0.001,12845,0.0,boulos,00h-00m-00s\n0.046,0.0,0.01,19125,0.0,ciro,00h-00m-00s\n0.243,0.0,0.002,25592,0.0,marina,00h-00m-00s\n0.0,0.0,0.003,6397,0.0,alvaro,00h-00m-00s\n0.043,0.0,0.016,19126,0.0,ciro,00h-00m-01s\n0.001,0.0,0.0,6398,0.001,alvaro,00h-00m-01s\n0.001,0.0,0.0,12846,0.0,boulos,00h-00m-01s\n0.022,0.0,0.0,1,0.0,alckmin,00h-00m-01s\n0.112,0.0,0.0,32062,0.0,meirelles,00h-00m-01s\n", "csv2_example": "neutral,anger,index,happiness\n0.996,0.001,12845,0.001\n0.626,0.0,25592,0.126\n0.563,0.0,25593,0.432\n0.939,0.002,19126,0.0\n0.303,0.0,6398,0.694\n0.998,0.0,12846,0.0\n0.977,0.0,1,0.0\n0.998,0.0,12847,0.0\n0.961,0.001,2,0.0\n0.942,0.0,6399,0.056\n", "csv1_path": "infiagent/merge_test/9_1_0.csv", "csv2_path": "infiagent/merge_test/9_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/9_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/9_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNo. of cases_max,Country,No. of cases_median,No. of deaths_max\n801000.0,Afghanistan,630308,510.0\n6661000.0,Angola,4615605,16600.0\n,Argentina,0,\n,Armenia,0,\n,Azerbaijan,0,\n36000.0,Bangladesh,32924,130.0\n,Belize,7,\n6552000.0,Benin,4111699,8920.0\n4200.0,Botswana,2989,20.0\n\nHeader and first few lines of CSV file 2:\nNo. of deaths,Country,No. of deaths_min,No. of cases_min,WHO Region\n298[110-510],Afghanistan,110.0,495000.0,Eastern Mediterranean\n0,Algeria,,,Africa\n13316[9970-16600],Angola,9970.0,3106000.0,Africa\n0,Argentina,,,Americas\n0,Armenia,,,Europe\n0,Azerbaijan,,,Europe\n76[3-130],Bangladesh,3.0,30000.0,South-East Asia\n0,Belize,,,Americas\n7328[5740-8920],Benin,5740.0,2774000.0,Africa\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "No. of cases_max,Country,No. of cases_median,No. of deaths_max\n801000.0,Afghanistan,630308,510.0\n6661000.0,Angola,4615605,16600.0\n,Argentina,0,\n,Armenia,0,\n,Azerbaijan,0,\n36000.0,Bangladesh,32924,130.0\n,Belize,7,\n6552000.0,Benin,4111699,8920.0\n4200.0,Botswana,2989,20.0\n236000.0,Brazil,217928,\n", "csv2_example": "No. of deaths,Country,No. of deaths_min,No. of cases_min,WHO Region\n298[110-510],Afghanistan,110.0,495000.0,Eastern Mediterranean\n0,Algeria,,,Africa\n13316[9970-16600],Angola,9970.0,3106000.0,Africa\n0,Argentina,,,Americas\n0,Armenia,,,Europe\n0,Azerbaijan,,,Europe\n76[3-130],Bangladesh,3.0,30000.0,South-East Asia\n0,Belize,,,Americas\n7328[5740-8920],Benin,5740.0,2774000.0,Africa\n0,Bhutan,,,South-East Asia\n", "csv1_path": "infiagent/merge_test/32_2_0.csv", "csv2_path": "infiagent/merge_test/32_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/32_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/32_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nTime\n09:15:59\n09:17:59\n09:19:59\n09:20:59\n09:21:59\n09:23:59\n09:24:59\n09:25:59\n09:26:59\n\nHeader and first few lines of CSV file 2:\nClose,High,Time\n317.7,319.4,09:15:59\n318.0,318.2,09:16:59\n318.55,318.85,09:17:59\n320.25,320.4,09:20:59\n320.05,320.3,09:21:59\n319.6,320.15,09:22:59\n319.4,319.65,09:23:59\n318.35,319.7,09:25:59\n319.05,319.15,09:26:59\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Time\n09:15:59\n09:17:59\n09:19:59\n09:20:59\n09:21:59\n09:23:59\n09:24:59\n09:25:59\n09:26:59\n09:27:59\n", "csv2_example": "Close,High,Time\n317.7,319.4,09:15:59\n318.0,318.2,09:16:59\n318.55,318.85,09:17:59\n320.25,320.4,09:20:59\n320.05,320.3,09:21:59\n319.6,320.15,09:22:59\n319.4,319.65,09:23:59\n318.35,319.7,09:25:59\n319.05,319.15,09:26:59\n319.65,319.7,09:27:59\n", "csv1_path": "infiagent/merge_test/49_2_0.csv", "csv2_path": "infiagent/merge_test/49_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/49_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/49_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nVolume,Time,Low\n143354,09:15:59,316.05\n52695,09:16:59,317.7\n47179,09:17:59,318.0\n44745,09:18:59,318.5\n57892,09:19:59,319.2\n67482,09:20:59,319.6\n56590,09:21:59,319.95\n52413,09:22:59,319.6\n56305,09:23:59,319.15\n\nHeader and first few lines of CSV file 2:\nOpen,High,Time\n319.25,319.4,09:15:59\n317.7,318.2,09:16:59\n318.0,318.85,09:17:59\n318.65,319.4,09:18:59\n319.3,319.85,09:19:59\n319.6,320.4,09:20:59\n320.25,320.3,09:21:59\n320.15,320.15,09:22:59\n319.65,319.65,09:23:59\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Volume,Time,Low\n143354,09:15:59,316.05\n52695,09:16:59,317.7\n47179,09:17:59,318.0\n44745,09:18:59,318.5\n57892,09:19:59,319.2\n67482,09:20:59,319.6\n56590,09:21:59,319.95\n52413,09:22:59,319.6\n56305,09:23:59,319.15\n36525,09:24:59,319.45\n", "csv2_example": "Open,High,Time\n319.25,319.4,09:15:59\n317.7,318.2,09:16:59\n318.0,318.85,09:17:59\n318.65,319.4,09:18:59\n319.3,319.85,09:19:59\n319.6,320.4,09:20:59\n320.25,320.3,09:21:59\n320.15,320.15,09:22:59\n319.65,319.65,09:23:59\n319.5,320.05,09:24:59\n", "csv1_path": "infiagent/merge_test/49_0_0.csv", "csv2_path": "infiagent/merge_test/49_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/49_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/49_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0\n0\n1\n2\n4\n5\n6\n7\n8\n9\n\nHeader and first few lines of CSV file 2:\nHigh,Close,Unnamed: 0,Volume\n90.61,90.0,0,36875013\n90.67,90.1,1,24159683\n90.28,90.14,2,25621164\n90.79,88.35,3,36599736\n89.78,89.6,4,24271531\n88.13,88.08,5,17808877\n88.19,87.82,6,18652201\n88.73,88.22,7,19484317\n88.58,88.28,8,22113049\n\nQuestion: Combine the contents of two tables, retaining only those rows that undergo a successful fusion.", "csv1_example": "Unnamed: 0\n0\n1\n2\n4\n5\n6\n7\n8\n9\n10\n", "csv2_example": "High,Close,Unnamed: 0,Volume\n90.61,90.0,0,36875013\n90.67,90.1,1,24159683\n90.28,90.14,2,25621164\n90.79,88.35,3,36599736\n89.78,89.6,4,24271531\n88.13,88.08,5,17808877\n88.19,87.82,6,18652201\n88.73,88.22,7,19484317\n88.58,88.28,8,22113049\n88.41,88.19,9,23407110\n", "csv1_path": "infiagent/merge_test/44_3_0.csv", "csv2_path": "infiagent/merge_test/44_3_1.csv", "instruction": "Combine the contents of two tables, retaining only those rows that undergo a successful fusion.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/44_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/44_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ncountry,gdpPercap_2007,gdpPercap_1962,gdpPercap_2002,gdpPercap_1972,gdpPercap_1982\nAfghanistan,974.5803384,853.10071,726.7340548,739.9811058,978.0114388\nBahrain,29796.04834,12753.27514,23403.55927,18268.65839,19211.14731\nBangladesh,1391.253792,686.3415538,1136.39043,630.2336265,676.9818656\nCambodia,1713.778686,496.9136476,896.2260153,421.6240257,624.4754784\nChina,4959.114854,487.6740183,3119.280896,676.9000921,962.4213805\nHong Kong China,39724.97867,4692.648272,30209.01516,8315.928145,14560.53051\nIndia,2452.210407,658.3471509,1746.769454,724.032527,855.7235377\nIndonesia,3540.651564,849.2897701,2873.91287,1111.107907,1516.872988\nIran,11605.71449,4187.329802,9240.761975,9613.818607,7608.334602\n\nHeader and first few lines of CSV file 2:\ngdpPercap_1957,gdpPercap_1977,gdpPercap_1967,country\n820.8530296,786.11336,836.1971382,Afghanistan\n11635.79945,19340.10196,14804.6727,Bahrain\n661.6374577,659.8772322,721.1860862,Bangladesh\n434.0383364,524.9721832,523.4323142,Cambodia\n575.9870009,741.2374699,612.7056934,China\n590.061996,813.337323,700.7706107,India\n858.9002707,1382.702056,762.4317721,Indonesia\n3290.257643,11888.59508,5906.731805,Iran\n6229.333562,14688.23507,8931.459811,Iraq\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "country,gdpPercap_2007,gdpPercap_1962,gdpPercap_2002,gdpPercap_1972,gdpPercap_1982\nAfghanistan,974.5803384,853.10071,726.7340548,739.9811058,978.0114388\nBahrain,29796.04834,12753.27514,23403.55927,18268.65839,19211.14731\nBangladesh,1391.253792,686.3415538,1136.39043,630.2336265,676.9818656\nCambodia,1713.778686,496.9136476,896.2260153,421.6240257,624.4754784\nChina,4959.114854,487.6740183,3119.280896,676.9000921,962.4213805\nHong Kong China,39724.97867,4692.648272,30209.01516,8315.928145,14560.53051\nIndia,2452.210407,658.3471509,1746.769454,724.032527,855.7235377\nIndonesia,3540.651564,849.2897701,2873.91287,1111.107907,1516.872988\nIran,11605.71449,4187.329802,9240.761975,9613.818607,7608.334602\nIraq,4471.061906,8341.737815,4390.717312,9576.037596,14517.90711\n", "csv2_example": "gdpPercap_1957,gdpPercap_1977,gdpPercap_1967,country\n820.8530296,786.11336,836.1971382,Afghanistan\n11635.79945,19340.10196,14804.6727,Bahrain\n661.6374577,659.8772322,721.1860862,Bangladesh\n434.0383364,524.9721832,523.4323142,Cambodia\n575.9870009,741.2374699,612.7056934,China\n590.061996,813.337323,700.7706107,India\n858.9002707,1382.702056,762.4317721,Indonesia\n3290.257643,11888.59508,5906.731805,Iran\n6229.333562,14688.23507,8931.459811,Iraq\n5385.278451,13306.61921,8393.741404,Israel\n", "csv1_path": "infiagent/merge_test/36_3_0.csv", "csv2_path": "infiagent/merge_test/36_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/36_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/36_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,High\nSep 17, 2017,110.94\nSep 16, 2017,116.01\nSep 15, 2017,113.75\nSep 14, 2017,117.38\nSep 13, 2017,123.7\nSep 12, 2017,130.91\nSep 11, 2017,128.2\nSep 10, 2017,129.7\nSep 09, 2017,131.51\n\nHeader and first few lines of CSV file 2:\nDate,Volume\nSep 17, 2017,5,350,380\nSep 16, 2017,5,683,580\nSep 15, 2017,8,539,660\nSep 14, 2017,6,367,800\nSep 13, 2017,6,315,510\nSep 11, 2017,7,083,720\nSep 09, 2017,6,225,680\nSep 08, 2017,7,051,050\nSep 05, 2017,8,537,600\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Date,High\nSep 17, 2017,110.94\nSep 16, 2017,116.01\nSep 15, 2017,113.75\nSep 14, 2017,117.38\nSep 13, 2017,123.7\nSep 12, 2017,130.91\nSep 11, 2017,128.2\nSep 10, 2017,129.7\nSep 09, 2017,131.51\nSep 08, 2017,139.78\n", "csv2_example": "Date,Volume\nSep 17, 2017,5,350,380\nSep 16, 2017,5,683,580\nSep 15, 2017,8,539,660\nSep 14, 2017,6,367,800\nSep 13, 2017,6,315,510\nSep 11, 2017,7,083,720\nSep 09, 2017,6,225,680\nSep 08, 2017,7,051,050\nSep 05, 2017,8,537,600\nSep 04, 2017,30,395,600\n", "csv1_path": "infiagent/merge_test/17_0_0.csv", "csv2_path": "infiagent/merge_test/17_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/17_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/17_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ndist,description,Unnamed: 0\n8,moderate rain,0\n8,moderate rain,1\n8,moderate rain,2\n8,moderate rain,3\n8,moderate rain,4\n8,moderate rain,5\n8,moderate rain,6\n8,moderate rain,8\n8,moderate rain,10\n\nHeader and first few lines of CSV file 2:\ntemp,humidity,day,Unnamed: 0\n32.18000000000001,54,2015-07-24 11:40:51,0\n32.370000000000005,62,2015-07-24 12:41:34,1\n32.79000000000002,75,2015-07-24 13:40:46,2\n32.75,79,2015-07-24 14:39:40,3\n32.72000000000003,70,2015-07-24 15:39:48,4\n32.09000000000003,74,2015-07-24 16:39:38,5\n31.840000000000032,37,2015-07-24 17:42:24,6\n31.400000000000038,74,2015-07-24 18:40:43,7\n29.350000000000023,62,2015-07-24 21:40:19,10\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "dist,description,Unnamed: 0\n8,moderate rain,0\n8,moderate rain,1\n8,moderate rain,2\n8,moderate rain,3\n8,moderate rain,4\n8,moderate rain,5\n8,moderate rain,6\n8,moderate rain,8\n8,moderate rain,10\n8,moderate rain,13\n", "csv2_example": "temp,humidity,day,Unnamed: 0\n32.18000000000001,54,2015-07-24 11:40:51,0\n32.370000000000005,62,2015-07-24 12:41:34,1\n32.79000000000002,75,2015-07-24 13:40:46,2\n32.75,79,2015-07-24 14:39:40,3\n32.72000000000003,70,2015-07-24 15:39:48,4\n32.09000000000003,74,2015-07-24 16:39:38,5\n31.840000000000032,37,2015-07-24 17:42:24,6\n31.400000000000038,74,2015-07-24 18:40:43,7\n29.350000000000023,62,2015-07-24 21:40:19,10\n28.680000000000007,57,2015-07-24 22:39:38,11\n", "csv1_path": "infiagent/merge_test/50_0_0.csv", "csv2_path": "infiagent/merge_test/50_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/50_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/50_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhumidity,dt,city,day,wind_deg,Unnamed: 0\n54,1437730851,Ravenna,2015-07-24 11:40:51,330.003,0\n62,1437734494,Ravenna,2015-07-24 12:41:34,20.0,1\n75,1437738046,Ravenna,2015-07-24 13:40:46,70.0,2\n79,1437741580,Ravenna,2015-07-24 14:39:40,70.0,3\n70,1437745188,Ravenna,2015-07-24 15:39:48,10.0,4\n74,1437748778,Ravenna,2015-07-24 16:39:38,80.0,5\n37,1437752544,Ravenna,2015-07-24 17:42:24,126.0,6\n74,1437756043,Ravenna,2015-07-24 18:40:43,100.0,7\n59,1437763244,Ravenna,2015-07-24 20:40:44,130.0,9\n\nHeader and first few lines of CSV file 2:\nwind_speed,pressure,temp,Unnamed: 0\n2.11,1010,32.18000000000001,0\n2.6,1010,32.370000000000005,1\n3.6,1009,32.79000000000002,2\n5.1,1009,32.75,3\n3.1,1008,32.72000000000003,4\n2.06,1007,31.840000000000032,6\n1.5,1007,30.52000000000004,9\n2.11,1008,28.680000000000007,11\n1.5,1007,27.78000000000003,12\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "humidity,dt,city,day,wind_deg,Unnamed: 0\n54,1437730851,Ravenna,2015-07-24 11:40:51,330.003,0\n62,1437734494,Ravenna,2015-07-24 12:41:34,20.0,1\n75,1437738046,Ravenna,2015-07-24 13:40:46,70.0,2\n79,1437741580,Ravenna,2015-07-24 14:39:40,70.0,3\n70,1437745188,Ravenna,2015-07-24 15:39:48,10.0,4\n74,1437748778,Ravenna,2015-07-24 16:39:38,80.0,5\n37,1437752544,Ravenna,2015-07-24 17:42:24,126.0,6\n74,1437756043,Ravenna,2015-07-24 18:40:43,100.0,7\n59,1437763244,Ravenna,2015-07-24 20:40:44,130.0,9\n62,1437766819,Ravenna,2015-07-24 21:40:19,160.0,10\n", "csv2_example": "wind_speed,pressure,temp,Unnamed: 0\n2.11,1010,32.18000000000001,0\n2.6,1010,32.370000000000005,1\n3.6,1009,32.79000000000002,2\n5.1,1009,32.75,3\n3.1,1008,32.72000000000003,4\n2.06,1007,31.840000000000032,6\n1.5,1007,30.52000000000004,9\n2.11,1008,28.680000000000007,11\n1.5,1007,27.78000000000003,12\n0.5,1007,27.230000000000015,13\n", "csv1_path": "infiagent/merge_test/50_3_0.csv", "csv2_path": "infiagent/merge_test/50_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/50_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/50_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,pressure,dist,description\n0,1010,8,moderate rain\n1,1010,8,moderate rain\n3,1009,8,moderate rain\n4,1008,8,moderate rain\n5,1008,8,moderate rain\n7,1007,8,moderate rain\n8,1007,8,moderate rain\n9,1007,8,moderate rain\n11,1008,8,moderate rain\n\nHeader and first few lines of CSV file 2:\nhumidity,day,dt,wind_deg,Unnamed: 0\n54,2015-07-24 11:40:51,1437730851,330.003,0\n70,2015-07-24 15:39:48,1437745188,10.0,4\n74,2015-07-24 16:39:38,1437748778,80.0,5\n37,2015-07-24 17:42:24,1437752544,126.0,6\n47,2015-07-24 19:39:29,1437759569,135.0,8\n59,2015-07-24 20:40:44,1437763244,130.0,9\n69,2015-07-25 00:39:16,1437777556,180.0,13\n59,2015-07-25 01:41:18,1437781278,193.502,14\n60,2015-07-25 02:40:51,1437784851,193.502,15\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Unnamed: 0,pressure,dist,description\n0,1010,8,moderate rain\n1,1010,8,moderate rain\n3,1009,8,moderate rain\n4,1008,8,moderate rain\n5,1008,8,moderate rain\n7,1007,8,moderate rain\n8,1007,8,moderate rain\n9,1007,8,moderate rain\n11,1008,8,moderate rain\n12,1007,8,moderate rain\n", "csv2_example": "humidity,day,dt,wind_deg,Unnamed: 0\n54,2015-07-24 11:40:51,1437730851,330.003,0\n70,2015-07-24 15:39:48,1437745188,10.0,4\n74,2015-07-24 16:39:38,1437748778,80.0,5\n37,2015-07-24 17:42:24,1437752544,126.0,6\n47,2015-07-24 19:39:29,1437759569,135.0,8\n59,2015-07-24 20:40:44,1437763244,130.0,9\n69,2015-07-25 00:39:16,1437777556,180.0,13\n59,2015-07-25 01:41:18,1437781278,193.502,14\n60,2015-07-25 02:40:51,1437784851,193.502,15\n62,2015-07-25 03:40:08,1437788408,224.004,16\n", "csv1_path": "infiagent/merge_test/50_2_0.csv", "csv2_path": "infiagent/merge_test/50_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/50_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/50_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nEQ7,Unnamed: 0,EQ2,EX8\n-0.0192616021801591,1,-0.208033710010569,0.200716505553366\n-0.0115054095747666,3,0.0990492914551702,0.307269712205153\n-0.025916462300798,4,0.203353027030218,0.323376592280424\n-0.0260687742401788,5,0.54078497298146,0.209389440978512\n-0.0049911450904791,6,0.474591056252882,0.039647704800665\n-0.0295016510277619,7,0.262601414175049,-0.0582325664259767\n-0.0303335085428418,8,0.27117763331324,-0.073100455726226\n-0.0260570579371495,9,-0.0264534083276613,-0.0179653662378013\n\nHeader and first few lines of CSV file 2:\nNZ,EQ3,EX7,EX6,EQ6,Unnamed: 0,EQ4\n0.362830325973427,-0.0209965026800568,-0.0563269342288962,-0.0272364598603708,0.0508805382290177,1,-0.0642619047779573\n0.3381555792905,-0.0419930053601136,-0.0850724761803679,-0.0095903027677361,0.0769627933528486,2,-0.0162345864702208\n-0.36930248144376,-0.0678689769728596,-0.103877970914975,-0.0061377937713511,0.0588109536382906,4,-0.0987604010271765\n-0.383436113240686,-0.0344520079186847,-0.0825650768824202,-0.0118919754319929,0.0137460533760731,5,-0.040586466175552\n-0.208110086071965,0.0344520079186847,-0.0462077870621788,-0.0149608723176685,-0.020795311517649,7,-0.071026315807216\n-0.313322994728527,0.0360784975629145,-0.0204173942832883,-0.06943379203841,0.0169182195397822,8,-0.0033822055146293\n-0.11687746493838,0.0156734456625776,0.066356531420687,0.0337578657424314,0.0803111909700971,10,-0.0493802005135883\n0.110178944111268,0.0122726036791881,0.0916991743249439,0.0602271013813833,0.0807391816429785,11,0.0385571428667744\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "EQ7,Unnamed: 0,EQ2,EX8\n-0.0192616021801591,1,-0.208033710010569,0.200716505553366\n-0.0115054095747666,3,0.0990492914551702,0.307269712205153\n-0.025916462300798,4,0.203353027030218,0.323376592280424\n-0.0260687742401788,5,0.54078497298146,0.209389440978512\n-0.0049911450904791,6,0.474591056252882,0.039647704800665\n-0.0295016510277619,7,0.262601414175049,-0.0582325664259767\n-0.0303335085428418,8,0.27117763331324,-0.073100455726226\n-0.0260570579371495,9,-0.0264534083276613,-0.0179653662378013\n-0.0709187822363157,10,-0.548757233025412,0.0291162832129883\n-0.0457755959354511,11,-0.37240117750767,0.0557545848759351\n", "csv2_example": "NZ,EQ3,EX7,EX6,EQ6,Unnamed: 0,EQ4\n0.362830325973427,-0.0209965026800568,-0.0563269342288962,-0.0272364598603708,0.0508805382290177,1,-0.0642619047779573\n0.3381555792905,-0.0419930053601136,-0.0850724761803679,-0.0095903027677361,0.0769627933528486,2,-0.0162345864702208\n-0.36930248144376,-0.0678689769728596,-0.103877970914975,-0.0061377937713511,0.0588109536382906,4,-0.0987604010271765\n-0.383436113240686,-0.0344520079186847,-0.0825650768824202,-0.0118919754319929,0.0137460533760731,5,-0.040586466175552\n-0.208110086071965,0.0344520079186847,-0.0462077870621788,-0.0149608723176685,-0.020795311517649,7,-0.071026315807216\n-0.313322994728527,0.0360784975629145,-0.0204173942832883,-0.06943379203841,0.0169182195397822,8,-0.0033822055146293\n-0.11687746493838,0.0156734456625776,0.066356531420687,0.0337578657424314,0.0803111909700971,10,-0.0493802005135883\n0.110178944111268,0.0122726036791881,0.0916991743249439,0.0602271013813833,0.0807391816429785,11,0.0385571428667744\n0.16968771629427,0.0007393134746498,0.0966244229459126,0.0590762650492549,0.0382422254180494,12,0.0121759398526656\n-0.180592131128098,0.0039922927631093,0.0935797237984047,0.0602271013813833,-0.0595662312963166,13,-0.020293233087776\n", "csv1_path": "infiagent/merge_test/31_3_0.csv", "csv2_path": "infiagent/merge_test/31_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/31_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/31_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\npeople_vaccinated_per_hundred,country,date,daily_vaccinations_raw,vaccines\n,Algeria,2021-01-29,,Sputnik V\n0.75,Andorra,2021-01-25,,Pfizer/BioNTech\n0.0,Anguilla,2021-02-04,,Oxford/AstraZeneca\n,Argentina,2020-12-29,,Sputnik V\n0.08,Austria,2021-01-10,,Moderna, Oxford/AstraZeneca, Pfizer/BioNTech\n0.0,Azerbaijan,2021-01-17,,Oxford/AstraZeneca, Sputnik V\n2.29,Bahrain,2020-12-23,,Pfizer/BioNTech, Sinopharm/Beijing\n0.0,Bangladesh,2021-01-26,,Oxford/AstraZeneca\n0.0,Belgium,2020-12-28,,Moderna, Oxford/AstraZeneca, Pfizer/BioNTech\n\nHeader and first few lines of CSV file 2:\nsource_website,people_fully_vaccinated,daily_vaccinations_per_million,people_vaccinated,daily_vaccinations,source_name,total_vaccinations_per_hundred,country\nhttps://www.aps.dz/regions/116777-blida-covid-19-trente-vaccines-au-matin-du-1er-jour-de-la-campagne,,,,,Ministry of Health,0.0,Algeria\nhttps://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,,,576.0,,Government of Andorra,0.75,Andorra\nhttps://www.facebook.com/MinistryofHealthAnguilla/posts/1391067271247845,,,0.0,,Ministry of Health,0.0,Anguilla\nhttp://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,,,,,Ministry of Health,0.0,Argentina\nhttps://info.gesundheitsministerium.gv.at/opendata/,,,6784.0,,Ministry of Health,,Austria\nhttps://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,,,0.0,,Government of Azerbaijan,0.0,Azerbaijan\nhttps://twitter.com/MOH_Bahrain/status/1362144927535267841,,,38965.0,,Ministry of Health,2.29,Bahrain\nhttps://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,,,0.0,,Directorate General of Health Services,0.0,Bangladesh\nhttps://epistat.wiv-isp.be/covid/,,,298.0,,Sciensano,0.0,Belgium\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "people_vaccinated_per_hundred,country,date,daily_vaccinations_raw,vaccines\n,Algeria,2021-01-29,,Sputnik V\n0.75,Andorra,2021-01-25,,Pfizer/BioNTech\n0.0,Anguilla,2021-02-04,,Oxford/AstraZeneca\n,Argentina,2020-12-29,,Sputnik V\n0.08,Austria,2021-01-10,,Moderna, Oxford/AstraZeneca, Pfizer/BioNTech\n0.0,Azerbaijan,2021-01-17,,Oxford/AstraZeneca, Sputnik V\n2.29,Bahrain,2020-12-23,,Pfizer/BioNTech, Sinopharm/Beijing\n0.0,Bangladesh,2021-01-26,,Oxford/AstraZeneca\n0.0,Belgium,2020-12-28,,Moderna, Oxford/AstraZeneca, Pfizer/BioNTech\n0.0,Bermuda,2021-01-10,,Pfizer/BioNTech\n", "csv2_example": "source_website,people_fully_vaccinated,daily_vaccinations_per_million,people_vaccinated,daily_vaccinations,source_name,total_vaccinations_per_hundred,country\nhttps://www.aps.dz/regions/116777-blida-covid-19-trente-vaccines-au-matin-du-1er-jour-de-la-campagne,,,,,Ministry of Health,0.0,Algeria\nhttps://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,,,576.0,,Government of Andorra,0.75,Andorra\nhttps://www.facebook.com/MinistryofHealthAnguilla/posts/1391067271247845,,,0.0,,Ministry of Health,0.0,Anguilla\nhttp://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,,,,,Ministry of Health,0.0,Argentina\nhttps://info.gesundheitsministerium.gv.at/opendata/,,,6784.0,,Ministry of Health,,Austria\nhttps://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,,,0.0,,Government of Azerbaijan,0.0,Azerbaijan\nhttps://twitter.com/MOH_Bahrain/status/1362144927535267841,,,38965.0,,Ministry of Health,2.29,Bahrain\nhttps://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,,,0.0,,Directorate General of Health Services,0.0,Bangladesh\nhttps://epistat.wiv-isp.be/covid/,,,298.0,,Sciensano,0.0,Belgium\nhttps://www.gov.bm/articles/covid-19-update-premiers-remarks-16-february-2021,,,0.0,,Government of Bermuda,0.0,Bermuda\n", "csv1_path": "infiagent/merge_test/22_2_0.csv", "csv2_path": "infiagent/merge_test/22_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/22_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/22_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nbmi,age\n27.9,19\n33.77,18\n33.0,28\n22.705,33\n28.88,32\n33.44,46\n27.74,37\n26.22,25\n26.29,62\n\nHeader and first few lines of CSV file 2:\nregion,children,smoker,sex,age\nsouthwest,0,yes,female,19\nsoutheast,1,no,male,18\nsoutheast,3,no,male,28\nnorthwest,0,no,male,33\nnorthwest,0,no,male,32\nsoutheast,1,no,female,46\nnorthwest,3,no,female,37\nnorthwest,0,no,female,60\nsoutheast,0,yes,male,27\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "bmi,age\n27.9,19\n33.77,18\n33.0,28\n22.705,33\n28.88,32\n33.44,46\n27.74,37\n26.22,25\n26.29,62\n34.4,23\n", "csv2_example": "region,children,smoker,sex,age\nsouthwest,0,yes,female,19\nsoutheast,1,no,male,18\nsoutheast,3,no,male,28\nnorthwest,0,no,male,33\nnorthwest,0,no,male,32\nsoutheast,1,no,female,46\nnorthwest,3,no,female,37\nnorthwest,0,no,female,60\nsoutheast,0,yes,male,27\nnortheast,1,no,female,52\n", "csv1_path": "infiagent/merge_test/43_2_0.csv", "csv2_path": "infiagent/merge_test/43_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/43_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/43_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nyear,pop\n1952,8425333\n1962,10267083\n1967,11537966\n1972,13079460\n1987,13867957\n1997,22227415\n2002,25268405\n2007,31889923\n\nHeader and first few lines of CSV file 2:\ncountry,continent,year\nafghanistan,asia,1952\nafghanistan,asia,1957\nafghanistan,asia,1967\nafghanistan,asia,1972\nafghanistan,asia,1977\nafghanistan,asia,1982\nafghanistan,asia,1987\nafghanistan,asia,1997\nafghanistan,asia,2002\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "year,pop\n1952,8425333\n1962,10267083\n1967,11537966\n1972,13079460\n1987,13867957\n1997,22227415\n2002,25268405\n2007,31889923\n", "csv2_example": "country,continent,year\nafghanistan,asia,1952\nafghanistan,asia,1957\nafghanistan,asia,1967\nafghanistan,asia,1972\nafghanistan,asia,1977\nafghanistan,asia,1982\nafghanistan,asia,1987\nafghanistan,asia,1997\nafghanistan,asia,2002\nafghanistan,asia,2007\n", "csv1_path": "infiagent/merge_test/35_2_0.csv", "csv2_path": "infiagent/merge_test/35_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/35_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/35_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nEVENTNUM,VISITORDESCRIPTION,EVENTMSGACTIONTYPE,SCORE,WCTIMESTRING,GAME_ID\n0,,0,,7:13 PM,20200722\n1,,0,,7:14 PM,20200722\n3,Peeler Lost Ball Turnover (P1.T1),2,,7:16 PM,20200722\n4,Nesterovic BLOCK (1 BLK),5,,7:16 PM,20200722\n6,,1,,7:16 PM,20200722\n8,Garnett Lost Ball Turnover (P1.T2),2,,7:16 PM,20200722\n10,,2,,7:17 PM,20200722\n11,Hudson Free Throw 1 of 2 (1 PTS),11,1 - 2,7:17 PM,20200722\n12,Hudson Free Throw 2 of 2 (2 PTS),12,2 - 2,7:17 PM,20200722\n\nHeader and first few lines of CSV file 2:\nPCTIMESTRING,EVENTNUM,NEUTRALDESCRIPTION,EVENTMSGTYPE\n12:00,0,,12\n12:00,1,,10\n11:46,2,,5\n11:43,3,,5\n11:20,5,,4\n11:16,6,,2\n10:57,8,,5\n10:52,9,,1\n10:38,11,,3\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "EVENTNUM,VISITORDESCRIPTION,EVENTMSGACTIONTYPE,SCORE,WCTIMESTRING,GAME_ID\n0,,0,,7:13 PM,20200722\n1,,0,,7:14 PM,20200722\n3,Peeler Lost Ball Turnover (P1.T1),2,,7:16 PM,20200722\n4,Nesterovic BLOCK (1 BLK),5,,7:16 PM,20200722\n6,,1,,7:16 PM,20200722\n8,Garnett Lost Ball Turnover (P1.T2),2,,7:16 PM,20200722\n10,,2,,7:17 PM,20200722\n11,Hudson Free Throw 1 of 2 (1 PTS),11,1 - 2,7:17 PM,20200722\n12,Hudson Free Throw 2 of 2 (2 PTS),12,2 - 2,7:17 PM,20200722\n13,,5,2 - 4,7:18 PM,20200722\n", "csv2_example": "PCTIMESTRING,EVENTNUM,NEUTRALDESCRIPTION,EVENTMSGTYPE\n12:00,0,,12\n12:00,1,,10\n11:46,2,,5\n11:43,3,,5\n11:20,5,,4\n11:16,6,,2\n10:57,8,,5\n10:52,9,,1\n10:38,11,,3\n10:38,12,,3\n", "csv1_path": "infiagent/merge_test/7_0_0.csv", "csv2_path": "infiagent/merge_test/7_0_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/7_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/7_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nPsychology,English,Social Sciences and History,Foreign Languages,Computer Science,Year,Engineering\n47.6,63.6642632,36.1,74.6,14.9,1972,1.2\n50.4,62.94150212,36.4,74.9,16.4,1973,1.6\n52.6,62.41341209,37.3,75.3,18.9,1974,2.2\n54.5,61.64720641,37.7,75.0,19.8,1975,3.2\n59.0,62.72306675,40.5,74.3,25.7,1977,6.8\n61.3,63.61912216,41.8,74.3,28.1,1978,8.4\n63.3,65.08838972,43.6,74.2,30.2,1979,9.4\n66.9,65.83832154,44.6,73.9,34.8,1981,11.6\n67.5,65.84735212,44.6,72.7,36.3,1982,12.4\n\nHeader and first few lines of CSV file 2:\nEducation,Year,Physical Sciences\n74.53532758,1970,13.8\n74.14920369,1971,14.9\n73.55451996,1972,14.8\n73.50181443,1973,16.5\n73.33681143,1974,18.2\n72.80185448,1975,19.1\n72.16652471,1976,20.0\n72.45639481,1977,21.3\n73.19282134,1978,22.5\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Psychology,English,Social Sciences and History,Foreign Languages,Computer Science,Year,Engineering\n47.6,63.6642632,36.1,74.6,14.9,1972,1.2\n50.4,62.94150212,36.4,74.9,16.4,1973,1.6\n52.6,62.41341209,37.3,75.3,18.9,1974,2.2\n54.5,61.64720641,37.7,75.0,19.8,1975,3.2\n59.0,62.72306675,40.5,74.3,25.7,1977,6.8\n61.3,63.61912216,41.8,74.3,28.1,1978,8.4\n63.3,65.08838972,43.6,74.2,30.2,1979,9.4\n66.9,65.83832154,44.6,73.9,34.8,1981,11.6\n67.5,65.84735212,44.6,72.7,36.3,1982,12.4\n68.2,65.74986233,44.1,72.1,36.8,1984,13.5\n", "csv2_example": "Education,Year,Physical Sciences\n74.53532758,1970,13.8\n74.14920369,1971,14.9\n73.55451996,1972,14.8\n73.50181443,1973,16.5\n73.33681143,1974,18.2\n72.80185448,1975,19.1\n72.16652471,1976,20.0\n72.45639481,1977,21.3\n73.19282134,1978,22.5\n73.82114234,1979,23.7\n", "csv1_path": "infiagent/merge_test/47_2_0.csv", "csv2_path": "infiagent/merge_test/47_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/47_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/47_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nTrust (Government Corruption),Region,Happiness Score,Dystopia Residual,Standard Error,Family,Country\n0.41978,Western Europe,7.587,2.51738,0.03411,1.34951,Switzerland\n0.14145,Western Europe,7.561,2.70201,0.04884,1.40223,Iceland\n0.48357,Western Europe,7.527,2.49204,0.03328,1.36058,Denmark\n0.36503,Western Europe,7.522,2.46531,0.0388,1.33095,Norway\n0.32957,North America,7.427,2.45176,0.03553,1.32261,Canada\n0.41372,Western Europe,7.406,2.61955,0.0314,1.31826,Finland\n0.31814,Western Europe,7.378,2.4657,0.02799,1.28017,Netherlands\n0.43844,Western Europe,7.364,2.37119,0.03157,1.28907,Sweden\n0.42922,Australia and New Zealand,7.286,2.26425,0.03371,1.31967,New Zealand\n\nHeader and first few lines of CSV file 2:\nEconomy (GDP per Capita),Happiness Rank,Country\n1.39651,1,Switzerland\n1.30232,2,Iceland\n1.32548,3,Denmark\n1.459,4,Norway\n1.32629,5,Canada\n1.29025,6,Finland\n1.32944,7,Netherlands\n1.33171,8,Sweden\n1.25018,9,New Zealand\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Trust (Government Corruption),Region,Happiness Score,Dystopia Residual,Standard Error,Family,Country\n0.41978,Western Europe,7.587,2.51738,0.03411,1.34951,Switzerland\n0.14145,Western Europe,7.561,2.70201,0.04884,1.40223,Iceland\n0.48357,Western Europe,7.527,2.49204,0.03328,1.36058,Denmark\n0.36503,Western Europe,7.522,2.46531,0.0388,1.33095,Norway\n0.32957,North America,7.427,2.45176,0.03553,1.32261,Canada\n0.41372,Western Europe,7.406,2.61955,0.0314,1.31826,Finland\n0.31814,Western Europe,7.378,2.4657,0.02799,1.28017,Netherlands\n0.43844,Western Europe,7.364,2.37119,0.03157,1.28907,Sweden\n0.42922,Australia and New Zealand,7.286,2.26425,0.03371,1.31967,New Zealand\n0.35637,Australia and New Zealand,7.284,2.26646,0.04083,1.30923,Australia\n", "csv2_example": "Economy (GDP per Capita),Happiness Rank,Country\n1.39651,1,Switzerland\n1.30232,2,Iceland\n1.32548,3,Denmark\n1.459,4,Norway\n1.32629,5,Canada\n1.29025,6,Finland\n1.32944,7,Netherlands\n1.33171,8,Sweden\n1.25018,9,New Zealand\n1.33358,10,Australia\n", "csv1_path": "infiagent/merge_test/3_3_0.csv", "csv2_path": "infiagent/merge_test/3_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/3_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/3_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ndeaths,dates_active,Unnamed: 0\n0.0,April 19 – 21,0\n2.0,June 19 – 20,1\n2.0,June 20 – 23,2\n0.0,July 5 – 7,3\n0.0,July 17 – 18,4\n0.0,July 30 – August 1,5\n0.0,August 7 – 10,6\n2.0,August 12 – 17,7\n107.0,August 17 – September 1,8\n\nHeader and first few lines of CSV file 2:\nmax_sust_wind,damage_USD,year,Unnamed: 0,damage_imputed\n43.4488,0.0,2017,0,0\n43.4488,3000000.0,2017,1,0\n52.13856,25000000.0,2017,2,0\n26.06928,0.0,2017,3,0\n43.4488,0.0,2017,4,0\n52.13856,10000000.0,2017,5,0\n73.86296,15000000.0,2017,6,0\n95.58736,0.0,2017,7,0\n112.96688,125000000000.0,2017,8,0\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "deaths,dates_active,Unnamed: 0\n0.0,April 19 – 21,0\n2.0,June 19 – 20,1\n2.0,June 20 – 23,2\n0.0,July 5 – 7,3\n0.0,July 17 – 18,4\n0.0,July 30 – August 1,5\n0.0,August 7 – 10,6\n2.0,August 12 – 17,7\n107.0,August 17 – September 1,8\n134.0,August 30 – September 12,9\n", "csv2_example": "max_sust_wind,damage_USD,year,Unnamed: 0,damage_imputed\n43.4488,0.0,2017,0,0\n43.4488,3000000.0,2017,1,0\n52.13856,25000000.0,2017,2,0\n26.06928,0.0,2017,3,0\n43.4488,0.0,2017,4,0\n52.13856,10000000.0,2017,5,0\n73.86296,15000000.0,2017,6,0\n95.58736,0.0,2017,7,0\n112.96688,125000000000.0,2017,8,0\n156.41568,64760000000.00001,2017,9,0\n", "csv1_path": "infiagent/merge_test/21_3_0.csv", "csv2_path": "infiagent/merge_test/21_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/21_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/21_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate\n2014-01-02\n2014-01-03\n2014-01-08\n2014-01-09\n2014-01-10\n2014-01-14\n2014-01-15\n2014-01-16\n2014-01-17\n\nHeader and first few lines of CSV file 2:\nClose,Date,Open,Volume\n79.02,2014-01-02,83.349,58791957\n77.7,2014-01-06,80.619,103359151\n77.15,2014-01-07,81.64800000000001,79432766\n77.64,2014-01-08,80.8185,64686685\n76.65,2014-01-09,82.0155,69905199\n76.13,2014-01-10,80.97600000000001,76320664\n76.53,2014-01-13,79.485,94860843\n78.06,2014-01-14,80.7345,83734371\n79.62,2014-01-15,83.0235,98472619\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Date\n2014-01-02\n2014-01-03\n2014-01-08\n2014-01-09\n2014-01-10\n2014-01-14\n2014-01-15\n2014-01-16\n2014-01-17\n2014-01-22\n", "csv2_example": "Close,Date,Open,Volume\n79.02,2014-01-02,83.349,58791957\n77.7,2014-01-06,80.619,103359151\n77.15,2014-01-07,81.64800000000001,79432766\n77.64,2014-01-08,80.8185,64686685\n76.65,2014-01-09,82.0155,69905199\n76.13,2014-01-10,80.97600000000001,76320664\n76.53,2014-01-13,79.485,94860843\n78.06,2014-01-14,80.7345,83734371\n79.62,2014-01-15,83.0235,98472619\n78.44,2014-01-21,81.144,82255544\n", "csv1_path": "infiagent/merge_test/29_3_0.csv", "csv2_path": "infiagent/merge_test/29_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/29_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/29_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nStart Date,Hospital Name,Provider Number,Number of Discharges\n07/01/2010,FROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,242\n07/01/2010,PROVIDENCE HOSPITAL,90006,247\n07/01/2010,BEAUFORT COUNTY MEMORIAL HOSPITAL,420067,586\n07/01/2010,ADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,965\n07/01/2010,BRAZOSPORT REGIONAL HEALTH SYSTEM,450072,149\n07/01/2010,WESTERN MISSOURI MEDICAL CENTER,260097,141\n07/01/2010,SAINT AGNES HOSPITAL,210011,390\n07/01/2010,MERCY HOSPITAL JEFFERSON,260023,178\n07/01/2010,ONSLOW MEMORIAL HOSPITAL,340042,98\n\nHeader and first few lines of CSV file 2:\nHospital Name,Expected Readmission Rate,End Date,Measure Name\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,5.6,06/30/2013,READM-30-HIP-KNEE-HRRP\nPROVIDENCE HOSPITAL,5.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nBEAUFORT COUNTY MEMORIAL HOSPITAL,4.8,06/30/2013,READM-30-HIP-KNEE-HRRP\nBRAZOSPORT REGIONAL HEALTH SYSTEM,5.4,06/30/2013,READM-30-HIP-KNEE-HRRP\nWESTERN MISSOURI MEDICAL CENTER,5.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nMERCY HOSPITAL JEFFERSON,6.1,06/30/2013,READM-30-HIP-KNEE-HRRP\nONSLOW MEMORIAL HOSPITAL,5.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,6.1,06/30/2013,READM-30-HIP-KNEE-HRRP\nMERCY ST ANNE HOSPITAL,6.3,06/30/2013,READM-30-HIP-KNEE-HRRP\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Start Date,Hospital Name,Provider Number,Number of Discharges\n07/01/2010,FROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,242\n07/01/2010,PROVIDENCE HOSPITAL,90006,247\n07/01/2010,BEAUFORT COUNTY MEMORIAL HOSPITAL,420067,586\n07/01/2010,ADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,965\n07/01/2010,BRAZOSPORT REGIONAL HEALTH SYSTEM,450072,149\n07/01/2010,WESTERN MISSOURI MEDICAL CENTER,260097,141\n07/01/2010,SAINT AGNES HOSPITAL,210011,390\n07/01/2010,MERCY HOSPITAL JEFFERSON,260023,178\n07/01/2010,ONSLOW MEMORIAL HOSPITAL,340042,98\n07/01/2010,FAUQUIER HOSPITAL,490023,256\n", "csv2_example": "Hospital Name,Expected Readmission Rate,End Date,Measure Name\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,5.6,06/30/2013,READM-30-HIP-KNEE-HRRP\nPROVIDENCE HOSPITAL,5.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nBEAUFORT COUNTY MEMORIAL HOSPITAL,4.8,06/30/2013,READM-30-HIP-KNEE-HRRP\nBRAZOSPORT REGIONAL HEALTH SYSTEM,5.4,06/30/2013,READM-30-HIP-KNEE-HRRP\nWESTERN MISSOURI MEDICAL CENTER,5.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nMERCY HOSPITAL JEFFERSON,6.1,06/30/2013,READM-30-HIP-KNEE-HRRP\nONSLOW MEMORIAL HOSPITAL,5.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,6.1,06/30/2013,READM-30-HIP-KNEE-HRRP\nMERCY ST ANNE HOSPITAL,6.3,06/30/2013,READM-30-HIP-KNEE-HRRP\nRESTON HOSPITAL CENTER,4.6,06/30/2013,READM-30-HIP-KNEE-HRRP\n", "csv1_path": "infiagent/merge_test/20_1_0.csv", "csv2_path": "infiagent/merge_test/20_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/20_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/20_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nAge,Date\n15,1991\n14,1992\n9,1994\n9,1996\n9,1997\n8,1998\n4,2000\n8,2002\n7,2003\n\nHeader and first few lines of CSV file 2:\nDate,Expungible,Count,Disqualifying_Offense\n1991,False,1.0,True\n1992,False,1.0,True\n1993,False,4.0,True\n1994,False,1.0,True\n1995,False,2.0,True\n1996,False,3.0,True\n1997,False,2.0,True\n1998,False,2.0,True\n1999,False,3.0,True\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Age,Date\n15,1991\n14,1992\n9,1994\n9,1996\n9,1997\n8,1998\n4,2000\n8,2002\n7,2003\n6,2005\n", "csv2_example": "Date,Expungible,Count,Disqualifying_Offense\n1991,False,1.0,True\n1992,False,1.0,True\n1993,False,4.0,True\n1994,False,1.0,True\n1995,False,2.0,True\n1996,False,3.0,True\n1997,False,2.0,True\n1998,False,2.0,True\n1999,False,3.0,True\n2000,False,1.0,True\n", "csv1_path": "infiagent/merge_test/10_1_0.csv", "csv2_path": "infiagent/merge_test/10_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/10_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/10_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nwage,married,female\n5.73,1,1\n4.28,1,1\n7.96,0,1\n11.57,1,0\n11.42,1,0\n3.91,1,1\n8.76,1,0\n7.69,0,0\n5.0,0,1\n\nHeader and first few lines of CSV file 2:\ngoodhlth,belavg,service,union,wage,black,abvavg,educ\n1,0,1,0,5.73,0,1,14\n1,0,0,0,4.28,0,0,12\n1,0,0,0,7.96,0,1,10\n1,0,1,0,11.57,0,0,16\n1,0,0,0,11.42,0,0,16\n0,0,0,0,3.91,0,0,12\n1,0,0,0,8.76,0,0,16\n1,0,0,1,7.69,0,1,16\n1,0,0,0,5.0,0,0,16\n\nQuestion: Combine the two tables, retaining only the rows that have been successfully combined.", "csv1_example": "wage,married,female\n5.73,1,1\n4.28,1,1\n7.96,0,1\n11.57,1,0\n11.42,1,0\n3.91,1,1\n8.76,1,0\n7.69,0,0\n5.0,0,1\n3.89,0,1\n", "csv2_example": "goodhlth,belavg,service,union,wage,black,abvavg,educ\n1,0,1,0,5.73,0,1,14\n1,0,0,0,4.28,0,0,12\n1,0,0,0,7.96,0,1,10\n1,0,1,0,11.57,0,0,16\n1,0,0,0,11.42,0,0,16\n0,0,0,0,3.91,0,0,12\n1,0,0,0,8.76,0,0,16\n1,0,0,1,7.69,0,1,16\n1,0,0,0,5.0,0,0,16\n1,0,0,0,3.89,0,0,12\n", "csv1_path": "infiagent/merge_test/15_0_0.csv", "csv2_path": "infiagent/merge_test/15_0_1.csv", "instruction": "Combine the two tables, retaining only the rows that have been successfully combined.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/15_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/15_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nclass1,class7,class9,ID\n0.0,0.0,0.0,1\n0.0,1.0,0.0,5\n0.0,1.0,0.0,10\n0.0,0.0,0.0,12\n1.0,0.0,0.0,13\n0.0,1.0,0.0,14\n0.0,1.0,0.0,21\n0.0,1.0,0.0,24\n0.0,1.0,0.0,26\n\nHeader and first few lines of CSV file 2:\nclass2,class6,ID\n0.0,0.0,1\n0.0,0.0,2\n0.0,0.0,5\n0.0,0.0,10\n1.0,0.0,12\n0.0,0.0,13\n0.0,0.0,14\n0.0,0.0,21\n0.0,0.0,24\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "class1,class7,class9,ID\n0.0,0.0,0.0,1\n0.0,1.0,0.0,5\n0.0,1.0,0.0,10\n0.0,0.0,0.0,12\n1.0,0.0,0.0,13\n0.0,1.0,0.0,14\n0.0,1.0,0.0,21\n0.0,1.0,0.0,24\n0.0,1.0,0.0,26\n0.0,0.0,0.0,29\n", "csv2_example": "class2,class6,ID\n0.0,0.0,1\n0.0,0.0,2\n0.0,0.0,5\n0.0,0.0,10\n1.0,0.0,12\n0.0,0.0,13\n0.0,0.0,14\n0.0,0.0,21\n0.0,0.0,24\n0.0,0.0,26\n", "csv1_path": "infiagent/merge_test/48_2_0.csv", "csv2_path": "infiagent/merge_test/48_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/48_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/48_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nz,carat,cut,color\n2.43,0.23,Ideal,E\n2.31,0.21,Premium,E\n2.63,0.29,Premium,I\n2.75,0.31,Good,J\n2.48,0.24,Very Good,J\n2.53,0.26,Very Good,H\n2.49,0.22,Fair,E\n2.73,0.3,Good,J\n2.27,0.2,Premium,E\n\nHeader and first few lines of CSV file 2:\ncarat,price\n0.23,326\n0.21,326\n0.29,334\n0.31,335\n0.26,337\n0.22,337\n0.3,339\n0.2,345\n0.32,345\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "z,carat,cut,color\n2.43,0.23,Ideal,E\n2.31,0.21,Premium,E\n2.63,0.29,Premium,I\n2.75,0.31,Good,J\n2.48,0.24,Very Good,J\n2.53,0.26,Very Good,H\n2.49,0.22,Fair,E\n2.73,0.3,Good,J\n2.27,0.2,Premium,E\n2.68,0.32,Premium,E\n", "csv2_example": "carat,price\n0.23,326\n0.21,326\n0.29,334\n0.31,335\n0.26,337\n0.22,337\n0.3,339\n0.2,345\n0.32,345\n0.33,403\n", "csv1_path": "infiagent/merge_test/28_2_0.csv", "csv2_path": "infiagent/merge_test/28_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/28_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/28_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nCountry,No. of deaths\nAfghanistan,298[110-510]\nAlgeria,0\nAngola,13316[9970-16600]\nArgentina,0\nArmenia,0\nAzerbaijan,0\nBangladesh,76[3-130]\nBelize,0\nBenin,7328[5740-8920]\n\nHeader and first few lines of CSV file 2:\nYear,WHO Region,Country,No. of deaths_median\n2017,Eastern Mediterranean,Afghanistan,298\n2017,Africa,Algeria,0\n2017,Africa,Angola,13316\n2017,Americas,Argentina,0\n2017,Europe,Armenia,0\n2017,Europe,Azerbaijan,0\n2017,South-East Asia,Bangladesh,76\n2017,Americas,Belize,0\n2017,Africa,Benin,7328\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Country,No. of deaths\nAfghanistan,298[110-510]\nAlgeria,0\nAngola,13316[9970-16600]\nArgentina,0\nArmenia,0\nAzerbaijan,0\nBangladesh,76[3-130]\nBelize,0\nBenin,7328[5740-8920]\nBhutan,0\n", "csv2_example": "Year,WHO Region,Country,No. of deaths_median\n2017,Eastern Mediterranean,Afghanistan,298\n2017,Africa,Algeria,0\n2017,Africa,Angola,13316\n2017,Americas,Argentina,0\n2017,Europe,Armenia,0\n2017,Europe,Azerbaijan,0\n2017,South-East Asia,Bangladesh,76\n2017,Americas,Belize,0\n2017,Africa,Benin,7328\n2017,South-East Asia,Bhutan,0\n", "csv1_path": "infiagent/merge_test/32_1_0.csv", "csv2_path": "infiagent/merge_test/32_1_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/32_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/32_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nExpected Readmission Rate,Number of Discharges,Hospital Name,End Date\n5.6,242,FROEDTERT MEMORIAL LUTHERAN HOSPITAL,06/30/2013\n4.8,586,BEAUFORT COUNTY MEMORIAL HOSPITAL,06/30/2013\n5.4,149,BRAZOSPORT REGIONAL HEALTH SYSTEM,06/30/2013\n5.3,141,WESTERN MISSOURI MEDICAL CENTER,06/30/2013\n5.2,390,SAINT AGNES HOSPITAL,06/30/2013\n6.1,178,MERCY HOSPITAL JEFFERSON,06/30/2013\n5.3,98,ONSLOW MEMORIAL HOSPITAL,06/30/2013\n5.0,256,FAUQUIER HOSPITAL,06/30/2013\n6.1,121,ROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,06/30/2013\n\nHeader and first few lines of CSV file 2:\nFootnote,Hospital Name,Number of Readmissions,Start Date\n,WESTERN MISSOURI MEDICAL CENTER,19.0,07/01/2010\n,SAINT AGNES HOSPITAL,38.0,07/01/2010\n,MERCY HOSPITAL JEFFERSON,24.0,07/01/2010\n,ROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,18.0,07/01/2010\n,RESTON HOSPITAL CENTER,59.0,07/01/2010\n,CAPITAL HEALTH MEDICAL CENTER - HOPEWELL,25.0,07/01/2010\n,THOMAS JEFFERSON UNIVERSITY HOSPITAL,116.0,07/01/2010\n,HOUSTON MEDICAL CENTER,26.0,07/01/2010\n,VALLEY VIEW MEDICAL CENTER,15.0,07/01/2010\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Expected Readmission Rate,Number of Discharges,Hospital Name,End Date\n5.6,242,FROEDTERT MEMORIAL LUTHERAN HOSPITAL,06/30/2013\n4.8,586,BEAUFORT COUNTY MEMORIAL HOSPITAL,06/30/2013\n5.4,149,BRAZOSPORT REGIONAL HEALTH SYSTEM,06/30/2013\n5.3,141,WESTERN MISSOURI MEDICAL CENTER,06/30/2013\n5.2,390,SAINT AGNES HOSPITAL,06/30/2013\n6.1,178,MERCY HOSPITAL JEFFERSON,06/30/2013\n5.3,98,ONSLOW MEMORIAL HOSPITAL,06/30/2013\n5.0,256,FAUQUIER HOSPITAL,06/30/2013\n6.1,121,ROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,06/30/2013\n6.3,108,MERCY ST ANNE HOSPITAL,06/30/2013\n", "csv2_example": "Footnote,Hospital Name,Number of Readmissions,Start Date\n,WESTERN MISSOURI MEDICAL CENTER,19.0,07/01/2010\n,SAINT AGNES HOSPITAL,38.0,07/01/2010\n,MERCY HOSPITAL JEFFERSON,24.0,07/01/2010\n,ROBERT WOOD JOHNSON UNIVERSITY HOSPITAL AT RAHWAY,18.0,07/01/2010\n,RESTON HOSPITAL CENTER,59.0,07/01/2010\n,CAPITAL HEALTH MEDICAL CENTER - HOPEWELL,25.0,07/01/2010\n,THOMAS JEFFERSON UNIVERSITY HOSPITAL,116.0,07/01/2010\n,HOUSTON MEDICAL CENTER,26.0,07/01/2010\n,VALLEY VIEW MEDICAL CENTER,15.0,07/01/2010\n,REGIONAL HOSPITAL OF JACKSON,31.0,07/01/2010\n", "csv1_path": "infiagent/merge_test/20_2_0.csv", "csv2_path": "infiagent/merge_test/20_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/20_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/20_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nSymbol,Close Price,Deliverable Qty,Turnover,Prev Close,Series,Date\nGODREJIND,578.55,360927,460836225.3,564.6,EQ,15-May-2017\nGODREJIND,584.8,210364,291930164.6,578.55,EQ,16-May-2017\nGODREJIND,588.6,261667,296814880.85,584.8,EQ,17-May-2017\nGODREJIND,574.6,99785,129878624.25,588.6,EQ,18-May-2017\nGODREJIND,578.0,68041,141692454.15,574.6,EQ,19-May-2017\nGODREJIND,565.95,131406,265093680.35,578.0,EQ,22-May-2017\nGODREJIND,561.3,185396,620507087.55,565.95,EQ,23-May-2017\nGODREJIND,569.95,251584,411258378.25,561.3,EQ,24-May-2017\nGODREJIND,583.9,300989,345447321.45,569.95,EQ,25-May-2017\n\nHeader and first few lines of CSV file 2:\n% Dly Qt to Traded Qty,Total Traded Quantity,No. of Trades,Average Price,Low Price,Date\n45.28,797171,21649,578.09,568.5,15-May-2017\n42.05,500223,17204,583.6,572.25,16-May-2017\n27.72,245436,4969,577.31,567.55,19-May-2017\n17.08,1085338,20749,571.72,555.55,23-May-2017\n34.63,726457,13113,566.12,551.8,24-May-2017\n50.37,597536,12426,578.12,569.75,25-May-2017\n34.64,424831,7144,588.04,577.35,26-May-2017\n32.9,181696,5018,588.57,582.75,29-May-2017\n26.24,541003,11183,599.27,580.7,30-May-2017\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "Symbol,Close Price,Deliverable Qty,Turnover,Prev Close,Series,Date\nGODREJIND,578.55,360927,460836225.3,564.6,EQ,15-May-2017\nGODREJIND,584.8,210364,291930164.6,578.55,EQ,16-May-2017\nGODREJIND,588.6,261667,296814880.85,584.8,EQ,17-May-2017\nGODREJIND,574.6,99785,129878624.25,588.6,EQ,18-May-2017\nGODREJIND,578.0,68041,141692454.15,574.6,EQ,19-May-2017\nGODREJIND,565.95,131406,265093680.35,578.0,EQ,22-May-2017\nGODREJIND,561.3,185396,620507087.55,565.95,EQ,23-May-2017\nGODREJIND,569.95,251584,411258378.25,561.3,EQ,24-May-2017\nGODREJIND,583.9,300989,345447321.45,569.95,EQ,25-May-2017\nGODREJIND,593.5,147182,249816977.85,583.9,EQ,26-May-2017\n", "csv2_example": "% Dly Qt to Traded Qty,Total Traded Quantity,No. of Trades,Average Price,Low Price,Date\n45.28,797171,21649,578.09,568.5,15-May-2017\n42.05,500223,17204,583.6,572.25,16-May-2017\n27.72,245436,4969,577.31,567.55,19-May-2017\n17.08,1085338,20749,571.72,555.55,23-May-2017\n34.63,726457,13113,566.12,551.8,24-May-2017\n50.37,597536,12426,578.12,569.75,25-May-2017\n34.64,424831,7144,588.04,577.35,26-May-2017\n32.9,181696,5018,588.57,582.75,29-May-2017\n26.24,541003,11183,599.27,580.7,30-May-2017\n30.82,693740,13900,611.53,601.55,31-May-2017\n", "csv1_path": "infiagent/merge_test/38_0_0.csv", "csv2_path": "infiagent/merge_test/38_0_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/38_0_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/38_0_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nUnnamed: 0,Weight,Hair color\n0,441.0,No Hair\n1,65.0,No Hair\n2,90.0,No Hair\n3,441.0,No Hair\n4,-99.0,Black\n5,122.0,No Hair\n6,-99.0,Blond\n9,81.0,Brown\n10,104.0,-\n\nHeader and first few lines of CSV file 2:\nRace,Height,Eye color,Unnamed: 0,Gender\nHuman,203.0,yellow,0,Male\nIcthyo Sapien,191.0,blue,1,Male\nUngaran,185.0,blue,2,Male\nHuman / Radiation,203.0,green,3,Male\nCosmic Entity,-99.0,blue,4,Male\nHuman,193.0,blue,5,Male\n-,-99.0,blue,6,Male\nHuman,185.0,blue,7,Male\n-,173.0,blue,8,Female\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Unnamed: 0,Weight,Hair color\n0,441.0,No Hair\n1,65.0,No Hair\n2,90.0,No Hair\n3,441.0,No Hair\n4,-99.0,Black\n5,122.0,No Hair\n6,-99.0,Blond\n9,81.0,Brown\n10,104.0,-\n12,90.0,Black\n", "csv2_example": "Race,Height,Eye color,Unnamed: 0,Gender\nHuman,203.0,yellow,0,Male\nIcthyo Sapien,191.0,blue,1,Male\nUngaran,185.0,blue,2,Male\nHuman / Radiation,203.0,green,3,Male\nCosmic Entity,-99.0,blue,4,Male\nHuman,193.0,blue,5,Male\n-,-99.0,blue,6,Male\nHuman,185.0,blue,7,Male\n-,173.0,blue,8,Female\nHuman,178.0,brown,9,Male\n", "csv1_path": "infiagent/merge_test/39_2_0.csv", "csv2_path": "infiagent/merge_test/39_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/39_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/39_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ntimestamp,avg. abandonment time,avg. num. agents staffed\nApr 13  2017 12:00:00 AM,00:00:00,4\nApr 13  2017 12:15:00 AM,00:00:00,4\nApr 13  2017 12:30:00 AM,00:00:00,4\nApr 13  2017 12:45:00 AM,00:00:00,4\nApr 13  2017 1:00:00 AM,00:00:00,4\nApr 13  2017 1:15:00 AM,00:00:00,4\nApr 13  2017 1:30:00 AM,00:00:00,4\nApr 13  2017 1:45:00 AM,00:00:00,4\nApr 13  2017 2:30:00 AM,00:00:00,4\n\nHeader and first few lines of CSV file 2:\nnum. calls timed out,timestamp,avg. wait time\n0,Apr 13  2017 12:15:00 AM,00:00:00\n0,Apr 13  2017 12:30:00 AM,00:00:00\n0,Apr 13  2017 12:45:00 AM,00:00:00\n0,Apr 13  2017 1:00:00 AM,00:00:00\n0,Apr 13  2017 1:15:00 AM,00:00:00\n0,Apr 13  2017 1:30:00 AM,00:00:00\n0,Apr 13  2017 1:45:00 AM,00:00:00\n0,Apr 13  2017 2:00:00 AM,00:00:00\n0,Apr 13  2017 2:15:00 AM,00:00:00\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "timestamp,avg. abandonment time,avg. num. agents staffed\nApr 13  2017 12:00:00 AM,00:00:00,4\nApr 13  2017 12:15:00 AM,00:00:00,4\nApr 13  2017 12:30:00 AM,00:00:00,4\nApr 13  2017 12:45:00 AM,00:00:00,4\nApr 13  2017 1:00:00 AM,00:00:00,4\nApr 13  2017 1:15:00 AM,00:00:00,4\nApr 13  2017 1:30:00 AM,00:00:00,4\nApr 13  2017 1:45:00 AM,00:00:00,4\nApr 13  2017 2:30:00 AM,00:00:00,4\nApr 13  2017 3:00:00 AM,00:00:00,4\n", "csv2_example": "num. calls timed out,timestamp,avg. wait time\n0,Apr 13  2017 12:15:00 AM,00:00:00\n0,Apr 13  2017 12:30:00 AM,00:00:00\n0,Apr 13  2017 12:45:00 AM,00:00:00\n0,Apr 13  2017 1:00:00 AM,00:00:00\n0,Apr 13  2017 1:15:00 AM,00:00:00\n0,Apr 13  2017 1:30:00 AM,00:00:00\n0,Apr 13  2017 1:45:00 AM,00:00:00\n0,Apr 13  2017 2:00:00 AM,00:00:00\n0,Apr 13  2017 2:15:00 AM,00:00:00\n0,Apr 13  2017 2:30:00 AM,00:00:00\n", "csv1_path": "infiagent/merge_test/4_2_0.csv", "csv2_path": "infiagent/merge_test/4_2_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/4_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/4_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\ny,carat\n3.98,0.23\n3.84,0.21\n4.23,0.29\n4.35,0.31\n3.96,0.24\n4.11,0.26\n3.78,0.22\n4.28,0.3\n3.75,0.2\n\nHeader and first few lines of CSV file 2:\nprice,depth,carat,table,z\n326,61.5,0.23,55.0,2.43\n326,59.8,0.21,61.0,2.31\n334,62.4,0.29,58.0,2.63\n335,63.3,0.31,58.0,2.75\n337,61.9,0.26,55.0,2.53\n337,65.1,0.22,61.0,2.49\n339,64.0,0.3,55.0,2.73\n345,60.2,0.2,62.0,2.27\n345,60.9,0.32,58.0,2.68\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "y,carat\n3.98,0.23\n3.84,0.21\n4.23,0.29\n4.35,0.31\n3.96,0.24\n4.11,0.26\n3.78,0.22\n4.28,0.3\n3.75,0.2\n4.42,0.32\n", "csv2_example": "price,depth,carat,table,z\n326,61.5,0.23,55.0,2.43\n326,59.8,0.21,61.0,2.31\n334,62.4,0.29,58.0,2.63\n335,63.3,0.31,58.0,2.75\n337,61.9,0.26,55.0,2.53\n337,65.1,0.22,61.0,2.49\n339,64.0,0.3,55.0,2.73\n345,60.2,0.2,62.0,2.27\n345,60.9,0.32,58.0,2.68\n403,61.8,0.33,55.0,2.78\n", "csv1_path": "infiagent/merge_test/28_3_0.csv", "csv2_path": "infiagent/merge_test/28_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/28_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/28_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nDate,Open\nSep 17, 2017,109.75\nSep 16, 2017,111.11\nSep 15, 2017,97.42\nSep 14, 2017,115.97\nSep 13, 2017,123.14\nSep 12, 2017,125.29\nSep 11, 2017,121.88\nSep 10, 2017,129.7\nSep 09, 2017,128.51\n\nHeader and first few lines of CSV file 2:\nDate,Market Cap,Low,Volume,Close,High\nSep 17, 2017,737,226,000,102.81,5,350,380,106.84,110.94\nSep 16, 2017,744,652,000,105.02,5,683,580,109.85,116.01\nSep 15, 2017,652,107,000,89.36,8,539,660,111.22,113.75\nSep 14, 2017,775,543,000,96.71,6,367,800,96.71,117.38\nSep 13, 2017,822,282,000,112.6,6,315,510,115.97,123.7\nSep 11, 2017,812,292,000,120.76,7,083,720,125.7,128.2\nSep 10, 2017,863,579,000,118.94,4,644,030,122.92,129.7\nSep 09, 2017,854,520,000,125.21,6,225,680,130.05,131.51\nSep 08, 2017,914,556,000,123.23,7,051,050,128.29,139.78\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "Date,Open\nSep 17, 2017,109.75\nSep 16, 2017,111.11\nSep 15, 2017,97.42\nSep 14, 2017,115.97\nSep 13, 2017,123.14\nSep 12, 2017,125.29\nSep 11, 2017,121.88\nSep 10, 2017,129.7\nSep 09, 2017,128.51\nSep 08, 2017,137.68\n", "csv2_example": "Date,Market Cap,Low,Volume,Close,High\nSep 17, 2017,737,226,000,102.81,5,350,380,106.84,110.94\nSep 16, 2017,744,652,000,105.02,5,683,580,109.85,116.01\nSep 15, 2017,652,107,000,89.36,8,539,660,111.22,113.75\nSep 14, 2017,775,543,000,96.71,6,367,800,96.71,117.38\nSep 13, 2017,822,282,000,112.6,6,315,510,115.97,123.7\nSep 11, 2017,812,292,000,120.76,7,083,720,125.7,128.2\nSep 10, 2017,863,579,000,118.94,4,644,030,122.92,129.7\nSep 09, 2017,854,520,000,125.21,6,225,680,130.05,131.51\nSep 08, 2017,914,556,000,123.23,7,051,050,128.29,139.78\nSep 07, 2017,901,778,000,132.52,7,564,730,137.61,138.67\n", "csv1_path": "infiagent/merge_test/17_1_0.csv", "csv2_path": "infiagent/merge_test/17_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/17_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/17_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nsource_website,daily_vaccinations,country,people_vaccinated\nhttps://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/,,Albania,0.0\nhttps://www.aps.dz/regions/116777-blida-covid-19-trente-vaccines-au-matin-du-1er-jour-de-la-campagne,,Algeria,\nhttps://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,,Andorra,576.0\nhttps://www.facebook.com/MinistryofHealthAnguilla/posts/1391067271247845,,Anguilla,0.0\nhttp://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,,Argentina,\nhttps://info.gesundheitsministerium.gv.at/opendata/,,Austria,6784.0\nhttps://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,,Azerbaijan,0.0\nhttps://twitter.com/MOH_Bahrain/status/1362144927535267841,,Bahrain,38965.0\nhttps://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,,Bangladesh,0.0\n\nHeader and first few lines of CSV file 2:\npeople_vaccinated_per_hundred,daily_vaccinations_raw,people_fully_vaccinated_per_hundred,source_name,date,daily_vaccinations_per_million,country\n,,,Ministry of Health,2021-01-29,,Algeria\n0.75,,,Government of Andorra,2021-01-25,,Andorra\n0.0,,,Ministry of Health,2021-02-04,,Anguilla\n0.08,,,Ministry of Health,2021-01-10,,Austria\n0.0,,,Government of Azerbaijan,2021-01-17,,Azerbaijan\n2.29,,,Ministry of Health,2020-12-23,,Bahrain\n0.0,,,Directorate General of Health Services,2021-01-26,,Bangladesh\n0.0,,,Sciensano,2020-12-28,,Belgium\n,,,Ministry of Health,2021-01-28,,Bolivia\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "source_website,daily_vaccinations,country,people_vaccinated\nhttps://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/,,Albania,0.0\nhttps://www.aps.dz/regions/116777-blida-covid-19-trente-vaccines-au-matin-du-1er-jour-de-la-campagne,,Algeria,\nhttps://www.govern.ad/comunicats/item/12423-els-casos-actius-de-la-covid-19-baixen-dels-500-per-primera-vegada-des-del-desembre-de-l-any-passat,,Andorra,576.0\nhttps://www.facebook.com/MinistryofHealthAnguilla/posts/1391067271247845,,Anguilla,0.0\nhttp://datos.salud.gob.ar/dataset/vacunas-contra-covid-19-dosis-aplicadas-en-la-republica-argentina,,Argentina,\nhttps://info.gesundheitsministerium.gv.at/opendata/,,Austria,6784.0\nhttps://eurasianet.org/azerbaijan-starts-mass-covid-19-vaccinations,,Azerbaijan,0.0\nhttps://twitter.com/MOH_Bahrain/status/1362144927535267841,,Bahrain,38965.0\nhttps://dghs.gov.bd/images/docs/Press%20Release/Covid-19-Vaccination-Update-17-2-2021.pdf,,Bangladesh,0.0\nhttps://www.gov.bm/articles/covid-19-update-premiers-remarks-16-february-2021,,Bermuda,0.0\n", "csv2_example": "people_vaccinated_per_hundred,daily_vaccinations_raw,people_fully_vaccinated_per_hundred,source_name,date,daily_vaccinations_per_million,country\n,,,Ministry of Health,2021-01-29,,Algeria\n0.75,,,Government of Andorra,2021-01-25,,Andorra\n0.0,,,Ministry of Health,2021-02-04,,Anguilla\n0.08,,,Ministry of Health,2021-01-10,,Austria\n0.0,,,Government of Azerbaijan,2021-01-17,,Azerbaijan\n2.29,,,Ministry of Health,2020-12-23,,Bahrain\n0.0,,,Directorate General of Health Services,2021-01-26,,Bangladesh\n0.0,,,Sciensano,2020-12-28,,Belgium\n,,,Ministry of Health,2021-01-28,,Bolivia\n0.0,,,Regional governments via Coronavirus Brasil,2021-01-16,,Brazil\n", "csv1_path": "infiagent/merge_test/22_3_0.csv", "csv2_path": "infiagent/merge_test/22_3_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/22_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/22_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nNZ,EQ3,EQ4,EQ5,Unnamed: 0,EX3\n0.362830325973427,-0.0209965026800568,-0.0642619047779573,0.134687464025311,1,-0.0080529851935652\n0.3381555792905,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,2,-0.0019217351030098\n0.0146529160116964,-0.0681647023627196,-0.085908020071585,0.116167937721831,3,-0.0111643658365336\n-0.36930248144376,-0.0678689769728596,-0.0987604010271765,0.11336194888797,4,-0.0183022390762845\n\nHeader and first few lines of CSV file 2:\nEQ7,Unnamed: 0,EQ1,EX6,EQ8,EX2,EX4,EX1,EQ6,EX7,EX5\n-0.0192616021801591,1,0.0008177611295828,-0.0272364598603708,-0.0219164236380736,0.0426168252660709,-0.0730019227052621,0.0410182786002027,0.0508805382290177,-0.0563269342288962,-0.110649838313963\n-0.0200465944831218,2,0.0338007933560907,-0.0095903027677361,-0.252665055370363,0.0263969887842081,-0.094665094890157,0.0300434883392789,0.0769627933528486,-0.0850724761803679,-0.098188303125206\n-0.0115054095747666,3,0.0166278096348511,-0.0333742536317219,-0.171574287909491,0.0257609167653115,-0.114399354483246,0.0237329839392477,0.0821742091932279,-0.111310618833892,-0.0780994489847098\n-0.0260687742401788,5,-0.0365266637880335,-0.0118919754319929,-0.319666693349616,0.0149476924440696,-0.0310109793605687,0.0026065126869694,0.0137460533760731,-0.0825650768824202,-0.0482347353426887\n\nQuestion: Combine the two tables and fill in missing values with NaN.", "csv1_example": "NZ,EQ3,EQ4,EQ5,Unnamed: 0,EX3\n0.362830325973427,-0.0209965026800568,-0.0642619047779573,0.134687464025311,1,-0.0080529851935652\n0.3381555792905,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,2,-0.0019217351030098\n0.0146529160116964,-0.0681647023627196,-0.085908020071585,0.116167937721831,3,-0.0111643658365336\n-0.36930248144376,-0.0678689769728596,-0.0987604010271765,0.11336194888797,4,-0.0183022390762845\n-0.383436113240686,-0.0344520079186847,-0.040586466175552,0.0488242057091753,5,-0.0130861009395434\n-0.154171177528628,0.0076888601363588,-0.055468170439921,-0.0202031196037967,6,-0.0049416045505968\n-0.208110086071965,0.0344520079186847,-0.071026315807216,-0.104382784619616,7,-0.0015556903214841\n-0.313322994728527,0.0360784975629145,-0.0033822055146293,-0.140860639459805,8,-0.0111643658365336\n-0.11687746493838,0.0156734456625776,-0.0493802005135883,-0.317076738226253,10,-0.04438292975999\n0.110178944111268,0.0122726036791881,0.0385571428667744,-0.402378798775617,11,-0.0484094223567726\n", "csv2_example": "EQ7,Unnamed: 0,EQ1,EX6,EQ8,EX2,EX4,EX1,EQ6,EX7,EX5\n-0.0192616021801591,1,0.0008177611295828,-0.0272364598603708,-0.0219164236380736,0.0426168252660709,-0.0730019227052621,0.0410182786002027,0.0508805382290177,-0.0563269342288962,-0.110649838313963\n-0.0200465944831218,2,0.0338007933560907,-0.0095903027677361,-0.252665055370363,0.0263969887842081,-0.094665094890157,0.0300434883392789,0.0769627933528486,-0.0850724761803679,-0.098188303125206\n-0.0115054095747666,3,0.0166278096348511,-0.0333742536317219,-0.171574287909491,0.0257609167653115,-0.114399354483246,0.0237329839392477,0.0821742091932279,-0.111310618833892,-0.0780994489847098\n-0.0260687742401788,5,-0.0365266637880335,-0.0118919754319929,-0.319666693349616,0.0149476924440696,-0.0310109793605687,0.0026065126869694,0.0137460533760731,-0.0825650768824202,-0.0482347353426887\n-0.0049911450904791,6,-0.0250780079738737,-0.0210986660890196,-0.316848867453293,0.0181280525385525,-0.0051932262087076,-0.0071336136696004,-0.0231114963355954,-0.0665356313705403,-0.052854097524728\n-0.0260570579371495,9,0.0128115910301312,-0.0444990048422959,0.159363709025421,0.002226252066138,-0.117366912316793,-0.0109747902609238,0.05772838899512,0.0227456936313826,-0.0060159135393999\n-0.0709187822363157,10,-0.0169003966780454,0.0337578657424314,-0.207579841029183,0.0073148282173106,-0.0501517273869484,-0.0219495805218476,0.0803111909700971,0.066356531420687,0.0553249191569815\n-0.0457755959354511,11,-0.0466123843862219,0.0602271013813833,-0.28616587435999,-0.0015901800472414,-0.0063802493421265,-0.0360796229827869,0.0807391816429785,0.0916991743249439,0.0772400327647956\n-0.0001405956363515,12,-0.0288942265785937,0.0590762650492549,-0.106138108761528,-0.0216264486424837,0.0151345449510909,-0.0437619761654336,0.0382422254180494,0.0966244229459126,0.0614482597238707\n-0.036402553512016,13,0.0198988541531824,0.0602271013813833,-0.0165938636116843,-0.0171739445102077,0.0111283418758022,-0.0604985313133424,-0.0595662312963166,0.0935797237984047,0.0401777082809924\n", "csv1_path": "infiagent/merge_test/31_2_0.csv", "csv2_path": "infiagent/merge_test/31_2_1.csv", "instruction": "Combine the two tables and fill in missing values with NaN.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/31_2_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/31_2_1.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='outer')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nmax_diffsel,abs_diffsel,site,min_diffsel\n1.5787387471316894,9.026365225783264,(HA2)121,-1.004167098795603\n0.7169223975734413,9.002764774505879,326,-1.2184218611180495\n0.9710714351184296,8.418637730396656,280,-1.0182673783732994\n1.0005541420596125,8.185717407618215,9,-0.8471518556126452\n1.37896385796,8.058662714496545,210,-1.2405474792520252\n1.263069276940311,8.015976108875662,192,-1.027758871160202\n0.6785731341245361,7.975893014675133,-12,-1.2624452625748657\n1.2322928805568882,7.856854881480676,171,-1.9128310938041404\n0.9115078157304708,7.846329400937815,(HA2)102,-1.663800562787797\n\nHeader and first few lines of CSV file 2:\nsite,negative_diffsel\n(HA2)121,-4.879263151890148\n326,-5.387163779625509\n280,-3.271699986071876\n-12,-4.581752096274572\n(HA2)102,-4.361519168815148\n242,-2.704191739007377\n299,-5.198842431213596\n(HA2)34,-4.453168832872083\n262,-4.343193848567695\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "max_diffsel,abs_diffsel,site,min_diffsel\n1.5787387471316894,9.026365225783264,(HA2)121,-1.004167098795603\n0.7169223975734413,9.002764774505879,326,-1.2184218611180495\n0.9710714351184296,8.418637730396656,280,-1.0182673783732994\n1.0005541420596125,8.185717407618215,9,-0.8471518556126452\n1.37896385796,8.058662714496545,210,-1.2405474792520252\n1.263069276940311,8.015976108875662,192,-1.027758871160202\n0.6785731341245361,7.975893014675133,-12,-1.2624452625748657\n1.2322928805568882,7.856854881480676,171,-1.9128310938041404\n0.9115078157304708,7.846329400937815,(HA2)102,-1.663800562787797\n1.3208015052141648,7.823952542822076,312,-1.4217237593217844\n", "csv2_example": "site,negative_diffsel\n(HA2)121,-4.879263151890148\n326,-5.387163779625509\n280,-3.271699986071876\n-12,-4.581752096274572\n(HA2)102,-4.361519168815148\n242,-2.704191739007377\n299,-5.198842431213596\n(HA2)34,-4.453168832872083\n262,-4.343193848567695\n222,-2.235978425911968\n", "csv1_path": "infiagent/merge_test/34_3_0.csv", "csv2_path": "infiagent/merge_test/34_3_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/34_3_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/34_3_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines two CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file 1:\nhour-per-week,age,workclass,income,education,sex\n40,39, State-gov, <=50K, Bachelors, Male\n13,50, Self-emp-not-inc, <=50K, Bachelors, Male\n40,38, Private, <=50K, HS-grad, Male\n40,53, Private, <=50K, 11th, Male\n40,28, Private, <=50K, Bachelors, Female\n16,49, Private, <=50K, 9th, Female\n45,52, Self-emp-not-inc, >50K, HS-grad, Male\n50,31, Private, >50K, Masters, Female\n40,42, Private, >50K, Bachelors, Male\n\nHeader and first few lines of CSV file 2:\ncapital-loos,native-country,final-weight,occupation,education-num,age,relationship\n0, United-States,77516, Adm-clerical,13,39, Not-in-family\n0, United-States,83311, Exec-managerial,13,50, Husband\n0, United-States,215646, Handlers-cleaners,9,38, Not-in-family\n0, United-States,234721, Handlers-cleaners,7,53, Husband\n0, Cuba,338409, Prof-specialty,13,28, Wife\n0, United-States,284582, Exec-managerial,14,37, Wife\n0, Jamaica,160187, Other-service,5,49, Not-in-family\n0, United-States,209642, Exec-managerial,9,52, Husband\n0, United-States,45781, Prof-specialty,14,31, Not-in-family\n\nQuestion: Combine the contents of two tables, retaining only the rows that undergo successful merging.", "csv1_example": "hour-per-week,age,workclass,income,education,sex\n40,39, State-gov, <=50K, Bachelors, Male\n13,50, Self-emp-not-inc, <=50K, Bachelors, Male\n40,38, Private, <=50K, HS-grad, Male\n40,53, Private, <=50K, 11th, Male\n40,28, Private, <=50K, Bachelors, Female\n16,49, Private, <=50K, 9th, Female\n45,52, Self-emp-not-inc, >50K, HS-grad, Male\n50,31, Private, >50K, Masters, Female\n40,42, Private, >50K, Bachelors, Male\n40,30, State-gov, >50K, Bachelors, Male\n", "csv2_example": "capital-loos,native-country,final-weight,occupation,education-num,age,relationship\n0, United-States,77516, Adm-clerical,13,39, Not-in-family\n0, United-States,83311, Exec-managerial,13,50, Husband\n0, United-States,215646, Handlers-cleaners,9,38, Not-in-family\n0, United-States,234721, Handlers-cleaners,7,53, Husband\n0, Cuba,338409, Prof-specialty,13,28, Wife\n0, United-States,284582, Exec-managerial,14,37, Wife\n0, Jamaica,160187, Other-service,5,49, Not-in-family\n0, United-States,209642, Exec-managerial,9,52, Husband\n0, United-States,45781, Prof-specialty,14,31, Not-in-family\n0, United-States,159449, Exec-managerial,13,42, Husband\n", "csv1_path": "infiagent/merge_test/18_1_0.csv", "csv2_path": "infiagent/merge_test/18_1_1.csv", "instruction": "Combine the contents of two tables, retaining only the rows that undergo successful merging.", "instruction_type": "Merge-SimpleMerge", "reference_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"infiagent/merge_test/18_1_0.csv\")\ndf2 = pd.read_csv(\"infiagent/merge_test/18_1_1.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n", "assistant_code": "import pandas as pd\n\ndf1 = pd.read_csv(\"data1.csv\")\ndf2 = pd.read_csv(\"data2.csv\")\n\ndf = pd.merge(df1, df2, how='inner')\nprint(df)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Aggregate the records by 'Date' and compute the sum of '7-Day Passes Purchased' for each date.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Aggregate the records by 'Date' and compute the sum of '7-Day Passes Purchased' for each date.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Loading the data\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Grouping records by 'Date' and calculating the total '7-Day Passes Purchased' for each date\ngrouped_data = data.groupby('Date')['7-Day Passes Purchased (midnight to 11:59 pm)'].sum()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Aggregate the data by 'Date' and compute the sum of '7-Day Passes Purchased'\ndf_aggregated = df.groupby('Date')['7-Day Passes Purchased (midnight to 11:59 pm)'].sum()\n\nprint(df_aggregated)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Calculate the maximum and minimum number of '24-Hour Passes Purchased (midnight to 11:59 pm)' within the given dates.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Calculate the maximum and minimum number of '24-Hour Passes Purchased (midnight to 11:59 pm)' within the given dates.", "instruction_type": "Query-Aggregation", "reference_code": "# Import the necessary libraries\nimport pandas as pd\n\n# Load the data from CSV file\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Calculate the max and min number of '24-Hour Passes Purchased (midnight to 11:59 pm)'\nmax_24hour_passes = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].max()\nmin_24hour_passes = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].min()\n\n# Print the result\nprint(f\"Maximum number of '24-Hour Passes Purchased (midnight to 11:59 pm)': {max_24hour_passes}\")\nprint(f\"Minimum number of '24-Hour Passes Purchased (midnight to 11:59 pm)': {min_24hour_passes}\")", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the '24-Hour Passes Purchased (midnight to 11:59 pm)' column to numeric\ndf['24-Hour Passes Purchased (midnight to 11:59 pm)'] = pd.to_numeric(df['24-Hour Passes Purchased (midnight to 11:59 pm)'], errors='coerce')\n\n# Get the maximum and minimum value\nmax_value = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].max()\nmin_value = df['24-Hour Passes Purchased (midnight to 11:59 pm)'].min()\n\n# Output the result\nprint(\"Maximum: \", max_value)\nprint(\"Minimum: \", min_value)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Find the total number of fleet_numbers or trips for each route_code.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Find the total number of fleet_numbers or trips for each route_code.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Calculate the total number of fleet_numbers or trips performed by each route_code\nresult = df.groupby('route_code')['fleet_number'].nunique()\n\nprint(result)", "assistant_code": "import pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Group by 'route_code' and count the number of occurrences of each fleet_number\nroute_counts = df.groupby('route_code')['fleet_number'].count()\n\n# Print the result\nprint(route_counts)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Count the total number of unique 'destination_station' in the table.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Count the total number of unique 'destination_station' in the table.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# count the total number of unique 'destination_station' in the table\nunique_destination_count = data['destination_station'].nunique()\n\nprint(unique_destination_count)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the total number of unique 'destination_station'\nunique_stations = df['destination_station'].nunique()\n\nprint(unique_stations)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Calculate the average 'travel_time' for each group of data based on 'origin_station'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Calculate the average 'travel_time' for each group of data based on 'origin_station'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Convert time columns to datetime\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\n\n# Calculate travel time in minutes\ndf['travel_time'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 60\n\n# Group data by 'origin_station' and calculate average 'travel_time'\nresult = df.groupby('origin_station')['travel_time'].mean()\n\nprint(result)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Convert 'start_time' and 'end_time' to datetime format\ndata['start_time'] = pd.to_datetime(data['start_time'])\ndata['end_time'] = pd.to_datetime(data['end_time'])\n\n# Calculate the travel time in seconds\ndata['travel_time'] = (data['end_time'] - data['start_time']).dt.total_seconds()\n\n# Group by 'origin_station' and calculate the average travel time\naverage_travel_time = data.groupby('origin_station')['travel_time'].mean()\n\nprint(average_travel_time)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Calculate the average 'Economy (GDP per Capita)' value for each region and show the results.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Calculate the average 'Economy (GDP per Capita)' value for each region and show the results.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/3.csv')\n\n# Aggregate data to find 'Economy (GDP per Capita)' average for each region\navg_gdp_per_region = data.groupby('Region')['Economy (GDP per Capita)'].mean()\n\nprint(avg_gdp_per_region)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'Economy (GDP per Capita)' value for each region\naverage_gdp = df.groupby('Region')['Economy (GDP per Capita)'].mean()\n\nprint(average_gdp)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Determine the median 'Family' score for each region and display the information.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Determine the median 'Family' score for each region and display the information.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Calculate the median 'Family' score for each region\nmedian_family_scores = df.groupby('Region')['Family'].median()\n\nprint(median_family_scores)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'Region' and calculate the median 'Family' score for each group\nmedian_family_scores = data.groupby('Region')['Family'].median()\n\nprint(median_family_scores)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Calculate the average number of calls answered per day and list them.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Calculate the average number of calls answered per day and list them.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Converting 'timestamp' column to datetime type\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Extracting date from timestamp\ndf['Date'] = df['timestamp'].dt.date\n\n# Group by date and calculate the average number of calls answered\navg_calls_answered = df.groupby('Date')['num. calls answered'].mean()\n\nprint(avg_calls_answered)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Convert timestamp to datetime and extract date\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata['date'] = data['timestamp'].dt.date\n\n# Group by date and calculate the average number of calls answered\navg_calls_answered_per_day = data.groupby('date')['num. calls answered'].mean()\n\nprint(avg_calls_answered_per_day)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Compute the sum of abandoned calls for every hour.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Compute the sum of abandoned calls for every hour.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv('infiagent/csv/4.csv')\n\n# Convert the 'timestamp' column to datetime\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# Extract the hour from the 'timestamp' column\ndata['hour'] = data['timestamp'].dt.hour\n\n# Group the data by hour and calculate the sum of 'num. calls abandoned'\nhourly_abandoned_calls = data.groupby('hour')['num. calls abandoned'].sum()\n\n# Print the result\nprint(hourly_abandoned_calls)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Convert timestamp to datetime and extract the hour\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\ndf['hour'] = df['timestamp'].dt.hour\n\n# Sum the abandoned calls for each hour\nabandoned_calls_per_hour = df.groupby('hour')['num. calls abandoned'].sum()\n\nprint(abandoned_calls_per_hour)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Calculate and display the overall total (sum) and average number of \"num. calls abandoned\".", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Calculate and display the overall total (sum) and average number of \"num. calls abandoned\".", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Calculate the total number of 'num. calls abandoned'\ntotal_calls_abandoned = df['num. calls abandoned'].sum()\n\n# Calculate the average number of 'num. calls abandoned'\navg_calls_abandoned = df['num. calls abandoned'].mean()\n\nprint(\"Total number of calls abandoned: \",total_calls_abandoned)\nprint(\"Average number of calls abandoned: \",avg_calls_abandoned)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the total number of calls abandoned\ntotal_calls_abandoned = df['num. calls abandoned'].sum()\n\n# Calculate the average number of calls abandoned\navg_calls_abandoned = df['num. calls abandoned'].mean()\n\nprint('Total number of calls abandoned:', total_calls_abandoned)\nprint('Average number of calls abandoned:', avg_calls_abandoned)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Find the timestamp when the number of \"num. calls transferred\" was greatest, only considering records where \"num. calls abandoned\" is less than 5.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Find the timestamp when the number of \"num. calls transferred\" was greatest, only considering records where \"num. calls abandoned\" is less than 5.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Filtering for num. calls abandonedless than 5\nfiltered_df = df[df['num. calls abandoned'] < 5]\n\n# Finding the timestamp when num. calls transferredis greatest\nmax_transferred_time = filtered_df.loc[filtered_df['num. calls transferred'].idxmax(),'timestamp']\n\nprint(max_transferred_time)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'num. calls transferred' and 'num. calls abandoned' to int for numerical operations\ndf['num. calls transferred'] = df['num. calls transferred'].astype(int)\ndf['num. calls abandoned'] = df['num. calls abandoned'].astype(int)\n\n# Filter the dataframe where 'num. calls abandoned' is less than 5\nfiltered_df = df[df['num. calls abandoned'] < 5]\n\n# Find the timestamp where 'num. calls transferred' is maximum\nmax_transferred_timestamp = filtered_df.loc[filtered_df['num. calls transferred'].idxmax(), 'timestamp']\n\nprint(max_transferred_timestamp)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: For each day, identify the time slot with the highest count of \"num. calls answered\".", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "For each day, identify the time slot with the highest count of \"num. calls answered\".", "instruction_type": "Query-Aggregation", "reference_code": "# import pandas library\nimport pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# convert the timestamp to datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# create new column to store only the date from timestamp\ndf['date_only'] = df['timestamp'].dt.date\n\n# Group by 'date_only' and find the timeslot with maximum nimber of 'num. calls answered'\ngrouped_df = df.loc[df.groupby('date_only')['num. calls answered'].idxmax()]\n\n# print the result\nprint(grouped_df[['date_only','timestamp','num. calls answered']])", "assistant_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Convert the 'timestamp' column to datetime\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# Set time as index\ndata.set_index('timestamp', inplace=True)\n\n# Group by 'time' and find the max of 'num. calls answered' for each group\ngrouped_data = data.resample('H').num_calls_answered.max()\n\n# Find the time slot with the highest count of 'num. calls answered' for each day\nhighest_answeredCalls_perDay = grouped_data.idxmax()\n\nprint(highest_answeredCalls_perDay)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Compute the sum of abandoned calls for every hour.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Compute the sum of abandoned calls for every hour.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv('infiagent/csv/4.csv')\n\n# Convert the 'timestamp' column to datetime\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# Extract the hour from the 'timestamp' column\ndata['hour'] = data['timestamp'].dt.hour\n\n# Group the data by hour and calculate the sum of 'num. calls abandoned'\nhourly_abandoned_calls = data.groupby('hour')['num. calls abandoned'].sum()\n\n# Print the result\nprint(hourly_abandoned_calls)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Convert the 'timestamp' column to datetime\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# Extract the hour from the timestamp and create a new column 'hour'\ndata['hour'] = data['timestamp'].dt.hour\n\n# Sum the 'num. calls abandoned' for each hour\nabandoned_calls_per_hour = data.groupby('hour')['num. calls abandoned'].sum()\n\nprint(abandoned_calls_per_hour)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Calculate the total weight for each 'Source' and display this alongside the source.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Calculate the total weight for each 'Source' and display this alongside the source.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data from csv\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# aggregate the total 'Weight' for each 'Source'\ntotal_weight_source = df.groupby('Source')['Weight'].sum()\n\nprint(total_weight_source)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the total weight for each 'Source'\ntotal_weight = df.groupby('Source')['Weight'].sum()\n\nprint(total_weight)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Summarize the 'Weight' column for each 'Source' group and arrange the results in descending order.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Summarize the 'Weight' column for each 'Source' group and arrange the results in descending order.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Group the data by 'Source' and sum 'Weight' of each group\ngrouped_df = df.groupby('Source')['Weight'].sum()\n\n# Sort the grouped data in descending order\nsorted_grouped_df = grouped_df.sort_values(ascending=False)\n\nprint(sorted_grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Summarize the 'Weight' column for each 'Source' group\ngrouped_df = df.groupby('Source')['Weight'].sum()\n\n# Arrange the results in descending order\nsorted_df = grouped_df.sort_values(ascending=False)\n\nprint(sorted_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Identify the 'Source' with the maximum 'Weight' sum.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Identify the 'Source' with the maximum 'Weight' sum.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# loading the data\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# grouping the data by 'Source' and finding sum of 'Weight' for each source\ngrouped_df = df.groupby('Source')['Weight'].sum()\n\n# Finding the source with maximum weight sum\nmax_weight_source = grouped_df.idxmax()\n\nprint(max_weight_source)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'Source' and calculate the sum of 'Weight' for each group\ngrouped_data = df.groupby('Source')['Weight'].sum()\n\n# Identify the 'Source' with the maximum 'Weight' sum\nmax_weight_source = grouped_data.idxmax()\n\nprint(max_weight_source)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Aggregate the data by 'TIME' and compute the sum of 'USFLUX' for each corresponding group.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Aggregate the data by 'TIME' and compute the sum of 'USFLUX' for each corresponding group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# group by 'TIME' and calculate the sum of 'USFLUX'\ndf_grouped = df.groupby('TIME').USFLUX.sum()\n\nprint(df_grouped)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Aggregate the data by 'TIME' and compute the sum of 'USFLUX'\ndf_aggregated = df.groupby('TIME')['USFLUX'].sum()\n\nprint(df_aggregated)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Retrieve the largest value present in the 'MEANGAM' column.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Retrieve the largest value present in the 'MEANGAM' column.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/6.csv')\n\n# Find the maximum value of the 'MEANGAM' column\nmax_MEANGAM = data['MEANGAM'].max()\n\nprint(max_MEANGAM)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Find the maximum value in the 'MEANGAM' column\nmax_value = df['MEANGAM'].max()\n\nprint(max_value)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Find the maximum difference between 'TOTUSJZ' and 'TOTUSJH'.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Find the maximum difference between 'TOTUSJZ' and 'TOTUSJH'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Calculate the absolute difference between 'TOTUSJZ' and 'TOTUSJH'\ndiff = abs(df['TOTUSJZ'] - df['TOTUSJH'])\n\n# Find the maximum difference\nmax_diff = diff.max()\n\nprint(max_diff)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the difference between 'TOTUSJZ' and 'TOTUSJH'\ndf['diff'] = df['TOTUSJZ'] - df['TOTUSJH']\n\n# Find the maximum difference\nmax_diff = df['diff'].max()\n\nprint(max_diff)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Calculate the sum of 'TOTPOT' for all rows where 'MEANGBH' is less than 30.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Calculate the sum of 'TOTPOT' for all rows where 'MEANGBH' is less than 30.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Filter the rows where 'MEANGBH' is less than 30\nfiltered_df = df[df['MEANGBH'] < 30]\n\n# Find the sum of 'TOTPOT' for these rows\nsum_totpot = filtered_df['TOTPOT'].sum()\n\nprint(sum_totpot)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the rows where 'MEANGBH' is less than 30\nfiltered_df = df[df['MEANGBH'] < 30]\n\n# Calculate the sum of 'TOTPOT' for the filtered rows\nsum_topot = filtered_df['TOTPOT'].sum()\n\nprint(sum_topot)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Count the distinct occurrences of each value in the EVENTMSGTYPE column.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Count the distinct occurrences of each value in the EVENTMSGTYPE column.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Count the unique values in the EVENTMSGTYPE column\neventmsgtype_counts = df['EVENTMSGTYPE'].value_counts()\n\nprint(eventmsgtype_counts)", "assistant_code": "import pandas as pd\n\n# Load the CSV file into a pandas DataFrame\ndf = pd.read_csv('data.csv')\n\n# Count the distinct occurrences of each value in the EVENTMSGTYPE column\nevent_msg_type_counts = df['EVENTMSGTYPE'].value_counts()\n\n# Print the result\nprint(event_msg_type_counts)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Calculate and show the average EVENTNUM for each PERIOD group.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Calculate and show the average EVENTNUM for each PERIOD group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Group by 'PERIOD' and calculate the average 'EVENTNUM'\naverage_eventnum = df.groupby('PERIOD')['EVENTNUM'].mean()\n\nprint(average_eventnum)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by PERIOD and calculate the average of EVENTNUM\navg_eventnum = df.groupby('PERIOD')['EVENTNUM'].mean()\n\nprint(avg_eventnum)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Compute the mean, maximum, and minimum of the length, height, and diameter for every 'Sex'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Compute the mean, maximum, and minimum of the length, height, and diameter for every 'Sex'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Group the data by 'Sex' and calculate the mean,max,and min for 'Length','Height' and 'Diameter'\ngrouped_data = data.groupby('Sex').agg({'Length': ['mean','max','min'],'Height': ['mean','max','min'],'Diameter': ['mean','max','min']})\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the mean, maximum, and minimum of the length, height, and diameter for every 'Sex'\ngrouped = df.groupby('Sex')[['Length', 'Height', 'Diameter']].agg(['mean', 'max', 'min'])\n\nprint(grouped)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Group the data by 'Sex' and calculate the average height, length and diameter.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Group the data by 'Sex' and calculate the average height, length and diameter.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load data from csv\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Group by 'Sex' and calculate the averages of the desired columns\ngrouped_df = df.groupby('Sex')[['Height','Length','Diameter']].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'Sex' and calculate the average height, length and diameter\ngrouped_data = data.groupby('Sex')[['Height', 'Length', 'Diameter']].mean()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Display the average emotion values (anger, contempt, disgust, fear, happiness, and sadness) for each person in the 'name' column and calculate them.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Display the average emotion values (anger, contempt, disgust, fear, happiness, and sadness) for each person in the 'name' column and calculate them.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# loading data into dataframe\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# calculating the mean of the emotions for each person\nresult = df.groupby('name')[['anger','contempt','disgust','fear','happiness','sadness']].mean()\n\n# preparing the output in a canonical format\noutput = result.to_string()\n\nprint(output)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'name' and calculate the mean for each emotion\naverage_emotions = df.groupby('name')[['anger', 'contempt', 'disgust', 'fear', 'happiness', 'sadness']].mean()\n\nprint(average_emotions)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Group the maximum and minimum blur values for each person by name.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Group the maximum and minimum blur values for each person by name.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# calculate min and max blur values per person\nresult = df.groupby('name')['blur'].agg(['min','max'])\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the maximum and minimum blur values for each person by name\ngrouped_df = df.groupby('name')['blur'].agg(['max', 'min'])\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Group the data based on the names and calculate the median of 'happiness' for each group.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Group the data based on the names and calculate the median of 'happiness' for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load Data\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Group data by name and calculate the median 'happiness' of each group\ngrouped_df = df.groupby('name')['happiness'].median()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data based on the 'name' column and calculate the median of 'happiness' for each group\ngrouped_data = df.groupby('name')['happiness'].median()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Find the timestamp when the maximum 'anger' emotion was recorded for 'alvaro'.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Find the timestamp when the maximum 'anger' emotion was recorded for 'alvaro'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load data from the csv file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Filtering the data for 'alvaro'\ndf_alvaro = df[df['name'] == 'alvaro']\n\n# Find the timestamp of maximum 'anger' emotion for 'alvaro'\ntimestamp_max_anger_alvaro = df_alvaro.loc[df_alvaro['anger'].idxmax()]['timestamp']\n\nprint(timestamp_max_anger_alvaro)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for 'alvaro'\nalvaro_data = df[df['name'] == 'alvaro']\n\n# Find the maximum 'anger' timestamp\nmax_anger_timestamp = alvaro_data[alvaro_data['anger'] == alvaro_data['anger'].max()]['timestamp'].values[0]\n\nprint(max_anger_timestamp)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Calculate and display the mean age and number of occurrences for each offense category.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Calculate and display the mean age and number of occurrences for each offense category.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Compute the average 'Age' and 'Count' for each 'Category'\nresult = df.groupby('Category')[['Age','Count']].mean()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'Category' and calculate the mean age and count for each group\nresult = data.groupby('Category').agg({'Age': 'mean', 'Offense': 'count'})\n\n# Rename the columns for better understanding\nresult.columns = ['Mean Age', 'Number of Occurrences']\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Aggregate the table based on the 'Offense' column and return the count of records for each group.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Aggregate the table based on the 'Offense' column and return the count of records for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the dataset\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# grouping by 'Offense'\nresult = df.groupby('Offense').size()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'Offense' column and count the number of records for each group\ngrouped_df = df.groupby('Offense').size()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Calculate the sum of 'Count' for each unique 'Offense' in the table.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Calculate the sum of 'Count' for each unique 'Offense' in the table.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/10.csv')\n\n# Compute the sum of 'Count' for each distinct 'Offense'\noffense_count_sum = data.groupby('Offense')['Count'].sum()\n\nprint(offense_count_sum)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the sum of 'Count' for each unique 'Offense'\noffense_group = df.groupby('Offense')['Count'].sum()\n\nprint(offense_group)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Calculate the maximum, minimum and average 'Count' for each year.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Calculate the maximum, minimum and average 'Count' for each year.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# convert the 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'],format='%Y')\n\n# calculate the maximum,minimum and average 'Count' for each year\nresult = df.groupby(df['Date'].dt.year)['Count'].agg(['max','min','mean'])\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Convert 'Date' column to datetime format\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y')\n\n# Extract year from 'Date'\ndata['Year'] = data['Date'].dt.year\n\n# Group by 'Year' and calculate the maximum, minimum and average 'Count'\ngrouped_data = data.groupby('Year')['Count'].agg(['max', 'min', 'mean'])\n\n# Reset index\ngrouped_data = grouped_data.reset_index()\n\n# Print the result\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Retrieve the maximum 'Age' for each offense category.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Retrieve the maximum 'Age' for each offense category.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/10.csv\")\n\n# Retrieve the maximum 'Age' for each offense category\nresult = df.groupby('Category')['Age'].max()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'Category' and get the maximum 'Age' for each group\nmax_age_per_category = data.groupby('Category')['Age'].max()\n\nprint(max_age_per_category)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Find the maximum 'weight' of vehicles for each unique 'horsepower'.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Find the maximum 'weight' of vehicles for each unique 'horsepower'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Perform the aggregation operation to find the maximum 'weight' for each unique 'horsepower'\ndf_aggregated = df.groupby('horsepower')['weight'].max()\n\nprint(df_aggregated)", "assistant_code": "import pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Group the dataframe by 'horsepower' and get the maximum 'weight' for each group\nmax_weight = df.groupby('horsepower')['weight'].max()\n\nprint(max_weight)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Organize the data according to 'cylinders' and determine the mean 'mpg' for each grouping.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Organize the data according to 'cylinders' and determine the mean 'mpg' for each grouping.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Group the data by 'cylinders' and calculate average 'mpg' for each group\ngrouped_df = df.groupby('cylinders')['mpg'].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'cylinders' and calculate the mean 'mpg'\ngrouped_df = df.groupby('cylinders')['mpg'].mean()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Group vehicles by their origin and calculate the median value of their mpg for each group.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Group vehicles by their origin and calculate the median value of their mpg for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the file from the specified location\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Group by origin and find the median mpg for each group\ngrouped_mpg = df.groupby('origin')['mpg'].median()\n\nprint(grouped_mpg)", "assistant_code": "import pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'origin' and calculate the median of 'mpg' for each group\nmedian_mpg = df.groupby('origin')['mpg'].median()\n\nprint(median_mpg)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: List the total 'ANNUAL_RT' salary for each department('DEPTID').", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "List the total 'ANNUAL_RT' salary for each department('DEPTID').", "instruction_type": "Query-Aggregation", "reference_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load data from csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Remove any non-numeric characters in 'ANNUAL_RT' column \ndf['ANNUAL_RT'] = df['ANNUAL_RT'].replace({'\\$': '',',': ''},regex=True).astype(float)\n\n# Group by 'DEPTID' and calculate the sum of 'ANNUAL_RT'\ngrouped_df = df.groupby('DEPTID')['ANNUAL_RT'].sum()\n\n# Print the summed 'ANNUAL_RT' for each 'DEPTID'\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Aggregating the 'ANNUAL_RT' salaries by department 'DEPTID'\ndepartment_salaries = df.groupby('DEPTID')['ANNUAL_RT'].sum()\n\nprint(department_salaries)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Compute and present the aggregated Gross value for DEPTID equal to A40001.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Compute and present the aggregated Gross value for DEPTID equal to A40001.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Filter rows for 'DEPTID' = 'A40001'\ndf_filtered = df[df['DEPTID'] == 'A40001']\n\n# Calculate the sum of 'Gross' for 'DEPTID' = 'A40001'\ngross_sum = df_filtered['Gross'].sum()\n\nprint(gross_sum)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe where DEPTID is A40001\ndf_filtered = df[df['DEPTID'] == 'A40001']\n\n# Compute the aggregated Gross value\naggregated_gross = df_filtered['Gross'].sum()\n\nprint(aggregated_gross)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: For each 'JOBTITLE', calculate the average 'Gross' and display the 'JOBTITLE' and its corresponding average 'Gross'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "For each 'JOBTITLE', calculate the average 'Gross' and display the 'JOBTITLE' and its corresponding average 'Gross'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the Data\ndf = pd.read_csv(\"infiagent/csv/12.csv\")\n\n# Clean and Convert the Gross Column to numeric\ndf['Gross'] = pd.to_numeric(df['Gross'],errors='coerce')\n\n# Groupby and calculate the mean\ngrouped_df = df.groupby('JOBTITLE')['Gross'].mean().reset_index()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Handle any missing values in 'Gross' column\ndf['Gross'] = df['Gross'].fillna(0)\n\n# Calculate the average 'Gross' for each 'JOBTITLE'\navg_gross = df.groupby('JOBTITLE')['Gross'].mean()\n\nprint(avg_gross)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Calculate the average 'GUSTS' value for the entire dataset.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Calculate the average 'GUSTS' value for the entire dataset.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Calculate the average of 'GUSTS'\naverage_gusts = data['GUSTS'].mean()\n\nprint(\"Average Gusts:\",average_gusts)", "assistant_code": "# Importing necessary library\nimport pandas as pd\n\n# Loading the data\ndf = pd.read_csv('data.csv')\n\n# Calculating the average 'GUSTS' value\naverage_gusts = df['GUSTS'].mean()\n\nprint(average_gusts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Find the frequency of each unique 'DIR' value.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Find the frequency of each unique 'DIR' value.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Count occurrences of each 'DIR' value\ndir_counts = data['DIR'].value_counts()\n\nprint(dir_counts)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Find the frequency of each unique 'DIR' value\ndir_frequency = df['DIR'].value_counts()\n\nprint(dir_frequency)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Determine the overall amount spent on player salaries who achieved a batting average exceeding 0.250.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Determine the overall amount spent on player salaries who achieved a batting average exceeding 0.250.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Filter the data for players who had a batting average above 0.250\nfiltered_df = df[df['batting_average'] > 0.25]\n\n# Calculate the total salary spent on these players\ntotal_salary = filtered_df['salary_in_thousands_of_dollars'].sum()\n\nprint(total_salary)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe for players with batting average exceeding 0.250\ndf_filtered = df[df['batting_average'] > 0.250]\n\n# Calculate the total salary for these players\ntotal_salary = df_filtered['salary_in_thousands_of_dollars'].sum()\n\nprint(total_salary)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Group by the indicator_of_arbitration_eligibility and compute the average batting_average for each of the group.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Group by the indicator_of_arbitration_eligibility and compute the average batting_average for each of the group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Read CSV file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Group by 'indicator_of_arbitration_eligibility' and compute the average 'batting_average'\ngrouped = df.groupby('indicator_of_arbitration_eligibility')['batting_average'].mean()\n\nprint(grouped)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by the 'indicator_of_arbitration_eligibility' and compute the mean of 'batting_average'\ngrouped_df = df.groupby('indicator_of_arbitration_eligibility')['batting_average'].mean()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Calculate the top 5 ratio of the total number_of_hits to number_of_runs for each player.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Calculate the top 5 ratio of the total number_of_hits to number_of_runs for each player.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n# Load the data\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Calculate the ratio\ndf['hits_to_runs'] = df['number_of_hits'] / df['number_of_runs']\n\n# Calculate the top 5 ratio\ntop_5_ratio = df.nlargest(5,'hits_to_runs')\n\nprint(top_5_ratio)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the ratio of number_of_hits to number_of_runs\ndf['hits_to_runs_ratio'] = df['number_of_hits'] / df['number_of_runs']\n\n# Sort the dataframe by hits_to_runs_ratio in descending order and get the top 5\ntop_5_ratio = df.sort_values('hits_to_runs_ratio', ascending=False).head(5)\n\nprint(top_5_ratio)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Calculate the average 'wage' for male employees (where 'female' is 0) and female employees (where 'female' is 1) separately.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Calculate the average 'wage' for male employees (where 'female' is 0) and female employees (where 'female' is 1) separately.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Filter the data where 'female' is 0 i.e.,males and then calculate their average wage\navg_wage_male = df[df['female'] == 0]['wage'].mean()\n\n# Filter the data where 'female' is 1 i.e.,females and then calculate their average wage\navg_wage_female = df[df['female'] == 1]['wage'].mean()\n\n# Print the average 'wage' for male and female employees separately\nprint('Average wage for male employees: ',avg_wage_male)\nprint('Average wage for female employees: ',avg_wage_female)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'wage' for male and female employees\nmale_avg_wage = df[df['female'] == 0]['wage'].mean()\nfemale_avg_wage = df[df['female'] == 1]['wage'].mean()\n\nprint('Average wage for male employees:', male_avg_wage)\nprint('Average wage for female employees:', female_avg_wage)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Count the number of records in each group by 'black' and 'female' columns.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Count the number of records in each group by 'black' and 'female' columns.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndata = pd.read_csv('infiagent/csv/15.csv')\n\n# Group by 'black' and 'female' columns and count the number of records in each group\ngrouped_data = data.groupby(['black','female']).size()\n\n# Print the result\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'black' and 'female' columns and count the number of records in each group\ngrouped_df = df.groupby(['black', 'female']).size().reset_index(name='counts')\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Retrieve records grouped by 'educ' level and calculate the average 'wage', maximum 'wage', and minimum 'wage' for each group.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Retrieve records grouped by 'educ' level and calculate the average 'wage', maximum 'wage', and minimum 'wage' for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file using pandas read_csv function\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Group by 'educ' and then calculate average,max and min 'wage' for each group\nresult = df.groupby('educ')['wage'].agg(['mean','max','min'])\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'educ' level and calculate the average, maximum and minimum wage\ngrouped_df = df.groupby('educ').agg({'wage': ['mean', 'max', 'min']})\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Please calculate the average 'atemp' value for each season.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Please calculate the average 'atemp' value for each season.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# calculate the average 'atemp' value for each season\navg_atemp_per_season = df.groupby('season')['atemp'].mean()\n\nprint(avg_atemp_per_season)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'season' and calculate the mean of 'atemp'\naverage_atemp = df.groupby('season')['atemp'].mean()\n\nprint(average_atemp)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Count the number of records for each 'weathersit' category and sort them in descending order.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Count the number of records for each 'weathersit' category and sort them in descending order.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load data from csv\ndata = pd.read_csv('infiagent/csv/16.csv')\n\n# Count the number of records for each 'weathersit' category\nweathersit_count = data['weathersit'].value_counts()\n\n# Sort the counts in descending order\nweathersit_count_sorted = weathersit_count.sort_values(ascending=False)\n\nprint(weathersit_count_sorted)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of records for each 'weathersit' category\nweathersit_counts = df['weathersit'].value_counts()\n\n# Sort the counts in descending order\nweathersit_counts_sorted = weathersit_counts.sort_values(ascending=False)\n\nprint(weathersit_counts_sorted)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Calculate the maximum, minimum, and average number of 'casual' users for each 'yr', 'mnth', and 'hr'.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Calculate the maximum, minimum, and average number of 'casual' users for each 'yr', 'mnth', and 'hr'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# loading the data from csv\ndata = pd.read_csv('infiagent/csv/16.csv')\n\n# group by yr,mnth,and hr and calculate max,min,and mean for 'casual' column\ngrouped_data = data.groupby(['yr','mnth','hr'])['casual'].agg(['max','min','mean'])\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the maximum, minimum, and average number of 'casual' users for each 'yr', 'mnth', and 'hr'.\ngrouped_df = df.groupby(['yr', 'mnth', 'hr'])\ncasual_max = grouped_df['casual'].max()\ncasual_min = grouped_df['casual'].min()\ncasual_avg = grouped_df['casual'].mean()\n\nresult = pd.DataFrame({\n    'casual_max': casual_max,\n    'casual_min': casual_min,\n    'casual_avg': casual_avg\n})\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Divide the data into groups based on the season and then determine the average number of casual, registered, and total users for each season group.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Divide the data into groups based on the season and then determine the average number of casual, registered, and total users for each season group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Group the data by 'season' and calculate the mean of 'casual','registered',and 'cnt'\ngrouped_df = df.groupby('season')[['casual','registered','cnt']].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by season and calculate the average for casual, registered and total users\ngrouped_df = df.groupby('season').agg({'casual':'mean', 'registered':'mean', 'cnt':'mean'})\n\n# Rename the columns to represent the new values\ngrouped_df.columns = ['avg_casual', 'avg_registered', 'avg_total']\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Calculate the arithmetic mean of all the 'Market Cap' values in the dataset.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Calculate the arithmetic mean of all the 'Market Cap' values in the dataset.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove commas from the 'Market Cap' column and convert to integer\ndf['Market Cap'] = df['Market Cap'].str.replace(',','').astype(int)\n\n# Compute the average 'Market Cap'\naverage_market_cap = df['Market Cap'].mean()\n\nprint(average_market_cap)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Remove commas from 'Market Cap' column and convert it to float\ndf['Market Cap'] = df['Market Cap'].str.replace(',', '').astype(float)\n\n# Calculate the arithmetic mean of 'Market Cap' column\nmean_market_cap = df['Market Cap'].mean()\n\nprint(mean_market_cap)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Group by 'Date' and calculate the sum of 'Volume' for each day.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Group by 'Date' and calculate the sum of 'Volume' for each day.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from provided CSV file\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove commas from 'Volume' column and convert it to a numeric type\ndf['Volume'] = df['Volume'].str.replace(',','').astype(float)\n\n# Group by 'Date' and calculate the sum of 'Volume' for each day\ngrouped_df = df.groupby('Date')['Volume'].sum()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove commas from 'Volume' column and convert it to numeric\ndf['Volume'] = df['Volume'].str.replace(',', '').astype(float)\n\n# Group by 'Date' and calculate the sum of 'Volume' for each day\ngrouped_df = df.groupby('Date')['Volume'].sum()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Compute the total of 'Volume' across all rows with 'Low' values beneath 100.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Compute the total of 'Volume' across all rows with 'Low' values beneath 100.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\nfile_path = 'infiagent/csv/17.csv'\ndata = pd.read_csv(file_path)\n\n# Calculate the sum of 'Volume' for all rows where the 'Low' value is less than 100\nsum_volume = data[data['Low'] < 100]['Volume'].str.replace(',','').astype(int).sum()\n\nprint(sum_volume)", "assistant_code": "# import necessary library\nimport pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# clean 'Volume' column, replace the \",\", \"\" with \"\", then convert the string to integer\ndf['Volume'] = df['Volume'].str.replace(\",\", \"\").astype(int)\n\n# filter rows where 'Low' values are beneath 100\ndf_filtered = df[df['Low'] < 100]\n\n# compute the total of 'Volume' across all rows with 'Low' values beneath 100\ntotal_volume = df_filtered['Volume'].sum()\n\nprint(total_volume)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Determine and enumerate the number of individuals based on their 'marital-status'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Determine and enumerate the number of individuals based on their 'marital-status'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Count the number of people according to their 'marital-status'\nmarital_status_counts = data['marital-status'].value_counts()\n\nprint(marital_status_counts)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'marital-status' and count the number of individuals\nmarital_status_counts = data['marital-status'].value_counts()\n\nprint(marital_status_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Calculate the median 'final-weight' for individuals grouped by 'income'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Calculate the median 'final-weight' for individuals grouped by 'income'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Fixing the extra spaces in the column names\ndf.columns = df.columns.str.strip()\n\n# Compute the median 'final-weight' for individuals by 'income'\nresult = df.groupby('income')['final-weight'].median()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median 'final-weight' for individuals grouped by 'income'\nmedian_final_weight = df.groupby('income')['final-weight'].median()\n\nprint(median_final_weight)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Calculate the total number of distinct 'coa_dept_id' values for each 'dept_group'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Calculate the total number of distinct 'coa_dept_id' values for each 'dept_group'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Compute the total number of unique 'coard_id' for each 'dept_group'\nresult = df.groupby('dept_group')['coa_dept_id'].nunique()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'dept_group' and count distinct 'coa_dept_id'\ngrouped_df = df.groupby('dept_group')['coa_dept_id'].nunique()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Calculate the mean 'Excess Readmission Ratio' for each 'State' and order the result in descending order.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Calculate the mean 'Excess Readmission Ratio' for each 'State' and order the result in descending order.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Calculate the mean 'Excess Readmission Ratio' for each 'State'\nstatewise_mean_ratio = df.groupby('State')['Excess Readmission Ratio'].mean()\n\n# Sort the result in descending order\nsorted_statewise_mean_ratio = statewise_mean_ratio.sort_values(ascending=False)\n\nprint(sorted_statewise_mean_ratio)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean 'Excess Readmission Ratio' for each 'State'\nmean_excess_readmission_ratio = df.groupby('State')['Excess Readmission Ratio'].mean()\n\n# Order the result in descending order\nresult = mean_excess_readmission_ratio.sort_values(ascending=False)\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Retrieve the number of hospitals in each state.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Retrieve the number of hospitals in each state.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Retrieve the number of hospitals in each state\nhospital_count_by_state = df['State'].value_counts()\n\nprint(hospital_count_by_state)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'State' and count the number of hospitals in each state\nstate_counts = df['State'].value_counts()\n\nprint(state_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Compute the aggregate count of 'Number of Discharges' and 'Number of Readmissions' records for each identified 'Measure Name'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Compute the aggregate count of 'Number of Discharges' and 'Number of Readmissions' records for each identified 'Measure Name'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Group by 'Measure Name' and sum 'Number of Discharges' and 'Number of Readmissions'\nagg_df = df.groupby('Measure Name')[['Number of Discharges','Number of Readmissions']].sum()\n\nprint(agg_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Compute the aggregate count of 'Number of Discharges' and 'Number of Readmissions' for each 'Measure Name'\ngrouped_df = df.groupby('Measure Name').agg({'Number of Discharges': 'sum', 'Number of Readmissions': 'sum'})\n\n# Print the result\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Calculate the total 'damage_USD' caused by hurricanes each year.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Calculate the total 'damage_USD' caused by hurricanes each year.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Group the data by 'year' and compute the sum of 'damage_USD' for each year\ntotal_damage_per_year = df.groupby('year')['damage_USD'].sum()\n\nprint(total_damage_per_year)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Handle non-standard data: Replace the string 'None' with NaN\ndf['damage_USD'] = pd.to_numeric(df['damage_USD'].replace('None', pd.NA))\n\n# Calculate the total 'damage_USD' caused by hurricanes each year\ntotal_damage_per_year = df.groupby('year')['damage_USD'].sum()\n\nprint(total_damage_per_year)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Calculate the mean 'deaths' for storms where 'max_storm_cat' is greater than 3.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Calculate the mean 'deaths' for storms where 'max_storm_cat' is greater than 3.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Select storms with 'max_storm_cat' greater than 3\nselected_storms = df[df['max_storm_cat'] > 3]\n\n# Calculate the average 'deaths'\naverage_deaths = selected_storms['deaths'].mean()\n\nprint(average_deaths)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter rows where 'max_storm_cat' is greater than 3\ndf_filtered = df[df['max_storm_cat'] > 3]\n\n# Calculate the mean 'deaths'\nmean_deaths = df_filtered['deaths'].mean()\n\nprint(mean_deaths)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Calculate the average total vaccinations per day for each country.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Calculate the average total vaccinations per day for each country.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Convert the 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Calculate the total vaccinations per day for each country\ndf['total_vaccinations_per_day'] = df.groupby(['country',df['date'].dt.date])['total_vaccinations'].transform('sum')\n\n# Calculate the average total vaccinations per day for each country\naverage_vaccinations_per_day = df.groupby('country')['total_vaccinations_per_day'].mean()\n\nprint(average_vaccinations_per_day)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove rows where 'daily_vaccinations' is null\ndf = df.dropna(subset=['daily_vaccinations'])\n\n# Calculate the average total vaccinations per day for each country\naverage_total_vaccinations_per_day = df.groupby('country')['daily_vaccinations'].mean()\n\nprint(average_total_vaccinations_per_day)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Calculate the total 'daily_vaccinations' for each group of 'vaccines'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Calculate the total 'daily_vaccinations' for each group of 'vaccines'.", "instruction_type": "Query-Aggregation", "reference_code": "# Necessary Libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Handle missing values as zero \ndf.fillna({'daily_vaccinations':0},inplace=True)\n\n# Group the data by 'vaccines' and 'daily_vaccinations'\ngrouped_df = df.groupby('vaccines')['daily_vaccinations'].sum()\n\n# Print the result\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the total 'daily_vaccinations' for each group of 'vaccines'\ntotal_daily_vaccinations = df.groupby('vaccines')['daily_vaccinations'].sum()\n\nprint(total_daily_vaccinations)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Group the data by 'country' and 'date' and calculate the sum of 'daily_vaccinations'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Group the data by 'country' and 'date' and calculate the sum of 'daily_vaccinations'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Handling missing data\ndf['daily_vaccinations'] = df['daily_vaccinations'].fillna(0)\n\n# Grouping the data by 'country' and 'date' and calculating the sum of 'daily_vaccinations'\ngrouped_df = df.groupby(['country','date'])['daily_vaccinations'].sum().reset_index()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'country' and 'date' and calculate the sum of 'daily_vaccinations'\ngrouped_df = df.groupby(['country', 'date'])['daily_vaccinations'].sum().reset_index()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,AL,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Determine the highest 'total_vaccinations' value for each 'vaccine' in every 'country'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,AL,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,AL,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Determine the highest 'total_vaccinations' value for each 'vaccine' in every 'country'.", "instruction_type": "Query-Aggregation", "reference_code": "# Importing the Necessary Libraries\nimport pandas as pd\n\n# Loading the Data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Replacing missing values (if any)\ndf.fillna(0,inplace=True)\n\n# Grouping by 'country' and 'vaccines' and finding the maximum 'total_vaccinations'\nfinal_data = df.groupby(['country','vaccines']).total_vaccinations.max().reset_index()\n\n# Sorting the dataframe\nfinal_data.sort_values(['country','vaccines'],inplace=True)\n\n# Printing the result\nprint(final_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Group by 'country' and 'vaccines' columns and get the max of 'total_vaccinations'\ngrouped_data = data.groupby(['country', 'vaccines'])['total_vaccinations'].max()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Calculate the total 'monthly_income' for all records.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Calculate the total 'monthly_income' for all records.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Compute the total monthly_income\ntotal_monthly_income = df['monthly_income'].sum()\n\nprint(total_monthly_income)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Fill in the missing values in 'monthly_income' column with 0\ndf['monthly_income'].fillna(0, inplace=True)\n\n# Calculate the total 'monthly_income' for all records\ntotal_monthly_income = df['monthly_income'].sum()\n\nprint(total_monthly_income)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Compute the average 'monthly_income' for users having 'serious_dlqin2yrs' equal to 1 and analyze the influence of income on delinquency.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Compute the average 'monthly_income' for users having 'serious_dlqin2yrs' equal to 1 and analyze the influence of income on delinquency.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv(\"infiagent/csv/23.csv\")\n\n# Filtering the data where 'serious_dlqin2yrs' is equal to 1\nfiltered_df = df[df['serious_dlqin2yrs'] == 1]\n\n# Calculate the average of 'monthly_income' and store it in 'average_income'\naverage_income = filtered_df['monthly_income'].mean()\n\n# Print the average 'monthly_income'\nprint(average_income)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data where 'serious_dlqin2yrs' equals to 1\ndf_filtered = df[df['serious_dlqin2yrs'] == 1]\n\n# Compute the average 'monthly_income' for the filtered data\naverage_income = df_filtered['monthly_income'].mean()\n\nprint(\"The average monthly income for users with serious_dlqin2yrs equal to 1 is: \", average_income)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Aggregate the records by 'number_of_dependents' and compute the count of 'serious_dlqin2yrs' for each group to comprehend the delinquency tendencies in accordance with the number of dependents.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Aggregate the records by 'number_of_dependents' and compute the count of 'serious_dlqin2yrs' for each group to comprehend the delinquency tendencies in accordance with the number of dependents.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Handle any missing values in the two columns we will be working with\ndf['serious_dlqin2yrs'].fillna(0,inplace=True)\ndf['number_of_dependents'].fillna(0,inplace=True)\n\n# Groupby 'number_of_dependents' and calculate the count for 'serious_dlqin2yrs' in each group\nresult = df.groupby('number_of_dependents')['serious_dlqin2yrs'].count().reset_index()\n\n# Rename the column to reflect count\nresult.rename(columns={'serious_dlqin2yrs': 'count_of_serious_dlqin2yrs'},inplace=True)\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Aggregate the data by 'number_of_dependents' and compute the count of 'serious_dlqin2yrs'\naggregated_df = df.groupby('number_of_dependents')['serious_dlqin2yrs'].sum()\n\nprint(aggregated_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Compute the average 'Rating' score for each 'Ethnicity'. Return the results, sorted by the average 'Rating' in descending order.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Compute the average 'Rating' score for each 'Ethnicity'. Return the results, sorted by the average 'Rating' in descending order.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Compute the average 'Rating' score for each 'Ethnicity'\navg_rating = df.groupby('Ethnicity')['Rating'].mean()\n\n# Sort the results by the average 'Rating' in descending order\navg_rating_sorted = avg_rating.sort_values(ascending=False)\n\nprint(avg_rating_sorted)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the average 'Rating' score for each 'Ethnicity'\navg_rating_by_ethnicity = df.groupby('Ethnicity')['Rating'].mean()\n\n# Sort the results in descending order\nsorted_avg_rating_by_ethnicity = avg_rating_by_ethnicity.sort_values(ascending=False)\n\nprint(sorted_avg_rating_by_ethnicity)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Calculate the average monthly income for the entire dataset, grouping by the column 'NumberOfDependents'.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Calculate the average monthly income for the entire dataset, grouping by the column 'NumberOfDependents'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Replace 'NA' with NaN for proper calculation\ndf.replace('NA',pd.NA,inplace=True)\n\n# Change the MonthlyIncome's type to float for proper calculation\ndf['MonthlyIncome'] = df['MonthlyIncome'].astype(float)\n\n# Calculate the average monthly income for the entire dataset,grouping by 'NumberOfDependents'\naverage_income = df.groupby('NumberOfDependents')['MonthlyIncome'].mean()\n\nprint(average_income)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Replace 'NA' values with NaN\ndf.replace('NA', pd.NA, inplace=True)\n\n# Convert 'MonthlyIncome' column to numeric\ndf['MonthlyIncome'] = pd.to_numeric(df['MonthlyIncome'], errors='coerce')\n\n# Group the data by 'NumberOfDependents' and calculate the mean of 'MonthlyIncome'\naverage_income = df.groupby('NumberOfDependents')['MonthlyIncome'].mean()\n\nprint(average_income)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Group all records by 'age' and calculate the sum of 'RevolvingUtilizationOfUnsecuredLines' for each age group.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Group all records by 'age' and calculate the sum of 'RevolvingUtilizationOfUnsecuredLines' for each age group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# fill NA values with 0 for 'RevolvingUtilizationOfUnsecuredLines' column\ndf['RevolvingUtilizationOfUnsecuredLines'] = df['RevolvingUtilizationOfUnsecuredLines'].fillna(0)\n\n# group by 'age' and calculate the sum of 'RevolvingUtilizationOfUnsecuredLines'\nresult = df.groupby('age')['RevolvingUtilizationOfUnsecuredLines'].sum()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill the missing values with 0\ndf.fillna(0, inplace=True)\n\n# Group by 'age' and calculate the sum of 'RevolvingUtilizationOfUnsecuredLines' for each age group\nresult = df.groupby('age')['RevolvingUtilizationOfUnsecuredLines'].sum()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Determine the overall count of individuals aged 60 to 89 days late, classified by their age.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Determine the overall count of individuals aged 60 to 89 days late, classified by their age.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# calculate the total count of people who are 60-89 days past due not worse,grouped by age\nresult = df.groupby('age')['NumberOfTime60-89DaysPastDueNotWorse'].sum()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Handle non-standard data (NA values)\ndata = data.replace('NA', None)\n\n# Convert relevant column to integer type\ndata['age'] = data['age'].astype(int)\n\n# Filter the data for individuals aged 60 to 89 days late\nfiltered_data = data[(data['NumberOfTime60-89DaysPastDueNotWorse'] == 1)]\n\n# Count the number of individuals for each age group\nresult = filtered_data['age'].value_counts()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Calculate the total sum of the 'MonthlyIncome' column.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Calculate the total sum of the 'MonthlyIncome' column.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(\"infiagent/csv/25.csv\")\n\n# Calculate the sum of the 'MonthlyIncome' column\nsum_monthly_income = df['MonthlyIncome'].sum()\n\nprint(sum_monthly_income)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Handle non-standard data (NA values in this case) in the 'MonthlyIncome' column\ndf['MonthlyIncome'] = pd.to_numeric(df['MonthlyIncome'], errors='coerce')\n\n# Calculate the total sum of the 'MonthlyIncome' column\ntotal_income = df['MonthlyIncome'].sum()\n\nprint(total_income)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Calculate the average number of 'Wins' for each school.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Calculate the average number of 'Wins' for each school.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Calculate the average 'Wins' for each school\naverage_wins_per_school = data.groupby('School')['Wins'].mean()\n\nprint(average_wins_per_school)", "assistant_code": "import pandas as pd\n\n# Loading the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filling the NaN values in 'School' column with 'Unknown' to handle non-standard data\ndf['School'] = df['School'].fillna('Unknown')\n\n# Grouping the data by 'School' column and calculating the average of 'Wins'\naverage_wins = df.groupby('School')['Wins'].mean()\n\n# Printing the result\nprint(average_wins)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Count the number of teams for each unique 'School'.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Count the number of teams for each unique 'School'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Count the number of teams for each school\nteam_counts_per_school = data.groupby('School').size()\n\nprint(team_counts_per_school)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of teams for each unique 'School'\nteam_counts = df['School'].value_counts()\n\nprint(team_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Calculate the sum of 'NUM ROUNDS' for every 'School'.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Calculate the sum of 'NUM ROUNDS' for every 'School'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Calculate the total 'NUM ROUNDS' for each 'School'\ntotal_rounds_per_school = data.groupby('School')['NUM ROUNDS'].sum()\n\nprint(total_rounds_per_school)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Fill the missing values of 'NUM ROUNDS' with 0\ndf['NUM ROUNDS'].fillna(0, inplace=True)\n\n# Convert 'NUM ROUNDS' to numeric type\ndf['NUM ROUNDS'] = pd.to_numeric(df['NUM ROUNDS'], errors='coerce')\n\n# Calculate the sum of 'NUM ROUNDS' for every 'School'\nsum_rounds = df.groupby('School')['NUM ROUNDS'].sum()\n\nprint(sum_rounds)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Count the number of records with 'Wins' above 4 for each school.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Count the number of records with 'Wins' above 4 for each school.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Count the number of records with 'Wins' above 4 for each school\nwins_above_4_per_school = data[data['Wins'] > 4].groupby('School').size()\n\nprint(wins_above_4_per_school)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Clean the data\ndf = df.dropna(subset=['Wins', 'School'])\n\n# Filter the data\ndf = df[df['Wins'] > 4]\n\n# Group by 'School' and count the number of records\nresult = df.groupby('School').size()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Calculate the minimum, maximum and average of column 'Z'.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Calculate the minimum, maximum and average of column 'Z'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Read the CSV data file\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Getting the basic computation for column 'Z'\nmin_z = df['Z'].min()\nmax_z = df['Z'].max()\navg_z = df['Z'].mean()\n\nprint(\"Minimum of Z: \",min_z)\nprint(\"Maximum of Z: \",max_z)\nprint(\"Average of Z: \",avg_z)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the minimum, maximum and average of column 'Z'\nmin_val = df['Z'].min()\nmax_val = df['Z'].max()\navg_val = df['Z'].mean()\n\n# Save the results in a dictionary\nresult = {\"Minimum of Z\": min_val, \"Maximum of Z\": max_val, \"Average of Z\": avg_val}\n\n# Print the result\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Group the data by the date and calculate the average X, Y, Z coordinates for each date.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Group the data by the date and calculate the average X, Y, Z coordinates for each date.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Parse the date from string to datetime format\ndf['Calendar Date (TDB)'] = pd.to_datetime(df['Calendar Date (TDB)'],format=' A.D. %Y-%b-%d %H:%M:%S.%f',errors='coerce')\n\n# Group the data by date and calculate the average of X,Y,and Z\ngrouped_df = df.groupby(df['Calendar Date (TDB)'].dt.date).mean()[['X','Y','Z']]\n\n# Results print statement\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Calendar Date (TDB)' column to datetime\ndf['Calendar Date (TDB)'] = pd.to_datetime(df['Calendar Date (TDB)'])\n\n# Group by 'Calendar Date (TDB)' and calculate the mean for X, Y, Z\ngrouped_df = df.groupby('Calendar Date (TDB)')[['X', 'Y', 'Z']].mean()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Aggregate the data by 'Date' and compute the sum of 'Volume' traded on each distinct date.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Aggregate the data by 'Date' and compute the sum of 'Volume' traded on each distinct date.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Grouping the data by 'Date' and summing 'Volume'\ndf_grouped = df.groupby('Date')['Volume'].sum()\n\nprint(df_grouped)", "assistant_code": "# import the necessary libraries\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# aggregate the data by 'Date' and compute the sum of 'Volume'\ngroup_df = df.groupby('Date').Volume.sum()\n\n# print the result\nprint(group_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Find the average 'Close' price of the year 2014.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Find the average 'Close' price of the year 2014.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Convert 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Filter the data for year 2014\ndf_2014 = df[df['Date'].dt.year == 2014]\n\n# Find the average 'Close' price for the filtered data\naverage_close_2014 = df_2014['Close'].mean()\n\nprint(average_close_2014)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Filter the data for the year 2014\ndf_2014 = df[df['Date'].dt.year == 2014]\n\n# Calculate the average 'Close' price\naverage_close_price = df_2014['Close'].mean()\n\nprint(average_close_price)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Count the number of entries for each date and group the results by 'Date'.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Count the number of entries for each date and group the results by 'Date'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Group by 'Date' and calculate the count of entries for each date\ndf_grouped = df.groupby('Date').size()\n\n# Print the grouped data\nprint(df_grouped)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of entries for each date\ndf_grouped = df.groupby('Date').size()\n\nprint(df_grouped)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Calculate the total number of votes ('total_votes') for each 'state_abbr'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Calculate the total number of votes ('total_votes') for each 'state_abbr'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Group the data by 'state_abbr' and calculate the sum of 'total_votes'\ntotal_votes_per_state = df.groupby('state_abbr')['total_votes'].sum()\n\nprint(total_votes_per_state)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'state_abbr' and calculate the sum of 'total_votes'\ngrouped_df = df.groupby('state_abbr')['total_votes'].sum()\n\n# Print the result\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Calculate the average 'votes_dem' and 'votes_gop' for each state_abbr in the table.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Calculate the average 'votes_dem' and 'votes_gop' for each state_abbr in the table.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/30.csv')\n\n# Remove the commas in the 'diff' column and convert it to numeric form\ndata['diff'] = data['diff'].str.replace(',','').astype(float)\n\n# Group by 'state_abbr' and calculate the average 'votes_dem' and 'votes_gop'\ngrouped_data = data.groupby('state_abbr')[['votes_dem','votes_gop']].mean()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'votes_dem' and 'votes_gop' for each state_abbr\navg_votes = df.groupby('state_abbr')[['votes_dem', 'votes_gop']].mean()\n\nprint(avg_votes)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Calculate the average value of each 'EX' column.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Calculate the average value of each 'EX' column.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Calculate the average value of each 'EX' column\nex_columns = [column for column in df.columns if 'EX' in column]\naverage_values = df[ex_columns].mean()\n\nprint(average_values)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Define the 'EX' columns\nex_columns = ['EX1', 'EX2', 'EX3', 'EX4', 'EX5', 'EX6', 'EX7', 'EX8']\n\n# Calculate the average value of each 'EX' column\nex_means = df[ex_columns].mean()\n\n# Print the result\nprint(ex_means)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Group the data by 'NZ' and calculate the sum of 'EQ2' for each group.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Group the data by 'NZ' and calculate the sum of 'EQ2' for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data from csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# group by 'NZ' and calculate the sum of 'EQ2'\ngrouped_df = df.groupby('NZ')['EQ2'].sum()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'NZ' and calculate the sum of 'EQ2' for each group\ngrouped_data = df.groupby('NZ')['EQ2'].sum()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Count the number of rows where 'EQ7' exceeds its average value.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Count the number of rows where 'EQ7' exceeds its average value.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Calculate the mean of the 'EQ7' column\nmean_eq7 = df['EQ7'].mean()\n\n# Filter rows where 'EQ7' is greater than its mean\nfiltered_df = df[df['EQ7'] > mean_eq7]\n\n# Count the number of rows \ncount = len(filtered_df)\n\nprint(count)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'EQ7'\navg_EQ7 = df['EQ7'].mean()\n\n# Count the number of rows where 'EQ7' exceeds its average\ncount = df[df['EQ7'] > avg_EQ7].shape[0]\n\nprint(count)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Group data by 'EX1', calculate the maximum, minimum and average of 'EQ2' for each group.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Group data by 'EX1', calculate the maximum, minimum and average of 'EQ2' for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Group data by 'EX1' and calculate the maximum,minimum and average of 'EQ2'\ngrouped_data = df.groupby('EX1')['EQ2'].agg(['max','min','mean'])\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Group by 'EX1' and calculate the max, min and mean of 'EQ2'\ngrouped_df = df.groupby('EX1')['EQ2'].agg(['max', 'min', 'mean'])\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Compute the overall count of cases and deaths for each WHO region and display the WHO region, total number of cases, and total number of deaths.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Compute the overall count of cases and deaths for each WHO region and display the WHO region, total number of cases, and total number of deaths.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data with using pandas read_csv function\ndata = pd.read_csv('infiagent/csv/32.csv')\n\n# cleaning the data,removing the non-numeric part from \"No. of cases\" and \"No. of deaths\" columns.\ndata['No. of cases'] = data['No. of cases'].str.replace(r'\\[.*\\]','',regex=True)\ndata['No. of deaths'] = data['No. of deaths'].str.replace(r'\\[.*\\]','',regex=True)\n\n# converting these columns to numeric since they are string initially.\ndata['No. of cases'] = pd.to_numeric(data['No. of cases'])\ndata['No. of deaths'] = pd.to_numeric(data['No. of deaths'])\n\n# group the dataframe by 'WHO Region' and calculate the sum of 'No. of cases' and 'No. of deaths' for each group\ngrouped_data = data.groupby('WHO Region').agg({'No. of cases':'sum','No. of deaths':'sum'}).reset_index()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Clean the data by removing unnecessary characters\ndf = df.replace({\"No. of cases\": df[\"No. of cases\"].str.replace('[\\d*]', '', regex=True).astype(int)\n                 ,\"No. of deaths\": df[\"No. of deaths\"].str.replace('[\\d*]', '', regex=True).astype(int)\n                }, regex=True)\n\n# Group by 'WHO Region' and sum the 'No. of cases' and 'No. of deaths'\ngrouped_df = df.groupby('WHO Region').agg({'No. of cases': 'sum', 'No. of deaths': 'sum'})\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Tabulate the data based on 'Year', and compute the aggregated 'No. of cases' and 'No. of deaths' for each year.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Tabulate the data based on 'Year', and compute the aggregated 'No. of cases' and 'No. of deaths' for each year.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Convert 'No. of cases' and 'No. of deaths' to numeric,# ignoring any rows where this isn't possible\ndf['No. of cases'] = pd.to_numeric(df['No. of cases'],errors='coerce')\ndf['No. of deaths'] = pd.to_numeric(df['No. of deaths'],errors='coerce')\n\n# Group by 'Year' and calculate total 'No. of cases' and 'No. of deaths'\ngrouped_df = df.groupby('Year')[['No. of cases','No. of deaths']].sum()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Clean the 'No. of cases' and 'No. of deaths' columns\ndf['No. of cases'] = df['No. of cases'].str.extract('(\\d+)', expand=False).astype(float)\ndf['No. of deaths'] = df['No. of deaths'].str.extract('(\\d+)', expand=False).astype(float)\n\n# Group by 'Year' and aggregate 'No. of cases' and 'No. of deaths' \ngrouped_df = df.groupby('Year').agg({'No. of cases': 'sum', 'No. of deaths': 'sum'}).reset_index()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Compute the highest ('No. of deaths_max') and lowest ('No. of deaths_min') death counts for each WHO region.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Compute the highest ('No. of deaths_max') and lowest ('No. of deaths_min') death counts for each WHO region.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the dataframe\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Group by 'WHO Region' and compute the max and min of 'No. of deaths_max' and 'No. of deaths_min'\nresult = df.groupby('WHO Region').agg({'No. of deaths_max': 'max','No. of deaths_min': 'min'})\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Extract the relevant columns\ndeaths_data = data[['WHO Region', 'No. of deaths_max', 'No. of deaths_min']]\n\n# Group by 'WHO Region' and compute the max and min of 'No. of deaths_max' and 'No. of deaths_min' respectively\nresult = deaths_data.groupby('WHO Region').agg({'No. of deaths_max':'max', 'No. of deaths_min':'min'})\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Calculate the average number of cases ('No. of cases') for each region and group the results by 'WHO Region'.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Calculate the average number of cases ('No. of cases') for each region and group the results by 'WHO Region'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Clean the data by removing brackets content in 'No. of cases' column\ndf['No. of cases'] = df['No. of cases'].str.split('[').str[0]\n\n# Convert 'No. of cases' column to numeric\ndf['No. of cases'] = pd.to_numeric(df['No. of cases'])\n\n# Group by 'WHO Region' and calculate mean number of cases\nresult = df.groupby('WHO Region')['No. of cases'].mean()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Clean the 'No. of cases' column\ndf['No. of cases'] = df['No. of cases'].str.extract('(\\d+)', expand=False).astype(float)\n\n# Group by 'WHO Region' and calculate the average of 'No. of cases'\navg_cases = df.groupby('WHO Region')['No. of cases'].mean()\n\nprint(avg_cases)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Display the total number of mentions for each source and arrange them in descending order.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Display the total number of mentions for each source and arrange them in descending order.", "instruction_type": "Query-Aggregation", "reference_code": "# Importing Required Libraries\nimport pandas as pd\n\n# Loading the Data\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# Counting total mentions for each Source\nsource_counts = data['source'].value_counts()\n\n# Sorting the counts in descending order\nsorted_counts = source_counts.sort_values(ascending=False)\n\n# Printing the result\nprint(sorted_counts)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Count the total number of mentions for each source\nsource_counts = data['source'].value_counts()\n\n# Sort the counts in descending order\nsource_counts = source_counts.sort_values(ascending=False)\n\nprint(source_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Group the records based on 'publishedAt' and 'source', then calculate the average 'pos', 'neu', and 'neg' scores for each group.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Group the records based on 'publishedAt' and 'source', then calculate the average 'pos', 'neu', and 'neg' scores for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# loading the csv file:\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# converting 'publishedAt' column to datetime \ndf['publishedAt'] = pd.to_datetime(df['publishedAt'])\n\n# rounding the date by the hour\ndf['publishedAt'] = df['publishedAt'].dt.round('H')\n\n# grouping data by 'publishedAt' and 'source' columns and calculating the average 'pos','neu',and 'neg' score\ngrouped_df = df.groupby(['publishedAt','source']).agg({'pos':'mean','neu':'mean','neg':'mean'}).reset_index()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'publishedAt' to datetime\ndf['publishedAt'] = pd.to_datetime(df['publishedAt'])\n\n# Group by 'publishedAt' and 'source' and calculate the mean of 'pos', 'neu', 'neg'\ngrouped_df = df.groupby(['publishedAt', 'source']).agg({'pos':'mean', 'neu':'mean', 'neg':'mean'})\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Determine the maximum length among all the 'site' strings and present the result.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Determine the maximum length among all the 'site' strings and present the result.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Add a new column to the dataframe that stores the length of each 'site' string \ndf['site_length'] = df['site'].apply(len)\n\n# Find the maximum 'site' length\nmax_site_length = df['site_length'].max()\n\nprint(max_site_length)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Determine the maximum length among all the 'site' strings\nmax_length = data['site'].apply(lambda x: len(str(x))).max()\n\nprint(max_length)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Group by 'site' and calculate the average 'abs_diffsel' value for each group.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Group by 'site' and calculate the average 'abs_diffsel' value for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Group by 'site' and calculate the mean of 'abs_diffsel'\nresult = df.groupby('site')['abs_diffsel'].mean()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'site' and calculate the average 'abs_diffsel' value for each group\ngrouped_df = df.groupby('site')['abs_diffsel'].mean()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Determine the highest value among the 'positive_diffsel' entries.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Determine the highest value among the 'positive_diffsel' entries.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv(\"infiagent/csv/34.csv\")\n\n# Find the maximum value among the 'positive_diffsel' values\nmax_positive_diffsel = df['positive_diffsel'].max()\n\nprint(max_positive_diffsel)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Find the highest value in the 'positive_diffsel' column\nhighest_positive_diffsel = df['positive_diffsel'].max()\n\nprint(highest_positive_diffsel)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Determine the aggregate population across all continents and present the individual continents along with their respective total populations.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Determine the aggregate population across all continents and present the individual continents along with their respective total populations.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load csv data\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Aggregate the data by 'continent' by summing the 'pop' column\ntotal_population = df.groupby('continent')['pop'].sum()\n\nprint(total_population)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by the 'continent' column and sum the 'pop' column\naggregate_population = df.groupby('continent')['pop'].sum()\n\nprint(aggregate_population)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Find out the minimum, maximum and median population for each continent for all the records.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Find out the minimum, maximum and median population for each continent for all the records.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the csv data\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# group by 'continent' and calculate the minimum,maximum and median population\nresult = df.groupby('continent')['pop'].agg(['min','max','median'])\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'continent' and then apply the aggregation function pop to get the minimum, maximum and median population for each continent\naggregated_data = data.groupby('continent')['pop'].agg(['min', 'max', 'median'])\n\nprint(aggregated_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Compute the average GDP per capita for each nation in the year 2007, and arrange the results in ascending order.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Compute the average GDP per capita for each nation in the year 2007, and arrange the results in ascending order.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas data frame\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Calculate the average GDP per capita for all countries in 2007\naverage_gdp_2007 = df['gdpPercap_2007'].mean()\n\n# List it in ascending order which is irrelevant in this case as it is a single value\nprint(average_gdp_2007)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the average GDP per capita for each nation in the year 2007\naverage_gdp = df['gdpPercap_2007'].mean()\n\n# Arrange the results in ascending order\nsorted_average_gdp = df['gdpPercap_2007'].sort_values()\n\nprint(sorted_average_gdp)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Calculate the total aggregate value of GDP per capita for all countries from 1952 to 2007.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Calculate the total aggregate value of GDP per capita for all countries from 1952 to 2007.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndata = pd.read_csv('infiagent/csv/36.csv')\n\n# create a list of column names except the 'country' column\ngdp_columns = [col for col in data.columns if 'gdpPercap' in col]\n\n# compute the total sum of GDP per capita for all the countries from 1952 to 2007\ntotal_gdp = data[gdp_columns].sum().sum()\n\nprint(total_gdp)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the total aggregate value of GDP per capita for all countries from 1952 to 2007\ndf_agg = df.iloc[:, 1:].stack().sum()\n\nprint(df_agg)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Compute the median GDP per capita across all countries for each year.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Compute the median GDP per capita across all countries for each year.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Read the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Compute median GDP per capita for each year\nmedians = df.median(numeric_only=True)\n\nprint(medians)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# List of years for which we need to compute the median GDP per capita\nyears = [1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002, 2007]\n\n# Initialize a dictionary to store the median GDP per capita for each year\nmedian_gdp = {year: [] for year in years}\n\n# For each year, compute the median GDP per capita across all countries\nfor year in years:\n    gdp_col = f'gdpPercap_{year}'\n    median_gdp[year] = data[gdp_col].median()\n\n# Print the median GDP per capita for each year\nfor year, median in median_gdp.items():\n    print(f'Year: {year}, Median GDP per capita: {median}')"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Compute the total of all entries in the \"Age\" column.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Compute the total of all entries in the \"Age\" column.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/37.csv')\n\n# Calculate the sum of all the values in the \"Age\" column\nage_sum = data['Age'].sum()\n\nprint(age_sum)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Compute the total of all entries in the \"Age\" column\ntotal_age = df['Age'].sum()\n\nprint(total_age)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Find the total number of different 'transcript_id' for each 'Age'.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Find the total number of different 'transcript_id' for each 'Age'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv data\ndata = pd.read_csv('infiagent/csv/37.csv')\n\n# Count the occurrence of each 'transcript_id' in each 'Age'\nresult = data.groupby('Age')['transcript_id'].nunique()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'Age' and count the unique 'transcript_id'\nresult = df.groupby('Age')['transcript_id'].nunique()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Find the maximum 'Close Price' for the dates where 'No. of Trades' were higher than average.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Find the maximum 'Close Price' for the dates where 'No. of Trades' were higher than average.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Preprocessing\n# Remove leading and trailing whitespace from column names\ndata.columns = data.columns.str.strip()\n\n# Remove leading whitespace from values (they are left aligned in your example)\nfor col in data.columns:\n    if data[col].dtype == 'object':\n        data[col] = data[col].str.lstrip()\n\n# Convert columns to appropriate data types\ndata[\"No. of Trades\"] = data[\"No. of Trades\"].astype(int)\ndata[\"Close Price\"] = data[\"Close Price\"].astype(float)\n\n# Calculate the average number of trades\naverage_trades = data[\"No. of Trades\"].mean()\n\n# Find out the rows where 'No. of Trades' were higher than average\nabove_average_trades_data = data[data[\"No. of Trades\"] > average_trades]\n\n# Find the maximum 'Close Price' for these dates\nmax_close_price = above_average_trades_data[\"Close Price\"].max()\n\nprint(max_close_price)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average number of trades\naverage_trades = df['No. of Trades'].mean()\n\n# Filter the dataframe to get the rows where number of trades is greater than average\ndf_filtered = df[df['No. of Trades'] > average_trades]\n\n# Find the maximum close price from the filtered dataframe\nmax_close_price = df_filtered['Close Price'].max()\n\nprint(max_close_price)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Show the average 'Close Price' grouped by 'Symbol'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Show the average 'Close Price' grouped by 'Symbol'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# Group by 'Symbol' and calculate the average 'Close Price'\naverage_close_price = df.groupby('Symbol')['Close Price'].mean()\n\nprint(average_close_price)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Cleaning the data\ndf['Close Price'] = df['Close Price'].apply(pd.to_numeric, errors='coerce')\n\n# Calculating the average 'Close Price' grouped by 'Symbol'\naverage_close_price = df.groupby('Symbol')['Close Price'].mean()\n\nprint(average_close_price)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Compute the overall 'Turnover' for all given dates.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Compute the overall 'Turnover' for all given dates.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Calculate the total 'Turnover'\ntotal_turnover = data['Turnover'].sum()\n\nprint(\"Total Turnover:\",total_turnover)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Compute the overall 'Turnover' for all given dates\ntotal_turnover = data['Turnover'].sum()\n\nprint(total_turnover)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Compute the mean 'Total Traded Quantity' across the entire dataset.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Compute the mean 'Total Traded Quantity' across the entire dataset.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Calculate the average 'Total Traded Quantity'\naverage_traded_quantity = data['Total Traded Quantity'].mean()\n\nprint(\"Average Total Traded Quantity:\",average_traded_quantity)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the mean 'Total Traded Quantity' across the entire dataset\nmean_traded_quantity = df['Total Traded Quantity'].mean()\n\nprint(mean_traded_quantity)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Calculate the average weight and height of superheroes for each publisher.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Calculate the average weight and height of superheroes for each publisher.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Group by 'Publisher' and calculate mean 'Weight' and 'Height'\ngrouped = df.groupby('Publisher')[['Weight','Height']].mean()\n\nprint(grouped)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average weight and height of superheroes for each publisher\ndf_grouped = df.groupby('Publisher').agg({'Weight': 'mean', 'Height': 'mean'})\n\nprint(df_grouped)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Aggregate the superheroes by 'Race' and retrieve the count for each group.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Aggregate the superheroes by 'Race' and retrieve the count for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the csv data\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Group the superheroes based on 'Race' and calculate the count for each group\ngrouped_df = df.groupby('Race').size()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Aggregate the data by 'Race' and count the number of superheroes in each group\nrace_count = df['Race'].value_counts()\n\nprint(race_count)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Find out the maximum and minimum height and weight among all superheroes.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Find out the maximum and minimum height and weight among all superheroes.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Loading the data from csv\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Calculate the maximum and minimum height and weight\nmax_height = df['Height'].max()\nmin_height = df['Height'].min()\nmax_weight = df['Weight'].max()\nmin_weight = df['Weight'].min()\n\nprint('Max Height: ',max_height,'Min Height: ',min_height,'Max Weight: ',max_weight,'Min Weight: ',min_weight)", "assistant_code": "import pandas as pd\n\n# Load the CSV file into a pandas DataFrame\ndf = pd.read_csv('data.csv')\n\n# Find the maximum and minimum height and weight\nmax_height = df['Height'].max()\nmin_height = df['Height'].min()\nmax_weight = df['Weight'].max()\nmin_weight = df['Weight'].min()\n\n# Save the answer in a variable\nanswer = {'Maximum Height': max_height, 'Minimum Height': min_height, 'Maximum Weight': max_weight, 'Minimum Weight': min_weight}\n\n# Print the variable\nprint(answer)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Determine the average star rating for each of the parent brand names.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Determine the average star rating for each of the parent brand names.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Handle missing values\ndf['parent_brand_name'] = df['parent_brand_name'].fillna('Unknown')\n\n# Calculate the average star rating for each of the parent brand names\navg_star_rating = df.groupby('parent_brand_name')['star_rating'].mean()\n\nprint(avg_star_rating)", "assistant_code": "import pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n\n# Group the dataframe by 'parent_brand_name' and calculate the mean of 'star_rating'\ndf_grouped = df.groupby('parent_brand_name')['star_rating'].mean()\n\n# Reset the index to make 'parent_brand_name' a column again\ndf_grouped = df_grouped.reset_index()\n\n# Print the result\nprint(df_grouped)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Calculate the average bubble score for each star rating.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Calculate the average bubble score for each star rating.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# group by star_rating and calculate the average bubble_score for each \ngrouped_df = df.groupby('star_rating')['bubble_score'].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by star rating and calculate the average bubble score\navg_bubble_score = df.groupby('star_rating')['bubble_score'].mean()\n\nprint(avg_bubble_score)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Group the houses by the number of 'floors' and calculate the average price in each group.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Group the houses by the number of 'floors' and calculate the average price in each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Group by 'floors' and calculate the average price\ngrouped_df = df.groupby('floors')['price'].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by the 'floors' column and calculate the mean of 'price' in each group\naverage_prices = df.groupby('floors')['price'].mean()\n\n# Print the result\nprint(average_prices)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Calculate the mean 'sqft_living' for properties having over 2 bathrooms.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Calculate the mean 'sqft_living' for properties having over 2 bathrooms.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Filter the DataFrame for properties with more than 2 bathrooms\nfiltered_df = df[df['bathrooms'] > 2]\n\n# Compute the average 'sqft_living' \naverage_sqft_living = filtered_df['sqft_living'].mean()\n\nprint(average_sqft_living)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for properties with more than 2 bathrooms\nfiltered_df = df[df['bathrooms'] > 2]\n\n# Calculate the mean 'sqft_living' for the filtered data\nmean_sqft_living = filtered_df['sqft_living'].mean()\n\nprint(mean_sqft_living)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Calculate the median 'price' for each 'zipcode' group.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Calculate the median 'price' for each 'zipcode' group.", "instruction_type": "Query-Aggregation", "reference_code": "# Required Libraries\nimport pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Group by 'zipcode' and calculate the median 'price' in each group.\nresult = df.groupby('zipcode')['price'].median()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median 'price' for each 'zipcode'\nmedian_prices = df.groupby('zipcode')['price'].median()\n\nprint(median_prices)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Find the average 'importance.score' for each distinct 'row m/z' value.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Find the average 'importance.score' for each distinct 'row m/z' value.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Group the data by 'row m/z' and calculate the mean of 'importance.score'\ngrouped_df = df.groupby('row m/z')['importance.score'].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Calculate the average 'importance.score' for each distinct 'row m/z' value\naverage_importance_score = data.groupby('row m/z')['importance.score'].mean()\n\nprint(average_importance_score)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Calculate the maximum, minimum, and average values of 'row retention time' for the records.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Calculate the maximum, minimum, and average values of 'row retention time' for the records.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Find the maximum of 'row retention time'\nmax_row_retention_time = df['row retention time'].max()\n\n# Find the minimum of 'row retention time'\nmin_row_retention_time = df['row retention time'].min()\n\n# Find the average of 'row retention time'\navg_row_retention_time = df['row retention time'].mean()\n\nprint('Maximum Row Retention Time: ',max_row_retention_time)\nprint('Minimum Row Retention Time: ',min_row_retention_time)\nprint('Average Row Retention Time: ',avg_row_retention_time)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Perform the aggregation operations on 'row retention time'\nmax_value = df['row retention time'].max()\nmin_value = df['row retention time'].min()\navg_value = df['row retention time'].mean()\n\n# Store the results in a dictionary\nresult = {\n    'max_value': max_value,\n    'min_value': min_value,\n    'avg_value': avg_value\n}\n\n# Print the result\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Count the number of records where the bmi is greater than 30.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Count the number of records where the bmi is greater than 30.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Find records where the bmi is greater than 30\nbmi_greater_than_30 = df[df['bmi'] > 30]\n\n# Count the number of these records\ncount_bmi_greater_than_30 = len(bmi_greater_than_30)\n\nprint(count_bmi_greater_than_30)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Count the number of records where the bmi is greater than 30\ncount = df[df['bmi'] > 30].shape[0]\n\nprint(count)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Calculate the average charge for each region and display them in descending order.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Calculate the average charge for each region and display them in descending order.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the dataset from csv\ndata = pd.read_csv('infiagent/csv/43.csv')\n\n# Calculate the average charge for each region\naverage_charge = data.groupby('region')['charges'].mean()\n\n# Sort the results in descending order\naverage_charge_sorted = average_charge.sort_values(ascending=False)\n\nprint(average_charge_sorted)", "assistant_code": "import pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Calculate the average charge for each region\naverage_charges = df.groupby('region')['charges'].mean()\n\n# Sort the average charges in descending order\naverage_charges_sorted = average_charges.sort_values(ascending=False)\n\n# Print the sorted average charges\nprint(average_charges_sorted)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Summarize the data by 'region' and 'sex', and calculate the average 'bmi'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Summarize the data by 'region' and 'sex', and calculate the average 'bmi'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/43.csv')\n\n# Group by 'region' and 'sex',then calculate the average 'bmi'\navg_bmi = data.groupby(['region','sex'])['bmi'].mean()\n\nprint(avg_bmi)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'region' and 'sex', and calculate the average 'bmi'\ngrouped_df = df.groupby(['region', 'sex'])['bmi'].mean().reset_index()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Compute the total sum of the \"Volume\" column across all rows in the table.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Compute the total sum of the \"Volume\" column across all rows in the table.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Calculate the sum of 'Volume'\ntotal_volume = df['Volume'].sum()\n\nprint(total_volume)", "assistant_code": "import pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('data.csv')\n\n# Compute the total sum of the \"Volume\" column\ntotal_volume = df['Volume'].sum()\n\nprint(total_volume)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Group the table by \"Date\" and compute the mean of \"Volume\", \"High\", and \"Low\" values for each group.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Group the table by \"Date\" and compute the mean of \"Volume\", \"High\", and \"Low\" values for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Group by 'Date' and calculate the mean for specified columns\ngrouped_df = df.groupby('Date')[['Volume','High','Low']].mean()\n\nprint(grouped_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by \"Date\" and compute the mean of \"Volume\", \"High\", and \"Low\" values for each group\ngrouped_data = data.groupby('Date')[['Volume', 'High', 'Low']].mean()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Find the maximum value of \"High\", the minimum value of \"Low\", and the sum of \"Volume\" across the entire table.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Find the maximum value of \"High\", the minimum value of \"Low\", and the sum of \"Volume\" across the entire table.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Calculate maximum \"High\"\nmax_high = df['High'].max()\n\n# Calculate minimum \"Low\"\nmin_low = df['Low'].min()\n\n# Calculate total \"Volume\"\ntotal_volume = df['Volume'].sum()\n\nprint(\"Maximum 'High':\",max_high,\",Minimum 'Low':\",min_low,\",Total 'Volume':\",total_volume)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Find the maximum value of \"High\"\nmax_high = df['High'].max()\n\n# Find the minimum value of \"Low\"\nmin_low = df['Low'].min()\n\n# Find the sum of \"Volume\"\nsum_volume = df['Volume'].sum()\n\n# Save the answers in a variable\nanswer = {'max_high': max_high, 'min_low': min_low, 'sum_volume': sum_volume}\n\n# Print the answer\nprint(answer)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Count the number of houses that have an average number of rooms greater than 5.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Count the number of houses that have an average number of rooms greater than 5.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Filter the data where AveRooms is greater than 5\nfiltered_df = df[df['AveRooms'] > 5]\n\n# Count the number of houses\nnum_of_houses = len(filtered_df)\n\nprint(num_of_houses)", "assistant_code": "import pandas as pd\n\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe to get the houses with average number of rooms greater than 5\nfiltered_df = df[df['AveRooms'] > 5]\n\n# Count the number of houses\ncount = len(filtered_df)\n\nprint(count)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Group the houses based on \"HouseAge\" and calculate the average \"MedInc\" for each group.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Group the houses based on \"HouseAge\" and calculate the average \"MedInc\" for each group.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Group the houses based on \"HouseAge\" and calculate the average \"MedInc\"\nresult = df.groupby('HouseAge')['MedInc'].mean()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'HouseAge' and calculate the mean of 'MedInc' for each group\naverage_income = df.groupby('HouseAge')['MedInc'].mean()\n\n# Print the result\nprint(average_income)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Generate a listing to display each 'SUBJECT' along with the average of 'Value' respective to 'LOCATION'.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Generate a listing to display each 'SUBJECT' along with the average of 'Value' respective to 'LOCATION'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/46.csv')\n\n# Group the data by 'SUBJECT' and 'LOCATION',and find the average of 'Value'\ngrouped_data = data.groupby(['SUBJECT','LOCATION'])['Value'].mean()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'LOCATION' and 'SUBJECT', then calculate the mean of 'Value'\ngrouped_df = df.groupby(['LOCATION', 'SUBJECT'])['Value'].mean().reset_index()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Display the number of records for each distinct 'SUBJECT'.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Display the number of records for each distinct 'SUBJECT'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Counting the number of records for each unique subject\ncount_records = df['SUBJECT'].value_counts()\n\nprint(count_records)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'SUBJECT' and count the number of records for each distinct 'SUBJECT'\nsubject_counts = df['SUBJECT'].value_counts()\n\nprint(subject_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Summarize the total 'Value' for every 'SUBJECT' under 'FREQUENCY' labeled as \"A\" in the year 2012.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Summarize the total 'Value' for every 'SUBJECT' under 'FREQUENCY' labeled as \"A\" in the year 2012.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Filter data for the year 2012 and frequency \"A\"\nfiltered_data = df[(df['TIME'] == 2012) & (df['FREQUENCY'] == 'A')]\n\n# Compute the sum of 'Value' for each 'SUBJECT' \nresult = filtered_data.groupby('SUBJECT')['Value'].sum()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data for 'FREQUENCY' labeled as \"A\" and the year 2012\nfiltered_data = data[(data['FREQUENCY'] == 'A') & (data['TIME'] == '2012')]\n\n# Summarize the total 'Value' for every 'SUBJECT'\ntotal_value = filtered_data.groupby('SUBJECT')['Value'].sum()\n\nprint(total_value)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Determine the highest and lowest values for 'Health Professions' spanning from 1970 to 1979.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Determine the highest and lowest values for 'Health Professions' spanning from 1970 to 1979.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Filter the rows from 1970 to 1979\nfiltered_df = df[(df['Year'] >= 1970) & (df['Year'] <= 1979)]\n\n# Calculate the maximum and minimum value for 'Health Professions'\nmax_val = filtered_df['Health Professions'].max()\nmin_val = filtered_df['Health Professions'].min()\n\nprint(max_val,min_val)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for 'Health Professions' column\nhealth_professions = df['Health Professions']\n\n# Find the highest and lowest values\nhighest_value = health_professions.max()\nlowest_value = health_professions.min()\n\n# Save the answer in a variable and print the variable\nanswer = (highest_value, lowest_value)\nprint(answer)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Compute the mean of 'Biology', 'Physical Sciences', and 'Engineering' for each distinct value of 'Year' by aggregating the data.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Compute the mean of 'Biology', 'Physical Sciences', and 'Engineering' for each distinct value of 'Year' by aggregating the data.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# group by 'Year' and calculate the mean for 'Biology','Physical Sciences' and 'Engineering'\ndf_group = df.groupby('Year')[['Biology','Physical Sciences','Engineering']].mean()\n\nprint(df_group)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the mean of 'Biology', 'Physical Sciences', and 'Engineering' for each distinct value of 'Year'\nresult = df.groupby('Year')[['Biology', 'Physical Sciences', 'Engineering']].mean()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Find the average 'Art and Performance' value for the years where 'Social Sciences and History' is above its overall average.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Find the average 'Art and Performance' value for the years where 'Social Sciences and History' is above its overall average.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/47.csv')\n\n# Calculate the overall average of 'Social Sciences and History'\naverage_social_sciences_history = data['Social Sciences and History'].mean()\n\n# Filter the data where 'Social Sciences and History' is above its overall average\nfiltered_data = data[data['Social Sciences and History'] > average_social_sciences_history]\n\n# Find the average 'Art and Performance' value for the filtered data\naverage_art_performance = filtered_data['Art and Performance'].mean()\n\nprint(average_art_performance)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall average for 'Social Sciences and History'\nsocial_sciences_avg = df['Social Sciences and History'].mean()\n\n# Filter the years where 'Social Sciences and History' is above its overall average\nfiltered_df = df[df['Social Sciences and History'] > social_sciences_avg]\n\n# Calculate the average 'Art and Performance' for the filtered years\nart_performance_avg = filtered_df['Art and Performance'].mean()\n\nprint(art_performance_avg)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Calculate the total sum of 'Agriculture' and 'Architecture' for all years combined.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Calculate the total sum of 'Agriculture' and 'Architecture' for all years combined.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load csv file into pandas DataFrame\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Find the total sum for Agriculture and Architecture\ntotal_agri = df['Agriculture'].sum()\ntotal_archi = df['Architecture'].sum()\n\n# Combine both for the desired result\ntotal_sum = total_agri + total_archi\n\nprint(total_sum)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Calculate the total sum of 'Agriculture' and 'Architecture' for all years combined\ntotal_sum = data[['Agriculture', 'Architecture']].sum().sum()\n\nprint(total_sum)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Compute the cumulative sum for each class column and label it as 'Total_class'.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Compute the cumulative sum for each class column and label it as 'Total_class'.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# calculate the total sum for each class column\ntotal_class_dict = df.iloc[:,1:].sum().to_dict()\n\nprint(total_class_dict)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the cumulative sum for each column\ndf['Total_class1'] = df['class1'].cumsum()\ndf['Total_class2'] = df['class2'].cumsum()\ndf['Total_class3'] = df['class3'].cumsum()\ndf['Total_class4'] = df['class4'].cumsum()\ndf['Total_class5'] = df['class5'].cumsum()\ndf['Total_class6'] = df['class6'].cumsum()\ndf['Total_class7'] = df['class7'].cumsum()\ndf['Total_class8'] = df['class8'].cumsum()\ndf['Total_class9'] = df['class9'].cumsum()\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Compute the frequency counts for each unique value in the 'class5' column.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Compute the frequency counts for each unique value in the 'class5' column.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Loading the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Group the data by 'class5' and get counts of each group\ngrouped_data = df.groupby('class5').size()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Compute the frequency counts for each unique value in the 'class5' column\nclass5_counts = df['class5'].value_counts()\n\nprint(class5_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Compute the maximum and minimum of \"High\" and \"Low\" prices across the entire dataset, as well as their average.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Compute the maximum and minimum of \"High\" and \"Low\" prices across the entire dataset, as well as their average.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/49.csv')\n\n# Calculate maximum,minimum and average of \"High\" prices\nmax_high = data['High'].max()\nmin_high = data['High'].min()\navg_high = data['High'].mean()\n\n# Calculate maximum,minimum and average of \"Low\" prices\nmax_low = data['Low'].max()\nmin_low = data['Low'].min()\navg_low = data['Low'].mean()\n\n# Output the results\nprint(\"Max High: \",max_high)\nprint(\"Min High: \",min_high)\nprint(\"Average High: \",avg_high)\nprint(\"Max Low: \",max_low)\nprint(\"Min Low: \",min_low)\nprint(\"Average Low: \",avg_low)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Compute the maximum and minimum of \"High\" and \"Low\" prices across the entire dataset\nmax_high = data['High'].max()\nmin_high = data['High'].min()\nmax_low = data['Low'].max()\nmin_low = data['Low'].min()\n\n# Compute the average of \"High\" and \"Low\" prices across the entire dataset\navg_high = data['High'].mean()\navg_low = data['Low'].mean()\n\nprint(\"Max High: \", max_high, \" Min High: \", min_high, \" Avg High: \", avg_high)\nprint(\"Max Low: \", max_low, \" Min Low: \", min_low, \" Avg Low: \", avg_low)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Group the data by the hour of the \"Time\", and calculate the total volume traded each hour.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Group the data by the hour of the \"Time\", and calculate the total volume traded each hour.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the given csv file\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Convert 'Time' series data into pandas datetime\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Extracting hour from 'Time' and creating a new column 'Hour'\ndf['Hour'] = df['Time'].dt.hour\n\n# Group the data by 'Hour' and calculate total volume traded each hour\ntraded_volume = df.groupby('Hour')['Volume'].sum()\n\nprint(traded_volume)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Time' column to datetime format\ndf['Time'] = pd.to_datetime(df['Time'])\n\n# Extract the hour from the 'Time' column\ndf['Hour'] = df['Time'].dt.hour\n\n# Group the data by 'Hour' and calculate the total volume traded each hour\ngrouped_df = df.groupby('Hour')['Volume'].sum()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Compute the total sum of \"Volume\" for each distinct \"Close\" price value.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Compute the total sum of \"Volume\" for each distinct \"Close\" price value.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/49.csv')\n\n# Group the data by \"Close\" and calculate sum of \"Volume\"\nresult = data.groupby('Close')['Volume'].sum()\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Compute the total sum of \"Volume\" for each distinct \"Close\" price value\nresult = data.groupby('Close')['Volume'].sum()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Compute the minimum, maximum, and average humidity levels for each day.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Compute the minimum, maximum, and average humidity levels for each day.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Convert the 'day' column to datetime\ndf['day'] = pd.to_datetime(df['day'])\n\n# Extract only the date from 'day' column\ndf['day'] = df['day'].dt.date\n\n# Compute the minimum,maximum,and average humidity levels for each day\nresult = df.groupby('day')['humidity'].agg(['min','max','mean'])\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'humidity' column to numeric type\ndf['humidity'] = pd.to_numeric(df['humidity'])\n\n# Group the data by 'day' and calculate the minimum, maximum, and average humidity levels\ngrouped_df = df.groupby('day')['humidity'].agg(['min', 'max', 'mean'])\n\n# Print the result\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Compute the mean wind speed for every distinct 'city' entry.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Compute the mean wind speed for every distinct 'city' entry.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Calculate the average wind speed for each unique 'city' value\naverage_wind_speed = df.groupby('city')['wind_speed'].mean()\n\n# Print the result\nprint(average_wind_speed)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Compute the mean wind speed for every distinct 'city' entry\nmean_wind_speed = df.groupby('city')['wind_speed'].mean()\n\nprint(mean_wind_speed)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Calculate the sum of 'dist' for each 'city' and display the city name along with the corresponding total distance.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Calculate the sum of 'dist' for each 'city' and display the city name along with the corresponding total distance.", "instruction_type": "Query-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/50.csv')\n\n# Group the data by 'city' and sum 'dist' for each city\ngrouped_data = data.groupby('city')['dist'].sum()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Calculate the sum of 'dist' for each 'city'\ngrouped_data = data.groupby('city')['dist'].sum()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Calculate the 'wage' to 'exper' ratio for each record and display the top 10 records with the highest ratio.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Calculate the 'wage' to 'exper' ratio for each record and display the top 10 records with the highest ratio.", "instruction_type": "Query-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Calculate the 'wage' to 'exper' ratio,store it in a new column 'wage_exper_ratio'\ndf['wage_exper_ratio'] = df['wage'] / df['exper']\n\n# Sort the dataframe based on 'wage_exper_ratio' in a descending order to get records with highest ratio\ndf_sorted = df.sort_values(by='wage_exper_ratio',ascending=False)\n\n# Display the top 10 records\ntop10_records = df_sorted\n\n# final result\nprint(top10_records)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the 'wage' to 'exper' ratio\ndf['wage_to_exper_ratio'] = df['wage'] / df['exper']\n\n# Sort the dataframe by the calculated ratio in descending order and select the top 10 records\ntop_10_ratio = df.sort_values(by='wage_to_exper_ratio', ascending=False).head(10)\n\nprint(top_10_ratio)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: For each storm, subtract the maximum sustained wind ('max_sust_wind') from the minimum pressure ('min_p') and identify the top 5 storms with the highest discrepancies.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "For each storm, subtract the maximum sustained wind ('max_sust_wind') from the minimum pressure ('min_p') and identify the top 5 storms with the highest discrepancies.", "instruction_type": "Query-Basic Computation", "reference_code": "# Required library for data manipulation\nimport pandas as pd\n\n# Load csv data\ndata = pd.read_csv('infiagent/csv/21.csv')\n\n# Subtract min_p from max_sust_wind and create a new column \"discrepancy\"\ndata['discrepancy'] = data['max_sust_wind'] - data['min_p']\n\n# Sort the data based on discrepancy and print top 5 storms\nsorted_data = data.sort_values(by=['discrepancy'],ascending=False).head(5)\n\nprint(sorted_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the discrepancy between min_p and max_sust_wind\ndf['discrepancy'] = df['min_p'] - df['max_sust_wind']\n\n# Sort the dataframe by discrepancy in descending order and select the top 5 rows\ntop_5_discrepancies = df.sort_values(by='discrepancy', ascending=False).head(5)\n\nprint(top_5_discrepancies)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Display the relative change in 'Close Price' between the first and last dates for stock symbol 'GODREJIND'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Display the relative change in 'Close Price' between the first and last dates for stock symbol 'GODREJIND'.", "instruction_type": "Query-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Remove leading whitespaces in column entries\ndata = data.applymap(lambda x: x.strip() if isinstance(x,str) else x)\n\n# Convert 'Close Price' column to float type\ndata['Close Price'] = data['Close Price'].astype(float)\n\n# Select the data for the symbol 'GODREJIND'\ndata_godrejind = data[data['Symbol'] == 'GODREJIND']\n\n# Calculate the percentage change in 'Close Price' \npercentage_change_close_price = ((data_godrejind['Close Price'].iloc[-1] - data_godrejind['Close Price'].iloc[0]) / data_godrejind['Close Price'].iloc[0]) * 100\n\nprint(f\"The percentage change in 'Close Price' from first date to last date for symbol 'GODREJIND' is {percentage_change_close_price}%.\")", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Clean the data\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Close Price'] = pd.to_numeric(df['Close Price'], errors='coerce')\n\n# Filter the data for the stock symbol 'GODREJIND'\ndf_godrej = df[df['Symbol'] == 'GODREJIND']\n\n# Calculate the relative change in 'Close Price' between the first and last dates\nrelative_change = (df_godrej['Close Price'].iloc[-1] - df_godrej['Close Price'].iloc[0]) / df_godrej['Close Price'].iloc[0] * 100\n\nprint(relative_change)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Calculate the median price for properties having 3 bedrooms.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Calculate the median price for properties having 3 bedrooms.", "instruction_type": "Query-Basic Computation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# filter the data for properties with 3 bedrooms\ndf = df[df['bedrooms'] == 3]\n\n# calculate the median price\nmedian_price = df['price'].median()\n\nprint(median_price)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for properties with 3 bedrooms\ndf_3_bedrooms = df[df['bedrooms'] == 3]\n\n# Calculate the median price\nmedian_price = df_3_bedrooms['price'].median()\n\nprint(median_price)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Determine the percentage variation in 'Math and Statistics' between 1970 and 1979.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Determine the percentage variation in 'Math and Statistics' between 1970 and 1979.", "instruction_type": "Query-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Select the rows for the years 1970 and 1979\ndf_selected_years = df[df['Year'].isin([1970,1979])]\n\n# Calculate the percent change in 'Math and Statistics' from 1970 to 1979\npercent_change = ((df_selected_years[df_selected_years['Year'] == 1979]['Math and Statistics'].values[0]\n                   - df_selected_years[df_selected_years['Year'] == 1970]['Math and Statistics'].values[0]) \n                   / df_selected_years[df_selected_years['Year'] == 1970]['Math and Statistics'].values[0]) * 100\n\nprint(percent_change)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Extract the values for 'Math and Statistics' in 1970 and 1979\nmath_stats_1970 = df.loc[df['Year'] == 1970, 'Math and Statistics'].values[0]\nmath_stats_1979 = df.loc[df['Year'] == 1979, 'Math and Statistics'].values[0]\n\n# Calculate the percentage variation\npercentage_variation = ((math_stats_1979 - math_stats_1970) / math_stats_1970) * 100\n\nprint(percentage_variation)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Compute the variance between the 'High' and 'Low' columns for each row.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Compute the variance between the 'High' and 'Low' columns for each row.", "instruction_type": "Query-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Calculate the difference between 'High' and 'Low' values\ndf['Difference'] = df['High'] - df['Low']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Compute the variance between the 'High' and 'Low' columns for each row\ndata['Price_Variance'] = data['High'] - data['Low']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Find the percentage difference in population for each country between 1952 and 1997.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Find the percentage difference in population for each country between 1952 and 1997.", "instruction_type": "Query-Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Convert the 'pop' column to numeric type, handling any errors (like conversion errors or missing values)\ndf['pop'] = pd.to_numeric(df['pop'], errors='coerce')\n\n# Group by country\ngrouped = df.groupby('country')\n\n# Define a function to calculate the percentage change within each group\ndef calculate_percentage_change(group):\n    # Select the population values for 1952 and 1997\n    pop_1952 = group[group['year'] == 1952]['pop'].iloc[0]\n    pop_1997 = group[group['year'] == 1997]['pop'].iloc[0]\n    \n    # Calculate the percentage change\n    percentage_change = ((pop_1997 - pop_1952) / pop_1952) * 100 if pop_1952 != 0 else None\n    return pd.Series({'percentage_change': percentage_change})\n\n# Apply the function to each group and store the results in a new DataFrame\npercentage_change_df = grouped.apply(calculate_percentage_change)\n\n# Reset the index to make the country column a regular column again\npercentage_change_df = percentage_change_df.reset_index()\n\n# Print the result\nprint(percentage_change_df[['country', 'percentage_change']])\n", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for the years 1952 and 1997\ndf_1952 = df[df['year'] == 1952]\ndf_1997 = df[df['year'] == 1997]\n\n# Calculate the population difference for each country\npopulation_diff = df_1997['pop'] / df_1952['pop']\n\n# Convert the result to a dictionary\npopulation_diff_dict = population_diff.to_dict()\n\nprint(population_diff_dict)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Display 'ANNUAL_RT' in US Dollar currency format.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Display 'ANNUAL_RT' in US Dollar currency format.", "instruction_type": "Query-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Formatting 'ANNUAL_RT' as currency in US Dollar format\ndf['ANNUAL_RT'] = df['ANNUAL_RT'].map('${:,.2f}'.format)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Format 'ANNUAL_RT' column in US Dollar currency format\ndf['ANNUAL_RT'] = df['ANNUAL_RT'].apply(lambda x: '${:,.2f}'.format(x))\n\n# Print the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Obtain the total number of rows within the table.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Obtain the total number of rows within the table.", "instruction_type": "Query-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/12.csv\")\n\n# Get the total number of rows\ntotal_rows = len(df)\n\nprint(total_rows)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Get the total number of rows\ntotal_rows = df.shape[0]\n\nprint(total_rows)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Calculate the average \"price\" of diamonds for each \"cut\" and rank them in descending order.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Calculate the average \"price\" of diamonds for each \"cut\" and rank them in descending order.", "instruction_type": "Query-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Calculate the average price for each cut\naverage_prices = df.groupby('cut')['price'].mean()\n\n# Rank the average prices in descending order\nranked_prices = average_prices.sort_values(ascending=False)\n\nprint(ranked_prices)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average price for each cut\naverage_price = df.groupby('cut')['price'].mean()\n\n# Sort the cuts by average price in descending order\naverage_price_sorted = average_price.sort_values(ascending=False)\n\nprint(average_price_sorted)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Classify the records by \"color\" and \"clarity\" and calculate the number of diamonds in each category.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Classify the records by \"color\" and \"clarity\" and calculate the number of diamonds in each category.", "instruction_type": "Query-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Group the data on 'color' and 'clarity' and count the number of records in each group\ngrouped_df = df.groupby(['color','clarity']).size().reset_index(name='count')\n\nprint(grouped_df)", "assistant_code": "# Importing the necessary libraries\nimport pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Group the data by 'color' and 'clarity' and count the number of diamonds in each category\ngrouped_data = data.groupby(['color', 'clarity']).size()\n\n# Print the result\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Find the sum, average, minimum, and maximum of the \"price\" for each \"color\" group, and arrange the results in descending order based on the sum of the \"price\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Find the sum, average, minimum, and maximum of the \"price\" for each \"color\" group, and arrange the results in descending order based on the sum of the \"price\".", "instruction_type": "Query-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Group by 'color' and calculate sum,average,min,max of 'price'\nresult = df.groupby('color')['price'].agg(['sum','mean','min','max'])\n\n# Sort the result by sum of 'price' in descending order\nresult = result.sort_values(by='sum',ascending=False)\n\n# Print the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'color' and calculate the sum, average, minimum, and maximum of the 'price'\ngrouped_df = df.groupby('color')['price'].agg(['sum', 'mean', 'min', 'max'])\n\n# Sort by the sum of the 'price' in descending order\nsorted_df = grouped_df.sort_values(by='sum', ascending=False)\n\n# Print the result\nprint(sorted_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Get the unique values in the 'standard_indentification_level_1' column.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Get the unique values in the 'standard_indentification_level_1' column.", "instruction_type": "Query-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Find the distinct values in the 'standard_indentification_level_1' column\ndistinct_values = df['standard_indentification_level_1'].unique()\n\nprint(distinct_values)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Get the unique values in the 'standard_indentification_level_1' column\nunique_values = df['standard_indentification_level_1'].unique()\n\nprint(unique_values)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Display the count of 'station' for each 'origin_station'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Display the count of 'station' for each 'origin_station'.", "instruction_type": "Query-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# Group the data by 'origin_station' and count the 'station'\ngrouped_data = data.groupby('origin_station')['station'].count()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'origin_station' and count the number of 'station'\nresult = df.groupby('origin_station')['station'].count()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Organize the records according to the 'DIR' category and calculate the number of 'VIS' occurrences in each group.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Organize the records according to the 'DIR' category and calculate the number of 'VIS' occurrences in each group.", "instruction_type": "Query-Grouping", "reference_code": "import pandas as pd\n\n# load the data\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# group the records by 'DIR' and count the number of 'VIS' in each group\ngrouped_data = data.groupby('DIR')['VIS'].count()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group by 'DIR' and count 'VIS' occurrences\ngrouped_df = df.groupby('DIR')['VIS'].count()\n\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Display the department names, grouped by 'dept_group'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Display the department names, grouped by 'dept_group'.", "instruction_type": "Query-Grouping", "reference_code": "# import necessary libraries\nimport pandas as pd\n\n# load the csv data\ndata = pd.read_csv('infiagent/csv/19.csv')\n\n# group by 'dept_group' and get the department names in each group\ngrouped = data.groupby('dept_group')['Department Name'].apply(list)\n\nprint(grouped)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Group by 'dept_group' and get the list of department names\ngrouped = df.groupby('dept_group')['Department Name'].apply(list)\n\nprint(grouped)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Group the records by 'State' and find the maximum 'Number of Discharges' in each group.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Group the records by 'State' and find the maximum 'Number of Discharges' in each group.", "instruction_type": "Query-Grouping", "reference_code": "import pandas as pd\n\n# Read the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\ndf['Number of Discharges'] = pd.to_numeric(df['Number of Discharges'], errors='coerce')\n\n# Group by 'State' and find the maximum 'Number of Discharges'\nresult = df.groupby('State')['Number of Discharges'].max()\n\n# Print the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'State' and find the maximum 'Number of Discharges' in each group\nresult = df.groupby('State')['Number of Discharges'].max()\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Group the data by 'max_storm_cat' and count the frequency of each category, representing the number of storms.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Group the data by 'max_storm_cat' and count the frequency of each category, representing the number of storms.", "instruction_type": "Query-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Group the dataframe by 'max_storm_cat' and count the number of storms in each category\nstorm_counts = df.groupby('max_storm_cat').size()\n\nprint(storm_counts)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'max_storm_cat' and count the frequency of each category\nstorm_cat_counts = df.groupby('max_storm_cat').size()\n\nprint(storm_cat_counts)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Display the total number of articles published by each author, sorted by the month in which they were published.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Display the total number of articles published by each author, sorted by the month in which they were published.", "instruction_type": "Query-Grouping", "reference_code": "import pandas as pd\n\n# load the csv data\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# convert 'publishedAt' from string to datetime format\ndf['publishedAt'] = pd.to_datetime(df['publishedAt'])\n\n# create a new column 'Month' for the month of publication\ndf['Month'] = df['publishedAt'].dt.month\n\n# Group by 'author' and 'Month' and count the number of articles\nauthor_month_counts = df.groupby(['author','Month']).size()\n\nprint(author_month_counts)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('data.csv')\n\n# Convert the 'publishedAt' column to datetime\ndata['publishedAt'] = pd.to_datetime(data['publishedAt'])\n\n# Extract the month from the 'publishedAt' column\ndata['month'] = data['publishedAt'].dt.month\n\n# Group by 'author' and 'month', count the number of articles for each group\narticles_count = data.groupby(['author', 'month']).size().reset_index(name='articles_count')\n\n# Sort the dataframe by 'author' and 'month'\narticles_count = articles_count.sort_values(['author', 'month'])\n\nprint(articles_count)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Aggregate all rows by continent and subsequently, group the data within each continent by country to display the average GDP per capita for each country.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Aggregate all rows by continent and subsequently, group the data within each continent by country to display the average GDP per capita for each country.", "instruction_type": "Query-Grouping", "reference_code": "import pandas as pd\n\n# load data from csv file\ndata = pd.read_csv('infiagent/csv/35.csv')\n\n# group data by continent and country,then calculate the average GDP per capita\ngrouped_data = data.groupby(['continent','country'])['gdppercap'].mean()\n\nprint(grouped_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Aggregate the data by continent and then by country\ngrouped_data = data.groupby(['continent', 'country']).agg({'gdppercap': 'mean'}).reset_index()\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Filter the data to show only records where the act_dep_time is between 05:00:00 and 06:00:00.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Filter the data to show only records where the act_dep_time is between 05:00:00 and 06:00:00.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the data\ndata = pd.read_csv(\"infiagent/csv/2.csv\")\n\n# Convert act_dep_time to datetime type\ndata['act_dep_time'] = pd.to_datetime(data['act_dep_time'])\n\n# Create a mask for the time range\nmask = (data['act_dep_time'].dt.time >= datetime.strptime('05:00:00','%H:%M:%S').time()) & \\\n       (data['act_dep_time'].dt.time <= datetime.strptime('06:00:00','%H:%M:%S').time())\n\n# Apply the mask to the data\nfiltered_data = data[mask]\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Convert the 'act_dep_time' column to datetime\ndata['act_dep_time'] = pd.to_datetime(data['act_dep_time'])\n\n# Filter the data to only include records where 'act_dep_time' is between 05:00:00 and 06:00:00\nfiltered_data = data[(data['act_dep_time'].dt.hour >= 5) & (data['act_dep_time'].dt.hour < 6)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Retrieve the data for all countries based in 'Western Europe'.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646", "csv_path": "infiagent/csv/3.csv", "instruction": "Retrieve the data for all countries based in 'Western Europe'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# selecting the data where Region is 'Western Europe'\nwestern_europe = df[df['Region'] == 'Western Europe']\n\nprint(western_europe)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for countries in 'Western Europe'\nwestern_europe_df = df[df['Region'] == 'Western Europe']\n\nprint(western_europe_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: It's needed to obtain records where the 'Trust (Government Corruption)' value is upper than 0.40 and the 'Region' is 'Western Europe'.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "It's needed to obtain records where the 'Trust (Government Corruption)' value is upper than 0.40 and the 'Region' is 'Western Europe'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/3.csv')\n\n# Filter the data where 'Trust (Government Corruption)' is greater than 0.40 and 'Region' is 'Western Europe'\nfiltered_data = data[(data['Trust (Government Corruption)'] > 0.40) & (data['Region'] == 'Western Europe')]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'Trust (Government Corruption)' > 0.40 and 'Region' is 'Western Europe'\nfiltered_data = data[(data['Trust (Government Corruption)'] > 0.40) & (data['Region'] == 'Western Europe')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Retrieve all entries with a 'Happiness Score' higher than the average 'Happiness Score' for all 'Western Europe' nations.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Retrieve all entries with a 'Happiness Score' higher than the average 'Happiness Score' for all 'Western Europe' nations.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Calculate the average 'Happiness Score' of all 'Western Europe' countries.\navg_western_europe_happiness_score = df[df['Region'] == 'Western Europe']['Happiness Score'].mean()\n\n# Fetch all entries where the 'Happiness Score' is above \n# the average 'Happiness Score' of all 'Western Europe' countries.\nresult_df = df[df['Happiness Score'] > avg_western_europe_happiness_score]\n\nprint(result_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'Happiness Score' for all 'Western Europe' nations\naverage_happiness_score = df[df['Region'] == 'Western Europe']['Happiness Score'].mean()\n\n# Retrieve all entries with a 'Happiness Score' higher than the average 'Happiness Score' for 'Western Europe'\nresult = df[df['Happiness Score'] > average_happiness_score]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Select rows where the 'timestamp' is between 12:00:00 AM and 3:00:00 AM.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Select rows where the 'timestamp' is between 12:00:00 AM and 3:00:00 AM.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\nfile_path = 'infiagent/csv/4.csv'\ndata = pd.read_csv(file_path)\n\n# Apply range screening\nresult = data[(data['timestamp'] >= '12:00:00 AM') & (data['timestamp'] <= '3:00:00 AM')]\n\n# Print the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'timestamp' column to datetime\ndf['timestamp'] = pd.to_datetime(df['timestamp'])\n\n# Set time as index\ndf.set_index('timestamp', inplace=True)\n\n# Select rows between 12:00:00 AM and 3:00:00 AM\ndf_selected = df.between_time('12:00:00', '03:00:00')\n\nprint(df_selected)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Select rows where the timestamp includes '1:00:00 AM'.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Select rows where the timestamp includes '1:00:00 AM'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Select rows where the timestamp contains '1:00:00 AM'\nfiltered_data = df[df['timestamp'].str.contains('1:00:00 AM')]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data to include only rows where the timestamp is '1:00:00 AM'\nfiltered_data = data[data['timestamp'] == 'Apr 13  2017 1:00:00 AM']\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Select the rows where the 'Source' is 0 and 'Weight' is greater than 10.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Select the rows where the 'Source' is 0 and 'Weight' is greater than 10.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Loading the csv file\ndataframe = pd.read_csv('infiagent/csv/5.csv')\n\n# Querying the data\nselected_rows = dataframe[(dataframe['Source'] == 0) & (dataframe['Weight'] > 10)]\n\nprint(selected_rows)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select the rows where 'Source' is 0 and 'Weight' is greater than 10\nselected_rows = data[(data['Source'] == 0) & (data['Weight'] > 10)]\n\nprint(selected_rows)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Retrieve all rows where the 'Type' column equals 'Directed'.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Retrieve all rows where the 'Type' column equals 'Directed'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# loading the csv file\ndata = pd.read_csv('infiagent/csv/5.csv')\n\n# filtering the rows where 'Type' column equals 'Directed'\nfiltered_data = data[data['Type'] == 'Directed']\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the rows where the 'Type' column equals 'Directed'\ndf_directed = df[df['Type'] == 'Directed']\n\nprint(df_directed)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Filter rows where 'USFLUX' column exceeds 1e+21.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Filter rows where 'USFLUX' column exceeds 1e+21.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load csv into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Select rows where 'USFLUX' is greater than 1e+21\nselected_rows = df[df['USFLUX'] > 1e+21]\n\nprint(selected_rows)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter rows where 'USFLUX' column exceeds 1e+21\nfiltered_df = df[df['USFLUX'] > 1e+21]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Select all distinct 'TIME' values from the table.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Select all distinct 'TIME' values from the table.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# select all distinct 'TIME' values\ndistinct_time_values = df['TIME'].unique()\n\nprint(distinct_time_values)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select all distinct 'TIME' values\ndistinct_time = df['TIME'].unique()\n\nprint(distinct_time)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Fetch all the records from the table where EVENTMSGTYPE is 5.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Fetch all the records from the table where EVENTMSGTYPE is 5.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Select records where EVENTMSGTYPE is 5\nresult_df = df[df['EVENTMSGTYPE'] == 5]\n\nprint(result_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data where EVENTMSGTYPE is 5\ndf_filtered = df[df['EVENTMSGTYPE'] == 5]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Filter the records based on a condition where the shell weight is greater than 0.2.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Filter the records based on a condition where the shell weight is greater than 0.2.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Select records where shell weight is more than 0.2\nselected_records = data[data['Shell weight'] > 0.2]\n\nprint(selected_records)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data where shell weight is greater than 0.2\nfiltered_df = df[df['Shell weight'] > 0.2]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Select the records where the total weight (Sum of 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight') is greater than the average total weight.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Select the records where the total weight (Sum of 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight') is greater than the average total weight.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# calculate the total weight for each record\ndf['Total weight'] = df['Whole weight'] + df['Shucked weight'] + df['Viscera weight'] + df['Shell weight']\n\n# calculate the average total weight\naverage_total_weight = df['Total weight'].mean()\n\n# retrieve the records with total weight more than the average total weight\nresult = df[df['Total weight'] > average_total_weight]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the total weight for each record\ndf['Total weight'] = df['Whole weight'] + df['Shucked weight'] + df['Viscera weight'] + df['Shell weight']\n\n# Calculate the average total weight\naverage_total_weight = df['Total weight'].mean()\n\n# Select the records where the total weight is greater than the average\nselected_records = df[df['Total weight'] > average_total_weight]\n\nprint(selected_records)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Select records where 'Whole weight' exceeds twice the value of 'Shucked weight'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Select records where 'Whole weight' exceeds twice the value of 'Shucked weight'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/8.csv')\n\n# Filter the data where 'Whole weight' is more than twice the 'Shucked weight'\nfiltered_data = data[data['Whole weight'] > 2*data['Shucked weight']]\n\n# print the final output\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select records where 'Whole weight' exceeds twice the value of 'Shucked weight'\nresult = df[df['Whole weight'] > 2 * df['Shucked weight']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Display the names and time stamps of recordings having an anger value exceeding 0.05.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Display the names and time stamps of recordings having an anger value exceeding 0.05.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Filter rows where anger > 0.05\nresult = df[df['anger'] > 0.05][['name','timestamp']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data where anger value is greater than 0.05\ndf_filtered = df[df['anger'] > 0.05]\n\n# Select only 'name' and 'timestamp' columns\nresult = df_filtered[['name', 'timestamp']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Retrieve all rows from the table where the 'happiness' value is greater than the average happiness value across all rows.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Retrieve all rows from the table where the 'happiness' value is greater than the average happiness value across all rows.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv(\"infiagent/csv/9.csv\")\n\n# Calculate the average happiness value\naverage_happiness = data['happiness'].mean()\n\n# Retrieve rows where happiness value is greater than the average happiness\nresult = data[data['happiness'] > average_happiness]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average happiness\naverage_happiness = df['happiness'].mean()\n\n# Filter rows where happiness is greater than average\ndf_happy = df[df['happiness'] > average_happiness]\n\nprint(df_happy)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Select and display the rows where 'anger' was the dominant emotion by checking if anger value was higher than all other emotions for that row.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Select and display the rows where 'anger' was the dominant emotion by checking if anger value was higher than all other emotions for that row.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load data\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Find rows where 'anger' emotion value is higher than all other emotion values\ndf_anger_dominant = df[df['anger'] > df[['contempt','disgust','fear','happiness','neutral','sadness','surprise']].max(axis=1)]\n\nprint(df_anger_dominant)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check if 'anger' is the dominant emotion in each row\ndf['dominant_emotion'] = df.apply(lambda row: 'anger' if row['anger'] == row[['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise', 'blur']].max() else 'other', axis=1)\n\n# Select rows where 'anger' is the dominant emotion\nanger_dominant_rows = df[df['dominant_emotion'] == 'anger']\n\nprint(anger_dominant_rows)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Retrieve from the table all rows where the 'Offense' column contains 'Burglary/Breaking & Entering' and the 'Age' column is less than 18.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Retrieve from the table all rows where the 'Offense' column contains 'Burglary/Breaking & Entering' and the 'Age' column is less than 18.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/10.csv')\n\n# Filter the data based on the condition where 'Offense' is 'Burglary/Breaking & Entering' and 'Age' is less than 18\nfiltered_data = data[(data['Offense'] == 'Burglary/Breaking & Entering') & (data['Age'] < 18)]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where the 'Offense' column contains 'Burglary/Breaking & Entering' and the 'Age' column is less than 18\nfiltered_data = data[(data['Offense'] == 'Burglary/Breaking & Entering') & (data['Age'] < 18)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Retrieve all rows where the 'Age' is greater than 15 and the 'Count' is less than 2.0.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Retrieve all rows where the 'Age' is greater than 15 and the 'Count' is less than 2.0.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\npath = \"infiagent/csv/10.csv\"\ndata = pd.read_csv(path)\n\n# Retrieve all rows where the 'Age' is greater than 15 and the 'Count' is less than 2.0\nfiltered_data = data[(data['Age'] > 15) & (data['Count'] < 2.0)]\n\n# Print the filtered data\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_df = df[(df['Age'] > 15) & (df['Count'] < 2.0)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Choose all vehicles that have a horsepower value higher than 200.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Choose all vehicles that have a horsepower value higher than 200.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Find vehicles with horsepower greater than 200\nselected_vehicles = df[df['horsepower'] > 200]\n\nprint(selected_vehicles)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Choose all vehicles that have a horsepower value higher than 200\ndf_high_horsepower = df[df['horsepower'] > 200]\n\nprint(df_high_horsepower)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Find the job title with the highest 'ANNUAL_RT' in every 'DEPTID'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Find the job title with the highest 'ANNUAL_RT' in every 'DEPTID'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load data from the file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Grouping the DataFrame by 'DEPTID' and then getting the job title with the highest 'ANNUAL_RT' in each group\ndf = df.loc[df.groupby('DEPTID')['ANNUAL_RT'].idxmax()]\n\nprint(df[['DEPTID','JOBTITLE','ANNUAL_RT']])", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Remove the space at the start of 'ANNUAL_RT' column\ndf['ANNUAL_RT'] = df['ANNUAL_RT'].str.lstrip()\n\n# Convert 'ANNUAL_RT' to numeric\ndf['ANNUAL_RT'] = pd.to_numeric(df['ANNUAL_RT'])\n\n# Group by 'DEPTID' and find the job title with the max 'ANNUAL_RT' in each group\nresult = df.groupby('DEPTID').apply(lambda x: x.loc[x['ANNUAL_RT'].idxmax(), 'JOBTITLE'])\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Filter rows where 'HIRE_DT' is greater than '01/01/2010' and 'ANNUAL_RT' exceeds 45000.00.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Filter rows where 'HIRE_DT' is greater than '01/01/2010' and 'ANNUAL_RT' exceeds 45000.00.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Convert 'HIRE_DT' column to datetime\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'],format='%m/%d/%Y')\n\n# Filter rows according to the conditions specified\nresult = df[(df['HIRE_DT'] > '01/01/2010') & (df['ANNUAL_RT'] > 45000.00)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'HIRE_DT' column to datetime format\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'], format='%d/%m/%Y')\n\n# Filter the dataframe based on the conditions\ndf_filtered = df[(df['HIRE_DT'] > '2010-01-01') & (df['ANNUAL_RT'] > 45000.00)]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Filter records where the 'WINDSPEED' is greater than 5 and the 'DIR' is less than 270.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Filter records where the 'WINDSPEED' is greater than 5 and the 'DIR' is less than 270.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Filter records based on 'WINDSPEED' and 'DIR' criteria\nfiltered_data = data[(data['WINDSPEED'] > 5) & (data['DIR'] < 270)]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the records where 'WINDSPEED' is greater than 5 and 'DIR' is less than 270\nfiltered_df = df[(df['WINDSPEED'] > 5) & (df['DIR'] < 270)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Filter the table to show all rows where the time in the 'DATE TIME' column has a value ending with '00:00'.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Filter the table to show all rows where the time in the 'DATE TIME' column has a value ending with '00:00'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Find records where 'DATE TIME' ends with '00:00'\nrecords_ending_0000 = data[data['DATE TIME'].str.endswith('00:00')]\n\nprint(records_ending_0000)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter rows where 'DATE TIME' ends with '00:00'\ndf_filtered = df[df['DATE TIME'].str.endswith('00:00')]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Retrieve all players in the table whose salary is greater than 2000.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Retrieve all players in the table whose salary is greater than 2000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Query all players with a salary of more than 2000\nresult = df[df['salary_in_thousands_of_dollars'] > 2000]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for players with salary greater than 2000\nfiltered_df = df[df['salary_in_thousands_of_dollars'] > 2000]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Select the records with over 20 home runs and under 5 errors.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Select the records with over 20 home runs and under 5 errors.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Filter the data based on the condition\nresult = df[(df['number_of_home_runs'] > 20) & (df['number_of_errors'] < 5)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data for records with over 20 home runs and under 5 errors\nfiltered_data = data[(data['number_of_home_runs'] > 20) & (data['number_of_errors'] < 5)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Identify players with a batting average equal to or higher than the average of the entire dataset.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Identify players with a batting average equal to or higher than the average of the entire dataset.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Calculate the average batting average\navg_batting_average = df['batting_average'].mean()\n\n# Select players who have at or above the average batting-average\nplayers = df[df['batting_average'] >= avg_batting_average]\n\nprint(players)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall batting average\noverall_batting_average = df['batting_average'].mean()\n\n# Filter out players with a batting average equal to or higher than the overall batting average\nplayers_with_higher_batting_average = df[df['batting_average'] >= overall_batting_average]\n\nprint(players_with_higher_batting_average)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Get all the records where the 'wage' is greater than 5.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Get all the records where the 'wage' is greater than 5.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# filter data where 'wage' is above 5\nresult = df[df['wage'] > 5]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'wage' is greater than 5\nfiltered_data = data[data['wage'] > 5]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Select all records where the 'wage' is greater than the average 'wage'.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Select all records where the 'wage' is greater than the average 'wage'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv data\ndata = pd.read_csv('infiagent/csv/15.csv')\n\n# Calculate the average wage\naverage_wage = data['wage'].mean()\n\n# Select all records where the 'wage' is greater than the average 'wage'\nabove_average_wage = data[data['wage'] > average_wage]\n\nprint(above_average_wage)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Calculate the average wage\naverage_wage = data['wage'].mean()\n\n# Select all records where the wage is greater than the average wage\nresult = data[data['wage'] > average_wage]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Select all records where the person is married, living in a small city, and has good health.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Select all records where the person is married, living in a small city, and has good health.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# filter the data\nfiltered_df = df[(df['married'] == 1) & (df['smllcity'] == 1) & (df['goodhlth'] == 1)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Select all records where the person is married, living in a small city, and has good health\nselected_records = df[(df['married'] == 1) & (df['smllcity'] == 1) & (df['goodhlth'] == 1)]\n\nprint(selected_records)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Please select all columns for the records where the yr is equal to 0.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Please select all columns for the records where the yr is equal to 0.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# filter dataframe where yr is equal to 0\nresult = df[df['yr'] == 0]\n\n# print the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select all columns for the records where the yr is equal to 0\ndf_filtered = df[df['yr'] == 0]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Using the table data, please return the records that occurred in the fourth quarter (month 10 - 12).", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Using the table data, please return the records that occurred in the fourth quarter (month 10 - 12).", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/16.csv')\n\n# Filter the records that occurred in the fourth quarter (month 10 - 12)\nfourth_quarter_records = data[(data['mnth'] >= 10) & (data['mnth'] <= 12)]\n\nprint(fourth_quarter_records)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for the fourth quarter\ndf_fourth_quarter = df[df['mnth'].isin([10, 11, 12])]\n\nprint(df_fourth_quarter)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Filter the records of registered users in the second half of 2011 and keep only the ones which are greater than the average of registered users in the first half of 2011.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Filter the records of registered users in the second half of 2011 and keep only the ones which are greater than the average of registered users in the first half of 2011.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv data into a DataFrame\ndata = pd.read_csv('infiagent/csv/16.csv')\n\n# Convert the 'dteday' column to datetime format\ndata['dteday'] = pd.to_datetime(data['dteday'])\n\n# Calculate the average of registered users in the first half of 2011\naverage_first_half = data.loc[(data['dteday'] >= '2011-01-01') & (data['dteday'] <= '2011-06-30'),'registered'].mean()\n\n# Extract the records from the second half of 2011 where the amount of registered users exceeded the first half average\nextracted_records = data.loc[(data['dteday'] > '2011-06-30') & (data['registered'] > average_first_half)]\n\nprint(extracted_records)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'dteday' column to datetime\ndf['dteday'] = pd.to_datetime(df['dteday'])\n\n# Filter the data for the first and second half of 2011\ndf_first_half = df[df['dteday'].dt.year*12 + df['dteday'].dt.month <= 2011*12 + 6]\ndf_second_half = df[df['dteday'].dt.year*12 + df['dteday'].dt.month > 2011*12 + 6]\n\n# Calculate the average number of registered users in the first half of 2011\navg_registered_first_half = df_first_half['registered'].mean()\n\n# Filter the records of registered users in the second half of 2011 that are greater than the average of the first half\nfiltered_df = df_second_half[df_second_half['registered'] > avg_registered_first_half]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Find records where the total number of users ('cnt') is greater than 100 and the 'weathersit' is 1, and it's not a holiday.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Find records where the total number of users ('cnt') is greater than 100 and the 'weathersit' is 1, and it's not a holiday.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndata = pd.read_csv('infiagent/csv/16.csv')\n\n# filter the data according to the conditions\nfiltered_data = data[(data['cnt'] > 100) & (data['weathersit'] == 1) & (data['holiday'] == 0)]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'weathersit' is 1 and it's not a holiday\nfiltered_data = data[(data['weathersit'] == 1) & (data['holiday'] == 0)]\n\n# Find records where the total number of users ('cnt') is greater than 100\nresult = filtered_data[filtered_data['cnt'] > 100]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Retrieve all records of individuals who are 50 years old or older.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Retrieve all records of individuals who are 50 years old or older.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load csv data\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Find all records of people aged 50 and above\nresult = data[data['age']>=50]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select records where age is 50 or older\nselected_records = df[df['age'] >= 50]\n\nprint(selected_records)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Extract data for individuals who are 'Married-civ-spouse' and work more than 40 hours per week.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Extract data for individuals who are 'Married-civ-spouse' and work more than 40 hours per week.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Screen for individuals who are 'Married-civ-spouse' and work more than 40 hours per week\nfiltered_df = df[(df['marital-status'] == ' Married-civ-spouse') & (df['hour-per-week'] > 40)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data for individuals who are 'Married-civ-spouse' and work more than 40 hours per week\nfiltered_data = df[(df['marital-status'] == 'Married-civ-spouse') & (df['hour-per-week'] > 40)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: For each 'workclass', calculate the average of the 'capital-gain' column, and display only those 'workclass' values whose average 'capital-gain' is greater than the overall average 'capital-gain' of all 'workclass' values.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "For each 'workclass', calculate the average of the 'capital-gain' column, and display only those 'workclass' values whose average 'capital-gain' is greater than the overall average 'capital-gain' of all 'workclass' values.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv(\"infiagent/csv/18.csv\")\n\n# Calculate the overall average of 'capital-gain'\noverall_avg_capital_gain = df['capital-gain'].mean()\n\n# Compute the average 'capital-gain' for each 'workclass'\naverage_capital_gain_by_workclass = df.groupby('workclass')['capital-gain'].mean()\n\n# Filter those workclasses whose average 'capital-gain' is above the overall average\nresult = average_capital_gain_by_workclass[average_capital_gain_by_workclass > overall_avg_capital_gain]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall average 'capital-gain'\noverall_avg = df['capital-gain'].mean()\n\n# Calculate the average 'capital-gain' for each 'workclass'\nworkclass_avg = df.groupby('workclass')['capital-gain'].mean()\n\n# Filter 'workclass' values whose average 'capital-gain' is greater than the overall average 'capital-gain'\nresult = workclass_avg[workclass_avg > overall_avg]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Find people who are 'Not-in-family', 'White', 'Male', and have an income of '>50K' using a fusion query.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Find people who are 'Not-in-family', 'White', 'Male', and have an income of '>50K' using a fusion query.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Filter the data to get the rows matching the conditions\nfiltered_data = data[(data['relationship'] == 'Not-in-family') & \n                     (data['race'] == 'White') & \n                     (data['sex'] == 'Male') & \n                     (data['income'] == '>50K')]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data according to the conditions\nfiltered_data = df[(df['relationship'] == 'Not-in-family') & (df['race'] == 'White') & (df['sex'] == 'Male') & (df['income'] == '>50K')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Find the 'Department Name' and 'budget_year_start' for the departments where 'coa_dept_id' is greater than the average 'coa_dept_id'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Find the 'Department Name' and 'budget_year_start' for the departments where 'coa_dept_id' is greater than the average 'coa_dept_id'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# calculate the average of 'coa_dept_id'\navg_dept_id = df['coa_dept_id'].mean()\n\n# create a sub-query for the departments where 'coa_dept_id' is greater than the average 'coa_dept_id'\nresult_df = df[df['coa_dept_id'] > avg_dept_id][['Department Name','budget_year_start']]\n\nprint(result_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'coa_dept_id'\naverage_dept_id = df['coa_dept_id'].mean()\n\n# Filter the rows where 'coa_dept_id' is greater than the average\nfiltered_df = df[df['coa_dept_id'] > average_dept_id]\n\n# Select the 'Department Name' and 'budget_year_start' columns\nresult = filtered_df[['Department Name', 'budget_year_start']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Choose department names where 'budget_year_start' is within the last 12 months.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Choose department names where 'budget_year_start' is within the last 12 months.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\nfrom datetime import datetime,timedelta\n\n# Load the data into a DataFrame\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Convert the 'budget_year_start' column to datetime format\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\n\n# Get the date 12 months ago from today's date\none_year_ago = datetime.now() - timedelta(days=365)\n\n# Filter the data to include only rows where 'budget_year_start' is within the last 12 months\ndf_recent_budget_start = df[df['budget_year_start'] > one_year_ago]\n\nprint(df_recent_budget_start['Department Name'].tolist())", "assistant_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'budget_year_start' to datetime\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\n\n# Get the date 12 months ago from today\ndate_12_months_ago = datetime.now() - pd.DateOffset(months=12)\n\n# Filter departments where 'budget_year_start' is within the last 12 months\ndepartments = df[df['budget_year_start'] > date_12_months_ago]['Department Name']\n\nprint(departments)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Filter out 'Department Name' and 'coa_dept_id' columns for all departments with 'dept_group' beginning with 'U'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Filter out 'Department Name' and 'coa_dept_id' columns for all departments with 'dept_group' beginning with 'U'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# filter out the departments where 'dept_group' starts with 'U'\nfiltered_df = df[df['dept_group'].str.startswith('U')][['Department Name','coa_dept_id']]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\ndf_filtered = df[df['dept_group'].str.startswith('U')][['Department Name', 'coa_dept_id']]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Choose the hospital data when the 'Excess Readmission Ratio' exceeds 1.7.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Choose the hospital data when the 'Excess Readmission Ratio' exceeds 1.7.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Loading the data from the csv file\ndata = pd.read_csv('infiagent/csv/20.csv')\n\n# Selecting the hospital information where 'Excess Readmission Ratio' is greater than 1.7\nselected_hospitals = data[data['Excess Readmission Ratio'] > 1.7]\n\nprint(selected_hospitals)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'Excess Readmission Ratio' exceeds 1.7\nfiltered_data = data[data['Excess Readmission Ratio'] > 1.7]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Find the 'Provider Number' and 'Number of Discharges' for the hospitals with 'Excess Readmission Ratio' greater than the overall average 'Excess Readmission Ratio'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Find the 'Provider Number' and 'Number of Discharges' for the hospitals with 'Excess Readmission Ratio' greater than the overall average 'Excess Readmission Ratio'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Calculate the overall average 'Excess Readmission Ratio'\navg_excess_readmission_ratio = df['Excess Readmission Ratio'].mean()\n\n# Use a mask to select rows where 'Excess Readmission Ratio' is greater than the average\nmask = df['Excess Readmission Ratio'] > avg_excess_readmission_ratio\n\n# Apply the mask and select 'Provider Number' and 'Number of Discharges' columns\nresult = df[mask][['Provider Number','Number of Discharges']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall average 'Excess Readmission Ratio'\naverage_excess_readmission_ratio = df['Excess Readmission Ratio'].mean()\n\n# Filter the hospitals with 'Excess Readmission Ratio' greater than the average\nfiltered_df = df[df['Excess Readmission Ratio'] > average_excess_readmission_ratio]\n\n# Get the 'Provider Number' and 'Number of Discharges' for the filtered hospitals\nresult = filtered_df[['Provider Number', 'Number of Discharges']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Sort by 'Number of Discharges' within each 'State' group, and select the hospital with the highest rank.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Sort by 'Number of Discharges' within each 'State' group, and select the hospital with the highest rank.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Convert 'Number of Discharges' to numeric and handle errors by setting them to NaN\ndf['Number of Discharges'] = pd.to_numeric(df['Number of Discharges'],errors='coerce')\n\n# Group by 'State' and find the max 'Number of Discharges' for each group\nmax_discharges = df.groupby(['State'])['Number of Discharges'].idxmax()\n\n# Retrieve the rows with maximum 'Number of Discharges' for each state\nresult = df.loc[max_discharges]\n\nprint(result[['Hospital Name','State','Number of Discharges']])", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Sort the data by 'Number of Discharges' within each 'State' group\ndf_sorted = df.sort_values(['State', 'Number of Discharges'], ascending=[True, False])\n\n# Select the hospital with the highest rank (i.e., the first row after sorting)\nhighest_discharge_hospitals = df_sorted.iloc[:]\n\nprint(highest_discharge_hospitals)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Identify the state with the most substantial 'Number of Discharges' and extract all hospitals located in that state.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Identify the state with the most substantial 'Number of Discharges' and extract all hospitals located in that state.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Convert the 'Number of Discharges' column to numeric (forcing non-numeric invalid inputs to NaN)\ndf['Number of Discharges'] = pd.to_numeric(df['Number of Discharges'],errors='coerce')\n\n# Group by state and get the sum of 'Number of Discharges',then get the state with the highest sum\nmax_discharge_state = df.groupby('State')['Number of Discharges'].sum().idxmax()\n\n# Select all hospitals in the state with the highest sum of 'Number of Discharges'\nhospitals_in_state = df.loc[df['State'] == max_discharge_state,'Hospital Name'].values\n\nprint(hospitals_in_state)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Identify the state with the most substantial 'Number of Discharges'\nmax_discharges_state = df['State'].value_counts().idxmax()\n\n# Extract all hospitals located in that state\nhospitals_in_state = df[df['State'] == max_discharges_state]\n\nprint(hospitals_in_state)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Retrieve information on all hurricanes whose names begin with the letter \"A\".", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Retrieve information on all hurricanes whose names begin with the letter \"A\".", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# filter the data\nhurricanes_with_A = df[df['name'].str.startswith('A')]\n\nprint(hurricanes_with_A)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data to include only rows where the name starts with \"A\"\ndf_filtered = df[df['name'].str.startswith('A')]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Choose all hurricanes that impacted Florida.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Choose all hurricanes that impacted Florida.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# select all hurricanes which affected 'Florida'\nflorida_affected = df[df['areas_affected'].str.contains('Florida',na=False)]\n\nprint(florida_affected)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter the data to include only rows where 'areas_affected' includes 'Florida'\nflorida_hurricanes = data[data['areas_affected'].str.contains('Florida')]\n\nprint(florida_hurricanes)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Retrieve the records where the total number of vaccinations exceeds 300.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Retrieve the records where the total number of vaccinations exceeds 300.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data into a DataFrame\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Filter DataFrame to only include rows where the 'total_vaccinations' column is greater than 300\ndf_filtered = df[df['total_vaccinations'] > 300]\n\n# Output the filtered DataFrame\nprint(df_filtered)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select the records where the total number of vaccinations exceeds 300\nresult = df[df['total_vaccinations'] > 300]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Obtain the information for the nation 'Albania' and the 'total_vaccinations' count greater than 200.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Obtain the information for the nation 'Albania' and the 'total_vaccinations' count greater than 200.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/22.csv')\n\n# Filter the data based on the conditions\nfiltered_data = data[(data['country'] == 'Albania') & (data['total_vaccinations'] > 200)]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filtering the data for the nation 'Albania' and 'total_vaccinations' count greater than 200\nalbania_df = df[(df['country'] == 'Albania') & (df['total_vaccinations'] > 200)]\n\nprint(albania_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Retrieve all records where the 'number_of_open_credit_lines_and_loans' is greater than 7.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Retrieve all records where the 'number_of_open_credit_lines_and_loans' is greater than 7.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Filter the data where 'number_of_open_credit_lines_and_loans' is greater than 7\nfiltered_df = df[df['number_of_open_credit_lines_and_loans'] > 7]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe where 'number_of_open_credit_lines_and_loans' is greater than 7\nfiltered_df = df[df['number_of_open_credit_lines_and_loans'] > 7]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Choose all records with an age higher than 30 and a 'revolving_utilization_of_unsecured_lines' below 0.5.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Choose all records with an age higher than 30 and a 'revolving_utilization_of_unsecured_lines' below 0.5.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Filter the records based on the conditions\nresult = df[(df['age'] > 30) & (df['revolving_utilization_of_unsecured_lines'] < 0.5)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data based on the conditions\nfiltered_df = df[(df['age'] > 30) & (df['revolving_utilization_of_unsecured_lines'] < 0.5)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Select all records where 'debt_ratio' is between 0.1 and 0.5.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Select all records where 'debt_ratio' is between 0.1 and 0.5.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Select the records where 'debt_ratio' is between 0.1 and 0.5\nfiltered_df = df[(df['debt_ratio'] >= 0.1) & (df['debt_ratio'] <= 0.5)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select records where 'debt_ratio' is between 0.1 and 0.5\nselected_records = df[(df['debt_ratio'] >= 0.1) & (df['debt_ratio'] <= 0.5)]\n\nprint(selected_records)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Find all rows where 'Education' is greater than 15 and 'Married' status is 'No'.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Find all rows where 'Education' is greater than 15 and 'Married' status is 'No'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Screen for 'Education' > 15 and 'Married' = 'No'\nresult = df[(df['Education'] > 15) & (df['Married'] == 'No')]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_df = df[(df['Education'] > 15) & (df['Married'] == 'No')]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Locate all rows with 'Female' in the 'Gender' column, 'Age' higher than 70, and 'Married' set to 'Yes'.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Locate all rows with 'Female' in the 'Gender' column, 'Age' higher than 70, and 'Married' set to 'Yes'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Filter the DataFrame based on the condition\ndf_filtered = df[(df['Gender'] == 'Female') & (df['Age'] > 70) & (df['Married'] == 'Yes')]\n\nprint(df_filtered)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data according to the conditions\nfiltered_data = data[(data['Gender'] == 'Female') & (data['Age'] > 70) & (data['Married'] == 'Yes')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Filter the table to show only rows where the value in the \"NumberOfTime30-59DaysPastDueNotWorse\" column is greater than 1.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Filter the table to show only rows where the value in the \"NumberOfTime30-59DaysPastDueNotWorse\" column is greater than 1.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# select the rows where 'NumberOfTime30-59DaysPastDueNotWorse' is greater than 1\ndf_selected = df[df['NumberOfTime30-59DaysPastDueNotWorse'] > 1]\n\n# print the selected rows\nprint(df_selected)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data based on the condition\nfiltered_df = df[df['NumberOfTime30-59DaysPastDueNotWorse'] > 1]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Retrieve the rows corresponding to the oldest and youngest individuals with a '90DaysLate' status.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Retrieve the rows corresponding to the oldest and youngest individuals with a '90DaysLate' status.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from CSV\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Identify the rows of the oldest & youngest people who are '90DaysLate' \noldest_row = data[data['NumberOfTimes90DaysLate'] > 0].sort_values('age',ascending=False).iloc[0]\nyoungest_row = data[data['NumberOfTimes90DaysLate'] > 0].sort_values('age',ascending=True).iloc[0]\n\nprint('Oldest person who is 90DaysLate:',oldest_row)\nprint('Youngest person who is 90DaysLate:',youngest_row)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Handle non-standard data (NA values)\ndf = df.replace('NA', pd.NA)\ndf = df.dropna(subset=['age'])\n\n# Select rows where 'NumberOfTimes90DaysLate' is greater than 0\ndf_90_days_late = df[df['NumberOfTimes90DaysLate'] > 0]\n\n# Find the minimum and maximum ages\noldest_age = df_90_days_late['age'].max()\nyoungest_age = df_90_days_late['age'].min()\n\n# Retrieve the rows corresponding to the oldest and youngest individuals\noldest_individual = df_90_days_late[df_90_days_late['age'] == oldest_age]\nyoungest_individual = df_90_days_late[df_90_days_late['age'] == youngest_age]\n\nprint(oldest_individual, youngest_individual)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Filter the table records where the 'Wins' column exceeds 3 and the 'School' column reads 'Archbishop Mitty'.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Filter the table records where the 'Wins' column exceeds 3 and the 'School' column reads 'Archbishop Mitty'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Filter records based on 'Wins' and 'School'\nfiltered_data = data[(data['Wins'] > 3) & (data['School'] == 'Archbishop Mitty')]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'Wins' > 3 and 'School' == 'Archbishop Mitty'\nfiltered_data = data[(data['Wins'] > 3) & (data['School'] == 'Archbishop Mitty')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Choose the 'Name' column for teams with 'NUM ROUNDS' falling within the top 25% for each school.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Choose the 'Name' column for teams with 'NUM ROUNDS' falling within the top 25% for each school.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Select 'Name' of teams with 'NUM ROUNDS' in the top 25% for their school\ntop_25_percent_rounds = data.groupby('School')['NUM ROUNDS'].transform(lambda x: x.quantile(0.75))\ntop_teams = data[data['NUM ROUNDS'] >= top_25_percent_rounds][['School','Name']]\n\nprint(top_teams)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Clean the data\ndata = data.dropna(subset=['NUM ROUNDS', 'School', 'Name'])\n\n# Calculate the top 25% rounds for each school\ntop_25_percent_rounds = data.groupby('School')['NUM ROUNDS'].quantile(0.75)\n\n# Join the original data with the calculated values\ndata = data.join(top_25_percent_rounds, on='School', rsuffix='_25thPerc')\n\n# Filter the data\nfiltered_data = data[data['NUM ROUNDS'] <= data['NUM ROUNDS_25thPerc']]\n\n# Get the 'Name' column\nresult = filtered_data['Name']\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Find all records where 'Name' contains 'Singh'.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Find all records where 'Name' contains 'Singh'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Find records where 'Name' contains 'Singh'\nrecords_with_singh = data[data['Name'].str.contains('Singh')]\n\nprint(records_with_singh)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the rows where 'Name' contains 'Singh'\nresult = df[df['Name'].str.contains('Singh')]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Identify the team with the maximum and minimum 'Wins'.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Identify the team with the maximum and minimum 'Wins'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Identify the team with the max and min 'Wins'\nmax_wins_team = data[data['Wins'] == data['Wins'].max()]\nmin_wins_team = data[data['Wins'] == data['Wins'].min()]\n\nprint(\"Team with Maximum Wins:\\n\",max_wins_team)\nprint(\"Team with Minimum Wins:\\n\",min_wins_team)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Handle missing data\ndf = df.fillna('')\n\n# Identify the team with the maximum 'Wins'\nmax_wins_team = df.loc[df['Wins'].idxmax(), 'Name']\n\n# Identify the team with the minimum 'Wins'\nmin_wins_team = df.loc[df['Wins'].idxmin(), 'Name']\n\nprint(max_wins_team, min_wins_team)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Select records where 'Wins' is between 4 and 5, and 'NUM ROUNDS' is equal to 5.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Select records where 'Wins' is between 4 and 5, and 'NUM ROUNDS' is equal to 5.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Select records based on 'Wins' and 'NUM ROUNDS'\nselected_records = data[(data['Wins'].between(4,5)) & (data['NUM ROUNDS'] == 5)]\n\nprint(selected_records)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select the records where 'Wins' is between 4 and 5, and 'NUM ROUNDS' is equal to 5\nselected_records = data[(data['Wins'].between(4, 5)) & (data['NUM ROUNDS'] == 5)]\n\nprint(selected_records)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Retrieve and present the day and month from the Calendar Date (TDB) where the value of column X exceeds -2.9E+08.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Retrieve and present the day and month from the Calendar Date (TDB) where the value of column X exceeds -2.9E+08.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Loading the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Filter the data where column 'X' is greater than -2.9E+08\ndf_filtered = df[df['X'] > -2.9E+08]\n\n# Extract day and month from 'Calendar Date (TDB)' column\ndf_filtered['Day_Month'] = df_filtered['Calendar Date (TDB)'].apply(lambda x: '-'.join(x.split('-')[1:3]))\n\nprint(df_filtered['Day_Month'])", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Calendar Date (TDB)' column to datetime\ndf['Calendar Date (TDB)'] = pd.to_datetime(df['Calendar Date (TDB)'])\n\n# Create a mask where the value of column 'X' exceeds -2.9E+08\nmask = df['X'] > -2.9e8\n\n# Apply the mask to the dataframe\ndf_filtered = df[mask]\n\n# Extract day and month from the 'Calendar Date (TDB)' column\ndf_filtered[['Day', 'Month']] = df_filtered['Calendar Date (TDB)'].dt.day_name() & df_filtered['Calendar Date (TDB)'].dt.month_name()\n\nprint(df_filtered[['Day', 'Month']])"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Select the data from column X where the Calendar Date (TDB) is after A.D. 1999-Feb-10 00:58:29.0000.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Select the data from column X where the Calendar Date (TDB) is after A.D. 1999-Feb-10 00:58:29.0000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/27.csv')\n\n# Convert the 'Calendar Date (TDB)' column to datetime\ndata['Calendar Date (TDB)'] = pd.to_datetime(data['Calendar Date (TDB)'],format=\" A.D. %Y-%b-%d %H:%M:%S.%f\")\n\n# Select the data from column X where the Calendar Date (TDB) is after A.D. 1999-Feb-10 00:58:29.0000\nselected_data = data.loc[data['Calendar Date (TDB)'] > '1999-02-10 00:58:29.0000','X']\n\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Calendar Date (TDB)' column to datetime\ndf['Calendar Date (TDB)'] = pd.to_datetime(df['Calendar Date (TDB)'])\n\n# Filter the data\nfiltered_df = df[df['Calendar Date (TDB)'] > '1999-02-10 00:58:29.0000']\n\n# Select the 'X' column\nresult = filtered_df['X']\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Retrieve all records where the value in column X is less than -2.9E+08 and the value in column Y is greater than 1.6E+07.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Retrieve all records where the value in column X is less than -2.9E+08 and the value in column Y is greater than 1.6E+07.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Filter the data based on the conditions\nfiltered_df = df[(df['X'] < -2.9E+08) & (df['Y'] > 1.6E+07)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Apply the conditions to filter the data\nresult = df[(df['X'] < -2.9E+08) & (df['Y'] > 1.6E+07)]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Retrieve all records with \"Ideal\" as the \"cut\" and show the \"carat\", \"color\", \"clarity\", and \"price\" columns.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Retrieve all records with \"Ideal\" as the \"cut\" and show the \"carat\", \"color\", \"clarity\", and \"price\" columns.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Select all records where the \"cut\" is \"Ideal\"\ndf_ideal = df[df['cut'] == \"Ideal\"]\n\n# Display the \"carat\",\"color\",\"clarity\" and \"price\"\nresult = df_ideal[['carat','color','clarity','price']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select the records with \"Ideal\" as the \"cut\"\nideal_cuts = df[df['cut'] == 'Ideal']\n\n# Select the specified columns\nselected_columns = ideal_cuts[['carat', 'color', 'clarity', 'price']]\n\nprint(selected_columns)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Retrieve the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" details for the diamonds that have a \"volume\" greater than the average \"volume\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Retrieve the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" details for the diamonds that have a \"volume\" greater than the average \"volume\".", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Calculate the volume of each diamond\ndf['volume'] = df['x'] * df['y'] * df['z']\n\n# Calculate the average volume\naverage_volume = df['volume'].mean()\n\n# Filter the diamonds that have above average volume\ndf_above_avg_volume = df[df['volume'] > average_volume]\n\n# Retrieve the details\nresult = df_above_avg_volume[[\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the volume of each diamond using the formula: volume = x * y * z\ndf['volume'] = df['x'] * df['y'] * df['z']\n\n# Calculate the average volume\naverage_volume = df['volume'].mean()\n\n# Filter the dataframe to get the diamonds with volume greater than the average volume\nfiltered_df = df[df['volume'] > average_volume]\n\n# Select the required columns\nresult = filtered_df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y', 'z']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Determine the proportion of \"x\" to \"y\" for each diamond and present only the diamonds with a ratio greater than 1.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Determine the proportion of \"x\" to \"y\" for each diamond and present only the diamonds with a ratio greater than 1.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Calculate the ratio of \"x\" to \"y\"\ndf['ratio'] = df['x'] / df['y']\n\n# Filter rows where ratio is greater than 1\nfiltered_df = df[df['ratio'] > 1]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the proportion of 'x' to 'y'\ndf['proportion'] = df['x'] / df['y']\n\n# Select only the rows where the proportion is greater than 1\ndf_greater_than_1 = df[df['proportion'] > 1]\n\nprint(df_greater_than_1)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Select the 'Date' and 'High' price, where 'High' price is greater than 78.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Select the 'Date' and 'High' price, where 'High' price is greater than 78.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/29.csv')\n\n# Select the 'Date' and 'High' price,where 'High' price is more than 78\nselected_data = data.loc[data['High'] > 78,['Date','High']]\n\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select the 'Date' and 'High' columns where 'High' is greater than 78\nresult = df.loc[df['High'] > 78, ['Date', 'High']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Generate a list of 'High' prices that are higher than the average 'High' price.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Generate a list of 'High' prices that are higher than the average 'High' price.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Calculate the average 'High' price\naverage_high = df['High'].mean()\n\n# Filter the DataFrame for 'High' prices greater than the average\nhigh_prices_above_average = df[df['High'] > average_high]['High'].tolist()\n\nprint(high_prices_above_average)", "assistant_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'High' prices\naverage_high_price = df['High'].mean()\n\n# Generate a list of 'High' prices that are higher than the average 'High' price\nhigh_prices_above_average = df[df['High'] > average_high_price]['High'].tolist()\n\nprint(high_prices_above_average)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Create a list of 'Low' prices that are below the typical 'Low' price.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Create a list of 'Low' prices that are below the typical 'Low' price.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load data\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Calculate average 'Low' price\navg_low = df['Low'].mean()\n\n# Get list of 'Low' prices lower than average\nlow_below_avg = df[df['Low'] < avg_low]['Low'].tolist()\n\nprint(low_below_avg)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the typical 'Low' price\ntypical_low = df['Low'].mean()\n\n# Create a list of 'Low' prices that are below the typical 'Low' price\nbelow_typical_low = df[df['Low'] < typical_low]['Low'].tolist()\n\nprint(below_typical_low)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Retrieve the rows where 'state_abbr' is \"AK\" and 'county_name' is \"Alaska\".", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Retrieve the rows where 'state_abbr' is \"AK\" and 'county_name' is \"Alaska\".", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file data into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Filter the rows where 'state_abbr' is \"AK\" and 'county_name' is \"Alaska\"\nfiltered_df = df[(df['state_abbr'] == \"AK\") & (df['county_name'] == \"Alaska\")]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe based on 'state_abbr' and 'county_name'\nfiltered_df = df[(df['state_abbr'] == 'AK') & (df['county_name'] == 'Alaska')]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Query all entries where 'per_dem' is greater than 0.4 and 'per_gop' is less than 0.6.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Query all entries where 'per_dem' is greater than 0.4 and 'per_gop' is less than 0.6.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the dataset \ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Replace the comma seperator in the 'diff' and percentage in 'per_point_diff' columns\ndf['diff'] = df['diff'].str.replace(',','').astype(float)\ndf['per_point_diff'] = df['per_point_diff'].replace('%','',regex=True).astype('float')\n\n# Query - entries where 'per_dem' is greater than 0.4 and 'per_gop' is less than 0.6\nresult = df[(df['per_dem'] > 0.4) & (df['per_gop'] < 0.6)]\n\n# Print the queried data \nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Query the data\nresult = df[(df['per_dem'] > 0.4) & (df['per_gop'] < 0.6)]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Select the rows where 'EQ1' is greater than 0.01.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Select the rows where 'EQ1' is greater than 0.01.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Select the rows where 'EQ1' is greater than 0.01\nselected_rows = df[df['EQ1'] > 0.01]\n\nprint(selected_rows)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select the rows where 'EQ1' is greater than 0.01\nselected_data = data[data['EQ1'] > 0.01]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Query 'EQ5' values which are larger than the average 'EQ5' value.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Query 'EQ5' values which are larger than the average 'EQ5' value.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('infiagent/csv/31.csv')\n\n# Calculate the average of 'EQ5'\navg_eq5 = data['EQ5'].mean()\n\n# Query 'EQ5' values which are larger than the average 'EQ5' value\nresult = data[data['EQ5'] > avg_eq5]['EQ5']\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of 'EQ5' column\naverage_EQ5 = df['EQ5'].mean()\n\n# Query 'EQ5' values which are larger than the average 'EQ5' value\nresult = df[df['EQ5'] > average_EQ5]\n\nprint(result['EQ5'])"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Select the rows where 'EX4' is between -0.1 and 0.1.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Select the rows where 'EX4' is between -0.1 and 0.1.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/31.csv')\n\n# Select the rows where 'EX4' is between -0.1 and 0.1\nresult = data[(data['EX4'] >= -0.1) & (data['EX4'] <= 0.1)]\n\n# Print the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the rows where 'EX4' is between -0.1 and 0.1\nfiltered_data = data[(data['EX4'] >= -0.1) & (data['EX4'] <= 0.1)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Obtain the data for 'No. of Cases' and 'No. of Deaths' specifically related to the nation 'Afghanistan' in the year 2017 from the table.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Obtain the data for 'No. of Cases' and 'No. of Deaths' specifically related to the nation 'Afghanistan' in the year 2017 from the table.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Loading the dataset from the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Filtering the dataset for the country 'Afghanistan' and year 2017\nfiltered_df = df[(df['Country'] == 'Afghanistan') & (df['Year'] == 2017)]\n\n# Selecting columns 'No. of cases' and 'No. of deaths'\nselected_data = filtered_df[['No. of cases','No. of deaths']]\n\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter the data for Afghanistan in 2017\nafghanistan_2017 = data[(data['Country'] == 'Afghanistan') & (data['Year'] == 2017)]\n\n# Extract the data for 'No. of Cases' and 'No. of Deaths'\nafghanistan_2017_data = afghanistan_2017[['No. of cases', 'No. of deaths']]\n\nprint(afghanistan_2017_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Retrieve records where the No. of cases_max is greater than 5000000.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Retrieve records where the No. of cases_max is greater than 5000000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Filter the dataframe where 'No. of cases_max' is greater than 5000000\nfiltered_df = df[df['No. of cases_max'] > 5000000]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Clean the 'No. of cases_max' column to remove commas and convert it to numeric\ndata['No. of cases_max'] = pd.to_numeric(data['No. of cases_max'].str.replace(',', ''))\n\n# Filter the data where 'No. of cases_max' is greater than 5000000\nfiltered_data = data[data['No. of cases_max'] > 5000000]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Choose every row in the table where the author field reads 'ABC News'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Choose every row in the table where the author field reads 'ABC News'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndata = pd.read_csv(\"infiagent/csv/33.csv\")\n\n# select rows where author is 'ABC News'\nselected_data = data[data['author'] == 'ABC News']\n\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select rows where author is 'ABC News'\ndf_selected = df[df['author'] == 'ABC News']\n\nprint(df_selected)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Extract all records where the author has exceeded 10 publication counts.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Extract all records where the author has exceeded 10 publication counts.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data from csv file\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# count number of articles each author has published\nauthor_counts = data['author'].value_counts()\n\n# get the authors who have published more than 10 articles\nauthors_more_than_10 = author_counts[author_counts > 10].index.tolist()\n\n# retrieve all entries where the author has published more than 10 articles\nresult = data[data['author'].isin(authors_more_than_10)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Create a count of publications per author\nauthor_counts = data['author'].value_counts()\n\n# Extract authors with more than 10 publications\nauthors_with_more_than_10_publications = author_counts[author_counts > 10].index\n\n# Filter the original data for these authors\nfiltered_data = data[data['author'].isin(authors_with_more_than_10_publications)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Retrieve the records where the length of the 'text' is greater than 200 characters.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Retrieve the records where the length of the 'text' is greater than 200 characters.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# retrieve the records where the length of the 'text' is greater than 200 characters\nfiltered_df = df[df['text'].astype(str).apply(len) > 200]\n\n# output the filtered dataframe\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter the records where the length of the 'text' is greater than 200 characters\nfiltered_data = data[data['text'].str.len() > 200]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Extract all rows with an 'abs_diffsel' column greater than 8.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Extract all rows with an 'abs_diffsel' column greater than 8.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndata = pd.read_csv('infiagent/csv/34.csv')\n\n# filter the data where 'abs_diffsel' is greater than 8\nfiltered_data = data[data['abs_diffsel'] > 8]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'abs_diffsel' is greater than 8\nfiltered_data = data[data['abs_diffsel'] > 8]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Select all the data where life expectancy is above 40 and GDP per capita is less than 800.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Select all the data where life expectancy is above 40 and GDP per capita is less than 800.", "instruction_type": "Query-Screening", "reference_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load data from csv\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Select the data where life expectancy is above 40 and GDP per capita is less than 800\nselected_data = df[(df['lifeexp'] > 40) & (df['gdppercap'] < 800)]\n\n# Print the selected data\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select the rows where life expectancy is above 40 and GDP per capita is less than 800\nselected_data = data[(data['lifeexp'] > 40) & (data['gdppercap'] < 800)]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Find all the records where the year is between 1980 and 2000, and population is more than 5 million.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Find all the records where the year is between 1980 and 2000, and population is more than 5 million.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Filter the data for the records where the year is between 1980 and 2000,and the population is more than 5 million\nfiltered_df = df[(df['year'] >= 1980) & (df['year'] <= 2000) & (df['pop'] > 5000000)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data where year is between 1980 and 2000 and population is more than 5 million\nfiltered_df = df[(df['year'] >= 1980) & (df['year'] <= 2000) & (df['pop'] > 5000000)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Extract the rows with 'part_of_speech' containing 'n0prop' and 'Age' exceeding 5.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Extract the rows with 'part_of_speech' containing 'n0prop' and 'Age' exceeding 5.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# filter the data\nfiltered_data = df[(df['part_of_speech'].str.contains('n0prop')) & (df['Age'] > 5)]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'part_of_speech' contains 'n0prop' and 'Age' is greater than 5\nfiltered_data = data[(data['part_of_speech'].str.contains('n0prop')) & (data['Age'] > 5)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Find the 'transcript_id' with the highest number of records and retrieve all the corresponding records.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Find the 'transcript_id' with the highest number of records and retrieve all the corresponding records.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Group by 'transcript_id' and count the number of records for each group\ngrouped_df = df.groupby('transcript_id').size()\n\n# Find the 'transcript_id' with the maximum number of records\nmax_records_id = grouped_df.idxmax()\n\n# Extract all records that belong to the 'transcript_id' with maximum number of records\nresult = df[df['transcript_id'] == max_records_id]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group the data by 'transcript_id' and count the number of records for each\ngrouped_df = df.groupby('transcript_id').size().reset_index(name='counts')\n\n# Find the 'transcript_id' with the highest number of records\nmax_transcript_id = grouped_df[grouped_df['counts'] == grouped_df['counts'].max()]['transcript_id'].values[0]\n\n# Retrieve all the corresponding records for the 'transcript_id' with the highest count\nresult = df[df['transcript_id'] == max_transcript_id]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Filter rows where 'Symbol' equals 'GODREJIND' and 'Series' is 'EQ'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Filter rows where 'Symbol' equals 'GODREJIND' and 'Series' is 'EQ'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# Filter the dataframe\nfiltered_df = df[(df['Symbol'] == 'GODREJIND') & (df['Series'] == 'EQ')]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_data = data[(data['Symbol'] == 'GODREJIND') & (data['Series'] == 'EQ')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Select from the table where 'High Price' is greater than 585 and 'Date' is after '18-May-2017'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Select from the table where 'High Price' is greater than 585 and 'Date' is after '18-May-2017'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'],format='%d-%b-%Y')\n\n# Screen the data based on 'High Price' and 'Date'\nfiltered_df = df[(df['High Price'] > 585) & (df['Date'] > '2017-05-18')]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Clean the 'Date' column and convert to datetime\ndf['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%Y')\n\n# Filter the data where 'High Price' is greater than 585 and 'Date' is after '18-May-2017'\nfiltered_df = df[(df['High Price'] > 585) & (df['Date'] > '2017-05-18')]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Show the rows where \"Prev Close\" price is greater than \"Open Price\".", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Show the rows where \"Prev Close\" price is greater than \"Open Price\".", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/38.csv\")\n\n# Select rows where 'Prev Close' is higher than 'Open Price'\nresult = df[df['Prev Close'] > df['Open Price']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove leading and trailing whitespaces from the columns\ndata = data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n\n# Filter the rows where \"Prev Close\" price is greater than \"Open Price\"\nresult = data[data[\"Prev Close\"] > data[\"Open Price\"]]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Select records where 'Close Price' is between 570 and 590.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Select records where 'Close Price' is between 570 and 590.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Select records where 'Close Price' is between 570 and 590\nfiltered_data = data[(data['Close Price'] >= 570) & (data['Close Price'] <= 590)]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Clean the data\ndata['Close Price'] = data['Close Price'].apply(pd.to_numeric, errors='coerce')\n\n# Select records where 'Close Price' is between 570 and 590\nselected_records = data[(data['Close Price'] >= 570) & (data['Close Price'] <= 590)]\n\nprint(selected_records)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Show a list of all male superheroes in the Marvel comics.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Show a list of all male superheroes in the Marvel comics.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndataframe = pd.read_csv('infiagent/csv/39.csv')\n\n# Filter the dataframe for male superheroes in the Marvel Comics\nmale_marvel_heroes = dataframe[(dataframe['Gender'] == 'Male') & (dataframe['Publisher'] == 'Marvel Comics')]\n\n# Get the list of names of these superheroes\nmale_marvel_heroes_list = male_marvel_heroes['name'].tolist()\n\nprint(male_marvel_heroes_list)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Filter the data to only include rows where the 'Gender' column is 'Male' and the 'Publisher' column is 'Marvel Comics'\nmale_marvel_heroes = df[(df['Gender'] == 'Male') & (df['Publisher'] == 'Marvel Comics')]\n\n# Select the 'name' column from the filtered data\nmale_marvel_heroes_names = male_marvel_heroes['name']\n\n# Print the names of the male Marvel heroes\nprint(male_marvel_heroes_names)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Show me all superheroes with 'good' alignment and 'Marvel Comics' as publisher.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Show me all superheroes with 'good' alignment and 'Marvel Comics' as publisher.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from given CSV file\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Filter the data based on alignment 'good' and publisher 'Marvel Comics'\nfiltered_df = df[(df['Alignment'] == 'good') & (df['Publisher'] == 'Marvel Comics')]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Filter data where Alignment is 'good' and Publisher is 'Marvel Comics'\nfiltered_df = df[(df['Alignment'] == 'good') & (df['Publisher'] == 'Marvel Comics')]\n\n# Print the filtered data\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Select the information where 'Eye color' is 'blue' and 'Hair color' is 'No Hair';", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Select the information where 'Eye color' is 'blue' and 'Hair color' is 'No Hair';", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Filter data where 'Eye color' is blue and 'Hair color' is 'No Hair'\nfiltered_df = df[(df['Eye color'] == 'blue') & (df['Hair color'] == 'No Hair')]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('data.csv')\n\n# Select the rows where 'Eye color' is 'blue' and 'Hair color' is 'No Hair'\nselected_data = data[(data['Eye color'] == 'blue') & (data['Hair color'] == 'No Hair')]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Eliminate all superheroes with 'Human' in their 'Race' and whose 'Alignment' is 'bad'.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Eliminate all superheroes with 'Human' in their 'Race' and whose 'Alignment' is 'bad'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Filter out the superheroes that are 'Human' and 'bad'\ndf_bad_humans = df[(df['Race'].str.contains('Human')) & (df['Alignment'] == 'bad')]\n\nprint(df_bad_humans)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove the rows where 'Race' contains 'Human' and 'Alignment' is 'bad'\ndata = data[~((data['Race'].str.contains('Human')) & (data['Alignment'] == 'bad'))]\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Retrieve the details of superheroes with 'Blond' hair color and 'Human' race.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Retrieve the details of superheroes with 'Blond' hair color and 'Human' race.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load data into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# filter out details of superheroes who have 'Hair color' as 'Blond' and are of 'Human' race\nfiltered_df = df[(df['Hair color'] == 'Blond') & (df['Race'] == 'Human')]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where 'Hair color' is 'Blond' and 'Race' is 'Human'\nfiltered_data = data[(data['Hair color'] == 'Blond') & (data['Race'] == 'Human')]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Fetch the hotel details where the hotel type is 'Hotel' and the star rating is equal to or more than 4.0.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Fetch the hotel details where the hotel type is 'Hotel' and the star rating is equal to or more than 4.0.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/40.csv')\n\n# Filter the data to include only rows where hotel_type is 'Hotel' and star_rating is greater than or equal to 4.0\nfiltered_data = data[(data['hotel_type'] == 'Hotel') & (data['star_rating'] >= 4.0)]\n\n# Output the filtered data\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe based on the conditions\nfiltered_df = df[(df['hotel_type'] == 'Hotel') & (df['star_rating'] >= 4.0)]\n\n# Print the filtered dataframe\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Retrieve hotel information with a star rating of 4.5 or higher and a review count exceeding 1000.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Retrieve hotel information with a star rating of 4.5 or higher and a review count exceeding 1000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Loading the data from the csv file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Filter the data based on the conditions\nresult = df[(df['star_rating'] >= 4.5) & (df['review_count'] > 1000)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data based on the conditions\nfiltered_df = df[(df['star_rating'] >= 4.5) & (df['review_count'] > 1000)]\n\n# Print the result\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Fetch all the hotels which have the number of reviews between 500 and 2000.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Fetch all the hotels which have the number of reviews between 500 and 2000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Filter hotels with review count between 500 and 2000\nfiltered_hotels = df[(df['review_count'] >= 500) & (df['review_count'] <= 2000)]\n\nprint(filtered_hotels)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the dataframe to get the hotels with review count between 500 and 2000\nfiltered_data = data[(data['review_count'] >= 500) & (data['review_count'] <= 2000)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Fetch the hotel details which belong to the parent brand name 'Choice Hotels International, Inc.'.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Fetch the hotel details which belong to the parent brand name 'Choice Hotels International, Inc.'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# read csv file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# filter hotels where the parent brand name is 'Choice Hotels International,Inc.'\nfiltered_df = df[df['parent_brand_name'] == 'Choice Hotels International,Inc.']\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe based on the condition\nfiltered_df = df[df['parent_brand_name'] == 'Choice Hotels International, Inc.']\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Select all properties where the number of bedrooms is greater than 3.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Select all properties where the number of bedrooms is greater than 3.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Select all properties where the number of bedrooms is greater than 3\nselected_properties = df[df['bedrooms'] > 3]\n\nprint(selected_properties)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select all properties where the number of bedrooms is greater than 3\nselected_data = data[data['bedrooms'] > 3]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Retrieve all records where the 'price' is greater than 500000.0 and the 'bathrooms' are more than 2.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Retrieve all records where the 'price' is greater than 500000.0 and the 'bathrooms' are more than 2.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Query the data for records where 'price' > 500000.0 and 'bathrooms' > 2\nresult = df[(df['price'] > 500000.0) & (df['bathrooms'] > 2)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_df = df[(df['price'] > 500000.0) & (df['bathrooms'] > 2)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Calculate the sum of the 'sqft_living' and 'sqft_lot' for each property and then find the record with the maximum sum.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Calculate the sum of the 'sqft_living' and 'sqft_lot' for each property and then find the record with the maximum sum.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Calculate the sum of 'sqft_living' and 'sqft_lot' for each property\ndf['sqft_total'] = df['sqft_living'] + df['sqft_lot']\n\n# Find the record with the maximum sum\nmax_sqft_record = df[df['sqft_total'] == df['sqft_total'].max()]\n\nprint(max_sqft_record)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the sum of 'sqft_living' and 'sqft_lot' for each property\ndf['sum_sqft'] = df['sqft_living'] + df['sqft_lot']\n\n# Find the record with the maximum sum\nmax_sum_record = df[df['sum_sqft'] == df['sum_sqft'].max()]\n\nprint(max_sum_record)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Filter the rows where 'standard_identification_level_1' is not empty.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Filter the rows where 'standard_identification_level_1' is not empty.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/42.csv')\n\n# Select all the rows where 'standard_indentification_level_1' is not null\nresult = data[data['standard_indentification_level_1'].notna()]\n\n# Output the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the rows where 'standard_identification_level_1' is not empty\ndf_filtered = df[df['standard_identification_level_1'].notna()]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Retrieve the female 'smoker' records with 'charges' greater than the overall average 'charges' table value.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Retrieve the female 'smoker' records with 'charges' greater than the overall average 'charges' table value.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Calculate the average charges\navg_charges = df['charges'].mean()\n\n# Filter the records of female smokers with charges above average\nresult = df[(df['sex'] == 'female') & (df['smoker'] == 'yes') & (df['charges'] > avg_charges)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall average 'charges'\naverage_charges = df['charges'].mean()\n\n# Filter the dataframe to get the female 'smoker' records with 'charges' greater than the average\nfiltered_df = df[(df['sex'] == 'female') & (df['smoker'] == 'yes') & (df['charges'] > average_charges)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Select the \"Date\", \"High\", and \"Low\" columns from the table.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Select the \"Date\", \"High\", and \"Low\" columns from the table.", "instruction_type": "Query-Screening", "reference_code": "# Required Library\nimport pandas as pd\n\n# Loading csv data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Selecting Date,High,and Low columns\nresult = df[['Date','High','Low']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select the \"Date\", \"High\", and \"Low\" columns\nselected_data = data[[\"Date\", \"High\", \"Low\"]]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Filter the table to show only rows where the value in the \"Volume\" column is greater than 30000000.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Filter the table to show only rows where the value in the \"Volume\" column is greater than 30000000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Filter the DataFrame to only include rows where the 'Volume' column is greater than 30000000\nfiltered_df = df[df['Volume'] > 30000000]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for rows where the \"Volume\" is greater than 30000000\nfiltered_df = df[df['Volume'] > 30000000]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Retrieve all records where the \"Close\" value was higher than the \"Open\" value and \"Volume\" was above 25000000.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Retrieve all records where the \"Close\" value was higher than the \"Open\" value and \"Volume\" was above 25000000.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Filter the data\nresult = df[(df['Close'] > df['Open']) & (df['Volume'] > 25000000)]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Filter the data where \"Close\" is greater than \"Open\" and \"Volume\" is greater than 25000000\nfiltered_data = data[(data['Close'] > data['Open']) & (data['Volume'] > 25000000)]\n\nprint(filtered_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Filter the houses based on median age greater than 30 years.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Filter the houses based on median age greater than 30 years.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Filter the houses that have a median age greater than 30 years\nselected_houses = df[df['HouseAge'] > 30]\n\nprint(selected_houses)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\nfiltered_df = df[df['HouseAge'] > 30]\n\n# Output the result\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Filter the dataset to identify rows with \"Population\" greater than twice the minimum value in the column.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Filter the dataset to identify rows with \"Population\" greater than twice the minimum value in the column.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Find the minimum population\nmin_population = df['Population'].min()\n\n# Get rows where 'Population' is more than twice the minimum population\nresult = df[df['Population'] > 2 * min_population]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Find the minimum population\nmin_population = df['Population'].min()\n\n# Filter the rows where population is more than twice the minimum population\nfiltered_df = df[df['Population'] > 2 * min_population]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Select entries with location equal to \"AUS\" and measure equal to \"PC_GDP\".", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Select entries with location equal to \"AUS\" and measure equal to \"PC_GDP\".", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Filter the data to retrieve entries where location is \"AUS\" and measure is \"PC_GDP\"\nfiltered_df = df[(df['LOCATION'] == 'AUS') & (df['MEASURE'] == 'PC_GDP')]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select entries with location equal to \"AUS\" and measure equal to \"PC_GDP\"\ndf_selected = df[(df['LOCATION'] == 'AUS') & (df['MEASURE'] == 'PC_GDP')]\n\nprint(df_selected)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Find all rows where the \"Value\" is greater than the average value of the \"Value\" column.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Find all rows where the \"Value\" is greater than the average value of the \"Value\" column.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv(\"infiagent/csv/46.csv\")\n\n# Find the average value of the \"Value\" column\naverage_value = data[\"Value\"].mean()\n\n# Find all rows where the \"Value\" is greater than the average value\nfiltered_data = data[data[\"Value\"] > average_value]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average value of the 'Value' column\naverage_value = df['Value'].mean()\n\n# Filter rows where the 'Value' is greater than the average value\nfiltered_df = df[df['Value'] > average_value]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Identify the 'SUBJECT' corresponding to each 'LOCATION' where the 'VALUE' falls below the average 'VALUE' across all 'LOCATIONS'.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Identify the 'SUBJECT' corresponding to each 'LOCATION' where the 'VALUE' falls below the average 'VALUE' across all 'LOCATIONS'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Compute the overall average value\navg_value = df['Value'].mean()\n\n# Find which values are lower than the average\nbelow_average = df[df['Value'] < avg_value]\n\n# Get the \"SUBJECT\" and \"LOCATION\" of those below average\nresult = below_average[['LOCATION','SUBJECT']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average value across all locations\naverage_value = df['Value'].mean()\n\n# Identify the 'SUBJECT' corresponding to each 'LOCATION' where the 'VALUE' falls below the average 'VALUE'\nresult = df[df['Value'] < average_value][['LOCATION', 'SUBJECT']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Filter the data to keep only the records where the indicator is 'EDUEXP' and the frequency is 'A' for the year 2013.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Filter the data to keep only the records where the indicator is 'EDUEXP' and the frequency is 'A' for the year 2013.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Filter the data\ndf_filtered = df[(df['INDICATOR'] == 'EDUEXP') & (df['FREQUENCY'] == 'A') & (df['TIME'] == '2013')]\n\nprint(df_filtered)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data\ndf_filtered = df[(df['INDICATOR'] == 'EDUEXP') & (df['FREQUENCY'] == 'A') & (df['TIME'] == '2013')]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Retrieve rows where the 'Math and Statistics' value is greater than 40 and 'Physical Sciences' is less than 20.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Retrieve rows where the 'Math and Statistics' value is greater than 40 and 'Physical Sciences' is less than 20.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Filter the data\nfiltered_df = df[(df['Math and Statistics'] > 40) & (df['Physical Sciences'] < 20)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Apply the condition to the dataframe\nresult = data[(data['Math and Statistics'] > 40) & (data['Physical Sciences'] < 20)]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Select all rows where 'class9' is not NULL or 'class9' is not 0.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Select all rows where 'class9' is not NULL or 'class9' is not 0.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# select the rows where 'class9' is not 0\ndf_selected = df[df['class9'] != 0.0]\n\nprint(df_selected)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select rows where 'class9' is not NULL or 'class9' is not 0\nselected_rows = data[(data['class9'].notnull()) | (data['class9'] != 0)]\n\nprint(selected_rows)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Fetch all the rows where 'class9' is 1 and 'class5' is 0.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Fetch all the rows where 'class9' is 1 and 'class5' is 0.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Extract the rows where class9 is 1 and class5 is 0\nfiltered_df = df[(df['class9'] == 1) & (df['class5'] == 0)]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Apply the condition to filter the rows\nresult = df[(df['class9'] == 1.0) & (df['class5'] == 0.0)]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Identify all distinct IDs in the table.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Identify all distinct IDs in the table.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Process data to get unique IDs\nunique_ids = df['ID'].unique()\n\nprint(unique_ids)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Get all distinct IDs\ndistinct_ids = df['ID'].unique()\n\nprint(distinct_ids)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Select all data where the \"Open\" price is higher than the \"Close\" price.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Select all data where the \"Open\" price is higher than the \"Close\" price.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Select data where the \"Open\" price is higher than the \"Close\" price\nresult = df[df['Open'] > df['Close']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Select rows where the \"Open\" price is higher than the \"Close\" price\ndf_reversed = df[df['Open'] > df['Close']]\n\nprint(df_reversed)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Select all rows where the 'Time' column contains the word '59'.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Select all rows where the 'Time' column contains the word '59'.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/49.csv')\n\n# Retrieve all rows where the 'Time' value contains the word '59'\nfiltered_data = data[data['Time'].str.contains('59')]\n\nprint(filtered_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select rows where 'Time' column contains '59'\ndf_filtered = df[df['Time'].str.contains('59')]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Choose records with a \"Close\" price greater than the average \"Close\" price.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Choose records with a \"Close\" price greater than the average \"Close\" price.", "instruction_type": "Query-Screening", "reference_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Loading the data\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Calculating the average of the \"Close\" column\naverage_close = df['Close'].mean()\n\n# Selecting the records where the \"Close\" price is higher than the average \"Close\" price\nresult = df[df['Close'] > average_close]\n\n# Printing the result\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average \"Close\" price\naverage_close_price = df['Close'].mean()\n\n# Filter the records with \"Close\" price greater than the average \"Close\" price\ndf_filtered = df[df['Close'] > average_close_price]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Query the data where the \"Volume\" is above the 75th percentile of the column's data.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Query the data where the \"Volume\" is above the 75th percentile of the column's data.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load data from csv\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Calculate the 75th percentile of the 'Volume' column\nvolume_75_percentile = df['Volume'].quantile(0.75)\n\n# Query rows where 'Volume' is above the 75th percentile\nresult = df[df['Volume'] > volume_75_percentile]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the 75th percentile of the 'Volume' column\nvolume_75th_percentile = df['Volume'].quantile(0.75)\n\n# Filter the data where 'Volume' is above the 75th percentile\ndf_filtered = df[df['Volume'] > volume_75th_percentile]\n\nprint(df_filtered)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Identify the dates and times at which the temperature surpassed the mean temperature.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Identify the dates and times at which the temperature surpassed the mean temperature.", "instruction_type": "Query-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Compute the average temperature\naverage_temp = df['temp'].mean()\n\n# Filter the data for instances where the temperature was above average\nabove_average_temp = df[df['temp'] > average_temp]\n\n# Get the dates and times when the temperature was above average\ndates_and_times = above_average_temp['day']\n\nprint(dates_and_times)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean temperature\nmean_temp = df['temp'].mean()\n\n# Filter the dataframe to get the rows where the temperature surpassed the mean\ndf_surpassed_mean = df[df['temp'] > mean_temp]\n\n# Extract the dates and times\ndates_times = df_surpassed_mean['dt']\n\nprint(dates_times)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Examine and display the count of 'Trips within the last 24 hours' on 10/4/2014.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78", "csv_path": "infiagent/csv/1.csv", "instruction": "Examine and display the count of 'Trips within the last 24 hours' on 10/4/2014.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas dataframe\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Convert 'Date' column to datetime format\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Filter the dataframe to get the required row\nfiltered_data = data[data['Date'] == '2014-10-04']\n\n# Get the number of 'Trips over the past 24-hours' for the date '10/4/2014'\ntrips_on_date = filtered_data['Trips over the past 24-hours (midnight to 11:59pm)'].values[0]\n\nprint(trips_on_date)", "assistant_code": "import pandas as pd\n\n# Load the csv file into a pandas dataframe\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe to get the row with date 10/4/2014\ndf_filtered = df[df['Date'] == '10/4/2014']\n\n# Get the count of 'Trips over the past 24-hours' for 10/4/2014\ntrips_count = df_filtered['Trips over the past 24-hours (midnight to 11:59pm)'].values[0]\n\nprint(trips_count)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Select origin_station, destination_station, route_code, start_time, end_time, and fleet_number from the table.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Select origin_station, destination_station, route_code, start_time, end_time, and fleet_number from the table.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Step 1: Load the data\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# Step 2: Selecting desired columns\nselected_data = data[['origin_station','destination_station','route_code','start_time','end_time','fleet_number']]\n\n# Print the selected data\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Select the required columns\ndf_selected = df[['origin_station', 'destination_station', 'route_code', 'start_time', 'end_time', 'fleet_number']]\n\nprint(df_selected)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Display the data types of each column in the table.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Display the data types of each column in the table.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Display data types\nprint(df.dtypes)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Display the data types of each column in the table\nprint(df.dtypes)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Identify the 'cylinders' and 'displacement' of the vehicles with the highest 5 'horsepower' values.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Identify the 'cylinders' and 'displacement' of the vehicles with the highest 5 'horsepower' values.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Let's first sort the DataFrame by 'horsepower' in descending order\ndf = df.sort_values('horsepower',ascending=False)\n\n# Now we get the data for the top 5 vehicles having the maximum 'horsepower'\ntop_5_vehicles = df.head(5)[['cylinders','displacement']]\n\nprint(top_5_vehicles)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Sort the data by 'horsepower' in descending order and select the top 5 rows\ntop5_hp = df.sort_values(by='horsepower', ascending=False).head(5)\n\n# Select the 'cylinders' and 'displacement' columns from the top 5 rows\nresult = top5_hp[['cylinders', 'displacement']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Extract and list all the employees who are working as a 'POLICE SERGEANT'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Extract and list all the employees who are working as a 'POLICE SERGEANT'.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the csv data into a dataframe\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Use the query function to filter rows where JOBTITLE equals 'POLICE SERGEANT'\nsergeants = df.query(\"`JOBTITLE` == 'POLICE SERGEANT'\")\n\nprint(sergeants)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe based on the 'JOBTITLE' column\npolice_sergeants = df[df['JOBTITLE'] == 'POLICE SERGEANT']\n\n# Extract the 'NAME' column\nnames = police_sergeants['NAME']\n\nprint(names.tolist())"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Obtain the count of records in the table.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Obtain the count of records in the table.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Retrieve the number of records\nnum_records = len(data)\n\n# Print the result\nprint(num_records)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the number of records\nrecord_count = df.shape[0]\n\nprint(record_count)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Retrieve the 'Date' and 'Volume' for all the rows in the dataset.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Retrieve the 'Date' and 'Volume' for all the rows in the dataset.", "instruction_type": "Query-Simple Selection", "reference_code": "# importing pandas library\nimport pandas as pd\n\n# load the data from csv\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Selecting 'Date' and 'Volume' column \nselected_data = df[['Date','Volume']]\n\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Select the 'Date' and 'Volume' columns\nselected_data = data[['Date', 'Volume']]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Display the departments associated with the 'Utility and Other Enterprises' dept_group.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Display the departments associated with the 'Utility and Other Enterprises' dept_group.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Filter the dataframe to select departments under 'Utility and Other Enterprises' dept_group\ndept_list = df[df['dept_group'] == 'Utility and Other Enterprises']['Department Name'].tolist()\n\nprint(dept_list)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe to only include rows where 'dept_group' is 'Utility and Other Enterprises'\nfiltered_df = df[df['dept_group'] == 'Utility and Other Enterprises']\n\n# Get the names of the departments in the filtered dataframe\ndepartments = filtered_df['Department Name'].tolist()\n\nprint(departments)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Filter the table to display only the rows where Gender equals 'Male'.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Filter the table to display only the rows where Gender equals 'Male'.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Select the rows where Gender is 'Male'\nmale_rows = df[df['Gender'] == 'Male']\n\nprint(male_rows)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the dataframe to only include rows where Gender equals 'Male'\nmale_df = df[df['Gender'] == 'Male']\n\nprint(male_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Display the first 5 characters of 'Ethnicity' column for all rows.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Display the first 5 characters of 'Ethnicity' column for all rows.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Fetch the first 5 characters of 'Ethnicity' column\ndf['Ethnicity'] = df['Ethnicity'].str[:5]\n\nprint(df)", "assistant_code": "# Importing pandas library\nimport pandas as pd\n\n# Loading the data from csv\ndf = pd.read_csv('data.csv')\n\n# Selecting the 'Ethnicity' column\nethnicity_column = df['Ethnicity']\n\n# Selecting the first 5 characters of 'Ethnicity' for all rows\nfirst_5_characters = ethnicity_column.str[:5]\n\n# Printing the result\nprint(first_5_characters)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Choose the 'Open' and 'Close' data for each date in the table.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Choose the 'Open' and 'Close' data for each date in the table.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Select 'Date','Open' and 'Close' columns from the DataFrame\ndf = df[['Date','Open','Close']]\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select the 'Open' and 'Close' columns\nselected_data = df[['Open', 'Close']]\n\nprint(selected_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Retrieve all records and sort them by 'abs_diffsel' in descending order, maintaining all other columns unchanged.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Retrieve all records and sort them by 'abs_diffsel' in descending order, maintaining all other columns unchanged.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Sort the dataframe by 'abs_diffsel' in descending order\ndf_sorted = df.sort_values('abs_diffsel',ascending=False)\n\nprint(df_sorted)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Sort the data by 'abs_diffsel' in descending order\ndf_sorted = df.sort_values(by='abs_diffsel', ascending=False)\n\nprint(df_sorted)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Retrieve all records with \"pop\" and \"country\" categories for the year 1962.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Retrieve all records with \"pop\" and \"country\" categories for the year 1962.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Filter the DataFrame to get the records for the year 1962\nfiltered_df = df[df['year'] == 1962][['pop','country']]\n\nprint(filtered_df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Filter the data for the year 1962 and select 'pop' and 'country' columns\nresult = df[df['year'] == 1962][['pop', 'country']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Obtaining the GDP per capita figure for Bahrain in the year 2002.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Obtaining the GDP per capita figure for Bahrain in the year 2002.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Filter the data for the country Bahrain and year 2002\ngdp_2002_bahrain = df[df['country'] == 'Bahrain']['gdpPercap_2002'].values[0]\n\nprint(gdp_2002_bahrain)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for Bahrain in the year 2002\nbahrain_2002 = df.loc[(df['country'] == 'Bahrain') & (df['gdpPercap_2002'] != '')]\n\n# Get the GDP per capita for Bahrain in 2002\ngdp_per_capita = bahrain_2002['gdpPercap_2002'].values[0]\n\nprint(gdp_per_capita)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Display a table containing the hotel names and their corresponding parent brand names from the database.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Display a table containing the hotel names and their corresponding parent brand names from the database.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Select the columns for hotel names and parent brand names\nselected_columns = df[['hotel_name','parent_brand_name']]\n\nprint(selected_columns)", "assistant_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Select the 'hotel_name' and 'parent_brand_name' columns\nselected_data = df[['hotel_name', 'parent_brand_name']]\n\n# Print the selected data\nprint(selected_data)\n"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Obtain the data for the first 10 entries with the maximum 'importance.score'.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Obtain the data for the first 10 entries with the maximum 'importance.score'.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# load the data from csv\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# sort the dataframe based on 'importance.score' in descending order\ndf.sort_values(by='importance.score',ascending=False,inplace=True)\n\n# select the top 10 records\ntop_10_records = df.head(10)\n\nprint(top_10_records)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Sort the data based on 'importance.score' in descending order and select the first 10 entries\ntop_10_importance = data.sort_values(by='importance.score', ascending=False).head(10)\n\nprint(top_10_importance)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Retrieve all records that belong to the year 2013.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Retrieve all records that belong to the year 2013.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Fetch all records for the year 2013\nresult = df[df['TIME'] == 2013]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Select the records that belong to the year 2013\ndf_2013 = df[df['TIME'] == '2013']\n\nprint(df_2013)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Fetch data for 1979, including the values of 'Agriculture', 'Biology' and 'Foreign Languages'.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Fetch data for 1979, including the values of 'Agriculture', 'Biology' and 'Foreign Languages'.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# filter the row for the year 1979 and select the required columns\nresult = df[df['Year'] == 1979][['Agriculture','Biology','Foreign Languages']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Filter the data for the year 1979\ndata_1979 = data[data['Year'] == 1979]\n\n# Select the required columns\nresult = data_1979[['Agriculture', 'Biology', 'Foreign Languages']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Find the rows with the maximum and minimum temperatures for each day.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Find the rows with the maximum and minimum temperatures for each day.", "instruction_type": "Query-Simple Selection", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Convert 'day' column to datetime\ndf['day'] = pd.to_datetime(df['day'])\n\n# Extract the date from the 'day' column\ndf['date'] = df['day'].dt.date\n\n# Find the rows with the max temperature for each date\nmax_temp_df = df.loc[df.groupby('date')['temp'].idxmax()]\n\n# Find the rows with the min temperature for each date\nmin_temp_df = df.loc[df.groupby('date')['temp'].idxmin()]\n\n# Combine the dataframes\nresult_df = pd.concat([max_temp_df,min_temp_df]).sort_values(by='day')\n\nprint(result_df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'temp' column to numeric data type\ndf['temp'] = pd.to_numeric(df['temp'], errors='coerce')\n\n# Group the data by 'day' and get the rows with max and min 'temp' for each group\ngrouped_df = df.groupby('day').agg({'temp': ['max', 'min']})\n\n# Print the result\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Select all rows from the table where the 'fleet_number' is listed in the top 10 fleets with more than 10 trips.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Select all rows from the table where the 'fleet_number' is listed in the top 10 fleets with more than 10 trips.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# load data\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# subquery to obtain the list of fleet_numbers that have made more than 10 trips\nfleet_numbers = data['fleet_number'].value_counts()\nfleet_numbers_more_than_10 = fleet_numbers[fleet_numbers > 10].index.tolist()\n\n# select all rows where 'fleet_number' is in the list\nselected_data = data[data['fleet_number'].isin(fleet_numbers_more_than_10)]\n\nprint(selected_data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Find the top 10 fleets with more than 10 trips\ntop_fleets = df['fleet_number'].value_counts().nlargest(10)[top_fleets > 10]\n\n# Filter the data\nfiltered_df = df[df['fleet_number'].isin(top_fleets.index)]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Sort each region's countries by their 'Health (Life Expectancy)' score.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Sort each region's countries by their 'Health (Life Expectancy)' score.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Group by 'Region' and sort countries in each region according to 'Health (Life Expectancy)'\ndf['Health Rank'] = df.groupby('Region')['Health (Life Expectancy)'].rank(ascending=False)\n\n# Sort the dataframe by 'Region' and 'Health Rank'\ndf_sorted = df.sort_values(by=['Region','Health Rank'])\n\n# Print the result\nprint(df_sorted[['Region','Country','Health Rank']])", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Sort the data by 'Health (Life Expectancy)' score for each region's countries\nsorted_df = df.sort_values(['Region', 'Health (Life Expectancy)'])\n\n# Print the sorted dataframe\n\nprint(sorted_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Fetch game records that have a PCTIMESTRING later than the average PCTIMESTRING of all records.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Fetch game records that have a PCTIMESTRING later than the average PCTIMESTRING of all records.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Convert the 'PCTIMESTRING' column to datetime format\ndf['PCTIMESTRING'] = pd.to_datetime(df['PCTIMESTRING'],format='%M:%S')\n\n# Calculate the average PCTIMESTRING\naverage_pctimestring = df['PCTIMESTRING'].mean()\n\n# Fetch game records that have a PCTIMESTRING later than the average PCTIMESTRING\ndf_later_than_average = df[df['PCTIMESTRING'] > average_pctimestring]\n\nprint(df_later_than_average)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert PCTIMESTRING to datetime format\ndf['PCTIMESTRING'] = pd.to_datetime(df['PCTIMESTRING'])\n\n# Calculate the average PCTIMESTRING\naverage_time = df['PCTIMESTRING'].mean()\n\n# Filter the records with PCTIMESTRING later than the average\nfiltered_df = df[df['PCTIMESTRING'] > average_time]\n\nprint(filtered_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Identify identical 'displacements' applicable to vehicles with 4, 6, or 8 'cylinders'.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Identify identical 'displacements' applicable to vehicles with 4, 6, or 8 'cylinders'.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data\ndata_path = 'infiagent/csv/11.csv'\ndf = pd.read_csv(data_path)\n\n# Filter the dataframe for vehicles with 4,6 or 8 cylinders\nfiltered_df = df[df['cylinders'].isin([4,6,8])]\n\n# Group by 'displacement' and filter for any groups with more than one unique 'cylinder' value\ngrouped_df = filtered_df.groupby('displacement').filter(lambda x: x['cylinders'].nunique() > 1)\n\n# Select the 'displacement' column and remove duplicates\nresult = grouped_df['displacement'].drop_duplicates()\n\nprint(result.to_list())", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter the data for vehicles with 4, 6, or 8 cylinders\nfiltered_df = df[df['cylinders'].isin([4, 6, 8])]\n\n# Find the displacements\ndisplacements = filtered_df['displacement'].unique()\n\nprint(displacements)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Select the 'BARO' values for times when 'WINDSPEED' was in its top 10% range.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Select the 'BARO' values for times when 'WINDSPEED' was in its top 10% range.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Select 'BARO' values for top 10% 'WINDSPEED'\ntop_10_percent_wind_speed = data['WINDSPEED'].quantile(0.9)\nbaro_top_wind_speed = data[data['WINDSPEED'] >= top_10_percent_wind_speed]['BARO']\n\nprint(baro_top_wind_speed)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the 90th percentile of WINDSPEED\ntop_10_percentile = df['WINDSPEED'].quantile(0.9)\n\n# Filter the dataframe where WINDSPEED is in the top 10%\nfiltered_df = df[df['WINDSPEED'] > top_10_percentile]\n\n# Select the 'BARO' values\nbaro_values = filtered_df['BARO']\n\nprint(baro_values)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Determine the highest 'Low' value in the table and associate it with its respective date.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Determine the highest 'Low' value in the table and associate it with its respective date.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# load the data from csv file\ndata = pd.read_csv('infiagent/csv/29.csv')\n\n# calculate the maximum 'Low' price\nmax_low = data['Low'].max()\n\n# get the row that has the maximum 'Low' price\nmax_low_row = data[data['Low'] == max_low]\n\n# get the date corresponding to maximum 'Low' price\ndate = max_low_row['Date'].values[0]\n\n# Output the date and corresponding max 'Low' price\nprint(\"Date: {},Max 'Low' Price: {}\".format(date,max_low))", "assistant_code": "# Required Libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Finding the highest 'Low' value and its date\nhighest_low = df['Low'].max()\ndate_of_highest_low = df[df['Low'] == highest_low]['Date'].values[0]\n\n# Output\nprint(\"The highest 'Low' value is: \", highest_low)\nprint(\"The date of the highest 'Low' value is: \", date_of_highest_low)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Find the 'state_abbr' and 'county_name' for the entries with the top 10 highest values in 'total_votes'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Find the 'state_abbr' and 'county_name' for the entries with the top 10 highest values in 'total_votes'.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/30.csv')\n\n# Adjust the datatype of 'total_votes' to float\ndata['total_votes'] = data['total_votes'].astype(float)\n\n# Use nlargest to get the top 10 highest 'total_votes' and the corresponding 'state_abbr' and 'county_name'\nresult = data.nlargest(10,'total_votes')[['state_abbr','county_name']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Sort the data based on 'total_votes' in descending order\ndf_sorted = df.sort_values(by='total_votes', ascending=False)\n\n# Get the top 10 entries\ntop_10 = df_sorted.head(10)\n\n# Get the 'state_abbr' and 'county_name' for the top 10 entries\nresult = top_10[['state_abbr', 'county_name']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Retrieve the countries with the highest number of cases in the table.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Retrieve the countries with the highest number of cases in the table.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Remove the square brackets and split the values to get the maximum number of cases\ndf['No. of cases'] = df['No. of cases'].str.split('[').str[0]\n\n# Convert the 'No. of cases' column to numeric\ndf['No. of cases'] = pd.to_numeric(df['No. of cases'],errors='coerce')\n\n# Get the country with the highest number of cases\ncountry_highest_cases = df.loc[df['No. of cases'].idxmax()]['Country']\n\nprint(country_highest_cases)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Clean the 'No. of cases' column and convert it to integer\ndata['No. of cases'] = data['No. of cases'].str.extract('(\\d+)', expand=False).astype(int)\n\n# Get the countries with the highest number of cases\nmax_cases = data['No. of cases'].max()\ncountries_with_max_cases = data[data['No. of cases'] == max_cases]['Country']\n\nprint(countries_with_max_cases)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\n\nQuestion: Select rows where 'gdpPercap_1977' is higher than the average 'gdpPercap_1977' of countries having 'gdpPercap_2007' greater than 20000.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Select rows where 'gdpPercap_1977' is higher than the average 'gdpPercap_1977' of countries having 'gdpPercap_2007' greater than 20000.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndata = pd.read_csv('infiagent/csv/36.csv')\n\n# Calculate the average 'gdpPercap_1977' for countries with 'gdpPercap_2007' greater than 20000\naverage_gdpPercap_1977 = data[data['gdpPercap_2007'] > 20000]['gdpPercap_1977'].mean()\n\n# Retrieve all rows where 'gdpPercap_1977' is greater than the average\nresult = data[data['gdpPercap_1977'] > average_gdpPercap_1977]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'gdpPercap_1977' of countries having 'gdpPercap_2007' greater than 20000\navg_gdp_1977 = df[df['gdpPercap_2007'] > 20000]['gdpPercap_1977'].mean()\n\n# Select rows where 'gdpPercap_1977' is higher than the calculated average\nresult = df[df['gdpPercap_1977'] > avg_gdp_1977]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Retrieve the names of all male superheroes who are taller than the average height of all superheroes.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Retrieve the names of all male superheroes who are taller than the average height of all superheroes.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/39.csv')\n\n# Calculate the average height of all superheroes\naverage_height = data['Height'].mean()\n\n# Use a query to find all male superheroes who are taller than the average height\ntaller_superheroes = data.query('Gender == \"Male\" and Height > @average_height')['name']\n\nprint(taller_superheroes)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average height of all superheroes\naverage_height = df['Height'].mean()\n\n# Retrieve the names of all male superheroes who are taller than the average height\nmale_superheroes_taller_than_average = df[(df['Gender'] == 'Male') & (df['Height'] > average_height)]['name']\n\nprint(male_superheroes_taller_than_average)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Find the date with the highest \"Range\" value in days for the high volume.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Find the date with the highest \"Range\" value in days for the high volume.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Calculate Range as the difference between High and Low for each day\ndf['Range'] = df['High'] - df['Low']\n\n# Classify volume into labels 'High' and 'Low' based on median value\ndf['Volume_Label'] = pd.qcut(df['Volume'],q=2,labels=['Low','High'])\n\n# Get the subset of data where volume label is 'High'\ndf_high_volume = df[df['Volume_Label'] == 'High']\n\n# Get the date with the highest Range value in days with high volume\nmax_range_date = df_high_volume[df_high_volume['Range'] == df_high_volume['Range'].max()]['Date'].iloc[0]\n\nprint(max_range_date)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Calculate the difference between consecutive dates and add it as a new column 'Range'\ndata['Range'] = data['Date'].diff().dt.days\n\n# Find the date with the highest 'Range' value for the high volume\nmax_range_date = data[data['Range'] == data['Range'].max()]['Date'].values[0]\n\nprint(max_range_date)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Utilizing a subquery, identify the dates and temperatures for the days with the maximum wind speed.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Utilizing a subquery, identify the dates and temperatures for the days with the maximum wind speed.", "instruction_type": "Query-Subquery", "reference_code": "import pandas as pd\n\n# Load data\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# First,find the max wind speed\nmax_wind_speed = df['wind_speed'].max()\n\n# Then,filter the dataframe with rows having max_wind_speed,and keep 'day' and 'temp'\nresult = df.loc[df['wind_speed'] == max_wind_speed,['day','temp']]\n\nprint(result)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove the '°C' from the 'temp' column and convert it to float\ndf['temp'] = df['temp'].str.replace('°C', '').astype(float)\n\n# Find the day with the maximum wind speed\nmax_wind_speed_day = df[df['wind_speed'] == df['wind_speed'].max()]\n\n# Get the dates and temperatures for the day with the maximum wind speed\nresult = max_wind_speed_day[['day', 'temp']]\n\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Group the data by 'Year-Month' and provide the count of 'Daily Trips' for each group.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Group the data by 'Year-Month' and provide the count of 'Daily Trips' for each group.", "instruction_type": "Update-Aggregation", "reference_code": "# Import the necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Convert 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create 'Year-Month' column \ndf['Year-Month'] = df['Date'].dt.to_period('M')\n\n# Count 'Daily Trips' for each 'Year-Month'\ndf.rename(columns={'Trips over the past 24-hours (midnight to 11:59pm)': 'Daily Trips'},inplace=True)\ndf_grouped = df.groupby('Year-Month')['Daily Trips'].count()\n\n# Print the result\nprint(df_grouped)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Remove the time component from the 'Date'\ndf['Date'] = df['Date'].dt.date\n\n# Group by 'Year-Month' and count the 'Daily Trips'\ngrouped_data = df.groupby('Date').agg({'Trips over the past 24-hours (midnight to 11:59pm': 'sum'})\n\nprint(grouped_data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Summarize the data by 'Date' and update 'Cumulative trips' to show the total of 'Daily Trips'.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Summarize the data by 'Date' and update 'Cumulative trips' to show the total of 'Daily Trips'.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/1.csv')\n# rename columns to make it easier to work with\ndf.columns = df.columns.str.replace(' ','_').str.replace('(','').str.replace(')','').str.replace(':','').str.replace('-','_').str.lower()\n\n# perform the grouping and summing operation\ndf['cumulative_trips'] = df.groupby('date')['trips_over_the_past_24_hours_midnight_to_1159pm'].transform('sum')\n\nprint(df)", "assistant_code": "# import the necessary libraries\nimport pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Rename 'Cumulative trips (since launch):' column to 'Cumulative trips'\ndf = df.rename(columns={'Cumulative trips (since launch):': 'Cumulative trips'})\n\n# Convert 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Group by 'Date' and sum 'Trips over the past 24-hours (midnight to 11:59pm)' and 'Cumulative trips'\ngrouped_df = df.groupby('Date').agg({'Trips over the past 24-hours (midnight to 11:59pm)': 'sum', 'Cumulative trips': 'sum'})\n\n# Print the summarized data\nprint(grouped_df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Update the 'Market Cap' to be the maximum value for all rows where the 'Volume' is greater than 6,000,000.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Update the 'Market Cap' to be the maximum value for all rows where the 'Volume' is greater than 6,000,000.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Update 'Market Cap' to be the maximum value for rows where 'Volume' is greater than 6,000,000\ndf.loc[df['Volume'] > '6,000,000','Market Cap'] = df.loc[df['Volume'] > '6,000,000','Market Cap'].max()\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Remove commas from 'Volume' and convert it to numeric\ndf['Volume'] = df['Volume'].str.replace(',', '').astype(int)\n\n# Find the maximum 'Market Cap' where 'Volume' is greater than 6000000\nmax_market_cap = df.loc[df['Volume'] > 6000000, 'Market Cap'].max()\n\n# Update 'Market Cap' for rows where 'Volume' is greater than 6000000\ndf.loc[df['Volume'] > 6000000, 'Market Cap'] = max_market_cap\n\n# Print the updated DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Update the 'min_diffsel' field with results from a sub-query that computes the average of 'min_diffsel' for each 'site'.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Update the 'min_diffsel' field with results from a sub-query that computes the average of 'min_diffsel' for each 'site'.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Calculate the average of 'min_diffsel' for each 'site'\navg_min_diffsel = df.groupby('site')['min_diffsel'].mean()\n\n# Update the 'min_diffsel' field with the results from the sub-query \ndf['min_diffsel'] = df['site'].map(avg_min_diffsel)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute the average of 'min_diffsel' for each 'site'\naverage_min_diffsel = df.groupby('site')['min_diffsel'].mean()\n\n# Update the 'min_diffsel' field with the computed averages\ndf['min_diffsel'] = df['site'].map(average_min_diffsel)\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Group data by 'site' and update each group's 'positive_diffsel' with its average.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Group data by 'site' and update each group's 'positive_diffsel' with its average.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Group by 'site' and calculate the average of 'positive_diffsel' for each group\ngroup_average = df.groupby('site')['positive_diffsel'].mean()\n\n# Update 'positive_diffsel' with the group average\ndf['positive_diffsel'] = df['site'].map(group_average)\n\n# Output result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Group by 'site' and calculate the mean of 'positive_diffsel' for each group\ngrouped_df = df.groupby('site')['positive_diffsel'].mean()\n\n# Update 'positive_diffsel' with the grouped mean values\ndf['positive_diffsel'] = df['site'].map(grouped_df)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Populate any empty cells in the 'Deliverable Qty' column with the median of the existing values in that column.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Populate any empty cells in the 'Deliverable Qty' column with the median of the existing values in that column.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# Replace empty cells in 'Deliverable Qty' column with NaN\ndf['Deliverable Qty'] = df['Deliverable Qty'].replace('',np.nan)\n\n# Calculate the median of 'Deliverable Qty' column without the NaN values\nmedian_value = df['Deliverable Qty'].median(skipna=True)\n\n# Replace the NaN values in 'Deliverable Qty' column with the median\ndf['Deliverable Qty'] = df['Deliverable Qty'].fillna(median_value)\n\n# Print the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'Deliverable Qty' column to numeric type\ndf['Deliverable Qty'] = pd.to_numeric(df['Deliverable Qty'], errors='coerce')\n\n# Calculate the median of 'Deliverable Qty' column\nmedian_value = df['Deliverable Qty'].median()\n\n# Fill the NaN values with the median\ndf['Deliverable Qty'].fillna(median_value, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Set the value of 'Last Price' for 'GODREJIND' to be the largest value of 'Close Price' in the dataset.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Set the value of 'Last Price' for 'GODREJIND' to be the largest value of 'Close Price' in the dataset.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# Find the maximum 'Close Price' in the dataset\nmax_close_price = df['Close Price'].max()\n\n# Update 'Last Price' of 'GODREJIND' to be the max 'Close Price'\ndf.loc[df['Symbol'] == 'GODREJIND','Last Price'] = max_close_price\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Get the largest 'Close Price' in the dataset\nmax_close_price = df['Close Price'].max()\n\n# Set the 'Last Price' for 'GODREJIND' to be the largest 'Close Price'\ndf.loc[df['Symbol'] == 'GODREJIND', 'Last Price'] = max_close_price\n\n# Output the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Set the 'review_count' field to the median value of 'review_count' when the current 'review_count' is missing.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Set the 'review_count' field to the median value of 'review_count' when the current 'review_count' is missing.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# load the data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# compute the median of 'review_count'\nmedian_review_count = df['review_count'].median()\n\n# update null values in 'review_count' with the median\ndf['review_count'] = df['review_count'].apply(lambda x: median_review_count if pd.isnull(x) else x)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of 'review_count'\nmedian_review_count = df['review_count'].median()\n\n# Fill the missing values in 'review_count' with the median value\ndf['review_count'] = df['review_count'].fillna(median_review_count)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Calculate the average value of the 'AveBedrms' column where the 'AveRooms' value is greater than 6 and update the 'AveBedrms' column with the calculated value.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Calculate the average value of the 'AveBedrms' column where the 'AveRooms' value is greater than 6 and update the 'AveBedrms' column with the calculated value.", "instruction_type": "Update-Aggregation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Find the mean of 'AveBedrms'\nmean_value = df['AveBedrms'].mean()\n\n# Update the 'AveBedrms' column\ndf.loc[df['AveRooms'] > 6,'AveBedrms'] = mean_value\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Filter rows where 'AveRooms' value is greater than 6\ndf_filtered = df[df['AveRooms'] > 6]\n\n# Calculate the average value of the 'AveBedrms' column for the filtered data\naverage_bedrms = df_filtered['AveBedrms'].mean()\n\n# Update the 'AveBedrms' column with the calculated average where 'AveRooms' is greater than 6\ndf.loc[df['AveRooms'] > 6, 'AveBedrms'] = average_bedrms\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Change 'Miles traveled today' to miles per trip for each day where 'Trips over the past 24-hours' exceeded 30000.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Change 'Miles traveled today' to miles per trip for each day where 'Trips over the past 24-hours' exceeded 30000.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Filtering out the dates where Trips exceeded 30000 and updating 'Miles traveled today' to miles per trip\ndf.loc[df['Trips over the past 24-hours (midnight to 11:59pm)'] > 30000,'Miles traveled today (midnight to 11:59 pm)'] = df['Miles traveled today (midnight to 11:59 pm)'] / df['Trips over the past 24-hours (midnight to 11:59pm)']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a condition where 'Trips over the past 24-hours' exceeded 30000\ncondition = df['Trips over the past 24-hours (midnight to 11:59pm)'] > 30000\n\n# Update 'Miles traveled today' to miles per trip\ndf.loc[condition, 'Miles traveled today (midnight to 11:59 pm)'] = df.loc[condition, 'Miles traveled today (midnight to 11:59 pm)'] / df.loc[condition, 'Trips over the past 24-hours (midnight to 11:59pm)']\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: For the 'route_code' column, subtract 1 from the existing values and update the column.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "For the 'route_code' column, subtract 1 from the existing values and update the column.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# Subtract 1 from 'route_code' and update the column\ndata['route_code'] = data['route_code'] - 1\n\nprint(data)", "assistant_code": "# Importing required library\nimport pandas as pd\n\n# Loading the data from csv\ndf = pd.read_csv('data.csv')\n\n# Decrementing the 'route_code' column by 1\ndf['route_code'] = df['route_code'].apply(lambda x: x - 1)\n\n# Printing the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: For all \"Dystopia Residual\" values less than 2.5, add 0.2.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "For all \"Dystopia Residual\" values less than 2.5, add 0.2.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd \n\n# Load data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Increase by 0.2 every 'Dystopia Residual' value that is less than 2.5\ndf.loc[df['Dystopia Residual'] < 2.5,'Dystopia Residual'] += 0.2\n\n# print df\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Filter 'Dystopia Residual' values less than 2.5 and add 0.2\ndf.loc[df['Dystopia Residual'] < 2.5, 'Dystopia Residual'] += 0.2\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Increase the value of the \"num. busy overflows\" field by 10% for each row in the table.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Increase the value of the \"num. busy overflows\" field by 10% for each row in the table.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Update the 'num. busy overflows' field by increasing its value by 10%\ndf['num. busy overflows'] = df['num. busy overflows'] * 1.1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('data.csv')\n\n# Increase the value of the \"num. busy overflows\" field by 10%\ndata['num. busy overflows'] = data['num. busy overflows'].apply(lambda x: x + x*0.1)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Convert the 'avg. abandonment time' values to minutes instead of the current format.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Convert the 'avg. abandonment time' values to minutes instead of the current format.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/4.csv')\n\n# Convert 'avg. abandonment time' to minutes\ndf['avg. abandonment time'] = pd.to_timedelta(df['avg. abandonment time']).dt.total_seconds() / 60\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Convert 'avg. abandonment time' from 'HH:MM:SS' format to minutes\ndf['avg. abandonment time'] = df['avg. abandonment time'].apply(lambda x: sum(int(t) * 60 ** i for i, t in enumerate(reversed(x.split(':')))))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Update the SCOREMARGIN column by dividing by 2 for each row to halve the score margin.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Update the SCOREMARGIN column by dividing by 2 for each row to halve the score margin.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Clean the SCOREMARGIN column and update it by halving the values\ndf['SCOREMARGIN'] = pd.to_numeric(df['SCOREMARGIN'],errors='coerce')/2\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the SCOREMARGIN column by dividing by 2\ndf['SCOREMARGIN'] = df['SCOREMARGIN'] / 2\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Normalise the 'Count' column by subtracting the mean and dividing by the standard deviation.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Normalise the 'Count' column by subtracting the mean and dividing by the standard deviation.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# load the csv file\ndata = pd.read_csv('infiagent/csv/10.csv')\n\n# calculate mean and standard deviation of 'Count' column\nmean_count = data['Count'].mean()\nstd_count = data['Count'].std()\n\n# normalise 'Count' column\ndata['Count'] = (data['Count'] - mean_count) / std_count\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean and standard deviation of the 'Count' column\nmean_count = df['Count'].mean()\nstd_count = df['Count'].std()\n\n# Normalize the 'Count' column\ndf['Count'] = (df['Count'] - mean_count) / std_count\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Multiply the contents of the 'acceleration' column by double.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Multiply the contents of the 'acceleration' column by double.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/11.csv')\n\n# Multiply all values in the 'acceleration' column by 2\ndata['acceleration'] = data['acceleration'] * 2\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Multiply the 'acceleration' column by 2\ndf['acceleration'] = df['acceleration'] * 2\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Normalize 'weight' in the data (divide by the max value).", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Normalize 'weight' in the data (divide by the max value).", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# Normalize 'weight' in the data\ndf['weight'] = df['weight'] / df['weight'].max()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Normalize 'weight' column\ndf['weight'] = df['weight'] / df['weight'].max()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Increase the value of 'ANNUAL_RT' by 2% for all rows where 'YEARS_OF_SERVICE' is greater than 10 years.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Increase the value of 'ANNUAL_RT' by 2% for all rows where 'YEARS_OF_SERVICE' is greater than 10 years.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\nfrom datetime import datetime\n\n# loading data from the csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# convert HIRE_DT to datetime\ndf['HIRE_DT'] = pd.to_datetime(df['HIRE_DT'])\n\n# calculate YEARS_OF_SERVICE\ndf['YEARS_OF_SERVICE'] = datetime.now().year - df['HIRE_DT'].dt.year\n\n# update 'ANNUAL_RT' with a increase by 2% for those who have more than 10 'YEARS_OF_SERVICE'\ndf.loc[df['YEARS_OF_SERVICE'] > 10,'ANNUAL_RT'] = df.loc[df['YEARS_OF_SERVICE'] > 10,'ANNUAL_RT']*1.02\n\n# print updated data frame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'ANNUAL_RT' to float type to allow for mathematical operations\ndf['ANNUAL_RT'] = df['ANNUAL_RT'].astype(float)\n\n# Create a new column 'YEARS_OF_SERVICE' which is derived from the 'HIRE_DT'\ndf['YEARS_OF_SERVICE'] = pd.to_datetime(df['HIRE_DT']).dt.year\n\n# Increase the value of 'ANNUAL_RT' by 2% for all rows where 'YEARS_OF_SERVICE' is greater than 10 years\ndf.loc[df['YEARS_OF_SERVICE'] > 10, 'ANNUAL_RT'] *= 1.02\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: For all players, decrease the salary_in_thousands_of_dollars by 5% if the number_of_strike_outs exceeds 80.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "For all players, decrease the salary_in_thousands_of_dollars by 5% if the number_of_strike_outs exceeds 80.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/14.csv')\n\n# Update 'salary_in_thousands_of_dollars' by subtracting 5% for players with 'number_of_strike_outs' more than 80\ndata.loc[data['number_of_strike_outs'] > 80,'salary_in_thousands_of_dollars'] = data.loc[data['number_of_strike_outs'] > 80,'salary_in_thousands_of_dollars'] * 0.95\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Decrease the salary by 5% if the number of strikeouts exceeds 80\ndf.loc[df['number_of_strike_outs'] > 80, 'salary_in_thousands_of_dollars'] *= 0.95\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Convert the 'windspeed' in each record to percentage form by multiplying it by 100.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Convert the 'windspeed' in each record to percentage form by multiplying it by 100.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Multiply the 'windspeed' by 100 for each record to convert it into percentage\ndf['windspeed'] = df['windspeed'] * 100\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the 'windspeed' column\ndf['windspeed'] = df['windspeed'] * 100\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Adjust the 'Volume' column by multiplying each value by 10, and record the updated values in the same column.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Adjust the 'Volume' column by multiplying each value by 10, and record the updated values in the same column.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Remove commas\ndf[\"Volume\"] = df[\"Volume\"].str.replace(',','').astype(float)\n\n# Multiply the 'Volume' by 10\ndf[\"Volume\"] = df[\"Volume\"] * 10\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Adjust the 'Volume' column by multiplying each value by 10\ndf['Volume'] = df['Volume'].str.replace(',', '').astype(float) * 10\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Update the 'Expected Readmission Rate' column by adding 10% to each existing value.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Update the 'Expected Readmission Rate' column by adding 10% to each existing value.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Update the 'Expected Readmission Rate' column by adding 10% to each existing value\ndf['Expected Readmission Rate'] = df['Expected Readmission Rate'] * 1.1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the 'Expected Readmission Rate' column by adding 10% to each existing value\ndf['Expected Readmission Rate'] = df['Expected Readmission Rate'] * 1.10\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Update the 'damage_USD' value to be 50% of the original value for all rows where the 'deaths' is greater than 100.0.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Update the 'damage_USD' value to be 50% of the original value for all rows where the 'deaths' is greater than 100.0.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Update the 'damage_USD' value for rows where 'deaths' is greater than 100.0\ndf.loc[df['deaths'] > 100.0,'damage_USD'] = df.loc[df['deaths'] > 100.0,'damage_USD'] * 0.5\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Where 'deaths' is greater than 100, set 'damage_USD' to 50% of its value\ndf.loc[df['deaths'] > 100.0, 'damage_USD'] = df.loc[df['deaths'] > 100.0, 'damage_USD'] * 0.5\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Update 'Limit' column by subtracting 'Rating' from its current value for all rows.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Update 'Limit' column by subtracting 'Rating' from its current value for all rows.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Subtract 'Rating' from 'Limit'\ndf['Limit'] = df['Limit'] - df['Rating']\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update 'Limit' column by subtracting 'Rating' from its current value for all rows\ndf['Limit'] = df['Limit'] - df['Rating']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Increase the 'Balance' column value by 10% of the 'Income' for each row.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Increase the 'Balance' column value by 10% of the 'Income' for each row.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# load the csv file into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# update the 'Balance' column by adding 10% of 'Income'\ndf['Balance'] = df['Balance'] + (0.1 * df['Income'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Increase the 'Balance' column value by 10% of the 'Income' for each row\ndf['Balance'] = df['Balance'] + (df['Income'] * 0.1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Divide the values in column 'X' by 1E+06.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Divide the values in column 'X' by 1E+06.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Update column 'X' with its value divided by 1E+06\ndf['X'] = df['X'] / 1E+06\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Divide the values in column 'X' by 1E+06\ndf['X'] = df['X'] / 1E+06\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Update the 'Close' column by adding 10% to its existing value for the dates where 'Volume' exceeds 80000000.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Update the 'Close' column by adding 10% to its existing value for the dates where 'Volume' exceeds 80000000.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Update 'Close' column by adding 10% to its existing value for the dates where 'Volume' exceeds 80000000\ndf.loc[df['Volume'] > 80000000,'Close'] = df.loc[df['Volume'] > 80000000,'Close'] * 1.1\n\n# Print the DataFrame\nprint(df)", "assistant_code": "# necessary library for data manipulation\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Increase the Close column value by 10% for rows where Volume > 80000000\ndf.loc[df['Volume'] > 80000000, 'Close'] = df.loc[df['Volume'] > 80000000, 'Close'] * 1.10\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Create a new column named 'votes_ratio' by dividing 'votes_dem' by 'votes_gop', and round the values to 2 decimal places.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Create a new column named 'votes_ratio' by dividing 'votes_dem' by 'votes_gop', and round the values to 2 decimal places.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from a CSV file\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Create a new column 'votes_ratio' by dividing 'votes_dem' by 'votes_gop' and round to 2 decimal places\ndf['votes_ratio'] = (df['votes_dem'] / df['votes_gop']).round(2)\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Create a new column 'votes_ratio' by dividing 'votes_dem' by 'votes_gop'\ndf['votes_ratio'] = df['votes_dem'] / df['votes_gop']\n\n# Round the values in 'votes_ratio' to 2 decimal places\ndf['votes_ratio'] = df['votes_ratio'].round(2)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: If 'NZ' is less than 0, multiply the corresponding cell in 'EQ_Sum' by -1.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "If 'NZ' is less than 0, multiply the corresponding cell in 'EQ_Sum' by -1.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndf = pd.read_csv(\"infiagent/csv/31.csv\")\n\n# Calculate the sum of EQ1 to EQ8 for each row and store it in a new column 'EQ_Sum'\ndf['EQ_Sum'] = df[[\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\"]].sum(axis=1)\n\n# Create a new column 'Updated_EQ_Sum' that contains the updated values for 'EQ_Sum' based on the condition\n# If 'NZ' is less than 0,the cell in 'EQ_Sum' is multiplied by -1\ndf['Updated_EQ_Sum'] = df.apply(lambda row: row['EQ_Sum']*-1 if row['NZ']<0 else row['EQ_Sum'],axis=1)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the sum of 'EQ' columns \ndf['EQ_Sum'] = df.filter(like='EQ').sum(axis=1)\n\n# If 'NZ' is less than 0, multiply the corresponding 'EQ_Sum' by -1\ndf.loc[df['NZ'] < 0, 'EQ_Sum'] *= -1\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Separate the 'EX1' value into three equal parts based on the top 10 percentile of corresponding 'NZ' values.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Separate the 'EX1' value into three equal parts based on the top 10 percentile of corresponding 'NZ' values.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data \ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Define the top 10 percentile threshold for 'NZ'\ntop_10_percentile = df['NZ'].quantile(0.9)\n\n# Define new values for 'EX1' where 'NZ' is in the top 10 percentile\ndf.loc[df['NZ'] > top_10_percentile,'EX1'] = df.loc[df['NZ'] > top_10_percentile,'EX1'] / 3\n\n# Print the updated table\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the 90th percentile of 'NZ' values\ntop_10_percentile = np.percentile(df['NZ'], 90)\n\n# Create a boolean mask based on 'NZ' values\nmask = df['NZ'] <= top_10_percentile\n\n# Separate the 'EX1' column into three parts based on the 'NZ' values\ndf['EX1_part1'] = df.loc[mask, 'EX1'].iloc[:int(len(mask)*0.33)].values\ndf['EX1_part2'] = df.loc[mask, 'EX1'].iloc[int(len(mask)*0.33):int(len(mask)*0.67)].values\ndf['EX1_part3'] = df.loc[mask, 'EX1'].iloc[int(len(mask)*0.67):].values\n\nprint(df[['EX1_part1', 'EX1_part2', 'EX1_part3']])"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Multiply the 'No. of cases' column by 1000 to express the data as thousands.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Multiply the 'No. of cases' column by 1000 to express the data as thousands.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/32.csv')\n\n# Removing square braces and its value for casting to integer\ndata['No. of cases'] = data['No. of cases'].str.split('[').str[0]\n\n# Data type conversion\ndata['No. of cases'] = pd.to_numeric(data['No. of cases'])\n\n# Multiply the number of cases by 1000\ndata['No. of cases'] = data['No. of cases']*1000\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Multiply the 'No. of cases' column by 1000\ndata['No. of cases'] = data['No. of cases'].str.replace('[\\d*]', '', regex=True).astype(float) * 1000\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Calculate the compound score by adding the values of 'pos' and 'neu', then subtracting the value of 'neg'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Calculate the compound score by adding the values of 'pos' and 'neu', then subtracting the value of 'neg'.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv data\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# Update the compound score\ndata['compound'] = data['pos'] + data['neu'] - data['neg']\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Calculate the compound score\ndf['compound_score'] = df['pos'] + df['neu'] - df['neg']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Calculate and store the cumulative sum of 'max_diffsel' and 'abs_diffsel' in the 'max_diffsel' column.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Calculate and store the cumulative sum of 'max_diffsel' and 'abs_diffsel' in the 'max_diffsel' column.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Update 'max_diffsel' column\ndf['max_diffsel'] = df['max_diffsel'] + df['abs_diffsel']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the cumulative sum of 'max_diffsel' and 'abs_diffsel'\ndf['max_diffsel'] = df['max_diffsel'].cumsum()\n\n# Store the result in a variable\nresult = df\n\n# Print the result\nprint(result)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsite,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n\nQuestion: Calculate the difference between the maximum and minimum values of 'min_diffsel' and store the result in 'min_diffsel'.", "csv_data_example": "site,abs_diffsel,positive_diffsel,negative_diffsel,max_diffsel,min_diffsel\n(HA2)121,9.026365225783264,4.147102073893115,-4.8792631518901475,1.5787387471316894,-1.004167098795603\n326,9.002764774505879,3.6156009948803702,-5.387163779625509,0.7169223975734413,-1.2184218611180495\n280,8.418637730396656,5.146937744324779,-3.271699986071876,0.9710714351184295,-1.0182673783732994\n9,8.185717407618215,4.420441253098646,-3.7652761545195683,1.0005541420596125,-0.8471518556126452\n210,8.058662714496545,5.824385887858723,-2.2342768266378226,1.37896385796,-1.2405474792520252\n192,8.015976108875662,4.7228763586939415,-3.293099750181721,1.263069276940311,-1.027758871160202\n-12,7.975893014675133,3.3941409184005606,-4.581752096274572,0.6785731341245361,-1.2624452625748657\n171,7.856854881480676,3.190966528591367,-4.665888352889309,1.2322928805568882,-1.9128310938041404\n(HA2)102,7.846329400937815,3.4848102321226664,-4.361519168815148,0.9115078157304708,-1.663800562787797\n312,7.823952542822076,4.061786995426102,-3.7621655473959748,1.3208015052141648,-1.4217237593217844\n", "csv_path": "infiagent/csv/34.csv", "instruction": "Calculate the difference between the maximum and minimum values of 'min_diffsel' and store the result in 'min_diffsel'.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/34.csv')\n\n# Calculate the difference between maximum and minimum value of 'min_diffsel'\ndiff_min_diffsel = df['min_diffsel'].max() - df['min_diffsel'].min()\n\n# Update 'min_diffsel' with the calculated difference\ndf['min_diffsel'] = diff_min_diffsel\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the difference between the maximum and minimum values of 'min_diffsel'\ndf['min_diffsel'] = df['min_diffsel'].max() - df['min_diffsel'].min()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Increase each value in the 'lifeexp' column by 5.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Increase each value in the 'lifeexp' column by 5.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/35.csv')\n\n# Update the 'lifeexp' column by adding 5 to each existing value\ndata['lifeexp'] = data['lifeexp'].apply(lambda x: x + 5)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Increase each value in the 'lifeexp' column by 5\ndf['lifeexp'] = df['lifeexp'] + 5\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Increment the 'Age' column for each record by 1.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Increment the 'Age' column for each record by 1.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Adjust the 'Age' by one year\ndf['Age'] = df['Age'] + 1\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('data.csv')\n\n# Increment the 'Age' column for each record by 1\ndf['Age'] = df['Age'].apply(lambda x: x + 1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: If 'Turnover' is zero, set 'Turnover' equal to the product of 'Close Price' and 'Total Traded Quantity'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "If 'Turnover' is zero, set 'Turnover' equal to the product of 'Close Price' and 'Total Traded Quantity'.", "instruction_type": "Update-Basic Computation", "reference_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# remove leading and trailing whitespaces from column names\ndata.columns = data.columns.str.strip()\n\n# remove leading and trailing whitespaces from data\nfor col in data.columns:\n    if data[col].dtype == 'object':\n        data[col] = data[col].str.strip()\n\n# convert columns to appropriate datatype\ndata['Close Price'] = data['Close Price'].astype(float)\ndata['Total Traded Quantity'] = data['Total Traded Quantity'].astype(int)\ndata['Turnover'] = data['Turnover'].astype(float)\n\n# update 'Turnover' column if it is zero\ndata.loc[data['Turnover'] == 0,'Turnover'] = data['Close Price'] * data['Total Traded Quantity']\n\n# print the updated dataframe\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Remove the unwanted spaces in the columns\ndata = data.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n\n# If 'Turnover' is zero, set 'Turnover' equal to the product of 'Close Price' and 'Total Traded Quantity'\ndata.loc[data['Turnover'] == 0, 'Turnover'] = data['Close Price'] * data['Total Traded Quantity']\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Increase the weight of all 'Human' superheroes by 10%.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Increase the weight of all 'Human' superheroes by 10%.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Increase the weight of all 'Human' superheroes by 10%\ndf.loc[df['Race'] == 'Human','Weight'] *= 1.1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Increase the weight of all 'Human' superheroes by 10%\ndf.loc[df['Race'] == 'Human', 'Weight'] *= 1.1\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Add 5 to the height of all superheroes whose 'Hair color' is 'No Hair'.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Add 5 to the height of all superheroes whose 'Hair color' is 'No Hair'.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/39.csv')\n\n# Locate rows where 'Hair color' is 'No Hair' \n# and add 5 to height of those superheroes.\ndata.loc[data['Hair color'] == 'No Hair','Height'] += 5\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Update the 'Height' of heroes whose 'Hair color' is 'No Hair'\ndata.loc[data['Hair color'] == 'No Hair', 'Height'] += 5\n\n# Print the updated data\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Modify the 'bubble_score' column by dividing its values by 5 and then multiplying them by 100 to convert them to rating percentages.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Modify the 'bubble_score' column by dividing its values by 5 and then multiplying them by 100 to convert them to rating percentages.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Update the 'bubble_score' column \ndf['bubble_score'] = df['bubble_score'].apply(lambda x: (x/5)*100)\n\n# Output the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Modify the 'bubble_score' column\ndf['bubble_score'] = (df['bubble_score'] / 5) * 100\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Update the 'importance.score' column by adding a uniform random noise between -0.01 to 0.01 to each entry to improve privacy.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Update the 'importance.score' column by adding a uniform random noise between -0.01 to 0.01 to each entry to improve privacy.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\nimport numpy as np\n\n# load the data\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# add noise to 'importance.score' column\ndf['importance.score'] = df['importance.score'] + np.random.uniform(-0.01,0.01,df.shape[0])\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'importance.score' column by adding a uniform random noise between -0.01 to 0.01\ndf['importance.score'] = df['importance.score'].apply(lambda x: x + np.random.uniform(-0.01, 0.01))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Increase the 'charges' by 10% for all records where 'smoker' is 'yes' and 'region' is 'northeast'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Increase the 'charges' by 10% for all records where 'smoker' is 'yes' and 'region' is 'northeast'.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Apply the 10% increase conditionally \ndf.loc[(df['smoker'] == 'yes') & (df['region'] == 'northeast'),'charges'] *= 1.1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Select the rows where 'smoker' is 'yes' and 'region' is 'northeast'\ndf_to_update = df[(df['smoker'] == 'yes') & (df['region'] == 'northeast')]\n\n# Increase the 'charges' by 10% for the selected rows\ndf.loc[df_to_update.index, 'charges'] = df.loc[df_to_update.index, 'charges'] * 1.1\n\n# Print the updated DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Subtract 5 from 'bmi' for all records where 'children' is 0.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Subtract 5 from 'bmi' for all records where 'children' is 0.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Update 'bmi' by subtracting 5 where 'children' is 0\ndf.loc[df['children'] == 0,'bmi'] = df.loc[df['children'] == 0,'bmi'] - 5\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update 'bmi' where 'children' is 0\ndf.loc[df['children'] == 0, 'bmi'] = df.loc[df['children'] == 0, 'bmi'] - 5\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Increment each value in the \"HouseAge\" column by 1.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Increment each value in the \"HouseAge\" column by 1.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas DataFrame\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Add 1 to all the values in the 'HouseAge' column\ndf['HouseAge'] = df['HouseAge'] + 1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the \"HouseAge\" column\ndf['HouseAge'] = df['HouseAge'].apply(lambda x: x + 1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Increase the 'Value' column by 10% for all rows with 'LOCATION' field equal to \"AUS\".", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Increase the 'Value' column by 10% for all rows with 'LOCATION' field equal to \"AUS\".", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Identify rows where 'LOCATION' is \"AUS\" and increase values in 'Value' column by 10%\ndf.loc[df['LOCATION'] == 'AUS','Value'] *= 1.1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Select the rows where 'LOCATION' is \"AUS\"\ndf_AUS = df[df['LOCATION'] == \"AUS\"]\n\n# Increase the 'Value' column by 10% for the selected rows\ndf.loc[df_AUS.index, 'Value'] = df_AUS['Value'] * 1.1\n\n# Output the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Change column 'Computer Science' format to percentage by multiplying each value in the column by 100.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Change column 'Computer Science' format to percentage by multiplying each value in the column by 100.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Convert 'Computer Science' values to percentage\ndf['Computer Science'] = df['Computer Science'] * 100\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the 'Computer Science' column\ndf['Computer Science'] = df['Computer Science'] * 100\n\n# Output the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Update 'Health Professions' column by reducing every value by a factor of 10.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Update 'Health Professions' column by reducing every value by a factor of 10.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/47.csv')\n\n# Update 'Health Professions' column by reducing every value by a factor of 10\ndata['Health Professions'] = data['Health Professions'] / 10\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update 'Health Professions' column by reducing every value by a factor of 10\ndf['Health Professions'] = df['Health Professions'] / 10\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Deduct 0.5% from every entry in the \"Open\" column to emulate a market decline at the start of trading.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Deduct 0.5% from every entry in the \"Open\" column to emulate a market decline at the start of trading.", "instruction_type": "Update-Basic Computation", "reference_code": "import pandas as pd\n\n# Load the data from .csv file\ndata = pd.read_csv('infiagent/csv/49.csv')\n\n# Subtracting 0.5% from all values in the 'Open' column\ndata['Open'] = data['Open'] - (0.005 * data['Open'])\n\n# print the updated dataframe\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Deduct 0.5% from every entry in the \"Open\" column\ndf['Open'] = df['Open'] * (1 - 0.005)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Replace any NA values in the 'sch_arr_time' and 'sch_dep_time' columns with the corresponding values from 'act_arr_time' and 'act_dep_time' respectively, and store the results in the same columns.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Replace any NA values in the 'sch_arr_time' and 'sch_dep_time' columns with the corresponding values from 'act_arr_time' and 'act_dep_time' respectively, and store the results in the same columns.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load data from the csv file\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# replace NA values in 'sch_arr_time' with corresponding values from 'act_arr_time'\ndf['sch_arr_time'] = df['sch_arr_time'].where(df['sch_arr_time'] != 'NA',df['act_arr_time'])\n\n# replace NA values in 'sch_dep_time' with corresponding values from 'act_dep_time'\ndf['sch_dep_time'] = df['sch_dep_time'].where(df['sch_dep_time'] != 'NA',df['act_dep_time'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Replace NA values in 'sch_arr_time' with the corresponding values from 'act_arr_time'\ndata['sch_arr_time'] = pd.to_datetime(data['sch_arr_time'])\ndata['act_arr_time'] = pd.to_datetime(data['act_arr_time'])\ndata['sch_arr_time'] = data['sch_arr_time'].fillna(data['act_arr_time'])\n\n# Replace NA values in 'sch_dep_time' with the corresponding values from 'act_dep_time'\ndata['sch_dep_time'] = pd.to_datetime(data['sch_dep_time'])\ndata['act_dep_time'] = pd.to_datetime(data['act_dep_time'])\ndata['sch_dep_time'] = data['sch_dep_time'].fillna(data['act_dep_time'])\n\n# Print the updated dataframe\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Fill in any blank or absent data points in the dataset with suitable default values.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Fill in any blank or absent data points in the dataset with suitable default values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/4.csv')\n\n# Replace null or missing values with appropriate defaults\ndata = data.replace(r'^\\s*$',np.nan,regex=True)\ndata = data.fillna(0)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Replace blank or absent data points with NaN\ndata = data.replace('', np.nan)\n\n# Convert data to appropriate datatypes\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\ndata['avg. num. agents talking'] = data['avg. num. agents talking'].astype(float)\ndata['avg. wait time'] = pd.to_timedelta(data['avg. wait time'])\ndata['avg. abandonment time'] = pd.to_timedelta(data['avg. abandonment time'])\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Fill in any missing values in the 'Weight' column with the average of the existing values.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Fill in any missing values in the 'Weight' column with the average of the existing values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Compute the mean of the 'Weight' column\nmean_weight = df['Weight'].mean()\n\n# Fill missing values in the 'Weight' column with the mean\ndf['Weight'] = df['Weight'].fillna(mean_weight)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of the 'Weight' column\naverage_weight = df['Weight'].mean()\n\n# Fill missing values in the 'Weight' column with the calculated average\ndf['Weight'].fillna(average_weight, inplace=True)\n\n# Print the updated DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Fill in any missing values in the 'R_VALUE' column with the mean value of that column.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Fill in any missing values in the 'R_VALUE' column with the mean value of that column.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv(\"infiagent/csv/6.csv\")\n\n# replace missing values in 'R_VALUE' with the column's mean value\ndf['R_VALUE'].fillna(df['R_VALUE'].mean(),inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill in missing values in 'R_VALUE' column with the mean of that column\ndf['R_VALUE'].fillna(df['R_VALUE'].mean(), inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Insert missing 'MEANGBZ' values using the median of 'MEANGBZ' values.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Insert missing 'MEANGBZ' values using the median of 'MEANGBZ' values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/6.csv')\n\n# Check for missing 'MEANGBZ' values \nif data['MEANGBZ'].isnull().any():\n    # Calculate the median of 'MEANGBZ'\n    median_val = data['MEANGBZ'].median()\n    # Fill missing values in 'MEANGBZ' with the median\n    data['MEANGBZ'].fillna(median_val,inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of 'MEANGBZ' column\nmedian_val = df['MEANGBZ'].median()\n\n# Fill the missing values in 'MEANGBZ' column with the median value\ndf['MEANGBZ'] = df['MEANGBZ'].fillna(median_val)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Convert all '0.0' emotion records to a null value (NaN) to indicate no recorded emotion.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Convert all '0.0' emotion records to a null value (NaN) to indicate no recorded emotion.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the CSV data\ndata = pd.read_csv(\"infiagent/csv/9.csv\")\n\n# Convert all '0.0' emotion records to a null value\nemotion_cols = ['anger','contempt','disgust','fear','happiness','neutral','sadness','surprise','blur']\ndata[emotion_cols] = data[emotion_cols].replace(0.0,np.nan)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Replace '0.0' with np.nan to indicate no recorded emotion\ndf = df.replace(0.0, np.nan)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Replace missing values in the 'horsepower' column with its mean.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Replace missing values in the 'horsepower' column with its mean.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/11.csv')\n\n# Calculate the mean of the 'horsepower' column\nmean_horsepower = data['horsepower'].mean()\n\n# Replace missing values in the 'horsepower' column with its mean\ndata['horsepower'].fillna(mean_horsepower,inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean of 'horsepower'\nmean_horsepower = df['horsepower'].mean()\n\n# Replace missing values in 'horsepower' with its mean\ndf['horsepower'].fillna(mean_horsepower, inplace=True)\n\n# Print the resulting dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Perform a grouping operation on the table based on the 'DEPTID' column and then calculate the median 'ANNUAL_RT' for each group. For any missing 'ANNUAL_RT' values in these groups, replace them with the calculated median value.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Perform a grouping operation on the table based on the 'DEPTID' column and then calculate the median 'ANNUAL_RT' for each group. For any missing 'ANNUAL_RT' values in these groups, replace them with the calculated median value.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the data\ndata = pd.read_csv('infiagent/csv/12.csv')\n\n# calculate the median ANNUAL_RT for each DEPTID\nmedian_annual_rt = data.groupby('DEPTID')['ANNUAL_RT'].transform('median')\n\n# fill missing ANNUAL_RT with the median ANNUAL_RT of its DEPTID group\ndata['ANNUAL_RT'].fillna(median_annual_rt,inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'ANNUAL_RT' to numeric, errors='coerce' will replace non-numeric values with NaN\ndf['ANNUAL_RT'] = pd.to_numeric(df['ANNUAL_RT'], errors='coerce')\n\n# Group by 'DEPTID' and calculate the median 'ANNUAL_RT'\ngrouped = df.groupby('DEPTID')['ANNUAL_RT'].median()\n\n# Replace missing 'ANNUAL_RT' values with the median of their respective group\ndf['ANNUAL_RT'] = df['DEPTID'].map(grouped).fillna(df['ANNUAL_RT'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Fill in the missing values in the number_of_doubles column with the average value of that column.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Fill in the missing values in the number_of_doubles column with the average value of that column.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# calculate the mean of the 'number_of_doubles' column\nmean_doubles = df['number_of_doubles'].mean()\n\n# replace NaN values in 'number_of_doubles' with the column mean\ndf['number_of_doubles'].replace(np.nan,mean_doubles,inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean of the 'number_of_doubles' column\nmean_doubles = df['number_of_doubles'].mean()\n\n# Fill missing values in 'number_of_doubles' with the mean\ndf['number_of_doubles'].fillna(mean_doubles, inplace=True)\n\n# Print the updated DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Set all wages below 5 to 5.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Set all wages below 5 to 5.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Update all 'wage' values below 5 to 5\ndf.loc[df['wage'] < 5,'wage'] = 5\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Ensure all wages below 5 are set to 5\ndata['wage'] = data['wage'].apply(lambda x: 5 if x < 5 else x)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Convert all commas in the 'Volume' column to blank spaces and refresh the table.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Convert all commas in the 'Volume' column to blank spaces and refresh the table.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Replace all commas in the 'Volume' column with blank spaces and update the table.\ndf['Volume'] = df['Volume'].str.replace(',',' ')\n\nprint(df)", "assistant_code": "# Import necessary library\nimport pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Replace all the commas in the 'Volume' column with blank spaces\ndata['Volume'] = data['Volume'].str.replace(',', ' ')\n\n# Print the dataframe\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: For 'Date', convert the format from 'MMM DD, YYYY' to 'YYYY-MM-DD' and update this column with these new values.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "For 'Date', convert the format from 'MMM DD, YYYY' to 'YYYY-MM-DD' and update this column with these new values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "# Import pandas library\nimport pandas as pd\n\n# Load csv data\ndf = pd.read_csv('infiagent/csv/17.csv')\n\n# Convert and update Date column\ndf['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n\n# Print updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Format 'Date' column in format 'YYYY-MM-DD'\ndf['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Replace all missing values in the 'workclass' column with 'Unknown'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Replace all missing values in the 'workclass' column with 'Unknown'.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Check if there are missing values in workclass column\nif data['workclass'].isnull().sum() > 0:\n    # Replace missing values with 'Unknown'\n    data['workclass'].fillna('Unknown',inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Replace all missing values in the 'workclass' column with 'Unknown'\ndf['workclass'].fillna('Unknown', inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Update the table by replacing all null values in the 'areas_affected' column with 'Not available'. Only output the instruction.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Update the table by replacing all null values in the 'areas_affected' column with 'Not available'. Only output the instruction.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Replace NaN values in 'areas_affected' column with 'Not available'\ndf['areas_affected'] = df['areas_affected'].fillna('Not available')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Replace null values in 'areas_affected' column with 'Not available'\ndf['areas_affected'] = df['areas_affected'].fillna('Not available')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Replace all null values in the 'total_vaccinations_per_hundred' column with 0.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Replace all null values in the 'total_vaccinations_per_hundred' column with 0.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Fill missing values in 'total_vaccinations_per_hundred' column with 0\ndf['total_vaccinations_per_hundred'] = df['total_vaccinations_per_hundred'].fillna(0)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Replace all null values in the 'total_vaccinations_per_hundred' column with 0\ndf['total_vaccinations_per_hundred'].fillna(0, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Substitute any null entries in the 'monthly_income' column with the average 'monthly_income' value of the entire table in order to avoid data loss.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Substitute any null entries in the 'monthly_income' column with the average 'monthly_income' value of the entire table in order to avoid data loss.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the csv\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Check if there is missing value in 'monthly_income' column\nif df['monthly_income'].isnull().sum() > 0:\n    # Calculate the average 'monthly_income' of the table\n    avg_monthly_income = df['monthly_income'].dropna().mean()\n    \n    # Replace the null values with the average 'monthly_income'\n    df['monthly_income'].fillna(avg_monthly_income,inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'monthly_income'\naverage_income = df['monthly_income'].mean()\n\n# Substitute null values in 'monthly_income' with the average income\ndf['monthly_income'].fillna(average_income, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Replace all 'NA' values in 'NumberRealEstateLoansOrLines' column with a default value of 0.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Replace all 'NA' values in 'NumberRealEstateLoansOrLines' column with a default value of 0.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Replace 'NA' values in 'NumberRealEstateLoansOrLines' column with 0\ndata['NumberRealEstateLoansOrLines'] = data['NumberRealEstateLoansOrLines'].replace('NA',0)\n\n# Print modified data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Replace 'NA' values with 0 in 'NumberRealEstateLoansOrLines' column\ndf['NumberRealEstateLoansOrLines'] = df['NumberRealEstateLoansOrLines'].replace('NA', 0)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Replace missing 'WEIGHTING' values with the average 'WEIGHTING' from the dataset.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Replace missing 'WEIGHTING' values with the average 'WEIGHTING' from the dataset.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Fill missing 'WEIGHTING' values with the average\naverage_weighting = data['WEIGHTING'].mean()\ndata['WEIGHTING'].fillna(average_weighting,inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Replace missing 'WEIGHTING' values with NaN\ndata['WEIGHTING'] = pd.to_numeric(data['WEIGHTING'], errors='coerce')\n\n# Calculate the average 'WEIGHTING'\naverage_weighting = data['WEIGHTING'].mean()\n\n# Replace NaN values with the average 'WEIGHTING'\ndata['WEIGHTING'].fillna(average_weighting, inplace=True)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Replace missing 'NUM ROUNDS' values with the median of 'NUM ROUNDS' for the respective school.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Replace missing 'NUM ROUNDS' values with the median of 'NUM ROUNDS' for the respective school.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Replace missing 'NUM ROUNDS' with the median for each school\nmedian_rounds_per_school = data.groupby('School')['NUM ROUNDS'].transform('median')\ndata['NUM ROUNDS'] = data['NUM ROUNDS'].fillna(median_rounds_per_school)\n\nprint(data)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of 'NUM ROUNDS' for each school\nmedians = df['NUM ROUNDS'].median(df['School'])\n\n# Replace missing 'NUM ROUNDS' values with the median of 'NUM ROUNDS' for the respective school\ndf['NUM ROUNDS'] = df['School'].map(medians).fillna(df['NUM ROUNDS'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Ensure all 'STANDARD TEAM NAME' values have a consistent number of characters, padding with spaces if necessary.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Ensure all 'STANDARD TEAM NAME' values have a consistent number of characters, padding with spaces if necessary.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/26.csv')\n\n# Pad 'STANDARD TEAM NAME' values to have consistent length\nmax_length = data['STANDARD TEAM NAME'].str.len().max()\ndata['STANDARD TEAM NAME'] = data['STANDARD TEAM NAME'].apply(lambda x: x.ljust(max_length))\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill the NaN values in 'STANDARD TEAM NAME' column with spaces\ndf['STANDARD TEAM NAME'].fillna(' ', inplace=True)\n\n# Make sure all 'STANDARD TEAM NAME' values have the same length by padding with spaces\nmax_length = df['STANDARD TEAM NAME'].str.len().max()\ndf['STANDARD TEAM NAME'] = df['STANDARD TEAM NAME'].str.pad(max_length, side='right')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Replace all zeros in the 'X' column with the average value of the entire column.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Replace all zeros in the 'X' column with the average value of the entire column.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Calculate the mean of 'X' column excluding zero\nmean_X = df[df['X'] != 0]['X'].mean()\n\n# Replace zeros in 'X' column with the mean value\ndf['X'] = df['X'].replace(0,mean_X)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average of the 'X' column\naverage_X = df['X'].mean()\n\n# Replace the zeros in the 'X' column with the average value\ndf['X'] = df['X'].replace(0, average_X)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Replace any missing values in the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" columns with suitable placeholder or average value.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Replace any missing values in the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" columns with suitable placeholder or average value.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Identify the columns that need missing values handling\ncolumns = [\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"]\n\n# Replace missing values with suitable placeholder or average value.\nfor column in columns:\n    if df[column].dtype == 'object':\n        df[column].fillna(df[column].mode()[0],inplace=True)  # If column is categorical,replace with mode\n    else:\n        df[column].fillna(df[column].mean(),inplace=True)  # If column is numerical,replace with mean\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Check for missing values in the columns\nmissing_values = df.isnull().sum()\n\n# Replace missing values in the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" columns with their respective means\ndf['carat'].replace(np.nan, df['carat'].mean(), inplace=True)\ndf['cut'].replace(np.nan, df['cut'].mode()[0], inplace=True)\ndf['color'].replace(np.nan, df['color'].mode()[0], inplace=True)\ndf['clarity'].replace(np.nan, df['clarity'].mode()[0], inplace=True)\ndf['depth'].replace(np.nan, df['depth'].mean(), inplace=True)\ndf['table'].replace(np.nan, df['table'].mean(), inplace=True)\ndf['price'].replace(np.nan, df['price'].mean(), inplace=True)\ndf['x'].replace(np.nan, df['x'].mean(), inplace=True)\ndf['y'].replace(np.nan, df['y'].mean(), inplace=True)\ndf['z'].replace(np.nan, df['z'].mean(), inplace=True)\n\n# Print the processed DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Substitute null values in 'Open' with the table's average 'Open' value.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Substitute null values in 'Open' with the table's average 'Open' value.", "instruction_type": "Update-Data Preprocessing", "reference_code": "# import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Compute the mean of 'Open' column\nmean_open = df['Open'].mean()\n\n# Replace null values in 'Open' column with the mean\ndf['Open'].replace(np.nan,mean_open,inplace=True)\n\nprint(df)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average 'Open' value\naverage_open = df['Open'].mean()\n\n# Substitute null values in 'Open' with the average 'Open' value\ndf['Open'].fillna(average_open, inplace=True)\n\n# Print the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Ensure that there are no missing values in 'votes_gop'. If any missing values are found, replace them with the median value of the column.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Ensure that there are no missing values in 'votes_gop'. If any missing values are found, replace them with the median value of the column.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Check for missing values\nif df['votes_gop'].isnull().any():\n    # Replace missing values with median\n    df['votes_gop'].fillna(df['votes_gop'].median(),inplace=True)\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of the 'votes_gop' column\nmedian_votes_gop = df['votes_gop'].median()\n\n# Fill missing values in the 'votes_gop' column with the median value\ndf['votes_gop'].fillna(median_votes_gop, inplace=True)\n\n# Check if there are any missing values left in the 'votes_gop' column\nmissing_values = df['votes_gop'].isnull().sum()\n\nprint(missing_values)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Replace all NULL values in 'EQ' columns with the average of the respective column.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Replace all NULL values in 'EQ' columns with the average of the respective column.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Identify columns starting with 'EQ'\neq_cols = [col for col in df.columns if col.startswith('EQ')]\n\n# Fill null values in 'EQ' columns with their respective average\nfor col in eq_cols:\n    df[col] = df[col].fillna(df[col].mean())\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Identify 'EQ' columns\neq_columns = df.columns[1:]\n\n# For each 'EQ' column, replace NULL values with the column's mean\nfor column in eq_columns:\n    df[column].replace(np.nan, df[column].mean(), inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Fill missing values in 'No. of deaths' with 0 if they are NA or blank.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Fill missing values in 'No. of deaths' with 0 if they are NA or blank.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# load the data\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# replace NAN or missing values in 'No. of deaths' column with 0\ndf['No. of deaths'] = df['No. of deaths'].replace(np.nan,0)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill missing values in 'No. of deaths' column with 0\ndf['No. of deaths'] = df['No. of deaths'].replace('', 0)\n\n# Convert 'No. of deaths' to numeric type\ndf['No. of deaths'] = pd.to_numeric(df['No. of deaths'], errors='coerce')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Locate any missing entries in the 'urlToImage' column and replace them with the corresponding 'url' values.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Locate any missing entries in the 'urlToImage' column and replace them with the corresponding 'url' values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "# Required Libraries\nimport pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/33.csv')\n\n# Check for missing values and replace them\nmissing_value_indexes = data[data['urlToImage'].isna()].index\ndata.loc[missing_value_indexes,'urlToImage'] = data.loc[missing_value_indexes,'url']\n\n# Output\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check for missing values in 'urlToImage' column\nmissing_image_urls = df['urlToImage'].isnull()\n\n# Replace missing 'urlToImage' values with corresponding 'url' values\ndf.loc[missing_image_urls, 'urlToImage'] = df.loc[missing_image_urls, 'url']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Fill any missing values in the 'lifeexp' column with the average of the existing 'lifeexp' values.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Fill any missing values in the 'lifeexp' column with the average of the existing 'lifeexp' values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Loading the CSV file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Calculating the average of 'lifeexp' column\naverage_lifeexp = df['lifeexp'].mean()\n\n# Filling missing values in 'lifeexp' column with the calculated average\ndf['lifeexp'].fillna(average_lifeexp,inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean of the 'lifeexp' column\nmean_lifeexp = df['lifeexp'].mean()\n\n# Fill missing values in the 'lifeexp' column with the mean\ndf['lifeexp'].fillna(mean_lifeexp, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Fill any missing GDP per capita values on the year 2007 using the previous year GDP per capita values for the respective country.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Fill any missing GDP per capita values on the year 2007 using the previous year GDP per capita values for the respective country.", "instruction_type": "Update-Data Preprocessing", "reference_code": "# Required Libraries\nimport pandas as pd\nimport numpy as np\n\n# Load Data\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Data Preprocessing\n# Using fillna function to fill missing values in 'gdpPercap_2007' column with the previous year's value\ndf['gdpPercap_2007'] = df['gdpPercap_2007'].fillna(df['gdpPercap_2002'])\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check for missing values\nmissing_values = df.isnull().sum()\n\n# Fill the missing value in 'gdpPercap_2007' column using the 'gdpPercap_2002' column\ndf['gdpPercap_2007'].fillna(df['gdpPercap_2002'], inplace=True)\n\n# Print the output\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Replace all occurrences of -99.0 in 'Height' and 'Weight' columns to None as these are invalid entries.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Replace all occurrences of -99.0 in 'Height' and 'Weight' columns to None as these are invalid entries.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file using pandas\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# Replace -99.0 in 'Height' and 'Weight' columns with None\ndf['Height'].replace(-99.0,None,inplace=True)\ndf['Weight'].replace(-99.0,None,inplace=True)\n\nprint(df)", "assistant_code": "# Import necessary libraries\nimport pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Replace -99.0 with None in 'Height' and 'Weight' columns\ndf['Height'] = df['Height'].replace(-99.0, None)\ndf['Weight'] = df['Weight'].replace(-99.0, None)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Set missing values in the 'brand_name' column to 'None' for all rows.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Set missing values in the 'brand_name' column to 'None' for all rows.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Replace missing values in 'brand_name' with 'None'\ndf['brand_name'] = df['brand_name'].fillna('None')\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Set missing values in the 'brand_name' column to 'None' \ndf['brand_name'] = df['brand_name'].fillna('None')\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Replace all null values in column 'LibraryID' with the string \"Unknown\".", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Replace all null values in column 'LibraryID' with the string \"Unknown\".", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Replace all null values in 'LibraryID' column with \"Unknown\"\ndf['LibraryID'].fillna('Unknown',inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Replace all null values in column 'LibraryID' with the string \"Unknown\"\ndata['LibraryID'].fillna('Unknown', inplace=True)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Fill all missing values in the column \"HouseAge\" with the average of other valid values.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Fill all missing values in the column \"HouseAge\" with the average of other valid values.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/45.csv')\n\n# Calculate the mean excluding null values\nmean_house_age = data['HouseAge'].mean(skipna=True)\n\n# Fill missing values with the calculated mean\ndata['HouseAge'].fillna(mean_house_age,inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Fill missing values in the 'HouseAge' column with the mean of the column\ndf['HouseAge'].fillna(df['HouseAge'].mean(), inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Modify the column 'Flag code' to 'IsFlagged' and replace its missing values with False.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Modify the column 'Flag code' to 'IsFlagged' and replace its missing values with False.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Rename the 'Flag Codes' column to 'IsFlagged'\ndf.rename(columns={'Flag Codes': 'IsFlagged'},inplace=True)\n\n# Replace missing values in 'IsFlagged' column with False\ndf['IsFlagged'].fillna(False,inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('data.csv')\n\n# Rename the column 'Flag Codes' to 'IsFlagged'\ndata = data.rename(columns={'Flag Codes': 'IsFlagged'})\n\n# Replace the missing values in the column 'IsFlagged' with False\ndata['IsFlagged'].fillna(False, inplace=True)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: If any cells in 'Math and Statistics' contain null values, substitute them with the median of the corresponding column.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "If any cells in 'Math and Statistics' contain null values, substitute them with the median of the corresponding column.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Check if there are any null values in column 'Math and Statistics'\nif df['Math and Statistics'].isnull().any():\n\n    # Calculate the median of the 'Math and Statistics' column\n    median = df['Math and Statistics'].median()\n\n    # Replace null values with the median\n    df['Math and Statistics'].fillna(median,inplace=True)\n\n# Save the dataframe to a new csv file\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of 'Math and Statistics' column\nmedian_value = df['Math and Statistics'].median()\n\n# Substitute null values in 'Math and Statistics' with the median\ndf['Math and Statistics'].fillna(median_value, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Identify any missing values in the table and set them to 0.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Identify any missing values in the table and set them to 0.", "instruction_type": "Update-Data Preprocessing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Identify and replace missing values with 0\ndf.fillna(0,inplace=True)\n\nprint(df)", "assistant_code": "# Import necessary library\nimport pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Fill all NaN values with 0\ndf.fillna(0, inplace=True)\n\n# Print the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Display the day of the week (Monday, Tuesday, etc.) in the 'Date' column instead of the date.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Display the day of the week (Monday, Tuesday, etc.) in the 'Date' column instead of the date.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Change the format of the 'Date' column to display the day of the week\ndata['Date'] = pd.to_datetime(data['Date']).dt.day_name()\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Create a new column 'Day_of_Week' that maps the date to the day of the week\ndf['Day_of_Week'] = df['Date'].dt.day_name()\n\n# Print the dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Modify the 'start_time' and 'end_time' columns in the table to display dates in a standardized format (MM/DD/YYYY).", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Modify the 'start_time' and 'end_time' columns in the table to display dates in a standardized format (MM/DD/YYYY).", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# loading the data\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# converting 'start_time' and 'end_time' columns to datetime\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\n\n# updating the date format to (MM/DD/YYYY)\ndf['start_time'] = df['start_time'].dt.strftime('%m/%d/%Y')\ndf['end_time'] = df['end_time'].dt.strftime('%m/%d/%Y')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'start_time' and 'end_time' to datetime objects\ndf['start_time'] = pd.to_datetime(df['start_time'])\ndf['end_time'] = pd.to_datetime(df['end_time'])\n\n# Convert the datetime objects to the desired format (MM/DD/YYYY)\ndf['start_time'] = df['start_time'].dt.strftime('%m/%d/%Y')\ndf['end_time'] = df['end_time'].dt.strftime('%m/%d/%Y')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Modify the 'station_type' value to 'Freight' for rows where the 'origin_station' is 'Machine Mist' and the 'destination_station' is 'Roll Test'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"", "csv_path": "infiagent/csv/2.csv", "instruction": "Modify the 'station_type' value to 'Freight' for rows where the 'origin_station' is 'Machine Mist' and the 'destination_station' is 'Roll Test'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load csv file\ndf = pd.read_csv('infiagent/csv/2.csv')\n\n# Update 'station_type' to 'Freight' where the 'origin_station' is 'Machine Mist' and 'destination_station' is 'Roll Test'\ndf.loc[(df['origin_station'] == 'Machine Mist') & (df['destination_station'] == 'Roll Test'),'station_type'] = 'Freight'\n\n# Output the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Update 'station_type' value \ndata.loc[(data['origin_station'] == 'Machine Mist') & (data['destination_station'] == 'Roll Test'), 'station_type'] = 'Freight'\n\n# Print the updated dataframe\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Update the column 'Region' so that all regions have their first letters capitalized.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Update the column 'Region' so that all regions have their first letters capitalized.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# update the 'Region' column so that all regions have their first letters capitalized\ndf['Region'] = df['Region'].str.title()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'Region' column to capitalize the first letter of each region\ndf['Region'] = df['Region'].str.capitalize()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n\nQuestion: Format the 'Happiness Rank' column as a string field.", "csv_data_example": "Country,Region,Happiness Rank,Happiness Score,Standard Error,Economy (GDP per Capita),Family,Health (Life Expectancy),Freedom,Trust (Government Corruption),Generosity,Dystopia Residual\nSwitzerland,Western Europe,1,7.587,0.03411,1.39651,1.34951,0.94143,0.66557,0.41978,0.29678,2.51738\nIceland,Western Europe,2,7.561,0.04884,1.30232,1.40223,0.94784,0.62877,0.14145,0.4363,2.70201\nDenmark,Western Europe,3,7.527,0.03328,1.32548,1.36058,0.87464,0.64938,0.48357,0.34139,2.49204\nNorway,Western Europe,4,7.522,0.0388,1.459,1.33095,0.88521,0.66973,0.36503,0.34699,2.46531\nCanada,North America,5,7.427,0.03553,1.32629,1.32261,0.90563,0.63297,0.32957,0.45811,2.45176\nFinland,Western Europe,6,7.406,0.0314,1.29025,1.31826,0.88911,0.64169,0.41372,0.23351,2.61955\nNetherlands,Western Europe,7,7.378,0.02799,1.32944,1.28017,0.89284,0.61576,0.31814,0.4761,2.4657\nSweden,Western Europe,8,7.364,0.03157,1.33171,1.28907,0.91087,0.6598,0.43844,0.36262,2.37119\nNew Zealand,Australia and New Zealand,9,7.286,0.03371,1.25018,1.31967,0.90837,0.63938,0.42922,0.47501,2.26425\nAustralia,Australia and New Zealand,10,7.284,0.04083,1.33358,1.30923,0.93156,0.65124,0.35637,0.43562,2.26646\n", "csv_path": "infiagent/csv/3.csv", "instruction": "Format the 'Happiness Rank' column as a string field.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/3.csv')\n\n# Modify the 'Happiness Rank' column to be a string field\ndf['Happiness Rank'] = df['Happiness Rank'].astype(str)\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Format the 'Happiness Rank' column as a string field\ndf['Happiness Rank'] = df['Happiness Rank'].astype(str)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Change the 'Type' to 'Indirected' for rows where 'Source' is 0 and 'Weight' is less than 100.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Change the 'Type' to 'Indirected' for rows where 'Source' is 0 and 'Weight' is less than 100.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/5.csv')\n\n# Updating the 'Type' where 'Source' is 0 and 'Weight' is less than 100\ndf.loc[(df['Source'] == 0) & (df['Weight'] < 100),'Type'] = 'Indirected'\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'Type' column where 'Source' is 0 and 'Weight' is less than 100\ndf.loc[(df['Source'] == 0) & (df['Weight'] < 100), 'Type'] = 'Indirected'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Modify the presentation of 'AREA_ACR' values to display two decimal points.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Modify the presentation of 'AREA_ACR' values to display two decimal points.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the DataFrame from CSV\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Decimal formatting\ndf['AREA_ACR'] = df['AREA_ACR'].map(\"{:,.2f}\".format)\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "# Import the pandas library\nimport pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Modify the 'AREA_ACR' column to display two decimal points\ndf['AREA_ACR'] = df['AREA_ACR'].round(2)\n\n# Print the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: For rows where 'R_VALUE' is greater than 1, replace 'R_VALUE' with 1.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "For rows where 'R_VALUE' is greater than 1, replace 'R_VALUE' with 1.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Update the 'R_VALUE' column as per the given condition \ndf.loc[df['R_VALUE'] > 1,'R_VALUE'] = 1\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# For rows where 'R_VALUE' is greater than 1, replace 'R_VALUE' with 1\ndf.loc[df['R_VALUE'] > 1, 'R_VALUE'] = 1\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n\nQuestion: Group the table by 'AREA_ACR' and calculate the median 'R_VALUE' for each group. Then, replace the individual 'R_VALUE' in each row with the corresponding group's median 'R_VALUE'.", "csv_data_example": "TRUE_TIME,TIME,USFLUX,MEANGAM,MEANGBT,MEANGBZ,MEANGBH,MEANJZD,TOTUSJZ,MEANJZH,TOTUSJH,ABSNJZH,SAVNCPP,MEANPOT,TOTPOT,MEANSHR,SHRGT45,R_VALUE,AREA_ACR\n2014.03.23_20:24:00_TAI,11.6,3.246502e+21,21.786,93.013,92.809,31.210,0.08746085,3.141588e+12,0.00286312,143.341,14.092,2.248874e+11,1.185247e+03,7.747525e+21,18.695,0.061,0.000,69.264130\n2014.03.23_20:36:00_TAI,11.8,3.908340e+21,21.740,89.953,89.779,31.535,0.15138598,3.745627e+12,0.00309745,173.704,18.216,4.651086e+11,1.155593e+03,9.025444e+21,18.172,0.000,0.000,83.896141\n2014.03.23_20:48:00_TAI,12.0,4.041844e+21,21.797,89.552,89.566,30.425,0.13912600,3.790352e+12,0.00293124,174.009,18.001,4.464203e+11,1.132300e+03,9.235995e+21,18.322,0.016,0.000,86.314224\n2014.03.23_21:00:00_TAI,12.2,4.096817e+21,21.654,89.355,89.499,30.440,0.23451930,3.604093e+12,0.00307095,164.412,19.141,7.636783e+11,1.100275e+03,9.107749e+21,18.134,0.048,0.000,87.762978\n2014.03.23_21:12:00_TAI,12.4,4.197154e+21,21.732,87.089,87.454,29.875,0.26665705,3.622492e+12,0.00341966,163.141,22.204,9.045709e+11,1.032512e+03,8.903345e+21,17.850,0.046,0.000,84.621979\n2014.03.23_21:36:00_TAI,12.8,4.271472e+21,21.737,86.248,86.595,29.165,0.18933275,3.534322e+12,0.00270168,157.135,18.042,6.605973e+11,9.804160e+02,8.694996e+21,17.608,0.045,0.000,93.034233\n2014.03.23_21:48:00_TAI,13.0,4.596958e+21,21.888,84.835,85.451,27.800,0.27554792,3.809157e+12,0.00314102,166.084,22.942,1.051558e+12,9.431147e+02,9.148247e+21,17.502,0.014,0.000,96.688416\n2014.03.23_22:00:00_TAI,13.2,4.684366e+21,22.237,84.526,85.236,27.636,0.00457878,3.761321e+12,0.00170337,162.369,12.963,1.820630e+10,9.062604e+02,9.159047e+21,17.662,0.026,2.070,96.260063\n2014.03.23_22:12:00_TAI,13.4,4.664280e+21,22.024,84.563,85.487,27.331,0.17288232,3.733276e+12,0.00244001,161.954,18.495,6.847288e+11,8.919001e+02,8.978380e+21,17.638,0.013,0.000,97.720245\n2014.03.23_22:24:00_TAI,13.6,4.671479e+21,22.004,86.069,87.034,27.117,0.17619905,3.714417e+12,0.00241245,161.277,18.303,6.985293e+11,8.809678e+02,8.876519e+21,17.485,0.013,0.000,97.474724\n", "csv_path": "infiagent/csv/6.csv", "instruction": "Group the table by 'AREA_ACR' and calculate the median 'R_VALUE' for each group. Then, replace the individual 'R_VALUE' in each row with the corresponding group's median 'R_VALUE'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load data\ndf = pd.read_csv('infiagent/csv/6.csv')\n\n# Group by 'AREA_ACR' and calculate the median 'R_VALUE' for each group\nmedians = df.groupby('AREA_ACR')['R_VALUE'].median()\n\n# Apply the group median 'R_VALUE' to each row\ndf['R_VALUE'] = df['AREA_ACR'].map(medians)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# group by 'AREA_ACR' and calculate the median 'R_VALUE'\ngrouped = df.groupby('AREA_ACR')['R_VALUE'].median()\n\n# replace individual 'R_VALUE' with corresponding group's median 'R_VALUE'\ndf['R_VALUE'] = df['AREA_ACR'].map(grouped)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Set EVENTNUM to 0 for all records where the score is '0 - 2'.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Set EVENTNUM to 0 for all records where the score is '0 - 2'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/7.csv')\n\n# Set EVENTNUM to 0 where the SCORE is '0 - 2'\ndata.loc[data['SCORE'] == '0 - 2','EVENTNUM'] = 0\n\n# Output the result\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Find the rows where the score is '0 - 2' and update the EVENTNUM to 0\ndf.loc[df['SCORE'] == '0 - 2', 'EVENTNUM'] = 0\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Calculate the average SCOREMARGIN across all records and use that value to replace the SCOREMARGIN in the current record.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Calculate the average SCOREMARGIN across all records and use that value to replace the SCOREMARGIN in the current record.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Handling non-standard data\ndf['SCOREMARGIN'] = pd.to_numeric(df['SCOREMARGIN'],errors='coerce')\n\n# Calculate the average SCOREMARGIN\naverage_scoremargin = df['SCOREMARGIN'].mean()\n\n# Replace SCOREMARGIN with the average SCOREMARGIN\ndf['SCOREMARGIN'] = average_scoremargin\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# calculate the average score margin\naverage_score_margin = df['SCOREMARGIN'].mean()\n\n# replace the score margin in the table with the average score margin\ndf['SCOREMARGIN'] = average_score_margin\n\n# print the updated table\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Convert the 'WCTIMESTRING' column from string format to datetime format.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Convert the 'WCTIMESTRING' column from string format to datetime format.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\nfrom datetime import datetime\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Convert 'WCTIMESTRING' column to datetime format\ndf['WCTIMESTRING'] = pd.to_datetime(df['WCTIMESTRING'])\n\nprint(df)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# convert 'WCTIMESTRING' column to datetime format\ndf['WCTIMESTRING'] = pd.to_datetime(df['WCTIMESTRING'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nGAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n\nQuestion: Set the SCORE of the game to the highest score achieved in the same PERIOD.", "csv_data_example": "GAME_ID,EVENTNUM,EVENTMSGTYPE,EVENTMSGACTIONTYPE,PERIOD,WCTIMESTRING,PCTIMESTRING,HOMEDESCRIPTION,NEUTRALDESCRIPTION,VISITORDESCRIPTION,SCORE,SCOREMARGIN\n0020200722,0,12,0,1,7:13 PM,12:00,,,,,\n0020200722,1,10,0,1,7:14 PM,12:00,Jump Ball Ilgauskas vs. Garnett: Tip to Palacio,,,,\n0020200722,2,5,1,1,7:15 PM,11:46,Ilgauskas Bad Pass Turnover (P1.T1),,Peeler STEAL (1 STL),,\n0020200722,3,5,2,1,7:16 PM,11:43,,,Peeler Lost Ball Turnover (P1.T1),,\n0020200722,4,2,5,1,7:16 PM,11:22,MISS Ilgauskas  Layup,,Nesterovic BLOCK (1 BLK),,\n0020200722,5,4,0,1,7:16 PM,11:20,Davis REBOUND (Off:1 Def:0),,,,\n0020200722,6,2,1,1,7:16 PM,11:16,MISS Davis 21' Jump Shot,,,,\n0020200722,7,4,0,1,7:16 PM,11:14,,,Garnett REBOUND (Off:0 Def:1),,\n0020200722,8,5,2,1,7:16 PM,10:57,Boozer STEAL (1 STL),,Garnett Lost Ball Turnover (P1.T2),,\n0020200722,9,1,5,1,7:16 PM,10:52,Palacio 1' Layup (2 PTS) (Ilgauskas 1 AST),,,0 - 2,2\n", "csv_path": "infiagent/csv/7.csv", "instruction": "Set the SCORE of the game to the highest score achieved in the same PERIOD.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/7.csv')\n\n# Convert SCORE column into numerical\ndf['SCORE'] = pd.to_numeric(df['SCORE'],errors='coerce')\n\n# Grouping data by PERIOD column and finding maximum score\ngrouped_df = df.groupby('PERIOD')['SCORE'].max().reset_index()\n\n# Merging the grouped_df with original df to update the SCORE column\ndf = df.merge(grouped_df,on='PERIOD',suffixes=('','_max'))\n\n# Replacing original SCORE column with the maximum score\ndf['SCORE'] = df['SCORE_max']\ndf.drop('SCORE_max',axis = 1,inplace=True)\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the SCORE column to integer\ndf['SCORE'] = df['SCORE'].str.extract('(\\d+)', expand=False).astype(int)\n\n# Get the maximum score in the same period\nmax_score = df.groupby('PERIOD')['SCORE'].max()\n\n# Set the maximum score to the corresponding games in the same period\ndf.loc[df['PERIOD'].isin(max_score.index), 'SCORE'] = max_score\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Change the sex values for records with less than 10 rings. Change 'M' to 'Y' and 'F' to 'X'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Change the sex values for records with less than 10 rings. Change 'M' to 'Y' and 'F' to 'X'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv data into a dataframe\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# find the records where Rings is less than 10,and update the Sex value accordingly\ndf.loc[(df['Rings'] < 10) & (df['Sex'] == 'M'),'Sex'] = 'Y'\ndf.loc[(df['Rings'] < 10) & (df['Sex'] == 'F'),'Sex'] = 'X'\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Replace 'M' with 'Y' and 'F' with 'X' where the number of rings is less than 10\ndf.loc[df['Rings'] < 10, 'Sex'] = df.loc[df['Rings'] < 10, 'Sex'].replace({'M': 'Y', 'F': 'X'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Update columns where the average weight is lower than the overall table average, set 'Sex' to 'U'.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Update columns where the average weight is lower than the overall table average, set 'Sex' to 'U'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Calculate the total average weight of the table\ntotal_avg_weight = df['Whole weight'].mean()\n\n# Using loc function to update the records where the 'Whole weight' is less than 'total_avg_weight'\ndf.loc[df['Whole weight'] < total_avg_weight,'Sex'] = 'U'\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall average weight\naverage_weight = df['Shucked weight'].mean()\n\n# Identify the rows where average weight is lower than the overall table average\nlower_weight_rows = df.groupby('Sex')['Shucked weight'].transform('mean') < average_weight\n\n# Update 'Sex' to 'U' in the identified rows\ndf.loc[lower_weight_rows, 'Sex'] = 'U'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Make all values in the 'Sex' column uppercase.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Make all values in the 'Sex' column uppercase.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Convert the values in the 'Sex' column to uppercase\ndf['Sex'] = df['Sex'].str.upper()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Make all values in the 'Sex' column uppercase\ndf['Sex'] = df['Sex'].str.upper()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n\nQuestion: Calculate the average height for each group based on 'Sex' and update the 'Height' column accordingly.", "csv_data_example": "Sex,Length,Diameter,Height,Whole weight,Shucked weight,Viscera weight,Shell weight,Rings\nM,0.455,0.365,0.095,0.514,0.2245,0.101,0.15,15\nM,0.35,0.265,0.09,0.2255,0.0995,0.0485,0.07,7\nF,0.53,0.42,0.135,0.677,0.2565,0.1415,0.21,9\nM,0.44,0.365,0.125,0.516,0.2155,0.114,0.155,10\nI,0.33,0.255,0.08,0.205,0.0895,0.0395,0.055,7\nI,0.425,0.3,0.095,0.3515,0.141,0.0775,0.12,8\nF,0.53,0.415,0.15,0.7775,0.237,0.1415,0.33,20\nF,0.545,0.425,0.125,0.768,0.294,0.1495,0.26,16\nM,0.475,0.37,0.125,0.5095,0.2165,0.1125,0.165,9\nF,0.55,0.44,0.15,0.8945,0.3145,0.151,0.32,19\n", "csv_path": "infiagent/csv/8.csv", "instruction": "Calculate the average height for each group based on 'Sex' and update the 'Height' column accordingly.", "instruction_type": "Update-Data Processing", "reference_code": "# Importing required libraries\nimport pandas as pd\n\n# Loading the data from the csv file\ndf = pd.read_csv('infiagent/csv/8.csv')\n\n# Calculating the average height for each 'Sex' group\naverage_height = df.groupby('Sex')['Height'].transform('mean')\n\n# Updating the 'Height' column with the average heights\ndf['Height'] = average_height\n\n# Printing the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the average height for each group based on 'Sex'\naverage_height = df.groupby('Sex')['Height'].mean()\n\n# Update the 'Height' column with the average height of the corresponding 'Sex' group\ndf['Height'] = df['Sex'].map(average_height)\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: In the 'name' column, update 'alvaro' to 'alvaro_dias' to indicate the full name of the person.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "In the 'name' column, update 'alvaro' to 'alvaro_dias' to indicate the full name of the person.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# Rename 'alvaro' to 'alvaro_dias' in the 'name' column\ndf['name'] = df['name'].replace('alvaro','alvaro_dias')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update 'alvaro' to 'alvaro_dias' in the 'name' column\ndf.loc[df['name'] == 'alvaro', 'name'] = 'alvaro_dias'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Modify the presentation of the 'name' column by converting it to uppercase for enhanced visibility.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Modify the presentation of the 'name' column by converting it to uppercase for enhanced visibility.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/9.csv')\n\n# change the 'name' field to uppercase\ndf['name'] = df['name'].str.upper()\n\nprint(df)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# convert 'name' column to uppercase\ndf['name'] = df['name'].str.upper()\n\n# print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nindex,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n\nQuestion: Find the maximum value in the 'happiness' column for rows where the 'neutral' value is greater than 0.9 and set it as the 'happiness' value for all those rows.", "csv_data_example": "index,name,timestamp,anger,contempt,disgust,fear,happiness,neutral,sadness,surprise,blur\n0,alckmin,00h-00m-00s,0.0,0.001,0.0,0.0,0.0,0.981,0.018,0.0,0.0\n12845,boulos,00h-00m-00s,0.001,0.001,0.0,0.0,0.001,0.996,0.002,0.0,0.05\n19125,ciro,00h-00m-00s,0.003,0.01,0.0,0.0,0.0,0.942,0.046,0.0,0.3\n25592,marina,00h-00m-00s,0.0,0.002,0.003,0.0,0.126,0.626,0.243,0.0,0.0\n32061,meirelles,00h-00m-00s,0.111,0.003,0.0,0.0,0.0,0.884,0.0,0.001,0.0\n6397,alvaro,00h-00m-00s,0.0,0.003,0.0,0.0,0.145,0.852,0.0,0.0,0.31\n25593,marina,00h-00m-01s,0.0,0.005,0.0,0.0,0.432,0.563,0.0,0.0,0.0\n19126,ciro,00h-00m-01s,0.002,0.016,0.0,0.0,0.0,0.939,0.043,0.0,0.18\n6398,alvaro,00h-00m-01s,0.0,0.0,0.0,0.0,0.694,0.303,0.001,0.001,0.89\n12846,boulos,00h-00m-01s,0.0,0.0,0.0,0.0,0.0,0.998,0.001,0.0,0.07\n", "csv_path": "infiagent/csv/9.csv", "instruction": "Find the maximum value in the 'happiness' column for rows where the 'neutral' value is greater than 0.9 and set it as the 'happiness' value for all those rows.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/9.csv')\n\n# Set the 'happiness' value to the maximum value in the 'happiness' column for all rows where the 'neutral' value is greater than 0.9\ndata.loc[data['neutral'] > 0.9,'happiness'] = data['happiness'].max()\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Find the maximum 'happiness' value where 'neutral' is greater than 0.9\nmax_happiness = df.loc[df['neutral'] > 0.9, 'happiness'].max()\n\n# Set this value as the 'happiness' value for all rows where 'neutral' is greater than 0.9\ndf.loc[df['neutral'] > 0.9, 'happiness'] = max_happiness\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Check the 'Offense' column and replace any slashes('/') in the text with dashes('-').", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Check the 'Offense' column and replace any slashes('/') in the text with dashes('-').", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/10.csv')\n\n# Replace slashes in the 'Offense' column with dashes\ndata['Offense'] = data['Offense'].str.replace('/','-')\n\n# Print the updated DataFrame\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Replace the '/' with '-' in the 'Offense' column\ndf['Offense'] = df['Offense'].str.replace('/', '-')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Set the 'Expungible' status of all offenses to false if the count exceeds the average count.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Set the 'Expungible' status of all offenses to false if the count exceeds the average count.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Calculate the average count\naverage_count = df['Count'].mean()\n\n# Update the 'Expungible' status of all crimes\ndf.loc[df['Count'] > average_count,'Expungible'] = False\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average count\naverage_count = df['Count'].mean()\n\n# Update 'Expungible' status\ndf.loc[df['Count'] > average_count, 'Expungible'] = False\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Modify the 'origin' attribute of all vehicles with a 'weight' greater than 4000 to 2.", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Modify the 'origin' attribute of all vehicles with a 'weight' greater than 4000 to 2.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndata = pd.read_csv('infiagent/csv/11.csv')\n\n# Update the 'origin' \ndata.loc[data['weight'] > 4000,'origin'] = 2\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Modify the 'origin' attribute of all vehicles with a 'weight' greater than 4000 to 2\ndf.loc[df['weight'] > 4000, 'origin'] = 2\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Normalize the 'DESCR' column by converting all the text to lowercase and removing any extra spaces.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Normalize the 'DESCR' column by converting all the text to lowercase and removing any extra spaces.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# convert the 'DESCR' column to lowercase\ndf['DESCR'] = df['DESCR'].str.lower()\n\n# remove any extra spaces in 'DESCR' column\ndf['DESCR'] = df['DESCR'].str.strip()\n\n# print updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Normalize the 'DESCR' column\ndf['DESCR'] = df['DESCR'].str.lower().str.strip()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nNAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n\nQuestion: Replace all instances of 'AIDE BLUE CHIP' in the 'JOBTITLE' column with 'BLUE CHIP AIDE'.", "csv_data_example": "NAME,JOBTITLE,DEPTID,DESCR,HIRE_DT,ANNUAL_RT,Gross\n\"AKROFI,BERNARD\",AUDITOR TRAINEE,A24002,COMP-Audits (002),02/04/2013,37407.00,14387.30\n\"Aaron,Keairah T\",AIDE BLUE CHIP,W02278,Youth Summer  (278),06/19/2013,11310.00,\n\"Aaron,Keontae E\",AIDE BLUE CHIP,W02200,Youth Summer  (200),06/10/2013,11310.00,\n\"Aaron,Patricia G\",Facilities/Office Services II,A03031,OED-Employment Dev (031),10/24/1979,51862.00,51771.01\n\"Aaron,Petra L\",ASSISTANT STATE'S ATTORNEY,A29005,States Attorneys Office (005),09/25/2006,64000.00,63909.03\n\"Abaineh,Yohannes T\",EPIDEMIOLOGIST,A65026,HLTH-Health Department (026),07/23/2009,57900.00,57428.85\n\"Abbey,Emmanuel\",CONTRACT SERV SPEC II,A40001,M-R Info Technology (001),05/01/2013,52000.00,3477.00\n\"Abdal-Rahim,Naim A\",EMT FIREFIGHTER,A64120,Fire Department (120),03/30/2011,40650.00,44159.59\n\"Abdi,Ezekiel W\",POLICE SERGEANT,A99123,Police Department (123),06/14/2007,68847.00,66496.24\n\"Abdul Adl,Attrice A\",RADIO DISPATCHER SHERIFF,A38410,Sheriff's Office (410),09/02/1999,41194.00,46254.02\n", "csv_path": "infiagent/csv/12.csv", "instruction": "Replace all instances of 'AIDE BLUE CHIP' in the 'JOBTITLE' column with 'BLUE CHIP AIDE'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/12.csv')\n\n# Replace 'AIDE BLUE CHIP' with 'BLUE CHIP AIDE' in 'JOBTITLE' column\ndf['JOBTITLE'] = df['JOBTITLE'].replace('AIDE BLUE CHIP','BLUE CHIP AIDE')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Replace 'AIDE BLUE CHIP' with 'BLUE CHIP AIDE' in 'JOBTITLE' column\ndf['JOBTITLE'] = df['JOBTITLE'].replace('AIDE BLUE CHIP', 'BLUE CHIP AIDE')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Format the \"DATE TIME\" field in the YYYY-MM-DD HH:MM format.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Format the \"DATE TIME\" field in the YYYY-MM-DD HH:MM format.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the dataset from specific path\ndf = pd.read_csv('infiagent/csv/13.csv')\n\n# Change the format of 'DATE TIME' to 'YYYY-MM-DD HH:MM' and update the column\ndf['DATE TIME'] = pd.to_datetime(df['DATE TIME'],format='%m/%d/%Y %H:%M').dt.strftime('%Y-%m-%d %H:%M')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'DATE TIME' column to datetime\ndf['DATE TIME'] = pd.to_datetime(df['DATE TIME'])\n\n# Format 'DATE TIME' column in YYYY-MM-DD HH:MM format\ndf['DATE TIME'] = df['DATE TIME'].dt.strftime('%Y-%m-%d %H:%M')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Format the 'DATE TIME' column to show dates in the format of 'dd-mm-yyyy hh:mm'.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Format the 'DATE TIME' column to show dates in the format of 'dd-mm-yyyy hh:mm'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the provided csv file\ndata = pd.read_csv('infiagent/csv/13.csv')\n\n# Format the 'DATE TIME' column\ndata['DATE TIME'] = pd.to_datetime(data['DATE TIME']).dt.strftime('%d-%m-%Y %H:%M')\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'DATE TIME' column to datetime\ndf['DATE TIME'] = pd.to_datetime(df['DATE TIME'])\n\n# Format the 'DATE TIME' column to 'dd-mm-yyyy hh:mm'\ndf['DATE TIME'] = df['DATE TIME'].dt.strftime('%d-%m-%Y %H:%M')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Modify all 'wage' and 'lwage' values to integer format.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Modify all 'wage' and 'lwage' values to integer format.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data \ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Convert 'wage' and 'lwage' to integers\ndf['wage'] = df['wage'].astype(int)\ndf['lwage'] = df['lwage'].astype(int)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Modify 'wage' and 'lwage' to integer format\ndf['wage'] = df['wage'].round().astype(int)\ndf['lwage'] = df['lwage'].round().astype(int)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Update the 'lwage' column to be the natural logarithm of the 'wage' column.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Update the 'lwage' column to be the natural logarithm of the 'wage' column.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/15.csv')\n\n# Update lwage column to be the natural logarithm of wage\ndf['lwage'] = np.log(df['wage'])\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Compute natural logarithm of 'wage' column\ndf['lwage'] = np.log(df['wage'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Alter the format of the 'dteday' column from 'yyyy-mm-dd' to 'dd-mm-yyyy' for improved readability.", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Alter the format of the 'dteday' column from 'yyyy-mm-dd' to 'dd-mm-yyyy' for improved readability.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Convert 'dteday' to datetime object\ndf['dteday'] = pd.to_datetime(df['dteday'])\n\n# Change the 'dteday' column format from 'yyyy-mm-dd' to 'dd-mm-yyyy'\ndf['dteday'] = df['dteday'].dt.strftime('%d-%m-%Y')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'dteday' column to datetime\ndf['dteday'] = pd.to_datetime(df['dteday'])\n\n# Alter the format of 'dteday' column to 'dd-mm-yyyy'\ndf['dteday'] = df['dteday'].dt.strftime('%d-%m-%Y')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Transform 'Market Cap' into an integer format while eliminating all non-numeric characters.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Transform 'Market Cap' into an integer format while eliminating all non-numeric characters.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/17.csv')\n\n# Convert 'Market Cap' to an integer format,replacing any non-numeric characters with empty space\ndata['Market Cap'] = data['Market Cap'].str.replace(',','').astype(int)\n\nprint(data)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# removing non-numeric characters from 'Market Cap' and converting it into integer format\ndf['Market Cap'] = df['Market Cap'].str.replace(',', '').astype(int)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: For all entries where 'Market Cap' is less than 800,000,000, set 'Close' to 0.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "For all entries where 'Market Cap' is less than 800,000,000, set 'Close' to 0.", "instruction_type": "Update-Data Processing", "reference_code": "# Required Libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/17.csv')\ndf['Market Cap'] = df['Market Cap'].replace(',','',regex=True).astype(float)\n\n# Update 'Close' where condition meets\ndf.loc[df['Market Cap'] < 800000000,'Close'] = 0\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'Market Cap' to numeric\ndf['Market Cap'] = df['Market Cap'].str.replace(',', '').astype(int)\n\n# Find all entries where 'Market Cap' is less than 800,000,000\ndf.loc[df['Market Cap'] < 800000000, 'Close'] = 0\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: For the 'relationship' column, change the value 'Not-in-family' to 'Single'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "For the 'relationship' column, change the value 'Not-in-family' to 'Single'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load csv data into a DataFrame\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Updating 'relationship' column as per the instructions\ndf['relationship'] = df['relationship'].replace('Not-in-family','Single')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Replace the value 'Not-in-family' with 'Single' in the 'relationship' column\ndata['relationship'] = data['relationship'].replace('Not-in-family', 'Single')\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: In the 'race' column, replace 'White' with 'Caucasian' and 'Black' with 'African American'.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "In the 'race' column, replace 'White' with 'Caucasian' and 'Black' with 'African American'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/18.csv')\n\n# Update 'White' to 'Caucasian' and 'Black' to 'African American' in 'race' column\ndf['race'] = df['race'].replace(['White','Black'],['Caucasian','African American'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# replace the values in the 'race' column\ndf['race'] = df['race'].replace({'White': 'Caucasian', 'Black': 'African American'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n\nQuestion: Fill in the missing 'occupation' column with the occupation that appears most often.", "csv_data_example": "age,workclass,final-weight,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loos,hour-per-week,native-country,income\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, <=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, <=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, <=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, <=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, <=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, <=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, >50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, >50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, >50K\n", "csv_path": "infiagent/csv/18.csv", "instruction": "Fill in the missing 'occupation' column with the occupation that appears most often.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/18.csv')\n\n# Replace missing values in the 'occupation' column with the most frequent occupation value\nmost_frequent_occupation = data['occupation'].mode()[0]\ndata['occupation'].fillna(most_frequent_occupation,inplace=True)\n\n# Output the results\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Find the most common occupation\nmost_common_occupation = df['occupation'].mode()[0]\n\n# Fill the missing values in the 'occupation' column with the most common occupation\ndf['occupation'].fillna(most_common_occupation, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Update values in the 'dept_group' column to 'Undocumented' if they are NULL or blank.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Update values in the 'dept_group' column to 'Undocumented' if they are NULL or blank.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Replace any NULL or blank values in 'dept_group' column to 'Undocumented'\ndf['dept_group'].fillna('Undocumented',inplace=True)\ndf['dept_group'].replace('','Undocumented',inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update values in the 'dept_group' column to 'Undocumented' where they are NULL or blank\ndf['dept_group'] = df['dept_group'].fillna('Undocumented')\ndf.loc[df['dept_group'] == '', 'dept_group'] = 'Undocumented'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Increase the 'coa_dept_id' of all departments in the 'Community Services' dept_group by 10%.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Increase the 'coa_dept_id' of all departments in the 'Community Services' dept_group by 10%.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load data from csv\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# select rows where 'dept_group' is 'Community Services'\ncommunity_services_df = df[df['dept_group'] == 'Community Services']\n\n# increase 'coa_dept_id' by 10%\ncommunity_services_df['coa_dept_id'] = community_services_df['coa_dept_id'] * 1.1\n\n# update original dataframe with modified values\ndf.update(community_services_df)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Identify the rows in 'Community Services' dept_group\ncommunity_services_df = df[df['dept_group'] == 'Community Services']\n\n# Increase 'coa_dept_id' by 10% for the identified rows\ncommunity_services_df['coa_dept_id'] = community_services_df['coa_dept_id'] * 1.1\n\n# Update the original dataframe with the new values\ndf.update(community_services_df)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Convert the 'budget_year_start' and 'budget_year_end' column formats to yyyy-mm-dd.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Convert the 'budget_year_start' and 'budget_year_end' column formats to yyyy-mm-dd.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load data from csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Convert 'budget_year_start' and 'budget_year_end' to datetime format\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\ndf['budget_year_end'] = pd.to_datetime(df['budget_year_end'])\n\n# Output the processed dataframe\nprint(df)", "assistant_code": "# Importing necessary libraries\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert 'budget_year_start' and 'budget_year_end' to datetime type\ndf['budget_year_start'] = pd.to_datetime(df['budget_year_start'])\ndf['budget_year_end'] = pd.to_datetime(df['budget_year_end'])\n\n# Format 'budget_year_start' and 'budget_year_end' in 'yyyy-mm-dd' format\ndf['budget_year_start'] = df['budget_year_start'].dt.strftime('%Y-%m-%d')\ndf['budget_year_end'] = df['budget_year_end'].dt.strftime('%Y-%m-%d')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Modify the value of 'github-dept-code' column to 'ENG' for rows where 'Department Name' is 'Austin Energy'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Modify the value of 'github-dept-code' column to 'ENG' for rows where 'Department Name' is 'Austin Energy'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Find the row where 'Department Name' equals 'Austin Energy' and update 'github-dept-code' to 'ENG'\ndf.loc[df['Department Name'] == 'Austin Energy','github-dept-code'] = 'ENG'\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the value of 'github-dept-code' column to 'ENG' for rows where 'Department Name' is 'Austin Energy'\ndf.loc[df['Department Name'] == 'Austin Energy', 'github-dept-code'] = 'ENG'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Update the 'dept_group' column to 'Services' for rows where the 'department name' is 'Animal Services' or 'Austin Code'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Update the 'dept_group' column to 'Services' for rows where the 'department name' is 'Animal Services' or 'Austin Code'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv data\ndata = pd.read_csv('infiagent/csv/19.csv')\n\n# update 'dept_group' to 'Services' where the 'department name' is 'Animal Services' or 'Austin Code'\ndata.loc[data['Department Name'].isin(['Animal Services','Austin Code']),'dept_group'] = 'Services'\n\n# print the updated data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'dept_group' column to 'Services' for rows where 'Department Name' is 'Animal Services' or 'Austin Code'\ndf.loc[df['Department Name'].isin(['Animal Services', 'Austin Code']), 'dept_group'] = 'Services'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Transform the 'State' column into uppercase to ensure consistency in the format of all data within that column.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Transform the 'State' column into uppercase to ensure consistency in the format of all data within that column.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/20.csv')\n\n# Convert the 'State' column to uppercase\ndf['State'] = df['State'].str.upper()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Transform the 'State' column into uppercase\ndf['State'] = df['State'].str.upper()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Convert the 'Number of Readmissions' column to float.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Convert the 'Number of Readmissions' column to float.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata_path = \"infiagent/csv/20.csv\"\ndf = pd.read_csv(data_path)\n\n# Directly modify the data type of the 'Number of Readmissions' column\ndf['Number of Readmissions'] = df['Number of Readmissions'].astype(float)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Convert 'Number of Readmissions' to float\ndf['Number of Readmissions'] = df['Number of Readmissions'].astype(float)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Find the storm with the highest 'damage_USD' and update its 'damage_imputed' value to 1.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Find the storm with the highest 'damage_USD' and update its 'damage_imputed' value to 1.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Identify the storm with the highest 'damage_USD'\nmax_damage_storm = df[df['damage_USD'] == df['damage_USD'].max()]\n\n# Update 'damage_imputed' value to 1 for the storm with the highest 'damage_USD'\ndf.loc[df['name'] == max_damage_storm['name'].values[0],'damage_imputed'] = 1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Find the storm with the highest 'damage_USD' \nmax_damage_index = df['damage_USD'].idxmax()\n\n# Update 'damage_imputed' value for that storm\ndf.at[max_damage_index, 'damage_imputed'] = 1\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Convert 'damage_USD' from float to integer type.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Convert 'damage_USD' from float to integer type.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Convert 'damage_USD' from float to integer type\ndf['damage_USD'] = df['damage_USD'].astype(int)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# convert 'damage_USD' from float to integer\ndf['damage_USD'] = df['damage_USD'].round().astype(int)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Adjust the 'total_vaccinations_per_hundred' column to display the percentage by multiplying the current values by 100.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Adjust the 'total_vaccinations_per_hundred' column to display the percentage by multiplying the current values by 100.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# handle non-standard data\ndf = df.replace({',': '.'},regex=True)\ndf['total_vaccinations_per_hundred'] = df['total_vaccinations_per_hundred'].astype(float)\n\n# update 'total_vaccinations_per_hundred' column\ndf['total_vaccinations_per_hundred'] = df['total_vaccinations_per_hundred'] * 100\n\n# output the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Adjust the 'total_vaccinations_per_hundred' column\ndf['total_vaccinations_per_hundred'] = df['total_vaccinations_per_hundred'] * 100\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Transform each date in the 'date' column to the 'yyyy-mm-dd' format.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Transform each date in the 'date' column to the 'yyyy-mm-dd' format.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Format 'date' column to 'yyyy-mm-dd'\ndf['date'] = df['date'].dt.strftime('%Y-%m-%d')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\n\n# Output the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Update the 'monthly_income' values to the median of their respective age group.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Update the 'monthly_income' values to the median of their respective age group.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Update the 'monthly_income' values to the median of their respective age group\ndf['monthly_income'] = df.groupby('age')['monthly_income'].transform(lambda x: x.fillna(x.median()))\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median income for each age group\nmedian_income_per_age = df.groupby('age')['monthly_income'].transform('median')\n\n# Update the 'monthly_income' with the median income of their respective age group\ndf['monthly_income'] = np.where(pd.isnull(df['monthly_income']), median_income_per_age, df['monthly_income'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Transform all values in 'number_of_dependents' into integers for streamlined analysis.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Transform all values in 'number_of_dependents' into integers for streamlined analysis.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/23.csv')\n\n# Convert values in 'number_of_dependents' to integers\ndata['number_of_dependents'] = data['number_of_dependents'].fillna(0).astype(int)\n\nprint(data)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# transform 'number_of_dependents' into integers\ndf['number_of_dependents'] = df['number_of_dependents'].astype(int)\n\n# print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Rounding the values in the 'debt_ratio' column to two decimal places to ensure consistency and ease of analysis.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Rounding the values in the 'debt_ratio' column to two decimal places to ensure consistency and ease of analysis.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Update the 'debt_ratio' column with rounded values\ndf['debt_ratio'] = df['debt_ratio'].round(2)\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Round the 'debt_ratio' column to two decimal places\ndf['debt_ratio'] = df['debt_ratio'].round(2)\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Replace all the 'NaN' values in the 'MonthlyIncome' column with the median of the column.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Replace all the 'NaN' values in the 'MonthlyIncome' column with the median of the column.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data from CSV\ndata = pd.read_csv('infiagent/csv/25.csv')\n\n# Calculate the median of 'MonthlyIncome' column\nmedian_monthly_income = data['MonthlyIncome'].median()\n\n# Replace NaN values in 'MonthlyIncome' column with the median\ndata['MonthlyIncome'].replace(np.nan,median_monthly_income,inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of the 'MonthlyIncome' column\nmedian_income = df['MonthlyIncome'].median()\n\n# Replace the 'NA' values with the median\ndf['MonthlyIncome'].replace('NA', median_income, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,\n\nQuestion: Format the 'Name' column to display last names in uppercase.", "csv_data_example": "JAMES LOGAN,,,,,,,,\nWins,Name,Code,School,,,STANDARD TEAM NAME,NUM ROUNDS,WEIGHTING\n5,Gatlin & Ramarao,Archbishop Mitty GR,Archbishop Mitty,,,Mitty GR,5,0.8\n5,Lahiri & Ponnuswamy,Archbishop Mitty LP,Archbishop Mitty,,,Mitty PL,,\n4,Gupta & Chatradhi,Archbishop Mitty GC,Archbishop Mitty,,,Mitty GuCh,,\n4,Patwa & Aggarwal,Archbishop Mitty PA,Archbishop Mitty,,,Mitty PA,,\n4,Shaikh & Singh,Archbishop Mitty SS,Archbishop Mitty,,,Mitty SS,,\n4,Arun & Schacter,Gunn Sr AS,Gunn Sr,,,Gunn AS,,\n4,Cheng & Parulekar,Gunn Sr CP,Gunn Sr,,,Gunn PC,,\n4,Prabhakar & Hamilton,Gunn Sr PH,Gunn Sr,,,Gunn PH,,\n4,Menotti & Bhasin,James Logan MB,James Logan,,,James Logan MB,,", "csv_path": "infiagent/csv/26.csv", "instruction": "Format the 'Name' column to display last names in uppercase.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Assuming the data is stored in a variable 'data'\ndf = pd.read_csv('infiagent/csv/26.csv')\n\n# Split the 'Name' column into two new columns: 'First Name' and 'Last Name'\ndf[['First Name', 'Last Name']] = df['Name'].str.split(' & ', expand=True)\n\n# Convert the 'Last Name' column to uppercase\ndf['Last Name'] = df['Last Name'].str.upper()\n\n# Combine 'First Name' and 'Last Name' columns and overwrite the original 'Name' column\ndf['Name'] = df[['Last Name']]\n\n# Finally, drop the 'First Name' and 'Last Name' columns, we don't need them anymore\ndf.drop(['First Name', 'Last Name'], axis=1, inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Loading the data from the CSV file\ndf = pd.read_csv('data.csv')\n\n# Extracting the last names from the 'Name' column and converting them to uppercase\ndf['Name'] = df['Name'].str.split(' & ').str[-1].str.upper()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Update the data in column 'Y' to its absolute value only for those rows where the value of 'Z' falls within the range (-2.735346128E+07, -2.575472613E+07).", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Update the data in column 'Y' to its absolute value only for those rows where the value of 'Z' falls within the range (-2.735346128E+07, -2.575472613E+07).", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Update the 'Y' column values to its absolute where 'Z' value is within given range\ndf.loc[(df['Z'] > -2.735346128E+07) & (df['Z'] < -2.575472613E+07),'Y'] = df.loc[(df['Z'] > -2.735346128E+07) & (df['Z'] < -2.575472613E+07),'Y'].abs()\n\n# Print updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Y' and 'Z' columns to numeric data type\ndf['Y'] = pd.to_numeric(df['Y'])\ndf['Z'] = pd.to_numeric(df['Z'])\n\n# Find the rows where 'Z' falls within the given range\ncondition = (df['Z'] >= -2.735346128E+07) & (df['Z'] <= -2.575472613E+07)\n\n# Update the 'Y' column to its absolute value for the selected rows\ndf.loc[condition, 'Y'] = df.loc[condition, 'Y'].abs()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Substitute all occurrences of 'A.D.' in the Calendar Date (TDB) column with 'Anno Domini'.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Substitute all occurrences of 'A.D.' in the Calendar Date (TDB) column with 'Anno Domini'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# loading the data\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# replacing 'A.D.' with 'Anno Domini' in the Calendar Date (TDB) column\ndf['Calendar Date (TDB)'] = df['Calendar Date (TDB)'].str.replace('A.D.','Anno Domini')\n\n# printing the DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Substitute 'A.D.' with 'Anno Domini' in the 'Calendar Date (TDB)' column\ndf['Calendar Date (TDB)'] = df['Calendar Date (TDB)'].str.replace('A.D.', 'Anno Domini')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Locate the record containing the highest 'Z' value and substitute all occurrences of 'Z' within the table with this uppermost value.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Locate the record containing the highest 'Z' value and substitute all occurrences of 'Z' within the table with this uppermost value.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/27.csv')\n\n# Find the maximum 'Z' value\nmax_Z = df['Z'].max()\n\n# Replace all 'Z' values with the maximum 'Z' value\ndf['Z'] = max_Z\n\n# Output the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Identify the maximum value in column 'Z'\nmax_z = df['Z'].max()\n\n# Substitute all occurrences of 'Z' with the maximum value\ndf['Z'] = max_z\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nJDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n\nQuestion: Convert 'Calendar Date (TDB)' to datetime format if it is not in datetime format.", "csv_data_example": "JDTDB,Calendar Date (TDB),X,Y,Z,\n2451214.540613426, A.D. 1999-Feb-05 00:58:29.0000, -2.887479204014895E+08,  2.475794555630772E+07,  2.212685553510034E+07,\n2451215.540613426, A.D. 1999-Feb-06 00:58:29.0000, -2.894465873203819E+08,  2.303593493346225E+07,  2.286678785207847E+07,\n2451216.540613426, A.D. 1999-Feb-07 00:58:29.0000, -2.901336496015852E+08,  2.131300030706115E+07,  2.360580331485712E+07,\n2451217.540613426, A.D. 1999-Feb-08 00:58:29.0000, -2.908091525744454E+08,  1.958921610934052E+07,  2.434387822074887E+07,\n2451218.540613426, A.D. 1999-Feb-09 00:58:29.0000, -2.914731418477991E+08,  1.786465590511964E+07,  2.508098923178605E+07,\n2451219.540613426, A.D. 1999-Feb-10 00:58:29.0000, -2.921256632967085E+08,  1.613939239760209E+07,  2.581711337101007E+07,\n2451220.540613426, A.D. 1999-Feb-11 00:58:29.0000, -2.927667630494693E+08,  1.441349743427704E+07,  2.655222801875284E+07,\n2451221.540613426, A.D. 1999-Feb-12 00:58:29.0000, -2.933964874748839E+08,  1.268704201291503E+07,  2.728631090891188E+07,\n2451222.540613426, A.D. 1999-Feb-13 00:58:29.0000, -2.940148831697997E+08,  1.096009628765337E+07,  2.801934012522100E+07,\n2451223.540613426, A.D. 1999-Feb-14 00:58:29.0000, -2.946219969469050E+08,  9.232729575167052E+06,  2.875129409751840E+07,\n", "csv_path": "infiagent/csv/27.csv", "instruction": "Convert 'Calendar Date (TDB)' to datetime format if it is not in datetime format.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load data\ntable = pd.read_csv(\"infiagent/csv/27.csv\")\n\n# Convert 'Calendar Date (TDB)' column to datetime\ntable['Calendar Date (TDB)'] = pd.to_datetime(table['Calendar Date (TDB)'].str.strip(), format = \"A.D. %Y-%b-%d %H:%M:%S.%f0\")\n\n# print(table)\nprint(table)\n", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Convert 'Calendar Date (TDB)' to datetime format\ndf['Calendar Date (TDB)'] = pd.to_datetime(df['Calendar Date (TDB)'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Modify the \"depth\" column to display only two decimal places.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Modify the \"depth\" column to display only two decimal places.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Update \"depth\" column to have only two decimal points precision \ndf['depth'] = df['depth'].round(2)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Modify the \"depth\" column to display only two decimal places\ndf['depth'] = df['depth'].round(2)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n\nQuestion: Subtract 1 from all 'Low' prices that were recorded on Fridays.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,83.349,79.58,78.86,79.02,58791957\n2014-01-03,82.929,79.1,77.2,77.28,98303870\n2014-01-06,80.619,78.11,76.23,77.7,103359151\n2014-01-07,81.64800000000001,77.99,76.85,77.15,79432766\n2014-01-08,80.8185,77.94,76.96,77.64,64686685\n2014-01-09,82.0155,78.12,76.48,76.65,69905199\n2014-01-10,80.97600000000001,77.26,75.87,76.13,76320664\n2014-01-13,79.485,77.5,75.7,76.53,94860843\n2014-01-14,80.7345,78.1,76.81,78.06,83734371\n2014-01-15,83.0235,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Subtract 1 from all 'Low' prices that were recorded on Fridays.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/29.csv')\n\n# Convert the 'Date' column to datetime format\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Determine the day of the week for each date\ndata['Day_of_Week'] = data['Date'].dt.day_name()\n\n# Deduct 1 from all 'Low' prices that were recorded on Fridays\ndata.loc[data['Day_of_Week'] == 'Friday','Low'] = data.loc[data['Day_of_Week'] == 'Friday','Low'] - 1\n\nprint(data)", "assistant_code": "# necessary libraries\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# find the rows where 'Low' prices were recorded on Fridays\nfriday_rows = df['Date'].dt.dayofweek == 4\n\n# subtract 1 from 'Low' prices for those rows\ndf.loc[friday_rows, 'Low'] = df.loc[friday_rows, 'Low'] - 1\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Modify the 'per_point_diff' column to remove the percentage symbol and convert the entries to decimal values.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Modify the 'per_point_diff' column to remove the percentage symbol and convert the entries to decimal values.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/30.csv')\n\n# Update the 'per_point_diff' column to eliminate the percentage sign and convert the values to decimals.\ndata['per_point_diff'] = data['per_point_diff'].str.rstrip('%').astype('float') / 100.0\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# The 'per_point_diff' column has values as strings with percentage symbol. Remove the symbol and convert the values to decimal.\ndf['per_point_diff'] = df['per_point_diff'].str.rstrip('%').astype('float') / 100.0\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Update the 'diff' column to replace any comma with an empty space.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Update the 'diff' column to replace any comma with an empty space.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Replace any comma with an empty space in 'diff' column\ndf['diff'] = df['diff'].str.replace(',','')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the 'diff' column to replace any comma with an empty space\ndf['diff'] = df['diff'].str.replace(',', '')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Convert all country names to uppercase for data consistency.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Convert all country names to uppercase for data consistency.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# Transform country names to uppercase for data consistency\ndf['Country'] = df['Country'].str.upper()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Convert all country names to uppercase for data consistency\ndata['Country'] = data['Country'].str.upper()\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Format the 'Year' column as a datetime.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Format the 'Year' column as a datetime.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# change the 'Year' column to datetime format\ndf['Year'] = pd.to_datetime(df['Year'],format='%Y')\n\nprint(df)", "assistant_code": "# importing pandas library\nimport pandas as pd\n\n# loading the data\ndf = pd.read_csv('data.csv')\n\n# converting 'Year' column to datetime\ndf['Year'] = pd.to_datetime(df['Year'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n\nQuestion: Update the 'source' column to change 'abc-news' to 'ABC News' for rows where the 'source' column is currently 'abc-news'.", "csv_data_example": ",author,description,publishedAt,source,title,url,urlToImage,text,neg,neu,pos,compound\n0,ABC News,\"Updates to the app include an \"\"about this account\"\" feature for users with large followings, and people can now request to be verified, too.\",2018-08-29 10:44:48,abc-news,WATCH: Instagram rolls out new features aimed at improving security,https://abcnews.go.com/Technology/video/instagram-rolls-features-aimed-improving-security-57469412,https://s.abcnews.com/images/Technology/180829_atm_techbytes_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. How much is Instagram worth? Plus, Samsung could introduce three new phones in January. Instagram reveals new video platform \"\"GMA\"\" got a first look at Instagram's IGTV, a major update that allows users to publish longform videos.  Now Playing: How much is Instagram worth? Now Playing: Instagram reveals new video platform Now Playing: Facebook and Instagram crack down on underage users Now Playing: Samsung to unveil 8K TV this fall Now Playing: New technology designed to keep ambulances safer  Now Playing: Trump administration wages battle with Google Now Playing: Instagram rolls out new features aimed at improving security Now Playing: Uber partners with Toyota to build self-driving cars Now Playing: YouTube plans to allow fewer ad skips Now Playing: Facebook increases security after 2016 election controversy Now Playing: Google accused of storing users' location data Now Playing: Netflix responds to criticism for running ads Now Playing: Kroger testing driverless grocery delivery Now Playing: Apple Car rumored to roll out by 2025 Now Playing: Asleep at the wheel: Exposing dangers of drowsy driving Now Playing: Sprint prepares for next generation of smartphones Now Playing: A serious warning about attacks on ATMs Now Playing: What to know about money payment app misfires  Now Playing: Spotify is testing a skip-ad option for nonpaying users Now Playing: Samsung unveils the Galaxy Note9  Now Playing: {{itm.title}}\",0.067,0.733,0.2,0.9746\n1,ABC News,\"Arizona primary voters Carlos Medina and Kaitlin Showers speak with ABC News' Chief National Correspondent Terry Moran on their faith in the political process in Tempe, Arizona on Tuesday.\",2018-08-29 01:22:02,abc-news,\"WATCH: In the heat of primary day, young Arizonans encourage faith in the political process\",https://abcnews.go.com/Politics/video/heat-primary-day-young-arizonans-encourage-faith-political-57462339,https://s.abcnews.com/images/Politics/180828_vod_az_voters_hpMain_16x9_992.jpg,\"Coming up in the next {{countdown}} {{countdownlbl}} Coming up next: {{nextVideo.title}} {{nextVideo.description}} Skip to this video now  Related Now Playing: {{currentVideo.title}}    Play Video  This transcript has been automatically generated and may not be 100% accurate. 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary A tense GOP primary race heated up this weekend when a campaign suggested John McCain was trying to mess with their timing. Now Playing: 'Start Here' podcast: Controversial Facebook post complicates Arizona Senate primary Now Playing: Arizona Senate candidates Joe Arpaio, Kelli Ward on why they're running Now Playing: WH officials deny connection to Maduro assassination attempt Now Playing: President Trump lashes out at 'totally dishonest' media Now Playing: Andrew Cuomo won't run against Trump in 2020 Now Playing: Joe Biden to pay tribute at John McCain's memorial today  Now Playing: GOP gubernatorial candidate uses 'monkey' comment while discussing black opponent Now Playing: GOP candidate uses 'monkey it up' discussing black opponent Now Playing: Family attend public viewing for Sen. John McCain Now Playing: The Briefing Room: Top White House lawyer leaving administration  Now Playing: Honoring an American hero Now Playing: FiveThirtyEight House forecast update for August 29, 2018 Now Playing: Sun belt showdowns: Progressive, Trump-aligned candidates win big  Now Playing: Black gubernatorial candidate makes history in Florida Now Playing: Primary elections: Andrew Gillum, Ron DeSantis win gubernatorial primaries in Florida Now Playing: Parkland survivor discusses Florida primary results Now Playing: GOP nominee considers potential Trump visit Now Playing: In the heat of primary day, young Arizonans encourage faith in the political process Now Playing: Lawyer for alleged Russian agent says video captures 'romance' Now Playing: The Briefing Room: Trump finally speaks about McCain Now Playing: {{itm.title}}\",0.062,0.735,0.204,0.9869\n2,Karma Allen,She works at a Texas hospital where a toddler tested positive for measles.,2018-08-28 11:04:51,abc-news,Nurse under investigation after posting on anti-vaccination page,https://abcnews.go.com/US/texas-nurse-investigation-posting-patients-measles-anti-vaccination/story?id=57443736,https://s.abcnews.com/images/Politics/measles-vaccine-gty-hb-180815_hpMain_16x9_992.jpg,\"\nA nurse at a hospital in Texas where a toddler tested positive for measles has been fired after she posted about the diagnosis on an anti-vaccination website, according to hospital officials.\n \nThe employee's firing comes just one day after Texas Children's Hospital said it was investigating the incident.\n \n\"\"We were made aware that one of our nurses posted protected health information regarding a patient on social media,\"\" the hospital said in a statement Tuesday night. \"\"We take these matters very seriously as the privacy and well-being of our patients is always a top priority. After an internal investigation, this individual is no longer with the organization.\"\"\n \nTexas Children's Hospital in Houston said Monday it stopped a nurse from seeing patients after she reportedly posted about a young boy?��__s condition on an anti-vaccination group on Facebook, according to a statement.\n", "csv_path": "infiagent/csv/33.csv", "instruction": "Update the 'source' column to change 'abc-news' to 'ABC News' for rows where the 'source' column is currently 'abc-news'.", "instruction_type": "Update-Data Processing", "reference_code": "# import necessary libraries\nimport pandas as pd\n\n# load the csv to a DataFrame\ndf = pd.read_csv('infiagent/csv/33.csv')\n\n# replace 'abc-news' with 'ABC News' in the 'source' column\ndf['source'] = df['source'].replace(['abc-news'],'ABC News')\n\n# print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update 'source' column\ndf['source'] = df['source'].replace('abc-news', 'ABC News')\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Update the 'country' column by converting all countries' names to uppercase.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Update the 'country' column by converting all countries' names to uppercase.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the data from the csv file \ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# updated the 'country' column by converting all countries' names to uppercase\ndf['country'] = df['country'].str.upper()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'country' column to uppercase\ndf['country'] = df['country'].str.upper()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Modify the format of the \"year\" column's date values to only display the four-digit year (\"YYYY\").", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Modify the format of the \"year\" column's date values to only display the four-digit year (\"YYYY\").", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Change the date format in 'year' row to \"YYYY\"\ndf['year'] = pd.to_datetime(df['year'],format='%Y').dt.strftime('%Y')\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Modify the \"year\" column to only display four-digit year\ndf['year'] = df['year'].apply(lambda x: str(x)[:4])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Modify the GDP per capita values to display up to two decimal points.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Modify the GDP per capita values to display up to two decimal points.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Update all the GDP per capita values to a 2 decimal point format\nfor column in df.columns[1:]:\n    df[column] = df[column].round(2)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the GDP per capita values to display up to two decimal points\ndf = df.round(2)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Replace all NaN values in the table with the mean GDP per capita of the respective year.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Replace all NaN values in the table with the mean GDP per capita of the respective year.", "instruction_type": "Update-Data Processing", "reference_code": "# import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# Replace all NaN values in the table with the mean GDP per capita of the respective year.\nfor col in df.columns[1:]:\n    df[col].replace(np.nan,df[col].mean(),inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean GDP per capita for each year\nyearly_means = df.dropna().groupby('country').mean()\n\n# Replace the NaN values with the mean GDP per capita of the respective year\ndf.fillna(yearly_means, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Convert the data type of the \"id\" column to text.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Convert the data type of the \"id\" column to text.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv(\"infiagent/csv/37.csv\")\n\n# Change the data type of the \"id\" column to text\ndf['id'] = df['id'].astype(str)\n\n# Print the updated table\nprint(df)", "assistant_code": "# Import pandas library\nimport pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the data type of the 'id' column to text\ndf['id'] = df['id'].astype(str)\n\n# Print the result\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Set the 'Gender' column value to 0 if in the same 'transcript_id', the number of 0 is more than 1. Otherwise, set the 'Gender' column value to 1.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Set the 'Gender' column value to 0 if in the same 'transcript_id', the number of 0 is more than 1. Otherwise, set the 'Gender' column value to 1.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndata = pd.read_csv('infiagent/csv/37.csv')\n\n# Group the data by 'transcript_id' and count the number of zeros in 'Gender'\ngrouped_data = data.groupby('transcript_id')['Gender'].transform(lambda x: (x == 0).sum())\n\n# Update the 'Gender' column based on the condition\ndata['Gender'] = data['Gender'].where(grouped_data <= 1,0)\n\n# Print the updated data\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Count the number of zeros in 'transcript_id'\ndf['zero_count'] = df['transcript_id'].apply(lambda x: str(x).count('0'))\n\n# Set the 'Gender' column value to 0 if zero count is more than 1, else set it to 1\ndf['Gender'] = df.apply(lambda row: 0 if row['zero_count'] > 1 else 1, axis=1)\n    \n# Drop the zero_count column as it is no longer needed\ndf = df.drop(['zero_count'], axis=1)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Format the 'transcript_id' column as string and add a prefix 'TransID-' to each id.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Format the 'transcript_id' column as string and add a prefix 'TransID-' to each id.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('infiagent/csv/37.csv')\n\n# Change the format of 'transcript_id' to string\ndata['transcript_id'] = data['transcript_id'].astype(str)\n\n# Append a prefix 'TransID-' to each id\ndata['transcript_id'] = 'TransID-' + data['transcript_id']\n\n# Print the results\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('data.csv')\n\n# Format the 'transcript_id' column as string and add a prefix 'TransID-' to each id\ndf['transcript_id'] = 'TransID-' + df['transcript_id'].astype(str)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Eliminate any whitespace from the column names and transform them into lowercase.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Eliminate any whitespace from the column names and transform them into lowercase.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/38.csv')\n\n# Remove any whitespace from the column names and convert them to lowercase\ndf.columns = df.columns.str.strip().str.lower()\n\nprint(df)", "assistant_code": "# import necessary libraries\nimport pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Eliminate any whitespace from the column names and transform them into lowercase\ndf.columns = df.columns.str.strip().str.lower()\n\n# Print the processed dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Capitalize the first letter of each word in the 'hotel_name' column.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Capitalize the first letter of each word in the 'hotel_name' column.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Capitalize the first letter of each word in the 'hotel_name' column\ndf['hotel_name'] = df['hotel_name'].str.title()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Capitalize the first letter of each word in 'hotel_name' column\ndf['hotel_name'] = df['hotel_name'].str.title()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Round the 'price' values to two decimal places in the table.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Round the 'price' values to two decimal places in the table.", "instruction_type": "Update-Data Processing", "reference_code": "# Required Libraries\nimport pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/41.csv')\n\n# Update the 'price' column format to two decimal places\ndata['price'] = data['price'].round(2)\n\n# Print the result\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Round the 'price' column to two decimal places\ndf['price'] = df['price'].round(2)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n\nQuestion: Change the display format of 'lat' and 'long' columns by adding a '?��' sign at the end to indicate that they are geographical coordinates.", "csv_data_example": ",price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n0,221900.0,3,1.0,1180,5650,1.0,0,0,3,7,1180,0,1955,0,98178,47.5112,-122.257,1340,5650\n1,538000.0,3,2.25,2570,7242,2.0,0,0,3,7,2170,400,1951,1991,98125,47.721000000000004,-122.319,1690,7639\n2,180000.0,2,1.0,770,10000,1.0,0,0,3,6,770,0,1933,0,98028,47.7379,-122.23299999999999,2720,8062\n3,604000.0,4,3.0,1960,5000,1.0,0,0,5,7,1050,910,1965,0,98136,47.5208,-122.39299999999999,1360,5000\n4,510000.0,3,2.0,1680,8080,1.0,0,0,3,8,1680,0,1987,0,98074,47.6168,-122.045,1800,7503\n5,1225000.0,4,4.5,5420,101930,1.0,0,0,3,11,3890,1530,2001,0,98053,47.6561,-122.005,4760,101930\n6,257500.0,3,2.25,1715,6819,2.0,0,0,3,7,1715,0,1995,0,98003,47.3097,-122.32700000000001,2238,6819\n7,291850.0,3,1.5,1060,9711,1.0,0,0,3,7,1060,0,1963,0,98198,47.4095,-122.315,1650,9711\n8,229500.0,3,1.0,1780,7470,1.0,0,0,3,7,1050,730,1960,0,98146,47.5123,-122.337,1780,8113\n9,323000.0,3,2.5,1890,6560,2.0,0,0,3,7,1890,0,2003,0,98038,47.3684,-122.031,2390,7570\n", "csv_path": "infiagent/csv/41.csv", "instruction": "Change the display format of 'lat' and 'long' columns by adding a '?��' sign at the end to indicate that they are geographical coordinates.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from CSV\ndf = pd.read_csv('infiagent/csv/41.csv')\n\n# Update columns 'lat' & 'long'\ndf['lat'] = df['lat'].astype(str) + '?��'\ndf['long'] = df['long'].astype(str) + '?��'\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Add '?' at the end of 'lat' and 'long' to indicate they are geographical coordinates\ndf['lat'] = df['lat'].astype(str) + '?'\ndf['long'] = df['long'].astype(str) + '?'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Set the value of the 'importance.score' column to 0.05 for all rows where the 'ID' column is less than 500.", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Set the value of the 'importance.score' column to 0.05 for all rows where the 'ID' column is less than 500.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv(\"infiagent/csv/42.csv\")\n\n# Update importance.score column\ndf.loc[df['row ID'] < 500,'importance.score'] = 0.05\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Set the value of the 'importance.score' column to 0.05 for all rows where the 'row ID' column is less than 500\ndf.loc[df['row ID'] < 500, 'importance.score'] = 0.05\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n\nQuestion: Format all values in the 'LibraryID' column to have consistent capitalization (upper case).", "csv_data_example": "#featureID,row ID,row m/z,row retention time,LibraryID,standard_indentification_level_1,_feature_id,importance.score\n358.3677167129743_3.65612984126984,241,358.3677167,3.6561298410000003,,,358.3677167129743_3.65612984126984,0.06705242252123918\n423.2744890715284_4.29798541001065,695,423.2744891,4.29798541,,,423.2744890715284_4.29798541001065,0.04059843417426111\n304.2993572401259_5.121302585521083,382,304.2993572,5.1213025860000005,Spectral Match to Benzyldodecyldimethylammonium from NIST14,,304.2993572401259_5.121302585521083,0.034141042934364416\n389.2691196723436_3.383737479270316,300,389.2691197,3.3837374789999997,,,389.2691196723436_3.383737479270316,0.032520681767450016\n332.3307817246258_5.3103554720133594,612,332.3307817,5.3103554719999995,Spectral Match to Benzyltetradecyldimethylammonium from NIST14,,332.3307817246258_5.3103554720133594,0.03225690801075316\n358.28219024838404_4.7314112935323385,168,358.2821902,4.731411294,,,358.28219024838404_4.7314112935323385,0.02723773303958672\n516.3001954425159_3.6121693593314776,822,516.3001954,3.6121693589999997,Spectral Match to Taurocholic acid from NIST14,,516.3001954425159_3.6121693593314776,0.02645730122829398\n326.377650457406_5.358003633720932,109,326.3776505,5.358003634,,,326.377650457406_5.358003633720932,0.023422235337951285\n357.27906873865504_4.755498426870746,100,357.2790687,4.755498427,Spectral Match to Chenodeoxycholic acid from NIST14,Chenodeoxycholic acid,357.27906873865504_4.755498426870746,0.02326156883657954\n358.28198237386744_5.002188993710692,17,358.28198239999995,5.002188994,,,358.28198237386744_5.002188993710692,0.021838301301350255\n", "csv_path": "infiagent/csv/42.csv", "instruction": "Format all values in the 'LibraryID' column to have consistent capitalization (upper case).", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load csv file\ndf = pd.read_csv('infiagent/csv/42.csv')\n\n# Update 'LibraryID' column to have consistent capitalization\ndf['LibraryID'] = df['LibraryID'].str.upper()\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndata = pd.read_csv('data.csv')\n\n# Format all values in the 'LibraryID' column to have consistent capitalization (upper case)\ndata['LibraryID'] = data['LibraryID'].str.upper()\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Modify the 'smoker' column by replacing 'yes' with '1' and 'no' with '0'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Modify the 'smoker' column by replacing 'yes' with '1' and 'no' with '0'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the data from the csv file\ndata = pd.read_csv('infiagent/csv/43.csv')\n\n# update the 'smoker' column values\ndata['smoker'] = data['smoker'].map({'yes': 1,'no': 0})\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Replace 'yes' with '1' and 'no' with '0' in the 'smoker' column\ndf['smoker'] = df['smoker'].replace({'yes': '1', 'no': '0'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nage,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n\nQuestion: Update the 'sex' field to 'unknown' when the 'age' is less than 20 and the 'region' is 'southwest'.", "csv_data_example": "age,sex,bmi,children,smoker,region,charges\n19,female,27.9,0,yes,southwest,16884.924\n18,male,33.77,1,no,southeast,1725.5523\n28,male,33,3,no,southeast,4449.462\n33,male,22.705,0,no,northwest,21984.47061\n32,male,28.88,0,no,northwest,3866.8552\n31,female,25.74,0,no,southeast,3756.6216\n46,female,33.44,1,no,southeast,8240.5896\n37,female,27.74,3,no,northwest,7281.5056\n37,male,29.83,2,no,northeast,6406.4107\n60,female,25.84,0,no,northwest,28923.13692\n", "csv_path": "infiagent/csv/43.csv", "instruction": "Update the 'sex' field to 'unknown' when the 'age' is less than 20 and the 'region' is 'southwest'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/43.csv')\n\n# Perform the update\ndf.loc[(df['age'] < 20) & (df['region'] == 'southwest'),'sex'] = 'unknown'\n\n# Print the updated data\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'sex' field to 'unknown' where 'age' is less than 20 and 'region' is 'southwest'\ndf.loc[(df['age'] < 20) & (df['region'] == 'southwest'), 'sex'] = 'unknown'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Format the \"Date\" column in the table as yyyy-mm-dd.", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Format the \"Date\" column in the table as yyyy-mm-dd.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the data from csv file\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# update \"Date\" column to a standardized date format\ndf['Date'] = pd.to_datetime(df['Date'],format='%d-%b-%y').dt.strftime('%Y-%m-%d')\n\nprint(df)", "assistant_code": "# import the necessary libraries\nimport pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Format 'Date' column in the format yyyy-mm-dd\ndf['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Set the \"Close\" column to the average \"Close\" value for rows where the \"Volume\" is below the overall average \"Volume\".", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Set the \"Close\" column to the average \"Close\" value for rows where the \"Volume\" is below the overall average \"Volume\".", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# Calculate the overall average of Volume\navg_volume = df['Volume'].mean()\n\n# Calculate the average Close value for rows where Volume is less than the overall average Volume\navg_close = df[df['Volume'] < avg_volume]['Close'].mean()\n\n# Update the Close value\ndf.loc[df['Volume'] < avg_volume,'Close'] = avg_close\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the overall average Volume\naverage_volume = df['Volume'].mean()\n\n# Apply the condition to set the Close value\ndf.loc[df['Volume'] < average_volume, 'Close'] = df['Close'].mean()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n\nQuestion: Update values of \"High\" and \"Low\", replace them with their respective average values taken from their specific Volume groups('Low', 'Medium', 'High').", "csv_data_example": ",Date,Open,High,Low,Close,Volume\n0,19-Jan-18,90.14,90.61,89.66,90.0,36875013\n1,18-Jan-18,89.8,90.67,89.66,90.1,24159683\n2,17-Jan-18,89.08,90.28,88.75,90.14,25621164\n3,16-Jan-18,90.1,90.79,88.01,88.35,36599736\n4,12-Jan-18,88.67,89.78,88.45,89.6,24271531\n5,11-Jan-18,88.13,88.13,87.24,88.08,17808877\n6,10-Jan-18,87.86,88.19,87.41,87.82,18652201\n7,9-Jan-18,88.65,88.73,87.86,88.22,19484317\n8,8-Jan-18,88.2,88.58,87.6,88.28,22113049\n9,5-Jan-18,87.66,88.41,87.43,88.19,23407110\n", "csv_path": "infiagent/csv/44.csv", "instruction": "Update values of \"High\" and \"Low\", replace them with their respective average values taken from their specific Volume groups('Low', 'Medium', 'High').", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\nimport numpy as np\n\n# load csv file\ndf = pd.read_csv('infiagent/csv/44.csv')\n\n# define three volume groups: low,medium and high\ndf['volume_group'] = pd.qcut(df['Volume'],3,labels=[\"Low\",\"Medium\",\"High\"])\n\n# calculate average \"High\" and \"Low\" for each volume group\naverage_high = df.groupby('volume_group')['High'].transform('mean')\naverage_low = df.groupby('volume_group')['Low'].transform('mean')\n\n# replace \"High\" and \"Low\" with their respective average values\ndf['High'] = df['volume_group'].map(average_high)\ndf['Low'] = df['volume_group'].map(average_low)\n\nprint(df)", "assistant_code": "# importing necessary libraries\nimport pandas as pd\n\n# loading the data\ndf = pd.read_csv('data.csv')\n\n# defining a function to calculate average based on volume\ndef calculate_average(df, column_name):\n    df[column_name] = df.groupby('Volume')[column_name].transform('mean')\n    return df\n\n# applying the function to \"High\" and \"Low\" columns\ndf = calculate_average(df, 'High')\ndf = calculate_average(df, 'Low')\n\n# print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Update the \"MedInc\" column by rounding off the values to the nearest whole number.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Update the \"MedInc\" column by rounding off the values to the nearest whole number.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Update the \"MedInc\" column by rounding off the values to the nearest whole number\ndf['MedInc'] = df['MedInc'].round(0)\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the \"MedInc\" column by rounding off the values to the nearest whole number\ndf['MedInc'] = df['MedInc'].round()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Set the update indicator for all 'AUT' location instances to \"UPDATED_INDICATOR\".", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Set the update indicator for all 'AUT' location instances to \"UPDATED_INDICATOR\".", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# Update the indicator for all instances of 'AUT' location\ndf.loc[df['LOCATION'] == 'AUT','INDICATOR'] = \"UPDATED_INDICATOR\"\n\n# Output the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'INDICATOR' for all rows where 'LOCATION' is 'AUT'\ndf.loc[df['LOCATION'] == 'AUT', 'INDICATOR'] = 'UPDATED_INDICATOR'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Set the value of 'class8' to 0.5 for all rows where 'class8' is equal to 1.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Set the value of 'class8' to 0.5 for all rows where 'class8' is equal to 1.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Update the 'class8' value where it is equal to 1\ndf.loc[df['class8'] == 1,'class8'] = 0.5\n\n# Output the results\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update 'class8' to 0.5 where 'class8' is 1\ndf.loc[df['class8'] == 1, 'class8'] = 0.5\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Update the values in 'class4' as follows: if the value is '0.0', change it to 'No'; if the value is '1.0', change it to 'Yes'.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Update the values in 'class4' as follows: if the value is '0.0', change it to 'No'; if the value is '1.0', change it to 'Yes'.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Update the values in 'class4'\ndf['class4'] = df['class4'].replace({0.0: 'No',1.0: 'Yes'})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the values in 'class4' column\ndf['class4'] = df['class4'].apply(lambda x: 'No' if x == 0.0 else 'Yes')\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Change the value of 'class6' to 1 where ID is less than 5.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Change the value of 'class6' to 1 where ID is less than 5.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/48.csv')\n\n# Identify the rows where ID is less than 5\nmask = df['ID'] < 5\n\n# Update the 'class6' column for these rows to 1\ndf.loc[mask,'class6'] = 1\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the value of 'class6' to 1 where ID is less than 5\ndf.loc[df['ID'] < 5, 'class6'] = 1\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Convert the \"Time\" column to a datetime format.", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Convert the \"Time\" column to a datetime format.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# load the csv data\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Convert \"Time\" column to datetime format\ndf['Time'] = pd.to_datetime(df['Time'])\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Convert the \"Time\" column to datetime format\ndf['Time'] = pd.to_datetime(df['Time'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Modify the 'description' column to 'light rain' whenever the wind speed is lower than 3.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Modify the 'description' column to 'light rain' whenever the wind speed is lower than 3.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Update the 'description' column \ndf.loc[df['wind_speed'] < 3,'description'] = 'light rain'\n\n# Output the dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('data.csv')\n\n# Modify the 'description' column to 'light rain' where 'wind_speed' is less than 3\ndf.loc[df['wind_speed'] < 3, 'description'] = 'light rain'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Capitalize all city names in the 'city' column.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Capitalize all city names in the 'city' column.", "instruction_type": "Update-Data Processing", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/50.csv')\n\n# Capitalize City Names\ndf['city'] = df['city'].str.capitalize()\n\n# Print the data\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Capitalize all city names\ndf['city'] = df['city'].str.upper()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Rename the column 'Trips over the past 24-hours (midnight to 11:59pm)' to 'Daily Trips'.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Rename the column 'Trips over the past 24-hours (midnight to 11:59pm)' to 'Daily Trips'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/1.csv')\n\n# Rename the column\ndf = df.rename(columns={'Trips over the past 24-hours (midnight to 11:59pm)': 'Daily Trips'})\n\nprint(df)", "assistant_code": "# Import the pandas library\nimport pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Rename the specified column\ndf = df.rename(columns={'Trips over the past 24-hours (midnight to 11:59pm)': 'Daily Trips'})\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n\nQuestion: Change the title of column 'Cumulative trips (since launch):' to 'Total trips since launch'.", "csv_data_example": "Date,Trips over the past 24-hours (midnight to 11:59pm),Cumulative trips (since launch):,Miles traveled today (midnight to 11:59 pm),Miles traveled to date:,Total Annual Memberships Sold,Annual Member Sign-Ups (midnight to 11:59 pm),24-Hour Passes Purchased (midnight to 11:59 pm),7-Day Passes Purchased (midnight to 11:59 pm)\n10/1/2014,31197,13296973,44612,23121175,124846,112,330,48\n10/2/2014,38286,13335259,60639,23181814,124959,113,602,86\n10/3/2014,38956,13374215,65739,23247553,125024,65,1276,107\n10/4/2014,15088,13389303,24254,23271807,125058,34,617,26\n10/5/2014,26247,13415550,48930,23320737,125109,51,1470,90\n10/6/2014,35651,13451201,56380,23377117,125203,94,710,99\n10/7/2014,36931,13488132,58740,23435857,125262,59,593,81\n10/8/2014,37870,13526002,60594,23496451,125354,92,667,86\n10/9/2014,36427,13562429,57865,23554316,125407,53,709,71\n10/10/2014,34690,13597119,54690,23609006,125464,57,905,78\n", "csv_path": "infiagent/csv/1.csv", "instruction": "Change the title of column 'Cumulative trips (since launch):' to 'Total trips since launch'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file\ndata = pd.read_csv('infiagent/csv/1.csv')\n\n# Rename the column 'Cumulative trips (since launch):' to 'Total trips since launch'\ndata = data.rename(columns={'Cumulative trips (since launch):': 'Total trips since launch'})\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Rename the column 'Cumulative trips (since launch):' to 'Total trips since launch'\ndf = df.rename(columns={'Cumulative trips (since launch):': 'Total trips since launch'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\nQuestion: Rename the column 'sch_arr_time' to 'scheduled_arrival_time' and 'sch_dep_time' to 'scheduled_departure_time'.", "csv_data_example": "\"origin_station_code\",\"origin_station\",\"destination_station_code\",\"destination_station\",\"route_code\",\"start_time\",\"end_time\",\"fleet_number\",\"station_code\",\"station\",\"station_type\",\"platform\",\"sch_arr_time\",\"sch_dep_time\",\"act_arr_time\",\"act_dep_time\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MAM\",\"Machine Mist\",\"Passenger\",\"Outbound\",NA,\"21/09/15 05:01:00\",NA,\"21/09/15 04:59:19\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"BUB\",\"Bucket Believe\",\"Passenger\",\"Outbound\",\"21/09/15 05:01:30\",\"21/09/15 05:02:00\",\"21/09/15 05:00:37\",\"21/09/15 05:01:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"MOM\",\"Monkey Machine\",\"Passenger\",\"Outbound\",\"21/09/15 05:04:30\",\"21/09/15 05:05:00\",\"21/09/15 05:03:10\",\"21/09/15 05:03:41\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FAF\",\"Father Form\",\"Passenger\",\"Outbound\",\"21/09/15 05:07:30\",\"21/09/15 05:08:00\",\"21/09/15 05:05:24\",\"21/09/15 05:06:04\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"GIS\",\"Giraffe Spot\",\"Passenger\",\"Outbound\",\"21/09/15 05:09:30\",\"21/09/15 05:10:00\",\"21/09/15 05:07:37\",\"21/09/15 05:08:20\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"SMO\",\"Smoke Soup\",\"Passenger\",\"Outbound\",\"21/09/15 05:11:30\",\"21/09/15 05:12:00\",\"21/09/15 05:09:10\",\"21/09/15 05:09:52\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"FEC\",\"Feast Clam\",\"Passenger\",\"Outbound\",\"21/09/15 05:13:30\",\"21/09/15 05:14:00\",\"21/09/15 05:11:30\",\"21/09/15 05:12:11\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"TRT\",\"Trail Test\",\"Passenger\",\"Outbound\",\"21/09/15 05:15:30\",\"21/09/15 05:16:00\",\"21/09/15 05:14:20\",\"21/09/15 05:15:08\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"PAB\",\"Page Bedroom\",\"Passenger\",\"Outbound\",\"21/09/15 05:17:30\",\"21/09/15 05:18:00\",\"21/09/15 05:16:23\",\"21/09/15 05:17:13\"\n\"MAM\",\"Machine Mist\",\"ROT\",\"Roll Test\",4,\"21/09/15 05:01:00\",\"21/09/15 05:41:00\",3088,\"OIW\",\"Oil Weight\",\"Passenger\",\"Outbound\",\"21/09/15 05:18:30\",\"21/09/15 05:19:00\",\"21/09/15 05:18:38\",\"21/09/15 05:18:39\"\n", "csv_path": "infiagent/csv/2.csv", "instruction": "Rename the column 'sch_arr_time' to 'scheduled_arrival_time' and 'sch_dep_time' to 'scheduled_departure_time'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the CSV data into a pandas dataframe\ndata = pd.read_csv('infiagent/csv/2.csv')\n\n# Rename the columns 'sch_arr_time' and 'sch_dep_time'\ndata.rename(columns={'sch_arr_time': 'scheduled_arrival_time','sch_dep_time': 'scheduled_departure_time'},inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Rename the columns\ndf = df.rename(columns={'sch_arr_time': 'scheduled_arrival_time', 'sch_dep_time': 'scheduled_departure_time'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ntimestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n\nQuestion: Rename the column 'num. calls answered' to 'answered calls' for better understanding.", "csv_data_example": "timestamp,num. busy overflows,num. calls answered,num. calls abandoned,num. calls transferred,num. calls timed out,avg. num. agents talking,avg. num. agents staffed,avg. wait time,avg. abandonment time\nApr 13  2017 12:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 12:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:30:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 1:45:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:00:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\nApr 13  2017 2:15:00 AM,0,0,0,0,0,0,4,00:00:00,00:00:00\n", "csv_path": "infiagent/csv/4.csv", "instruction": "Rename the column 'num. calls answered' to 'answered calls' for better understanding.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv(\"infiagent/csv/4.csv\")\n\n# Rename the column\ndf = df.rename(columns={'num. calls answered': 'answered calls'})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# rename the column 'num. calls answered' to 'answered calls'\ndf.rename(columns={'num. calls answered': 'answered calls'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nmpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n\nQuestion: Rename column \"modelyear\" to \"year_of_model\"", "csv_data_example": "mpg,cylinders,displacement,horsepower,weight,acceleration,modelyear,origin\n18.0,8,307.0,130.0,3504.,12.0,70,1\t\n15.0,8,350.0,165.0,3693.,11.5,70,1\t\n18.0,8,318.0,150.0,3436.,11.0,70,1\t\n16.0,8,304.0,150.0,3433.,12.0,70,1\t\n17.0,8,302.0,140.0,3449.,10.5,70,1\t\n15.0,8,429.0,198.0,4341.,10.0,70,1\t\n14.0,8,454.0,220.0,4354.,9.0,70,1\t\n14.0,8,440.0,215.0,4312.,8.5,70,1\t\n14.0,8,455.0,225.0,4425.,10.0,70,1\t\n15.0,8,390.0,190.0,3850.,8.5,70,1\t\n", "csv_path": "infiagent/csv/11.csv", "instruction": "Rename column \"modelyear\" to \"year_of_model\"", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/11.csv')\n\n# rename the column 'modelyear' to 'year_of_model'\ndf.rename(columns={'modelyear': 'year_of_model'},inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Rename the column \"modelyear\" to \"year_of_model\"\ndf.rename(columns={\"modelyear\": \"year_of_model\"}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n\nQuestion: Rename the column 'RELHUM' to 'Relative Humidity'.", "csv_data_example": "DATE TIME,WINDSPEED,DIR,GUSTS,AT,BARO,RELHUM,VIS\n01/01/2015 00:00,2.72,288,5.25,27.7,1023.0,,\n01/01/2015 01:00,3.89,273,7.00,26.8,1022.7,,\n01/01/2015 02:00,4.86,268,6.41,27.0,1022.1,,\n01/01/2015 03:00,4.47,294,7.19,26.6,1021.4,,\n01/01/2015 04:00,4.08,283,7.19,25.9,1020.9,,\n01/01/2015 05:00,5.64,271,9.33,25.5,1020.5,,\n01/01/2015 06:00,8.55,260,11.47,26.1,1019.5,,\n01/01/2015 07:00,7.58,264,10.50,25.7,1019.1,,\n01/01/2015 08:00,6.61,267,10.30,25.2,1018.9,,\n01/01/2015 09:00,9.14,274,13.41,25.2,1018.5,,\n", "csv_path": "infiagent/csv/13.csv", "instruction": "Rename the column 'RELHUM' to 'Relative Humidity'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('infiagent/csv/13.csv')\n\n# rename the column 'RELHUM' to 'Relative Humidity'\ndf.rename(columns={'RELHUM': 'Relative Humidity'},inplace=True)\n\nprint(df)", "assistant_code": "# importing pandas library\nimport pandas as pd\n\n# loading the data\ndf = pd.read_csv('data.csv')\n\n# renaming the column 'RELHUM' to 'Relative Humidity'\ndf = df.rename(columns={'RELHUM': 'Relative Humidity'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nwage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n\nQuestion: Rename the column 'goodhlth' to 'good health'.", "csv_data_example": "wage,lwage,belavg,abvavg,exper,looks,union,goodhlth,black,female,married,south,bigcity,smllcity,service,expersq,educ,Nservice,Slooks\n5.73,1.745715,0,1,30,4,0,1,0,1,1,0,0,1,1,900,14,0,4\n4.28,1.453953,0,0,28,3,0,1,0,1,1,1,0,1,0,784,12,1,0\n7.96,2.074429,0,1,35,4,0,1,0,1,0,0,0,1,0,1225,10,1,0\n11.57,2.448416,0,0,38,3,0,1,0,0,1,0,1,0,1,1444,16,0,3\n11.42,2.435366,0,0,27,3,0,1,0,0,1,0,0,1,0,729,16,1,0\n3.91,1.363537,0,0,20,3,0,0,0,1,1,0,1,0,0,400,12,1,0\n8.76,2.170196,0,0,12,3,0,1,0,0,1,0,1,0,0,144,16,1,0\n7.69,2.039921,0,1,5,4,1,1,0,0,0,0,0,1,0,25,16,1,0\n5,1.609438,0,0,5,3,0,1,0,1,0,0,0,0,0,25,16,1,0\n3.89,1.358409,0,0,12,3,0,1,0,1,0,0,0,1,0,144,12,1,0\n", "csv_path": "infiagent/csv/15.csv", "instruction": "Rename the column 'goodhlth' to 'good health'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/15.csv')\n\n# Renaming the column goodhlth to good health\ndata = data.rename(columns={'goodhlth': 'good health'})\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# rename the column\ndf = df.rename(columns={'goodhlth': 'good health'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ninstant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n\nQuestion: Update the column title \"cnt\" to \"Total Users\".", "csv_data_example": "instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt\n1,2011-01-01,1,0,1,0,0,6,0,1,0.24,0.2879,0.81,0,3,13,16\n2,2011-01-01,1,0,1,1,0,6,0,1,0.22,0.2727,0.8,0,8,32,40\n3,2011-01-01,1,0,1,2,0,6,0,1,0.22,0.2727,0.8,0,5,27,32\n4,2011-01-01,1,0,1,3,0,6,0,1,0.24,0.2879,0.75,0,3,10,13\n5,2011-01-01,1,0,1,4,0,6,0,1,0.24,0.2879,0.75,0,0,1,1\n6,2011-01-01,1,0,1,5,0,6,0,2,0.24,0.2576,0.75,0.0896,0,1,1\n7,2011-01-01,1,0,1,6,0,6,0,1,0.22,0.2727,0.8,0,2,0,2\n8,2011-01-01,1,0,1,7,0,6,0,1,0.2,0.2576,0.86,0,1,2,3\n9,2011-01-01,1,0,1,8,0,6,0,1,0.24,0.2879,0.75,0,1,7,8\n10,2011-01-01,1,0,1,9,0,6,0,1,0.32,0.3485,0.76,0,8,6,14\n", "csv_path": "infiagent/csv/16.csv", "instruction": "Update the column title \"cnt\" to \"Total Users\".", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/16.csv')\n\n# Rename 'cnt' column to 'Total Users'\ndf = df.rename(columns={'cnt': 'Total Users'})\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the column title \"cnt\" to \"Total Users\"\ndf.rename(columns={'cnt': 'Total Users'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDepartment Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n\nQuestion: Update the column title 'Dept Group' to 'Department Group'.", "csv_data_example": "Department Name,coa_dept_id,github-dept-code,dept_group,budget_year_start,budget_year_end\nAnimal Services,92,ANM,Community Services,10/1/2016,9/30/2017\nAustin Code,16,COD,Community Services,10/1/2016,9/30/2017\nAustin Convention Center,88,CON,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Energy,11,ENE,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Public Library,85,LIB,Community Services,10/1/2016,9/30/2017\nAustin Resource Recovery,15,RES,Utility and Other Enterprises,10/1/2016,9/30/2017\nAustin Transportation,24,TRA,Infrastructure/Transportation,10/1/2016,9/30/2017\nAustin Water,22,WAT,Utility and Other Enterprises,10/1/2016,9/30/2017\nAviation,81,AVI,Utility and Other Enterprises,10/1/2016,9/30/2017\nBuilding Services,75,BLD,Support Services,10/1/2016,9/30/2017\n", "csv_path": "infiagent/csv/19.csv", "instruction": "Update the column title 'Dept Group' to 'Department Group'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/19.csv')\n\n# Rename the column 'Dept Group' to 'Department Group'\ndf = df.rename(columns={'Dept Group': 'Department Group'})\n\n# Print the first 10 rows of the processed table\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the column title 'Dept Group' to 'Department Group'\ndf.rename(columns={'dept_group': 'Department Group'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nHospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n\nQuestion: Change the column title 'Provider Number' to 'Provider ID'.", "csv_data_example": "Hospital Name,Provider Number,State,Measure Name,Number of Discharges,Footnote,Excess Readmission Ratio,Predicted Readmission Rate,Expected Readmission Rate,Number of Readmissions,Start Date,End Date\nFROEDTERT MEMORIAL LUTHERAN HOSPITAL,520177,WI,READM-30-HIP-KNEE-HRRP,242,,1.9095,10.8,5.6,38,07/01/2010,06/30/2013\nPROVIDENCE HOSPITAL,090006,DC,READM-30-HIP-KNEE-HRRP,247,,1.7521,9.2,5.3,33,07/01/2010,06/30/2013\nBEAUFORT COUNTY MEMORIAL HOSPITAL,420067,SC,READM-30-HIP-KNEE-HRRP,586,,1.5836,7.6,4.8,53,07/01/2010,06/30/2013\nADVOCATE CHRIST HOSPITAL & MEDICAL CENTER,140208,IL,READM-30-HIP-KNEE-HRRP,965,,1.5760,9.0,5.7,95,07/01/2010,06/30/2013\nBRAZOSPORT REGIONAL HEALTH SYSTEM,450072,TX,READM-30-HIP-KNEE-HRRP,149,,1.5308,8.2,5.4,20,07/01/2010,06/30/2013\nWESTERN MISSOURI MEDICAL CENTER,260097,MO,READM-30-HIP-KNEE-HRRP,141,,1.5189,8.1,5.3,19,07/01/2010,06/30/2013\nSAINT AGNES HOSPITAL,210011,MD,READM-30-HIP-KNEE-HRRP,390,,1.5079,7.8,5.2,38,07/01/2010,06/30/2013\nMERCY HOSPITAL JEFFERSON,260023,MO,READM-30-HIP-KNEE-HRRP,178,,1.5019,9.2,6.1,24,07/01/2010,06/30/2013\nONSLOW MEMORIAL HOSPITAL,340042,NC,READM-30-HIP-KNEE-HRRP,98,,1.4953,7.9,5.3,15,07/01/2010,06/30/2013\nFAUQUIER HOSPITAL,490023,VA,READM-30-HIP-KNEE-HRRP,256,,1.4844,7.4,5.0,26,07/01/2010,06/30/2013\n", "csv_path": "infiagent/csv/20.csv", "instruction": "Change the column title 'Provider Number' to 'Provider ID'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/20.csv')\n\n# Rename the column\ndata.rename(columns={'Provider Number': 'Provider ID'},inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Rename the column\ndf = df.rename(columns={'Provider Number': 'Provider ID'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n\nQuestion: Rename the 'damage_USD' column to 'Damage in USD'.", "csv_data_example": ",name,dates_active,max_storm_cat,max_sust_wind,min_p,areas_affected,damage_USD,deaths,year,damage_imputed\n0,ARLENE,April_19__ 21,1,43.4488,990.0,None,0.0,0.0,2017,0\n1,BRET,June_19__ 20,1,43.4488,1007.0,\"Guyana, Venezuela, Trinidad and Tobago, Windward Islands\",3000000.0,2.0,2017,0\n2,CINDY,June_20__ 23,1,52.13856,991.0,\"Honduras, Belize, Cayman Islands, Yucat??n Peninsula, Cuba, Southern United States, Eastern United States\",25000000.0,2.0,2017,0\n3,FOUR,July_5__ 7,0,26.06928,1009.0,None,0.0,0.0,2017,0\n4,DON,July_17__ 18,1,43.4488,1005.0,\"Windward Islands, Barbados, Trinidad and Tobago\",0.0,0.0,2017,0\n5,EMILY,July_30__ August_1,1,52.13856,1001.0,Florida,10000000.0,0.0,2017,0\n6,FRANKLIN,August_7__ 10,2,73.86296,981.0,\"Nicaragua, Honduras, Guatemala, Belize, Yucat??n Peninsula, Central Mexico\",15000000.0,0.0,2017,0\n7,GERT,August_12__ 17,3,95.58735999999999,962.0,\"Bermuda, East Coast of the United States, Atlantic Canada\",0.0,2.0,2017,0\n8,HARVEY,August_17__ September_1,5,112.96688,937.0,\"Barbados, Suriname, Guyana, Windward Islands, Nicaragua, Belize, Yucat??n Peninsula, Northeastern Mexico, Southern United States (Texas, Louisiana), Eastern United States\",125000000000.0,107.0,2017,0\n9,IRMA,August_30__ September 12,6,156.41568,914.0,\"Cape Verde, Leeward Islands (Barbuda, Saint Martin, Saint Barthelemy, U.S. Virgin Islands), Puerto Rico, Hispaniola, Turks and Caicos Islands, The Bahamas, Cuba, Southeastern United States (Florida and Georgia), Northeastern United States\",64760000000.00001,134.0,2017,0\n", "csv_path": "infiagent/csv/21.csv", "instruction": "Rename the 'damage_USD' column to 'Damage in USD'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/21.csv')\n\n# Rename the 'damage_USD' column to 'Damage in USD'\ndf.rename(columns={'damage_USD': 'Damage in USD'},inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Rename the 'damage_USD' column to 'Damage in USD'\ndata = data.rename(columns={'damage_USD': 'Damage in USD'})\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Rename the column \"people_fully_vaccinated_per_hundred\" to \"percentage_fully_vaccinated\".", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Rename the column \"people_fully_vaccinated_per_hundred\" to \"percentage_fully_vaccinated\".", "instruction_type": "Update-Format and Content", "reference_code": "# necessary libraries\nimport pandas as pd\n\n# load data\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# rename a column\ndf.rename(columns={'people_fully_vaccinated_per_hundred': 'percentage_fully_vaccinated'},inplace=True)\n\n# output results\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Rename the column\ndata.rename(columns={'people_fully_vaccinated_per_hundred': 'percentage_fully_vaccinated'}, inplace=True)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Rename the column 'source_name' to 'vaccine_source'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Rename the column 'source_name' to 'vaccine_source'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the CSV file\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Rename the 'source_name' column to 'vaccine_source'\ndf.rename(columns={'source_name': 'vaccine_source'},inplace=True)\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('data.csv')\n\n# Rename the column 'source_name' to 'vaccine_source'\ndf = df.rename(columns={'source_name': 'vaccine_source'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nserious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n\nQuestion: Rename the column 'number_of_time60-89_days_past_due_not_worse' as 'number_of_times_late'.", "csv_data_example": "serious_dlqin2yrs,revolving_utilization_of_unsecured_lines,age,number_of_time30-59_days_past_due_not_worse,debt_ratio,monthly_income,number_of_open_credit_lines_and_loans,number_of_times90_days_late,number_real_estate_loans_or_lines,number_of_time60-89_days_past_due_not_worse,number_of_dependents\n1,0.76612660899999996,45,2,0.8029821290000001,9120.0,13,0,6,0,2.0\n0,0.95715101899999999,40,0,0.121876201,2600.0,4,0,0,0,1.0\n0,0.65818014000000002,38,1,0.085113375000000005,3042.0,2,1,0,0,0.0\n0,0.233809776,30,0,0.036049682,3300.0,5,0,0,0,0.0\n0,0.90723940000000003,49,1,0.024925695000000001,63588.0,7,0,1,0,0.0\n0,0.21317868200000001,74,0,0.37560696900000001,3500.0,3,0,1,0,1.0\n0,0.30568246500000001,57,0,5710.0,,8,0,3,0,0.0\n0,0.75446364799999999,39,0,0.20994001700000001,3500.0,8,0,0,0,0.0\n0,0.11695064400000001,27,0,46.0,,2,0,0,0,0.0\n0,0.189169052,57,0,0.60629090100000005,23684.0,9,0,4,0,2.0\n", "csv_path": "infiagent/csv/23.csv", "instruction": "Rename the column 'number_of_time60-89_days_past_due_not_worse' as 'number_of_times_late'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from the CSV file\ndf = pd.read_csv('infiagent/csv/23.csv')\n\n# Rename the column as per the instruction\ndf = df.rename(columns={'number_of_time60-89_days_past_due_not_worse': 'number_of_times_late'})\n\n# Print the updated dataframe\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Rename the column\ndf = df.rename(columns={'number_of_time60-89_days_past_due_not_worse': 'number_of_times_late'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n\nQuestion: Rename the column 'SeriousDlqin2yrs' to 'Delinquencyin2Years'.", "csv_data_example": ",SeriousDlqin2yrs,RevolvingUtilizationOfUnsecuredLines,age,NumberOfTime30-59DaysPastDueNotWorse,DebtRatio,MonthlyIncome,NumberOfOpenCreditLinesAndLoans,NumberOfTimes90DaysLate,NumberRealEstateLoansOrLines,NumberOfTime60-89DaysPastDueNotWorse,NumberOfDependents\n1,,0.88551908,43,0,0.177512717,5700,4,0,0,0,0\n2,,0.463295269,57,0,0.527236928,9141,15,0,4,0,2\n3,,0.043275036,59,0,0.687647522,5083,12,0,1,0,2\n4,,0.280308229,38,1,0.925960637,3200,7,0,2,0,0\n5,,0.9999999,27,0,0.019917227,3865,4,0,0,0,1\n6,,0.509791452,63,0,0.342429365,4140,4,0,0,0,1\n7,,0.587778161,50,0,1048,0,5,0,0,0,3\n8,,0.046148938,79,1,0.3691702,3301,8,0,1,0,1\n9,,0.013527027,68,0,2024,NA,4,0,1,0,0\n10,,0.9999999,23,98,0,0,0,98,0,98,0\n", "csv_path": "infiagent/csv/25.csv", "instruction": "Rename the column 'SeriousDlqin2yrs' to 'Delinquencyin2Years'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/25.csv')\n\n# Changing the 'SeriousDlqin2yrs' column name to 'Delinquencyin2Years'\ndf.rename(columns={'SeriousDlqin2yrs':'Delinquencyin2Years'},inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Rename the column 'SeriousDlqin2yrs' to 'Delinquencyin2Years'\ndf = df.rename(columns={'SeriousDlqin2yrs': 'Delinquencyin2Years'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Rename the column \"table\" to \"table_value\" to avoid confusion with database terminology.", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Rename the column \"table\" to \"table_value\" to avoid confusion with database terminology.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Rename column 'table' to 'table_value'\ndf.rename(columns={'table': 'table_value'},inplace=True)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Rename the column \"table\" to \"table_value\"\ndf.rename(columns={'table': 'table_value'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nvotes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n\nQuestion: Rename the column 'state_abbr' to 'state'.", "csv_data_example": "votes_dem,votes_gop,total_votes,per_dem,per_gop,diff,per_point_diff,state_abbr,county_name,combined_fips\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02013\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02016\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02020\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02050\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02060\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02068\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02070\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02090\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02100\n93003.0,130413.0,246588.0,0.37715947248000004,0.528870018006,\"37,410\",15.17%,AK,Alaska,02105\n", "csv_path": "infiagent/csv/30.csv", "instruction": "Rename the column 'state_abbr' to 'state'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/30.csv')\n\n# Rename the column 'state_abbr' to 'state'\ndf = df.rename(columns={'state_abbr': 'state'})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Rename the column 'state_abbr' to 'state'\ndf = df.rename(columns={'state_abbr': 'state'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Transform each column label to lowercase.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Transform each column label to lowercase.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file into a pandas dataframe\ndf = pd.read_csv(\"infiagent/csv/31.csv\")\n\n# Convert the column names to lower case\ndf.columns = df.columns.str.lower()\n\n# Print the dataframe to verify the changes\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Transform each column label to lowercase\ndf.columns = df.columns.str.lower()\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nCountry,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n\nQuestion: Rename the column 'No. of cases' to 'Case Count' and 'No. of deaths' to 'Death Count'.", "csv_data_example": "Country,Year,No. of cases,No. of deaths,No. of cases_median,No. of cases_min,No. of cases_max,No. of deaths_median,No. of deaths_min,No. of deaths_max,WHO Region\nAfghanistan, 2017,630308[495000-801000],298[110-510],630308,495000,801000,298,110,510,Eastern Mediterranean\nAlgeria, 2017,0,0,0,,,0,,,Africa\nAngola, 2017,4615605[3106000-6661000],13316[9970-16600],4615605,3106000,6661000,13316,9970,16600,Africa\nArgentina, 2017,0,0,0,,,0,,,Americas\nArmenia, 2017,0,0,0,,,0,,,Europe\nAzerbaijan, 2017,0,0,0,,,0,,,Europe\nBangladesh, 2017,32924[30000-36000],76[3-130],32924,30000,36000,76,3,130,South-East Asia\nBelize, 2017,7,0,7,,,0,,,Americas\nBenin, 2017,4111699[2774000-6552000],7328[5740-8920],4111699,2774000,6552000,7328,5740,8920,Africa\nBhutan, 2017,11,0,11,,,0,,,South-East Asia\n", "csv_path": "infiagent/csv/32.csv", "instruction": "Rename the column 'No. of cases' to 'Case Count' and 'No. of deaths' to 'Death Count'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# load the csv file\ndf = pd.read_csv('infiagent/csv/32.csv')\n\n# rename the columns\ndf = df.rename(columns={'No. of cases': 'Case Count','No. of deaths': 'Death Count'})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('data.csv')\n\n# Rename the columns\ndata = data.rename(columns={'No. of cases': 'Case Count', 'No. of deaths': 'Death Count'})\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nyear,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n\nQuestion: Modify the column title 'gdppercap' to 'GDP-per-capita'.", "csv_data_example": "year,pop,lifeexp,gdppercap,country,continent\n1952,8425333,28.801,779.4453145,afghanistan,asia\n1957,9240934,30.332,820.8530296,afghanistan,asia\n1962,10267083,31.997,853.10071,afghanistan,asia\n1967,11537966,34.02,836.1971382,afghanistan,asia\n1972,13079460,36.088,739.9811058,afghanistan,asia\n1977,14880372,38.438,786.11336,afghanistan,asia\n1982,12881816,39.854,978.0114388,afghanistan,asia\n1987,13867957,40.822,852.3959448,afghanistan,asia\n1992,16317921,41.674,649.3413952,afghanistan,asia\n1997,22227415,41.763,635.341351,afghanistan,asia\n", "csv_path": "infiagent/csv/35.csv", "instruction": "Modify the column title 'gdppercap' to 'GDP-per-capita'.", "instruction_type": "Update-Format and Content", "reference_code": "# Import necessary library\nimport pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/35.csv')\n\n# Rename the column 'gdppercap' to 'GDP-per-capita'\ndf = df.rename(columns={'gdppercap': 'GDP-per-capita'})\n\n# Print the result\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Rename the column 'gdppercap' to 'GDP-per-capita'\ndata.rename(columns={'gdppercap': 'GDP-per-capita'}, inplace=True)\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Rename the column \"gdpPercap_1967\" to \"GDP_per_capita_1967\".", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Rename the column \"gdpPercap_1967\" to \"GDP_per_capita_1967\".", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# loading the data\ndata = pd.read_csv('infiagent/csv/36.csv')\n\n# renaming the column\ndata.rename(columns={\"gdpPercap_1967\": \"GDP_per_capita_1967\"},inplace=True)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Rename the column\ndf = df.rename(columns={'gdpPercap_1967': 'GDP_per_capita_1967'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n\nQuestion: Rename the column 'No. of Trades' to 'Number of Trades'.", "csv_data_example": "\"Symbol\",\"Series\",\"Date\",\"Prev Close\",\"Open Price\",\"High Price\",\"Low Price\",\"Last Price\",\"Close Price\",\"Average Price\",\"Total Traded Quantity\",\"Turnover\",\"No. of Trades\",\"Deliverable Qty\",\"% Dly Qt to Traded Qty\"\n\"GODREJIND\",\"EQ\",\"15-May-2017\",\"        564.60\",\"        581.00\",\"        584.00\",\"        568.50\",\"        578.90\",\"        578.55\",\"        578.09\",\"     797171\",\"           460836225.30\",\"      21649\",\"     360927\",\"         45.28\"\n\"GODREJIND\",\"EQ\",\"16-May-2017\",\"        578.55\",\"        581.45\",\"        589.00\",\"        572.25\",\"        583.80\",\"        584.80\",\"        583.60\",\"     500223\",\"           291930164.60\",\"      17204\",\"     210364\",\"         42.05\"\n\"GODREJIND\",\"EQ\",\"17-May-2017\",\"        584.80\",\"        583.00\",\"        594.00\",\"        576.85\",\"        584.90\",\"        588.60\",\"        588.74\",\"     504155\",\"           296814880.85\",\"       8567\",\"     261667\",\"         51.90\"\n\"GODREJIND\",\"EQ\",\"18-May-2017\",\"        588.60\",\"        582.00\",\"        588.85\",\"        571.20\",\"        572.25\",\"        574.60\",\"        580.90\",\"     223583\",\"           129878624.25\",\"       7144\",\"      99785\",\"         44.63\"\n\"GODREJIND\",\"EQ\",\"19-May-2017\",\"        574.60\",\"        581.00\",\"        585.80\",\"        567.55\",\"        579.85\",\"        578.00\",\"        577.31\",\"     245436\",\"           141692454.15\",\"       4969\",\"      68041\",\"         27.72\"\n\"GODREJIND\",\"EQ\",\"22-May-2017\",\"        578.00\",\"        584.45\",\"        586.75\",\"        562.35\",\"        567.00\",\"        565.95\",\"        576.10\",\"     460156\",\"           265093680.35\",\"       9762\",\"     131406\",\"         28.56\"\n\"GODREJIND\",\"EQ\",\"23-May-2017\",\"        565.95\",\"        576.00\",\"        581.55\",\"        555.55\",\"        558.80\",\"        561.30\",\"        571.72\",\"    1085338\",\"           620507087.55\",\"      20749\",\"     185396\",\"         17.08\"\n\"GODREJIND\",\"EQ\",\"24-May-2017\",\"        561.30\",\"        560.75\",\"        573.60\",\"        551.80\",\"        569.00\",\"        569.95\",\"        566.12\",\"     726457\",\"           411258378.25\",\"      13113\",\"     251584\",\"         34.63\"\n\"GODREJIND\",\"EQ\",\"25-May-2017\",\"        569.95\",\"        570.40\",\"        585.00\",\"        569.75\",\"        584.50\",\"        583.90\",\"        578.12\",\"     597536\",\"           345447321.45\",\"      12426\",\"     300989\",\"         50.37\"\n\"GODREJIND\",\"EQ\",\"26-May-2017\",\"        583.90\",\"        584.00\",\"        596.65\",\"        577.35\",\"        594.80\",\"        593.50\",\"        588.04\",\"     424831\",\"           249816977.85\",\"       7144\",\"     147182\",\"         34.64\"\n", "csv_path": "infiagent/csv/38.csv", "instruction": "Rename the column 'No. of Trades' to 'Number of Trades'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/38.csv')\n\n# Rename the column 'No. of Trades' to 'Number of Trades'\ndata = data.rename(columns={'No. of Trades': 'Number of Trades'})\n\nprint(data)", "assistant_code": "# import necessary library\nimport pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# rename the column\ndf = df.rename(columns={'No. of Trades': 'Number of Trades'})\n\n# print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n\nQuestion: Renames column 'name' to 'Superhero Name' for enhanced clarity.", "csv_data_example": ",name,Gender,Eye color,Race,Hair color,Height,Publisher,Skin color,Alignment,Weight\n0,A-Bomb,Male,yellow,Human,No Hair,203.0,Marvel Comics,-,good,441.0\n1,Abe Sapien,Male,blue,Icthyo Sapien,No Hair,191.0,Dark Horse Comics,blue,good,65.0\n2,Abin Sur,Male,blue,Ungaran,No Hair,185.0,DC Comics,red,good,90.0\n3,Abomination,Male,green,Human / Radiation,No Hair,203.0,Marvel Comics,-,bad,441.0\n4,Abraxas,Male,blue,Cosmic Entity,Black,-99.0,Marvel Comics,-,bad,-99.0\n5,Absorbing Man,Male,blue,Human,No Hair,193.0,Marvel Comics,-,bad,122.0\n6,Adam Monroe,Male,blue,-,Blond,-99.0,NBC - Heroes,-,good,-99.0\n7,Adam Strange,Male,blue,Human,Blond,185.0,DC Comics,-,good,88.0\n8,Agent 13,Female,blue,-,Blond,173.0,Marvel Comics,-,good,61.0\n9,Agent Bob,Male,brown,Human,Brown,178.0,Marvel Comics,-,good,81.0\n", "csv_path": "infiagent/csv/39.csv", "instruction": "Renames column 'name' to 'Superhero Name' for enhanced clarity.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# load the data from the csv into a pandas dataframe\ndf = pd.read_csv('infiagent/csv/39.csv')\n\n# rename the 'name' column to 'Superhero Name'\ndf = df.rename(columns={'name': 'Superhero Name'})\n\nprint(df)", "assistant_code": "# Importing required libraries\nimport pandas as pd\n\n# Loading the data from csv\ndf = pd.read_csv('data.csv')\n\n# Renaming the column 'name' to 'Superhero Name'\ndf.rename(columns={'name': 'Superhero Name'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nhotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n\nQuestion: Rename the column 'hotel_id' to 'Hotel_ID'.", "csv_data_example": "hotel_id,hotel_name,city_name,star_rating,bubble_score,review_count,hotel_type,brand_name,parent_brand_name\n75737,Night Theater District,New York City,4.0,40.0,2291,Hotel,Night Hotel,Wyndham Hotel Group\n93401,Heritage Hotel New York City,New York City,2.5,35.0,968,Hotel,None,\n224217,Clarion Hotel Park Avenue,New York City,2.5,35.0,462,Hotel,Clarion,\"Choice Hotels International, Inc.\"\n488793,Solita Soho Hotel,New York City,3.0,40.0,520,Hotel,Ascend Collection,\"Choice Hotels International, Inc.\"\n1028569,Greenwich Hotel,New York City,5.0,45.0,582,Hotel,The Leading Hotels of the World,\"The Leading Hotels of the World, Ltd\"\n1383001,Comfort Inn Manhattan Bridge,New York City,2.0,40.0,578,Hotel,Comfort Inn,\"Choice Hotels International, Inc.\"\n1733337,Sutton Court Hotel Residences,New York City,3.5,45.0,72,Hotel,,\n1776857,The Langham New York Fifth Avenue,New York City,5.0,45.0,2608,Hotel,Langham,Langham Hospitality Group\n2643161,The NoMad Hotel New York,New York City,5.0,45.0,964,Hotel,,\n7148761,Cassa Hotel Times Square,New York City,4.0,45.0,1601,Hotel,None,\n", "csv_path": "infiagent/csv/40.csv", "instruction": "Rename the column 'hotel_id' to 'Hotel_ID'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/40.csv')\n\n# Rename 'hotel_id' to 'Hotel_ID'\ndf = df.rename(columns={'hotel_id': 'Hotel_ID'})\n\n# Print the updated DataFrame\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Rename the column 'hotel_id' to 'Hotel_ID'\ndf.rename(columns={'hotel_id': 'Hotel_ID'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n\nQuestion: Update the column label \"AveBedrms\" to \"AverageBedrooms\".", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.67616191904048,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.038555691554468,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.586357039187228,0.9825834542815676,1486.0,2.816011574632264,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.051282051282051,779.0,2.816011574632264,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.816011574632264,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577464,814.0,3.821596244131456,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.816011574632264,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Update the column label \"AveBedrms\" to \"AverageBedrooms\".", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Rename the column \"AveBedrms\" to \"AverageBedrooms\"\ndf = df.rename(columns={'AveBedrms': 'AverageBedrooms'})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Rename the column \"AveBedrms\" to \"AverageBedrooms\"\ndf = df.rename(columns={'AveBedrms': 'AverageBedrooms'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n\nQuestion: Rename the column 'SUBJECT' to 'TOPIC'.", "csv_data_example": "??\"LOCATION\",\"INDICATOR\",\"SUBJECT\",\"MEASURE\",\"FREQUENCY\",\"TIME\",\"Value\",\"Flag Codes\"\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.6,\n\"AUS\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",4,\n\"AUS\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.9,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.8,\n\"AUT\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.7,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2012\",3.1,\n\"AUT\",\"EDUEXP\",\"PRY_NTRY\",\"PC_GDP\",\"A\",\"2013\",3.2,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2012\",1.4,\n\"BEL\",\"EDUEXP\",\"TRY\",\"PC_GDP\",\"A\",\"2013\",1.4,\n", "csv_path": "infiagent/csv/46.csv", "instruction": "Rename the column 'SUBJECT' to 'TOPIC'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# loading the dataset\ndf = pd.read_csv('infiagent/csv/46.csv')\n\n# renaming the 'SUBJECT' column to 'TOPIC'\ndf = df.rename(columns={'SUBJECT': 'TOPIC'})\n\nprint(df)", "assistant_code": "# import pandas library\nimport pandas as pd\n\n# load the data from csv\ndf = pd.read_csv('data.csv')\n\n# rename the column 'SUBJECT' to 'TOPIC'\ndf.rename(columns={'SUBJECT': 'TOPIC'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nYear,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n\nQuestion: Rename the column 'Public Administration' to 'Pub Admin'.", "csv_data_example": "Year,Agriculture,Architecture,Art and Performance,Biology,Business,Communications and Journalism,Computer Science,Education,Engineering,English,Foreign Languages,Health Professions,Math and Statistics,Physical Sciences,Psychology,Public Administration,Social Sciences and History\n1970,4.22979798,11.92100539,59.7,29.08836297,9.064438975,35.3,13.6,74.53532758,0.8,65.57092343,73.8,77.1,38,13.8,44.4,68.4,36.8\n1971,5.452796685,12.00310559,59.9,29.39440285,9.503186594,35.5,13.6,74.14920369,1,64.55648516,73.9,75.5,39,14.9,46.2,65.5,36.2\n1972,7.42071022,13.21459351,60.4,29.81022105,10.5589621,36.6,14.9,73.55451996,1.2,63.6642632,74.6,76.9,40.2,14.8,47.6,62.6,36.1\n1973,9.653602412,14.7916134,60.2,31.14791477,12.80460152,38.4,16.4,73.50181443,1.6,62.94150212,74.9,77.4,40.9,16.5,50.4,64.3,36.4\n1974,14.07462346,17.44468758,61.9,32.99618284,16.20485038,40.5,18.9,73.33681143,2.2,62.41341209,75.3,77.9,41.8,18.2,52.6,66.1,37.3\n1975,18.33316153,19.13404767,60.9,34.44990213,19.68624931,41.5,19.8,72.80185448,3.2,61.64720641,75,78.9,40.7,19.1,54.5,63,37.7\n1976,22.25276005,21.39449143,61.3,36.07287146,23.4300375,44.3,23.9,72.16652471,4.5,62.14819377,74.4,79.2,41.5,20,56.9,65.6,39.2\n1977,24.6401766,23.74054054,62,38.33138629,27.16342715,46.9,25.7,72.45639481,6.8,62.72306675,74.3,80.5,41.1,21.3,59,69.3,40.5\n1978,27.14619175,25.84923973,62.5,40.11249564,30.52751868,49.9,28.1,73.19282134,8.4,63.61912216,74.3,81.9,41.6,22.5,61.3,71.5,41.8\n1979,29.63336549,27.77047744,63.2,42.06555109,33.62163381,52.3,30.2,73.82114234,9.4,65.08838972,74.2,82.3,42.3,23.7,63.3,73.3,43.6\n", "csv_path": "infiagent/csv/47.csv", "instruction": "Rename the column 'Public Administration' to 'Pub Admin'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/47.csv')\n\n# Rename the column\ndf = df.rename(columns={'Public Administration': 'Pub Admin'})\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Rename the column\ndf.rename(columns={'Public Administration': 'Pub Admin'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n\nQuestion: Rename the column 'class_three'.", "csv_data_example": "ID,class1,class2,class3,class4,class5,class6,class7,class8,class9\n1,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n2,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n5,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n10,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n12,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n13,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n14,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n21,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n24,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n26,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0\n", "csv_path": "infiagent/csv/48.csv", "instruction": "Rename the column 'class_three'.", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the data from csv file\ndata = pd.read_csv('infiagent/csv/48.csv')\n\n# Rename the column 'class3' to 'class_three'\ndata = data.rename(columns={'class3':'class_three'})\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# load the data\ndf = pd.read_csv('data.csv')\n\n# rename the column 'class_three'\ndf = df.rename(columns={'class3': 'class_three'})\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Rename the column \"Volume\" to \"Trading Volume\".", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Rename the column \"Volume\" to \"Trading Volume\".", "instruction_type": "Update-Format and Content", "reference_code": "import pandas as pd\n\n# Load the CSV file into a DataFrame\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Rename the column \"Volume\" to \"Trading Volume\"\ndf = df.rename(columns={'Volume': 'Trading Volume'})\n\nprint(df)", "assistant_code": "# import necessary library\nimport pandas as pd\n\n# load the csv file\ndf = pd.read_csv('data.csv')\n\n# rename the column \"Volume\" to \"Trading Volume\"\ndf.rename(columns={'Volume': 'Trading Volume'}, inplace=True)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Update the number_of_runs column value to 'High' if its value is above 70, 'Medium' if it's between 40 and 70, and 'Low' if it's less than 40.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Update the number_of_runs column value to 'High' if its value is above 70, 'Medium' if it's between 40 and 70, and 'Low' if it's less than 40.", "instruction_type": "Update-Grouping", "reference_code": "import pandas as pd\n\n# Load the data from csv file into a Pandas DataFrame\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Define a function to categorize 'number_of_runs' into 'High','Medium','Low'\ndef categorize_runs(val):\n    if val > 70:\n        return 'High'\n    elif 40 <= val <= 70:\n        return 'Medium'\n    else:\n        return 'Low'\n        \n# Apply the function to 'number_of_runs' column to update the data\ndf['number_of_runs'] = df['number_of_runs'].apply(categorize_runs)\n\n# Print the DataFrame to show the updated data\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'number_of_runs' column based on the given conditions\ndf['number_of_runs'] = df['number_of_runs'].apply(lambda x: 'High' if x > 70 else ('Medium' if 40 <= x <= 70 else 'Low'))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n\nQuestion: Update the 'description' column to group the similar weather conditions into broader categories such as 'Rain', 'Cloud', 'Sun', etc.", "csv_data_example": ",temp,humidity,pressure,description,dt,wind_speed,wind_deg,city,day,dist\n0,32.18000000000001,54,1010,moderate rain,1437730851,2.11,330.003,Ravenna,2015-07-24 11:40:51,8\n1,32.370000000000005,62,1010,moderate rain,1437734494,2.6,20,Ravenna,2015-07-24 12:41:34,8\n2,32.79000000000002,75,1009,moderate rain,1437738046,3.6,70,Ravenna,2015-07-24 13:40:46,8\n3,32.75,79,1009,moderate rain,1437741580,5.1,70,Ravenna,2015-07-24 14:39:40,8\n4,32.72000000000003,70,1008,moderate rain,1437745188,3.1,10,Ravenna,2015-07-24 15:39:48,8\n5,32.09000000000003,74,1008,moderate rain,1437748778,2.6,80,Ravenna,2015-07-24 16:39:38,8\n6,31.840000000000032,37,1007,moderate rain,1437752544,2.06,126,Ravenna,2015-07-24 17:42:24,8\n7,31.400000000000034,74,1007,moderate rain,1437756043,1.5,100,Ravenna,2015-07-24 18:40:43,8\n8,31.32000000000005,47,1007,moderate rain,1437759569,2.06,135,Ravenna,2015-07-24 19:39:29,8\n9,30.52000000000004,59,1007,moderate rain,1437763244,1.5,130,Ravenna,2015-07-24 20:40:44,8\n", "csv_path": "infiagent/csv/50.csv", "instruction": "Update the 'description' column to group the similar weather conditions into broader categories such as 'Rain', 'Cloud', 'Sun', etc.", "instruction_type": "Update-Grouping", "reference_code": "import pandas as pd\n\n# Load the data\ndata = pd.read_csv('infiagent/csv/50.csv')\n\n# Function to categorize description\ndef categorize_weather(description):\n    if 'rain' in description.lower():\n        return 'Rain'\n    elif 'cloud' in description.lower():\n        return 'Cloud'\n    elif 'sun' in description.lower():\n        return 'Sun'\n    else:\n        return 'Other'\n\n# Update 'description' column\ndata['description'] = data['description'].apply(categorize_weather)\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Update the 'description' column \ndf['description'] = df['description'].apply(lambda x: 'Rain' if 'rain' in x else ('Cloud' if 'cloud' in x else 'Sun' if 'sun' in x else x))\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nSource,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n\nQuestion: Change the 'Type' to 'Undirected' for rows where the 'Weight' is less than 10.", "csv_data_example": "Source,Target,Weight,Type,lng_org,lat_org,lng_dest,lat_dest\n0,1.0,10,Directed,51.048332,16.960160000000002,51.089356,17.001061\n0,13.0,129,Directed,51.048332,16.960160000000002,51.053396,16.971785\n0,14.0,168,Directed,51.048332,16.960160000000002,51.047068,16.95721\n0,15.0,1,Directed,51.048332,16.960160000000002,51.116737,17.033555\n0,16.0,1,Directed,51.048332,16.960160000000002,51.117021,17.042163000000002\n0,19.0,5,Directed,51.048332,16.960160000000002,51.098761,17.036521\n0,20.0,8,Directed,51.048332,16.960160000000002,51.094988,17.032866000000002\n0,24.0,1,Directed,51.048332,16.960160000000002,51.091722,17.04042\n0,26.0,8,Directed,51.048332,16.960160000000002,51.094516,16.980204\n0,27.0,1,Directed,51.048332,16.960160000000002,51.09599,16.988189000000002\n", "csv_path": "infiagent/csv/5.csv", "instruction": "Change the 'Type' to 'Undirected' for rows where the 'Weight' is less than 10.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the CSV data file\ndata = pd.read_csv('infiagent/csv/5.csv')\n\n# When 'Weight' is less than 10,update the 'Type' value to 'Undirected'\ndata.loc[data['Weight'] < 10,'Type'] = 'Undirected'\n\nprint(data)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update the 'Type' column to 'Undirected' where 'Weight' is less than 10\ndf.loc[df['Weight'] < 10, 'Type'] = 'Undirected'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n\nQuestion: Set 'Disqualifying_Offense' to True for rows where 'Age' is no greater than 16.", "csv_data_example": "Date,Offense,Age,Count,Category,Disqualifying_Offense,Offense_Type,Expungible\n1991,Aggravated Assault,15,1.0,Crimes Against Person,True,misdemeanor,False\n1991,All Other Larceny,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,All Other Offenses,13,1.0,Group B Offenses,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,10,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,14,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,15,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,16,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,17,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,18,1.0,Crimes Against Property,False,misdemeanor,True\n1991,Burglary/Breaking & Entering,19,1.0,Crimes Against Property,False,misdemeanor,True\n", "csv_path": "infiagent/csv/10.csv", "instruction": "Set 'Disqualifying_Offense' to True for rows where 'Age' is no greater than 16.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the data from csv\ndf = pd.read_csv('infiagent/csv/10.csv')\n\n# Update 'Disqualifying_Offense' based on condition\ndf.loc[df['Age'] <= 16,'Disqualifying_Offense'] = True\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Set 'Disqualifying_Offense' to True for rows where 'Age' is no greater than 16\ndf.loc[df['Age'] <= 16, 'Disqualifying_Offense'] = True\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n\nQuestion: Set the 'Close' value to 0 for all rows where the 'Volume' is less than 5,000,000.", "csv_data_example": "Date,Open,High,Low,Close,Volume,Market Cap\n\"Sep 17, 2017\",109.75,110.94,102.81,106.84,\"5,350,380\",\"737,226,000\"\n\"Sep 16, 2017\",111.11,116.01,105.02,109.85,\"5,683,580\",\"744,652,000\"\n\"Sep 15, 2017\",97.42,113.75,89.36,111.22,\"8,539,660\",\"652,107,000\"\n\"Sep 14, 2017\",115.97,117.38,96.71,96.71,\"6,367,800\",\"775,543,000\"\n\"Sep 13, 2017\",123.14,123.70,112.60,115.97,\"6,315,510\",\"822,282,000\"\n\"Sep 12, 2017\",125.29,130.91,122.22,123.50,\"6,861,080\",\"835,911,000\"\n\"Sep 11, 2017\",121.88,128.20,120.76,125.70,\"7,083,720\",\"812,292,000\"\n\"Sep 10, 2017\",129.70,129.70,118.94,122.92,\"4,644,030\",\"863,579,000\"\n\"Sep 09, 2017\",128.51,131.51,125.21,130.05,\"6,225,680\",\"854,520,000\"\n\"Sep 08, 2017\",137.68,139.78,123.23,128.29,\"7,051,050\",\"914,556,000\"\n", "csv_path": "infiagent/csv/17.csv", "instruction": "Set the 'Close' value to 0 for all rows where the 'Volume' is less than 5,000,000.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\ntable = pd.read_csv('infiagent/csv/17.csv')\n\ntable['Volume'] = table['Volume'].str.replace(',', '').astype(int)\ntable.loc[table['Volume'] < 5000000, 'Close'] = 0\n\nprint(table)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Replace the ',' with '' in 'Volume' column and convert it to numeric\ndata['Volume'] = pd.to_numeric(data['Volume'].str.replace(',', ''))\n\n# Set the 'Close' value to 0 where 'Volume' is less than 5000000\ndata.loc[data['Volume'] < 5000000, 'Close'] = 0\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n\nQuestion: Update all 'iso_code' where 'country' is 'Albania' to 'AL'.", "csv_data_example": "country,iso_code,date,total_vaccinations,people_vaccinated,people_fully_vaccinated,daily_vaccinations_raw,daily_vaccinations,total_vaccinations_per_hundred,people_vaccinated_per_hundred,people_fully_vaccinated_per_hundred,daily_vaccinations_per_million,vaccines,source_name,source_website\nAlbania,ALB,2021-01-10,0.0,0.0,,,,0.0,0.0,,,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-11,,,,,64.0,,,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-12,128.0,128.0,,,64.0,0.0,0.0,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-13,188.0,188.0,,60.0,63.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-14,266.0,266.0,,78.0,66.0,0.01,0.01,,23.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-15,308.0,308.0,,42.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-16,369.0,369.0,,61.0,62.0,0.01,0.01,,22.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-17,405.0,405.0,,36.0,58.0,0.01,0.01,,20.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-18,447.0,447.0,,42.0,55.0,0.02,0.02,,19.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\nAlbania,ALB,2021-01-19,483.0,483.0,,36.0,51.0,0.02,0.02,,18.0,Pfizer/BioNTech,Ministry of Health,https://shendetesia.gov.al/vaksinimi-anticovid-kryhen-424-vaksinime-ne-dy-qendrat-e-vaksinimit-ne-shkoder-dhe-tirane/\n", "csv_path": "infiagent/csv/22.csv", "instruction": "Update all 'iso_code' where 'country' is 'Albania' to 'AL'.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/22.csv')\n\n# Check if the country column contains 'Albania'\nhas_albania = df['country'].str.contains('Albania')\n\n# Update 'iso_code' where 'country' is 'Albania' to 'AL'\ndf.loc[has_albania,'iso_code'] = 'AL'\n\n# Save the updated dataframe to the csv file\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndata = pd.read_csv('data.csv')\n\n# Update the 'iso_code' where 'country' is 'Albania' to 'AL'\ndata.loc[data['country'] == 'Albania', 'iso_code'] = 'AL'\n\nprint(data)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Update the 'Income' column by replacing all values less than 50 with the mean 'Income' value.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Update the 'Income' column by replacing all values less than 50 with the mean 'Income' value.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Calculate the mean of 'Income' column\nmean_income = df['Income'].mean()\n\n# Replace all values below 50 in 'Income' column with the mean 'Income' value\ndf.loc[df['Income'] < 50,'Income'] = mean_income\n\nprint(df)", "assistant_code": "import pandas as pd\nimport numpy as np\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the mean of 'Income'\nmean_income = df['Income'].mean()\n\n# Replace values in 'Income' less than 50 with the mean 'Income'\ndf['Income'] = np.where(df['Income'] < 50, mean_income, df['Income'])\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n\nQuestion: Modify the \"color\" to \"Colorless\" for rows where \"color\" is either \"D\" or \"E\" and \"clarity\" is either \"IF\" or \"VVS1\".", "csv_data_example": "\"carat\",\"cut\",\"color\",\"clarity\",\"depth\",\"table\",\"price\",\"x\",\"y\",\"z\"\n0.23,\"Ideal\",\"E\",\"SI2\",61.5,55,326,3.95,3.98,2.43\n0.21,\"Premium\",\"E\",\"SI1\",59.8,61,326,3.89,3.84,2.31\n0.23,\"Good\",\"E\",\"VS1\",56.9,65,327,4.05,4.07,2.31\n0.29,\"Premium\",\"I\",\"VS2\",62.4,58,334,4.2,4.23,2.63\n0.31,\"Good\",\"J\",\"SI2\",63.3,58,335,4.34,4.35,2.75\n0.24,\"Very Good\",\"J\",\"VVS2\",62.8,57,336,3.94,3.96,2.48\n0.24,\"Very Good\",\"I\",\"VVS1\",62.3,57,336,3.95,3.98,2.47\n0.26,\"Very Good\",\"H\",\"SI1\",61.9,55,337,4.07,4.11,2.53\n0.22,\"Fair\",\"E\",\"VS2\",65.1,61,337,3.87,3.78,2.49\n0.23,\"Very Good\",\"H\",\"VS1\",59.4,61,338,4,4.05,2.39\n", "csv_path": "infiagent/csv/28.csv", "instruction": "Modify the \"color\" to \"Colorless\" for rows where \"color\" is either \"D\" or \"E\" and \"clarity\" is either \"IF\" or \"VVS1\".", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the csv file into a DataFrame\ndf = pd.read_csv('infiagent/csv/28.csv')\n\n# Apply the conditional update to the 'color' column\ndf.loc[(df['color'].isin(['D','E'])) & (df['clarity'].isin(['IF','VVS1'])),'color'] = 'Colorless'\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Modify the \"color\" to \"Colorless\" for rows where \"color\" is \"D\" or \"E\" and \"clarity\" is \"IF\" or \"VVS1\"\ndf.loc[((df['color'].isin(['D', 'E'])) & (df['clarity'].isin(['IF', 'VVS1'])))], 'color'] = 'Colorless'\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nDate,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n\nQuestion: Update the 'Open' price and increase it by 5% for all entries in the year 2014.", "csv_data_example": "Date,Open,High,Low,Close,Volume\n2014-01-02,79.38,79.58,78.86,79.02,58791957\n2014-01-03,78.98,79.1,77.2,77.28,98303870\n2014-01-06,76.78,78.11,76.23,77.7,103359151\n2014-01-07,77.76,77.99,76.85,77.15,79432766\n2014-01-08,76.97,77.94,76.96,77.64,64686685\n2014-01-09,78.11,78.12,76.48,76.65,69905199\n2014-01-10,77.12,77.26,75.87,76.13,76320664\n2014-01-13,75.7,77.5,75.7,76.53,94860843\n2014-01-14,76.89,78.1,76.81,78.06,83734371\n2014-01-15,79.07,80.03,78.81,79.62,98472619\n", "csv_path": "infiagent/csv/29.csv", "instruction": "Update the 'Open' price and increase it by 5% for all entries in the year 2014.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load data from the csv file\ndf = pd.read_csv('infiagent/csv/29.csv')\n\n# Convert 'Date' column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Update the 'Open' price and increase it by 5% for all entries in the year 2014\ndf.loc[df['Date'].dt.year == 2014,'Open'] = df.loc[df['Date'].dt.year == 2014,'Open'] * 1.05\n\n# Save the changes back to the csv file\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('data.csv')\n\n# Convert the 'Date' column to datetime format\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Extract the year from the 'Date' column\ndf['Year'] = df['Date'].dt.year\n\n# Update 'Open' price by increasing it by 5% for all entries in the year 2014\ndf.loc[df['Year'] == 2014, 'Open'] = df.loc[df['Year'] == 2014, 'Open'] * 1.05\n\n# Print the updated DataFrame\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\nQuestion: Set all negative values in columns 'EX1' to 'EX8' to zero.", "csv_data_example": "\"\",\"EQ1\",\"EQ2\",\"EQ3\",\"EQ4\",\"EQ5\",\"EQ6\",\"EQ7\",\"EQ8\",\"EX1\",\"EX2\",\"EX3\",\"EX4\",\"EX5\",\"EX6\",\"EX7\",\"EX8\",\"NZ\"\n\"1\",0.00081776112958284,-0.208033710010569,-0.0209965026800568,-0.0642619047779573,0.134687464025311,0.0508805382290177,-0.0192616021801591,-0.0219164236380736,0.0410182786002027,0.0426168252660709,-0.0080529851935652,-0.0730019227052621,-0.110649838313963,-0.0272364598603708,-0.0563269342288962,0.200716505553366,0.362830325973427\n\"2\",0.0338007933560907,0.0303187465307899,-0.0419930053601136,-0.0162345864702208,0.0869856538496802,0.0769627933528486,-0.0200465944831218,-0.252665055370363,0.0300434883392789,0.0263969887842081,-0.00192173510300988,-0.094665094890157,-0.098188303125206,-0.00959030276773619,-0.0850724761803679,0.241603201129052,0.3381555792905\n\"3\",0.0166278096348511,0.0990492914551702,-0.0681647023627196,-0.085908020071585,0.116167937721831,0.0821742091932279,-0.0115054095747666,-0.171574287909491,0.0237329839392477,0.0257609167653115,-0.0111643658365336,-0.114399354483246,-0.0780994489847098,-0.0333742536317219,-0.111310618833892,0.307269712205153,0.0146529160116964\n\"4\",0.00463397973430276,0.203353027030218,-0.0678689769728596,-0.0987604010271765,0.11336194888797,0.0588109536382906,-0.025916462300798,-0.144022212478769,0.0128953785565855,0.0337118170015188,-0.0183022390762845,-0.0908072697065456,-0.0642413624385921,-0.00613779377135116,-0.103877970914975,0.323376592280424,-0.36930248144376\n\"5\",-0.0365266637880335,0.54078497298146,-0.0344520079186847,-0.040586466175552,0.0488242057091753,0.0137460533760731,-0.0260687742401788,-0.319666693349616,0.0026065126869694,0.0149476924440696,-0.0130861009395434,-0.0310109793605687,-0.0482347353426887,-0.0118919754319929,-0.0825650768824202,0.209389440978512,-0.383436113240686\n\"6\",-0.0250780079738737,0.474591056252882,0.00768886013635882,-0.055468170439921,-0.0202031196037967,-0.0231114963355954,-0.00499114509047918,-0.316848867453293,-0.00713361366960046,0.0181280525385525,-0.00494160455059683,-0.00519322620870767,-0.052854097524728,-0.0210986660890196,-0.0665356313705403,0.039647704800665,-0.154171177528628\n\"7\",-0.0264409431898451,0.262601414175049,0.0344520079186847,-0.071026315807216,-0.104382784619616,-0.020795311517649,-0.0295016510277619,0.0284913507294957,-0.0111119751391853,0.03721021310545,-0.00155569032148419,0,-0.06606762190591,-0.0149608723176685,-0.0462077870621788,-0.0582325664259767,-0.208110086071965\n\"8\",-0.0109034817277712,0.27117763331324,0.0360784975629145,-0.00338220551462933,-0.140860639459805,0.0169182195397822,-0.0303335085428418,0.317788142752067,-0.00727079854786201,0.0222625206613803,-0.0111643658365336,-0.0847237761477737,-0.0526392434697494,-0.06943379203841,-0.0204173942832883,-0.073100455726226,-0.313322994728527\n\"9\",0.0128115910301312,-0.0264534083276613,0.0220315415445666,-0.0121759398526656,-0.202031196037967,0.05772838899512,-0.0260570579371495,0.159363709025421,-0.0109747902609238,0.00222625206613803,-0.0291920713266738,-0.117366912316793,-0.00601591353939993,-0.0444990048422959,0.0227456936313826,-0.0179653662378013,-0.0604205326815528\n\"10\",-0.0169003966780454,-0.548757233025412,0.0156734456625776,-0.0493802005135883,-0.317076738226253,0.0803111909700971,-0.0709187822363157,-0.207579841029183,-0.0219495805218476,0.00731482821731068,-0.04438292975999,-0.0501517273869484,0.0553249191569815,0.0337578657424314,0.066356531420687,0.0291162832129883,-0.11687746493838\n", "csv_path": "infiagent/csv/31.csv", "instruction": "Set all negative values in columns 'EX1' to 'EX8' to zero.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/31.csv')\n\n# Columns that needs to be updated\ncols_to_update = ['EX1','EX2','EX3','EX4','EX5','EX6','EX7','EX8']\n\n# Replace negative values with 0 in the specified columns\nfor col in cols_to_update:\n    df[col] = df[col].apply(lambda x : 0 if x < 0 else x)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Set all negative values in columns 'EX1' to 'EX8' to zero\nfor col in df.columns:\n    if 'EX' in col:\n        df[col] = df[col].apply(lambda x: x if x >= 0 else 0)\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\ncountry,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\n\nQuestion: Increase the GDP per capita for 2007 by 10% for countries that had a GDP per capita less than $1000 in 2002.", "csv_data_example": "country,gdpPercap_1952,gdpPercap_1957,gdpPercap_1962,gdpPercap_1967,gdpPercap_1972,gdpPercap_1977,gdpPercap_1982,gdpPercap_1987,gdpPercap_1992,gdpPercap_1997,gdpPercap_2002,gdpPercap_2007\nAfghanistan,779.4453145,820.8530296,853.10071,836.1971382,739.9811058,786.11336,978.0114388,852.3959448,649.3413952,635.341351,726.7340548,974.5803384\nBahrain,9867.084765,11635.79945,12753.27514,14804.6727,18268.65839,19340.10196,19211.14731,18524.02406,19035.57917,20292.01679,23403.55927,29796.04834\nBangladesh,684.2441716,661.6374577,686.3415538,721.1860862,630.2336265,659.8772322,676.9818656,751.9794035,837.8101643,972.7700352,1136.39043,1391.253792\nCambodia,368.4692856,434.0383364,496.9136476,523.4323142,421.6240257,524.9721832,624.4754784,683.8955732,682.3031755,734.28517,896.2260153,1713.778686\nChina,400.4486107,575.9870009,487.6740183,612.7056934,676.9000921,741.2374699,962.4213805,1378.904018,1655.784158,2289.234136,3119.280896,4959.114854\nHong Kong China,3054.421209,3629.076457,4692.648272,6197.962814,8315.928145,11186.14125,14560.53051,20038.47269,24757.60301,28377.63219,30209.01516,39724.97867\nIndia,546.5657493,590.061996,658.3471509,700.7706107,724.032527,813.337323,855.7235377,976.5126756,1164.406809,1458.817442,1746.769454,2452.210407\nIndonesia,749.6816546,858.9002707,849.2897701,762.4317721,1111.107907,1382.702056,1516.872988,1748.356961,2383.140898,3119.335603,2873.91287,3540.651564\nIran,3035.326002,3290.257643,4187.329802,5906.731805,9613.818607,11888.59508,7608.334602,6642.881371,7235.653188,8263.590301,9240.761975,11605.71449\nIraq,4129.766056,6229.333562,8341.737815,8931.459811,9576.037596,14688.23507,14517.90711,11643.57268,3745.640687,3076.239795,4390.717312,4471.061906\n", "csv_path": "infiagent/csv/36.csv", "instruction": "Increase the GDP per capita for 2007 by 10% for countries that had a GDP per capita less than $1000 in 2002.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# load the data from the specified csv file\ndf = pd.read_csv('infiagent/csv/36.csv')\n\n# filter the countries by condition 'GDP per capita less than $1000 in 2002'\nfiltered_df = df[df['gdpPercap_2002'] < 1000]\n\n# update the values of GDP per capita for 2007 for the filtered countries\nfiltered_df['gdpPercap_2007'] = filtered_df['gdpPercap_2007'] * 1.10\n\n# update the values in the original dataframe\ndf.update(filtered_df)\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Check the GDP per capita for 2002 and increase it by 10% for the countries that had GDP per capita less than $1000 in 2002\ndf.loc[df['gdpPercap_2002'] < 1000, 'gdpPercap_2007'] *= 1.1\n\n# Print the updated GDP per capita for 2007\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n\nQuestion: Set the 'gender' of all members with an 'Age' less than 10 to 0.", "csv_data_example": ",id,transcript_id,gloss,part_of_speech,Impaired,Age,Gender\n0,3772732,9573,and she asked um where do you wanna go,coord pro0sub v adv0int mod pro0per v v,1,10,0\n1,3772777,9573,and the kids I guess answered Mcdonalds,coord det0art n v n0prop,1,10,0\n2,3772812,9573,and so they went to Mcdonalds,coord co pro0sub v prep n0prop,1,10,0\n3,3772852,9573,and Lisa couldn't make up her mind,coord n0prop mod v prep det0poss n,1,10,0\n4,3772884,9573,and um she was deciding between a Big_Mac or a happymeal,coord pro0sub aux part prep det0art n0prop coord det0art,1,10,0\n5,3772934,9573,and Raymond ordered cheeseburger a large vanilla vanilla shake and french fries,coord n0prop v det0art adj n n coord adj n,1,10,0\n6,3772996,9573,and the mother ordered a salad,coord det0art n v det0art n,1,10,0\n7,3773020,9573,and the mother reached down to get the purse,coord det0art n v adv inf v det0art n,1,10,0\n8,3773061,9573,and um sh it wasn't there,coord pro0per cop adv,1,10,0\n9,3773088,9573,and so that's it,coord co pro0dem pro0per,1,10,0\n", "csv_path": "infiagent/csv/37.csv", "instruction": "Set the 'gender' of all members with an 'Age' less than 10 to 0.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('infiagent/csv/37.csv')\n\n# Update the 'Gender' of all members with 'Age' smaller than 10 to 0\ndf.loc[df['Age'] < 10,'Gender'] = 0\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Set the 'Gender' of all members with an 'Age' less than 10 to 0\ndf.loc[df['Age'] < 10, 'Gender'] = 0\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nMedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n\nQuestion: Update the \"AveOccup\" values where Latitude is greater than 35 to the median of 'AveOccup'.", "csv_data_example": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n0.9298,36.0,3.6761619190404797,1.1004497751124438,2664.0,3.9940029985007497,33.93,-118.25,1.0\n2.7006,17.0,4.499388004895961,1.039779681762546,3331.0,2.0385556915544676,32.79,-117.03,1.663\n5.0286,30.0,6.184375,1.06875,999.0,3.121875,34.89,-120.43,1.58\n3.9038,21.0,3.5863570391872277,0.9825834542815675,1486.0,2.1567489114658924,37.36,-122.02,2.438\n7.1754,52.0,7.42948717948718,1.0512820512820513,779.0,2.496794871794872,37.9,-122.28,3.629\n4.1292,28.0,3.9095238095238094,1.0333333333333334,1575.0,3.75,33.82,-118.27,2.019\n2.6062,33.0,6.997518610421836,1.2779156327543424,976.0,2.4218362282878414,41.4,-120.12,0.526\n5.222,42.0,5.8474576271186445,1.0527306967984935,1253.0,2.35969868173258,34.05,-118.39,5.00001\n2.2917,37.0,3.727699530516432,0.9859154929577465,814.0,3.8215962441314555,33.97,-118.25,1.12\n3.2292,52.0,7.075313807531381,1.2594142259414225,618.0,2.585774058577406,36.47,-120.95,2.25\n", "csv_path": "infiagent/csv/45.csv", "instruction": "Update the \"AveOccup\" values where Latitude is greater than 35 to the median of 'AveOccup'.", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('infiagent/csv/45.csv')\n\n# Calculate the median of `AveOccup`\nmedian_ave_occup = df['AveOccup'].median()\n\n# Update the 'AveOccup' values where Latitude is greater than 35 to the median of 'AveOccup'\ndf.loc[df['Latitude'] > 35,'AveOccup'] = median_ave_occup\n\n# Save the results\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of 'AveOccup'\nmedian_aveoccup = df['AveOccup'].median()\n\n# Update the 'AveOccup' values where Latitude is greater than 35 to the median of 'AveOccup'\ndf.loc[df['Latitude'] > 35, 'AveOccup'] = median_aveoccup\n\n# Print the updated dataframe\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nTime,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n\nQuestion: Set \"High\" to \"Close\" if \"High\" is less than \"Open\".", "csv_data_example": "Time,Open,High,Low,Close,Volume\n09:15:59,319.25,319.4,316.05,317.7,143354\n09:16:59,317.7,318.2,317.7,318,52695\n09:17:59,318,318.85,318,318.55,47179\n09:18:59,318.65,319.4,318.5,319.2,44745\n09:19:59,319.3,319.85,319.2,319.65,57892\n09:20:59,319.6,320.4,319.6,320.25,67482\n09:21:59,320.25,320.3,319.95,320.05,56590\n09:22:59,320.15,320.15,319.6,319.6,52413\n09:23:59,319.65,319.65,319.15,319.4,56305\n09:24:59,319.5,320.05,319.45,319.7,36525\n", "csv_path": "infiagent/csv/49.csv", "instruction": "Set \"High\" to \"Close\" if \"High\" is less than \"Open\".", "instruction_type": "Update-Screening", "reference_code": "import pandas as pd\n\n# Load the csv data\ndf = pd.read_csv('infiagent/csv/49.csv')\n\n# Update 'High' column based on condition\ndf.loc[df['High'] < df['Open'],'High'] = df['Close']\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Update 'High' to 'Close' if 'High' is less than 'Open'\ndf.loc[df['High'] < df['Open'], 'High'] = df['Close']\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\nsalary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n\nQuestion: Update the indicator_of_free_agency_eligibility column to 1 for all players who had more number_of_runs than the average number_of_runs.", "csv_data_example": "salary_in_thousands_of_dollars,batting_average,on_base_percentage,number_of_runs,number_of_hits,number_of_doubles,number_of_triples,number_of_home_runs,number_of_runs_batted_in,number_of_walks,number_of_strike_outs,number_of_stolen_bases,number_of_errors,indicator_of_free_agency_eligibility,indicator_of_free_agent_in_1991_1992,indicator_of_arbitration_eligibility,indicator_of_arbitration_in_1991_1992\n3300,0.272,0.302,69,153,21,4,31.0,104,22,80.0,4,3.0,1.0,0,0.0,0.0\n2600,0.269,0.335,58,111,17,2,18.0,66,39,69.0,0,3.0,1.0,1,0.0,0.0\n2500,0.249,0.337,54,115,15,1,17.0,73,63,116.0,6,5.0,1.0,0,0.0,0.0\n2475,0.26,0.292,59,128,22,7,12.0,50,23,64.0,21,21.0,0.0,0,1.0,0.0\n2313,0.273,0.346,87,169,28,5,8.0,58,70,53.0,3,8.0,0.0,0,1.0,0.0\n2175,0.291,0.379,104,170,32,2,26.0,100,87,89.0,22,4.0,1.0,0,0.0,0.0\n600,0.258,0.37,34,86,14,1,14.0,38,15,45.0,0,10.0,1.0,0,0.0,0.0\n460,0.228,0.279,16,38,7,2,3.0,21,11,32.0,2,3.0,0.0,0,0.0,0.0\n240,0.25,0.327,40,61,11,0,1.0,18,24,26.0,14,2.0,0.0,0,0.0,0.0\n200,0.203,0.24,39,64,10,1,10.0,33,14,96.0,13,6.0,0.0,0,0.0,0.0\n", "csv_path": "infiagent/csv/14.csv", "instruction": "Update the indicator_of_free_agency_eligibility column to 1 for all players who had more number_of_runs than the average number_of_runs.", "instruction_type": "Update-Subquery", "reference_code": "import pandas as pd\n\n# Load the csv file\ndf = pd.read_csv('infiagent/csv/14.csv')\n\n# Calculate the average number_of_runs\naverage_runs = df['number_of_runs'].mean()\n\n# Update the indicator_of_free_agency_eligibility column\ndf.loc[df['number_of_runs'] > average_runs,'indicator_of_free_agency_eligibility'] = 1\n\n# Save the updates to a new csv\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data\ndf = pd.read_csv('data.csv')\n\n# Calculate the average number of runs\naverage_runs = df['number_of_runs'].mean()\n\n# Update the 'indicator_of_free_agency_eligibility' column\ndf.loc[df['number_of_runs'] > average_runs, 'indicator_of_free_agency_eligibility'] = 1\n\nprint(df)"}
{"prompt": "Below are the first few lines of a CSV file. You need to write a Python program to solve the provided question.\n\nHeader and first few lines of CSV file:\n,Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n\nQuestion: Update the 'Education' column to hold the value 'Higher Education' for rows where 'Education' is above the median.", "csv_data_example": ",Income,Limit,Rating,Cards,Age,Education,Gender,Student,Married,Ethnicity,Balance\n1,14.891,3606,283,2,34,11,Male,No,Yes,Caucasian,333\n2,106.025,6645,483,3,82,15,Female,Yes,Yes,Asian,903\n3,104.593,7075,514,4,71,11,Male,No,No,Asian,580\n4,148.924,9504,681,3,36,11,Female,No,No,Asian,964\n5,55.882,4897,357,2,68,16,Male,No,Yes,Caucasian,331\n6,80.18,8047,569,4,77,10,Male,No,No,Caucasian,1151\n7,20.996,3388,259,2,37,12,Female,No,No,African American,203\n8,71.408,7114,512,2,87,9,Male,No,No,Asian,872\n9,15.125,3300,266,5,66,13,Female,No,No,Caucasian,279\n10,71.061,6819,491,3,41,19,Female,Yes,Yes,African American,1350\n", "csv_path": "infiagent/csv/24.csv", "instruction": "Update the 'Education' column to hold the value 'Higher Education' for rows where 'Education' is above the median.", "instruction_type": "Update-Subquery", "reference_code": "import pandas as pd\n\n# Load the dataset\ndf = pd.read_csv('infiagent/csv/24.csv')\n\n# Calculation the median of Education column\nmedian_education = df['Education'].median()\n\n# Subquery to update Education column\ndf.loc[df['Education'] > median_education,'Education'] = 'Higher Education'\n\nprint(df)", "assistant_code": "import pandas as pd\n\n# Load the data from the csv file\ndf = pd.read_csv('data.csv')\n\n# Calculate the median of the 'Education' column\nmedian_education = df['Education'].median()\n\n# Update the 'Education' column to 'Higher Education' for rows where 'Education' is above the median\ndf.loc[df['Education'] > median_education, 'Education'] = 'Higher Education'\n\nprint(df)"}
