{"csv_path": "infiagent/csv/2.csv", "questions": ["Remove all rows with duplicate fleet_number values.", "Clear the table except for rows with station type 'Passenger'.", "Delete all rows from the table where the 'start_time' is later than the 'end_time'.", "Eliminate all records where the 'station_code' corresponds to any 'station_code' in a subquery that includes the 'station' column with the keyword 'Believe'.", "Create a bar chart illustrating the frequency of trips based on the 'route_code'.", "Create a pie chart showing the percentage of each 'origin_station_code' compared to the entire dataset.", "Create a line graph that displays the relationship between 'origin_station' and 'destination_station', using 'start_time' as the x-axis and 'end_time' as the y-axis.", "Create a new column 'travel_time' that represents the difference between 'act_dep_time' and 'act_arr_time' for each station.", "Find the total number of fleet_numbers or trips for each route_code.", "Count the total number of unique 'destination_station' in the table.", "Calculate the average 'travel_time' for each group of data based on 'origin_station'.", "Display the count of 'station' for each 'origin_station'.", "Filter the data to show only records where the act_dep_time is between 05:00:00 and 06:00:00.", "Select origin_station, destination_station, route_code, start_time, end_time, and fleet_number from the table.", "Select all rows from the table where the 'fleet_number' is listed in the top 10 fleets with more than 10 trips.", "For the 'route_code' column, subtract 1 from the existing values and update the column.", "Replace any NA values in the 'sch_arr_time' and 'sch_dep_time' columns with the corresponding values from 'act_arr_time' and 'act_dep_time' respectively, and store the results in the same columns.", "Modify the 'start_time' and 'end_time' columns in the table to display dates in a standardized format (MM/DD/YYYY).", "Modify the 'station_type' value to 'Freight' for rows where the 'origin_station' is 'Machine Mist' and the 'destination_station' is 'Roll Test'.", "Rename the column 'sch_arr_time' to 'scheduled_arrival_time' and 'sch_dep_time' to 'scheduled_departure_time'."]}
{"csv_path": "infiagent/csv/3.csv", "questions": ["Eliminate rows containing incomplete data.", "Eliminate the rows from the table that have a 'Happiness Score' lower than 7.0.", "Delete records from the table where 'Economy (GDP per Capita)' is less than the average 'Economy (GDP per Capita)' for all countries.", "Develop a bar chart depicting the 'Happiness Score' for each country in 'Western Europe'.", "Create a pie chart illustrating the distribution of 'Dystopia Residual' among nations in the 'Western Europe' region.", "Create a line graph representing the 'Freedom' in relation to 'Generosity' for each country.", "Generate a scatter plot illustrating the relationship between 'Happiness Score' and 'Economy (GDP per Capita)' for every country.", "Create a new column which shows the ratio between 'Health (Life Expectancy)' and 'Economy (GDP per Capita)' for each country.", "Create a new column by multiplying 'Health (Life Expectancy)' and 'Economy (GDP per Capita)' for each country.", "Insert a column that indicates the length of each country's name.", "Create a new column in the table to assign grades to countries based on their 'Happiness Score'. The grading scale will be A, B, C, D, and F.", "Calculate the average 'Economy (GDP per Capita)' value for each region and show the results.", "Determine the median 'Family' score for each region and display the information.", "Retrieve the data for all countries based in 'Western Europe'.", "It's needed to obtain records where the 'Trust (Government Corruption)' value is upper than 0.40 and the 'Region' is 'Western Europe'.", "Retrieve all entries with a 'Happiness Score' higher than the average 'Happiness Score' for all 'Western Europe' nations.", "Sort each region's countries by their 'Health (Life Expectancy)' score.", "For all \"Dystopia Residual\" values less than 2.5, add 0.2.", "Update the column 'Region' so that all regions have their first letters capitalized.", "Format the 'Happiness Rank' column as a string field."]}
{"csv_path": "infiagent/csv/4.csv", "questions": ["Identify and delete duplicate rows from the table, if any.", "Delete any duplicate rows in the table.", "Delete all rows where \"num. calls transferred\" is zero.", "Exclude rows with 'num. busy overflows' exceeding 10.", "Delete rows where \"num. calls abandoned\" is more than 'num. calls answered'.", "Add a new column to the data denoting \"Total calls activity\", which is the sum of \"num. calls answered\", \"num. calls abandoned\", \"num. calls transferred\", \"num. calls timed out\".", "Add a column displaying the call answer rate, which is obtained by dividing \"num. calls answered\" by the total of \"num. calls answered\" and \"num. calls abandoned\".", "Transform the 'timestamp' column into a DateTime format and separate it into distinct 'Date' and 'Time' columns.", "Calculate the average number of calls answered per day and list them.", "Compute the sum of abandoned calls for every hour.", "Calculate and display the overall total (sum) and average number of \"num. calls abandoned\".", "Find the timestamp when the number of \"num. calls transferred\" was greatest, only considering records where \"num. calls abandoned\" is less than 5.", "For each day, identify the time slot with the highest count of \"num. calls answered\".", "Compute the sum of abandoned calls for every hour.", "Select rows where the 'timestamp' is between 12:00:00 AM and 3:00:00 AM.", "Select rows where the timestamp includes '1:00:00 AM'.", "Increase the value of the \"num. busy overflows\" field by 10% for each row in the table.", "Convert the 'avg. abandonment time' values to minutes instead of the current format.", "Fill in any blank or absent data points in the dataset with suitable default values.", "Rename the column 'num. calls answered' to 'answered calls' for better understanding."]}
{"csv_path": "infiagent/csv/5.csv", "questions": ["Eliminate rows where 'Target' is null or contains no value.", "If there are any duplicates in the source and target columns, keep only the first occurrence and discard the rest.", "Filter the table to exclude rows where the value in the 'Weight' column is greater than 1.5 times the Interquartile Range of the column.", "Delete the rows where 'Type' is not 'Directed'.", "Remove all rows where 'Weight' is less than 5.", "Filter the rows where 'Source' is not equal to 0 and 'Weight' is greater than or equal to 10.", "Draw a bar chart showing the count of 'Source' for each unique 'Weight' value.", "Create a scatter plot to display the connection between 'lng_org' and 'lng_dest'.", "Create a new column labeled \"weight_Increased_by_10\" in the table, and populate it with the values obtained by multiplying the \"weight\" column by 10.", "Introduce a new column named 'Lng_Change' that displays the difference between 'lng_dest' and 'lng_org' in the table.", "Create a new column indicating whether the 'Weight' value is missing or not.", "Add a new column 'Is_Directed' to the table, where it will display 'Yes' if the 'Type' column is 'Directed', and 'No' if it's not.", "Calculate the total weight for each 'Source' and display this alongside the source.", "Summarize the 'Weight' column for each 'Source' group and arrange the results in descending order.", "Identify the 'Source' with the maximum 'Weight' sum.", "Select the rows where the 'Source' is 0 and 'Weight' is greater than 10.", "Retrieve all rows where the 'Type' column equals 'Directed'.", "Fill in any missing values in the 'Weight' column with the average of the existing values.", "Change the 'Type' to 'Indirected' for rows where 'Source' is 0 and 'Weight' is less than 100.", "Change the 'Type' to 'Undirected' for rows where the 'Weight' is less than 10."]}
{"csv_path": "infiagent/csv/6.csv", "questions": ["Identify and eliminate duplicate rows from the table.", "Delete the rows where the 'MEANGAM' is less than 21.", "Delete rows where 'TRUE_TIME' year is not 2014.", "Create a bar graph displaying the mean 'MEANJZD' figure for each 'TIME' category.", "Plot a line graph to show the trend of 'MEANSHR' over time.", "Plot a scatter graph with 'USFLUX' on the horizontal axis and 'MEANSHR' on the vertical axis.", "Add a column 'USFLUX_Billion', where values equal to 'USFLUX' divided by 1e9.", "Create a new column categorizing 'Weight' into different weight classes (e.g., light, medium, heavy).", "Aggregate the data by 'TIME' and compute the sum of 'USFLUX' for each corresponding group.", "Retrieve the largest value present in the 'MEANGAM' column.", "Find the maximum difference between 'TOTUSJZ' and 'TOTUSJH'.", "Calculate the sum of 'TOTPOT' for all rows where 'MEANGBH' is less than 30.", "Filter rows where 'USFLUX' column exceeds 1e+21.", "Select all distinct 'TIME' values from the table.", "Display the data types of each column in the table.", "Fill in any missing values in the 'R_VALUE' column with the mean value of that column.", "Insert missing 'MEANGBZ' values using the median of 'MEANGBZ' values.", "Modify the presentation of 'AREA_ACR' values to display two decimal points.", "For rows where 'R_VALUE' is greater than 1, replace 'R_VALUE' with 1.", "Group the table by 'AREA_ACR' and calculate the median 'R_VALUE' for each group. Then, replace the individual 'R_VALUE' in each row with the corresponding group's median 'R_VALUE'."]}
{"csv_path": "infiagent/csv/7.csv", "questions": ["Delete all rows with missing values in the SCOREMARGIN column.", "Eliminate rows where the 'VISITORDESCRIPTION' field is left blank.", "Remove any duplicate rows from the dataset.", "Clear all table records where the visitor description includes the term 'turnover'.", "Remove any rows from the dataset where the GAME_ID appears less than 5 times.", "Create a pie chart displaying the percentage of each distinct NEUTRALDESCRIPTION in the table.", "Create a bar chart showing the frequency of EVENTMSGTYPE for improved data visualization.", "Create a line chart displaying the SCOREMARGIN values for GAME_ID '0020200722' across various PERIODS.", "Create a new column representing the time difference between 'WCTIMESTRING' and 'PCTIMESTRING'.", "Generate a new column to denote whether the 'EVENTMSGACTIONTYPE' represents a missed shot or not.", "Retrieve the time from the WCTIMESTRING column, and show only the hours for each row.", "Count the distinct occurrences of each value in the EVENTMSGTYPE column.", "Calculate and show the average EVENTNUM for each PERIOD group.", "Fetch all the records from the table where EVENTMSGTYPE is 5.", "Fetch game records that have a PCTIMESTRING later than the average PCTIMESTRING of all records.", "Update the SCOREMARGIN column by dividing by 2 for each row to halve the score margin.", "Set EVENTNUM to 0 for all records where the score is '0 - 2'.", "Calculate the average SCOREMARGIN across all records and use that value to replace the SCOREMARGIN in the current record.", "Convert the 'WCTIMESTRING' column from string format to datetime format.", "Set the SCORE of the game to the highest score achieved in the same PERIOD."]}
{"csv_path": "infiagent/csv/8.csv", "questions": ["Eliminate and remove the redundant records by identifying them.", "Identify and remove entries where 'Height', 'Length' or 'Diameter' is not provided.", "Delete the column 'Viscera weight'.", "Remove the entries where the gender is 'I'.", "Delete records where the 'Height' is more than the 95th percentile of the height distribution.", "Draw a histogram of the 'Rings' column to illustrate the distribution of age.", "Create a scatter diagram to illustrate the correlation between 'Shell weight' and 'Whole weight'.", "Insert a new column \"Density\" to table, calculated as Whole weight divided by the product of Length, Diameter and Height.", "Create a new column 'Weight Loss' that represents the difference between 'Whole weight' and the sum of 'Shucked weight', 'Viscera weight' and 'Shell weight'.", "Based on the shell weight being between 0.1 and 0.2, create a new column named \"Weight Category\" and fill it with the value 'Medium'.", "Add a new column 'Age Category' to the table and categorize the values in the 'Rings' column as 'Young' if they are less than 10, 'Adult' if they are between 10 and 20, and 'Old' if they are more than 20.", "Compute the mean, maximum, and minimum of the length, height, and diameter for every 'Sex'.", "Group the data by 'Sex' and calculate the average height, length and diameter.", "Filter the records based on a condition where the shell weight is greater than 0.2.", "Select the records where the total weight (Sum of 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight') is greater than the average total weight.", "Select records where 'Whole weight' exceeds twice the value of 'Shucked weight'.", "Change the sex values for records with less than 10 rings. Change 'M' to 'Y' and 'F' to 'X'.", "Update columns where the average weight is lower than the overall table average, set 'Sex' to 'U'.", "Make all values in the 'Sex' column uppercase.", "Calculate the average height for each group based on 'Sex' and update the 'Height' column accordingly."]}
{"csv_path": "infiagent/csv/9.csv", "questions": ["Check and delete any duplicate recordings based on 'index' and 'timestamp' columns.", "Delete any rows that have missing values in any of the emotion columns.", "Remove all duplicate rows from the table based on the 'name' column.", "Eliminate rows with blur values exceeding 0.5 to enhance overall data precision.", "Draw a pie chart to illustrate the proportion of different emotions recorded for 'marina' based on average values.", "Plot the 'sadness' emotion values over the 'timestamp', filtered for person named 'ciro'. Represent each point as a dot on the graph.", "Create a new column called 'Total_emotion_score' that calculates the sum of all emotion values for each row.", "Create a new column named 'emotion_dominant' that indicates the emotion with the highest score in each row.", "Insert a column that calculates the total seconds equivalent of the timestamp in hours-minutes-seconds format for more convenient time analysis.", "Display the average emotion values (anger, contempt, disgust, fear, happiness, and sadness) for each person in the 'name' column and calculate them.", "Group the maximum and minimum blur values for each person by name.", "Group the data based on the names and calculate the median of 'happiness' for each group.", "Find the timestamp when the maximum 'anger' emotion was recorded for 'alvaro'.", "Display the names and time stamps of recordings having an anger value exceeding 0.05.", "Retrieve all rows from the table where the 'happiness' value is greater than the average happiness value across all rows.", "Select and display the rows where 'anger' was the dominant emotion by checking if anger value was higher than all other emotions for that row.", "Convert all '0.0' emotion records to a null value (NaN) to indicate no recorded emotion.", "In the 'name' column, update 'alvaro' to 'alvaro_dias' to indicate the full name of the person.", "Modify the presentation of the 'name' column by converting it to uppercase for enhanced visibility.", "Find the maximum value in the 'happiness' column for rows where the 'neutral' value is greater than 0.9 and set it as the 'happiness' value for all those rows."]}
{"csv_path": "infiagent/csv/10.csv", "questions": ["Eliminate rows with matching 'Category' and 'Offense-Type' combinations from the database.", "Remove all rows where the 'Age' column is missing.", "Filter the table to exclude rows where 'Expungible' is False and 'Offense' contains 'Assault'", "Eliminate all rows where 'Offense' appears in the top 5 most frequently occurring 'Offenses'.", "Create a pie chart to display the proportion of various offenses.", "Draw a line graph tracking the sum of 'Count' for each year.", "Create a histogram displaying the distribution of ages in the data.", "Create a new column 'Count_Score' in the table, where its value is the result of dividing 'Count' by 'Age' for each row.", "Create a new column 'Offense_Detail' by combining 'Offense' and 'Offense_Type' values with a hyphen(-).", "Calculate and display the mean age and number of occurrences for each offense category.", "Aggregate the table based on the 'Offense' column and return the count of records for each group.", "Calculate the sum of 'Count' for each unique 'Offense' in the table.", "Calculate the maximum, minimum and average 'Count' for each year.", "Retrieve the maximum 'Age' for each offense category.", "Retrieve from the table all rows where the 'Offense' column contains 'Burglary/Breaking & Entering' and the 'Age' column is less than 18.", "Retrieve all rows where the 'Age' is greater than 15 and the 'Count' is less than 2.0.", "Normalise the 'Count' column by subtracting the mean and dividing by the standard deviation.", "Check the 'Offense' column and replace any slashes('/') in the text with dashes('-').", "Set the 'Expungible' status of all offenses to false if the count exceeds the average count.", "Set 'Disqualifying_Offense' to True for rows where 'Age' is no greater than 16."]}
{"csv_path": "infiagent/csv/11.csv", "questions": ["Eliminate duplicate rows in the data set based on the 'weight' column.", "Delete all rows from the vehicles table where the value of the 'cylinders' column is less than 5.", "Delete all records where 'modelyear' is less than 1975 and 'origin' is 3.", "Create a box plot displaying the 'mpg' values for each 'modelyear' category.", "Draw a line graph of the trend of 'mpg' over 'modelyear' for each 'cylinders' group.", "Create a scatter plot with 'weight' plotted on the x-axis and 'mpg' plotted on the y-axis, where the color of each point represents the corresponding value of 'modelyear'.", "Create a new column named 'efficiency' that calculates the miles per gallon per horsepower.", "Add a column for the ratio of 'displacement' to the vehicle's average 'displacement' within its 'cylinders' group.", "Create a new column to classify cars according to their 'weight' ('light' for weights under 3000, 'medium' for weights between 3000 and 4000, and 'heavy' for weights over 4000).", "Find the maximum 'weight' of vehicles for each unique 'horsepower'.", "Organize the data according to 'cylinders' and determine the mean 'mpg' for each grouping.", "Group vehicles by their origin and calculate the median value of their mpg for each group.", "Choose all vehicles that have a horsepower value higher than 200.", "Identify the 'cylinders' and 'displacement' of the vehicles with the highest 5 'horsepower' values.", "Identify identical 'displacements' applicable to vehicles with 4, 6, or 8 'cylinders'.", "Multiply the contents of the 'acceleration' column by double.", "Normalize 'weight' in the data (divide by the max value).", "Replace missing values in the 'horsepower' column with its mean.", "Modify the 'origin' attribute of all vehicles with a 'weight' greater than 4000 to 2.", "Rename column \"modelyear\" to \"year_of_model\""]}
{"csv_path": "infiagent/csv/12.csv", "questions": ["Identify and delete any duplicate entries from the table based on the 'NAME' and 'JOBTITLE' columns.", "Exclude rows where 'Gross' data is missing.", "Eliminate rows where 'ANNUAL_RT' is less than 'Gross'.", "Create a bar graph displaying the count of 'JOBTITLE' within each 'DEPTID' division from the dataset.", "Plot a Scatter plot between 'ANNUAL_RT' and 'Gross' for visual correlation.", "Calculate the monthly rate from the given 'ANNUAL_RT' and add this as a new column 'MONTHLY_RT'.", "Calculate the number of years from the hire date to the current date and add the result to a new column called 'YEARS_OF_SERVICE'.", "Create a new column 'HIRE_YEAR' by extracting the year from 'HIRE_DT'.", "List the total 'ANNUAL_RT' salary for each department('DEPTID').", "Compute and present the aggregated Gross value for DEPTID equal to A40001.", "For each 'JOBTITLE', calculate the average 'Gross' and display the 'JOBTITLE' and its corresponding average 'Gross'.", "Display 'ANNUAL_RT' in US Dollar currency format.", "Obtain the total number of rows within the table.", "Find the job title with the highest 'ANNUAL_RT' in every 'DEPTID'.", "Filter rows where 'HIRE_DT' is greater than '01/01/2010' and 'ANNUAL_RT' exceeds 45000.00.", "Extract and list all the employees who are working as a 'POLICE SERGEANT'.", "Increase the value of 'ANNUAL_RT' by 2% for all rows where 'YEARS_OF_SERVICE' is greater than 10 years.", "Perform a grouping operation on the table based on the 'DEPTID' column and then calculate the median 'ANNUAL_RT' for each group. For any missing 'ANNUAL_RT' values in these groups, replace them with the calculated median value.", "Normalize the 'DESCR' column by converting all the text to lowercase and removing any extra spaces.", "Replace all instances of 'AIDE BLUE CHIP' in the 'JOBTITLE' column with 'BLUE CHIP AIDE'."]}
{"csv_path": "infiagent/csv/13.csv", "questions": ["Remove any duplicates in the dataset.", "Remove all rows where 'AT' (Air Temperature) is missing.", "Remove the rows where 'GUSTS' is greater than 10 and the 'DIR' is greater than 200.", "Generate a line plot showing 'BARO' trends over the given time period.", "Add a new column 'TEMP_FAHRENHEIT' by converting 'AT' from Celsius to Fahrenheit.", "Replace missing 'RELHUM' values with the median of the existing 'RELHUM' values.", "Create a new column displaying the length of 'DATE TIME' values.", "Add a column to convert 'DATE TIME' to epoch format (the number of seconds that have elapsed since January 1, 1970).", "Create a new column and set the default value to 'N/A' for all existing records.", "Add a new column 'UNIQUE_ID' ensuring each row has a unique identifier.", "Calculate the average 'GUSTS' value for the entire dataset.", "Find the frequency of each unique 'DIR' value.", "Organize the records according to the 'DIR' category and calculate the number of 'VIS' occurrences in each group.", "Filter records where the 'WINDSPEED' is greater than 5 and the 'DIR' is less than 270.", "Filter the table to show all rows where the time in the 'DATE TIME' column has a value ending with '00:00'.", "Obtain the count of records in the table.", "Select the 'BARO' values for times when 'WINDSPEED' was in its top 10% range.", "Format the \"DATE TIME\" field in the YYYY-MM-DD HH:MM format.", "Format the 'DATE TIME' column to show dates in the format of 'dd-mm-yyyy hh:mm'.", "Rename the column 'RELHUM' to 'Relative Humidity'."]}
{"csv_path": "infiagent/csv/14.csv", "questions": ["Delete rows where the batting average is NaN or 0.", "Delete any duplicate rows in the table based on all columns.", "Eliminate the rows where both the indicator of free agency eligibility and the indicator of arbitration eligibility are equal to 0.", "Create a bar chart illustrating the count of players according to their respective salary ranges (0-1000, 1000-2000, 2000-3000, and 3000+).", "Plot a histogram of the number_of_strike_outs to understand its distribution.", "Create a pie chart displaying the distribution of players between those who are free agent eligible and those who are not.", "Create a scatter plot of salary vs. number_of_home_runs; one for players with free agency eligibility and one for those without.", "Insert a column 'batting_efficiency' that is calculated as the number_of_hits divided by number_of_runs.", "Add a column that flags a player as 'Veteran' if they have more than 100 number_of_runs_batted_in and 'Rookie' if not.", "Create a new column that will receive the value \"yes\" if the 'number_of_runs' exceeds 50, and \"no\" otherwise.", "Determine the overall amount spent on player salaries who achieved a batting average exceeding 0.250.", "Group by the indicator_of_arbitration_eligibility and compute the average batting_average for each of the group.", "Calculate the top 5 ratio of the total number_of_hits to number_of_runs for each player.", "Retrieve all players in the table whose salary is greater than 2000.", "Select the records with over 20 home runs and under 5 errors.", "Identify players with a batting average equal to or higher than the average of the entire dataset.", "For all players, decrease the salary_in_thousands_of_dollars by 5% if the number_of_strike_outs exceeds 80.", "Fill in the missing values in the number_of_doubles column with the average value of that column.", "Update the number_of_runs column value to 'High' if its value is above 70, 'Medium' if it's between 40 and 70, and 'Low' if it's less than 40.", "Update the indicator_of_free_agency_eligibility column to 1 for all players who had more number_of_runs than the average number_of_runs."]}
{"csv_path": "infiagent/csv/15.csv", "questions": ["Eliminate any repetitive records.", "Eliminate any duplicate rows within the table that share the same values in the 'wage' column.", "Remove all entries with a 'looks' rating less than 3.", "Delete all records where the 'union' is 1 and 'goodhlth' is 0.", "Draw a bar chart representing the number of individuals in each 'looks' category.", "Create a line graph that displays the average 'wage' for each 'educ' level.", "Create a scatter plot with 'exper' on the x-axis and 'wage' on the y-axis.", "Add a column 'income group' to the table, wherein those with 'wage' greater than 7 are classified as 'high', those with 'wage' between 5 and 7 are 'middle', and those with 'wage' less than 5 are 'low'.", "Create a new column 'avgwage_community' in the table, which represents the average wage of individuals residing in the same city (identified by 'bigcity' and 'smllcity' columns).", "Calculate the average 'wage' for male employees (where 'female' is 0) and female employees (where 'female' is 1) separately.", "Count the number of records in each group by 'black' and 'female' columns.", "Retrieve records grouped by 'educ' level and calculate the average 'wage', maximum 'wage', and minimum 'wage' for each group.", "Calculate the 'wage' to 'exper' ratio for each record and display the top 10 records with the highest ratio.", "Get all the records where the 'wage' is greater than 5.", "Select all records where the 'wage' is greater than the average 'wage'.", "Select all records where the person is married, living in a small city, and has good health.", "Set all wages below 5 to 5.", "Modify all 'wage' and 'lwage' values to integer format.", "Update the 'lwage' column to be the natural logarithm of the 'wage' column.", "Rename the column 'goodhlth' to 'good health'."]}
{"csv_path": "infiagent/csv/17.csv", "questions": ["Remove any duplicate rows based on the 'Date' column.", "Remove all entries in the table that have a 'Market Cap' below 750,000,000.", "Filter the table to show only rows where the value in the 'Close' column is above or equal to 100.", "Display a pie chart representation of 'Volume' traded per day, using 'Date' for grouping.", "Create a bar graph displaying the 'Market Cap' values for each date.", "Create a new column indicating the difference between the 'High' and 'Low' values.", "Add a new column that shows whether the 'Open' value surpasses a specific threshold, such as 120, by assigning a value of 1 if true and 0 if false.", "Create a new column that divides the values in the 'Open' column into different categories, such as 'Low', 'Medium', and 'High'.", "Add two new columns with the average of 'High' and 'Low' for the date 'Sep 15, 2017' derived from a subquery.", "Calculate the arithmetic mean of all the 'Market Cap' values in the dataset.", "Group by 'Date' and calculate the sum of 'Volume' for each day.", "Compute the total of 'Volume' across all rows with 'Low' values beneath 100.", "Retrieve the 'Date' and 'Volume' for all the rows in the dataset.", "Update the 'Market Cap' to be the maximum value for all rows where the 'Volume' is greater than 6,000,000.", "Adjust the 'Volume' column by multiplying each value by 10, and record the updated values in the same column.", "Convert all commas in the 'Volume' column to blank spaces and refresh the table.", "For 'Date', convert the format from 'MMM DD, YYYY' to 'YYYY-MM-DD' and update this column with these new values.", "Transform 'Market Cap' into an integer format while eliminating all non-numeric characters.", "For all entries where 'Market Cap' is less than 800,000,000, set 'Close' to 0.", "Set the 'Close' value to 0 for all rows where the 'Volume' is less than 5,000,000."]}
{"csv_path": "infiagent/csv/18.csv", "questions": ["Eliminate any repetitive rows, if present.", "Exclude all records with occupation 'Handlers-cleaners'.", "Remove rows where 'capital gain' minus 'capital loss' is less than zero.", "Eliminate any 'education' groups with fewer than 10 individuals.", "Plot a bar chart indicating the number of individuals from each 'native-country'.", "Create a histogram showing the distribution of the 'age' variable.", "Add a new column 'is_high_income' where the value is True if 'income' is ' >50K', False otherwise.", "Introduce a new column 'education_level' in the table, which classifies individuals into 'Low' if 'education-num' is 5 or less, 'Medium' if it is greater than 5 and less than or equal to 9, and 'High' if it is greater than 9.", "In the table, add a new column that shows whether the 'hour-per-week' value is less than the average value or not.", "Create a 'working_hours_category' column that categorizes individuals as 'Part-time' if they work less than 30 hours per week, 'Full-time' if they work between 30-40 hours, and 'Over-time' if they work more than 40 hours.", "Determine and enumerate the number of individuals based on their 'marital-status'.", "Calculate the median 'final-weight' for individuals grouped by 'income'.", "Retrieve all records of individuals who are 50 years old or older.", "Extract data for individuals who are 'Married-civ-spouse' and work more than 40 hours per week.", "For each 'workclass', calculate the average of the 'capital-gain' column, and display only those 'workclass' values whose average 'capital-gain' is greater than the overall average 'capital-gain' of all 'workclass' values.", "Find people who are 'Not-in-family', 'White', 'Male', and have an income of '>50K' using a fusion query.", "Replace all missing values in the 'workclass' column with 'Unknown'.", "For the 'relationship' column, change the value 'Not-in-family' to 'Single'.", "In the 'race' column, replace 'White' with 'Caucasian' and 'Black' with 'African American'.", "Fill in the missing 'occupation' column with the occupation that appears most often."]}
{"csv_path": "infiagent/csv/20.csv", "questions": ["Delete rows that contain missing values for all columns in the table.", "Remove rows from the table where the 'Number of Discharges' is less than 50.", "Filter the table to show only rows where 'Measure Name' is equal to 'READM-30-HIP-KNEE-HRRP'.", "Create a histogram to analyze the distribution of the 'Excess Readmission Ratio'.", "Draw a line graph showing changes in 'Number of Discharges' over time (based on 'Start Date' and 'End Date') for the 'FROEDTERT MEMORIAL LUTHERAN HOSPITAL'.", "Create a new column labeled 'Days of Service' to determine the total days of service for each record by subtracting the Start Date from the End Date.", "Insert a new column 'Date Range', which concatenates 'Start Date' and 'End Date' with a delimiter '-'.", "Create a new column named 'Hospital Rating' based on the 'Excess Readmission Ratio' of each hospital. If the 'Excess Readmission Ratio' is less than 1.5, assign a 'Good' rating; if it's between 1.5 and 1.6, assign an 'Average' rating; and if it's greater than 1.6, assign a 'Poor' rating.", "Calculate the mean 'Excess Readmission Ratio' for each 'State' and order the result in descending order.", "Retrieve the number of hospitals in each state.", "Compute the aggregate count of 'Number of Discharges' and 'Number of Readmissions' records for each identified 'Measure Name'.", "Group the records by 'State' and find the maximum 'Number of Discharges' in each group.", "Choose the hospital data when the 'Excess Readmission Ratio' exceeds 1.7.", "Find the 'Provider Number' and 'Number of Discharges' for the hospitals with 'Excess Readmission Ratio' greater than the overall average 'Excess Readmission Ratio'.", "Sort by 'Number of Discharges' within each 'State' group, and select the hospital with the highest rank.", "Identify the state with the most substantial 'Number of Discharges' and extract all hospitals located in that state.", "Update the 'Expected Readmission Rate' column by adding 10% to each existing value.", "Transform the 'State' column into uppercase to ensure consistency in the format of all data within that column.", "Convert the 'Number of Readmissions' column to float.", "Change the column title 'Provider Number' to 'Provider ID'."]}
{"csv_path": "infiagent/csv/21.csv", "questions": ["Remove duplicate rows from the data.", "Eliminate all storm records with 'max_storm_cat' below 3.", "Eliminate all rows where the value for 'deaths' is 0.", "Eliminate all rows where the 'damage_USD' is less than 1000000.0.", "Create a pie chart displaying the relative proportion of each 'max_storm_cat' in terms of its contribution to the overall 'damage_USD' amount.", "Create a bar graph illustrating the distribution of 'max_storm_cat'.", "Add a column 'max_wind_per_cat' calculating the wind speed per storm category by dividing 'max_sust_wind' by 'max_storm_cat'.", "Add a column 'damage_in_billion_USD' converting 'damage_USD' into billions.", "Introduce a new column to denote whether the value of 'deaths' is missing or not.", "Calculate the total 'damage_USD' caused by hurricanes each year.", "Calculate the mean 'deaths' for storms where 'max_storm_cat' is greater than 3.", "For each storm, subtract the maximum sustained wind ('max_sust_wind') from the minimum pressure ('min_p') and identify the top 5 storms with the highest discrepancies.", "Group the data by 'max_storm_cat' and count the frequency of each category, representing the number of storms.", "Retrieve information on all hurricanes whose names begin with the letter \"A\".", "Choose all hurricanes that impacted Florida.", "Update the 'damage_USD' value to be 50% of the original value for all rows where the 'deaths' is greater than 100.0.", "Update the table by replacing all null values in the 'areas_affected' column with 'Not available'. Only output the instruction.", "Find the storm with the highest 'damage_USD' and update its 'damage_imputed' value to 1.", "Convert 'damage_USD' from float to integer type.", "Rename the 'damage_USD' column to 'Damage in USD'."]}
{"csv_path": "infiagent/csv/22.csv", "questions": ["Remove all rows where date is null.", "Remove all rows that have duplicate information in the table.", "Filter out all rows where the 'vaccines' field reads 'Pfizer/BioNTech'.", "Prepare a bar chart for 'daily_vaccinations' for the country 'Albania'.", "Draw a time series plot for 'total_vaccinations' for all dates for the country 'Albania'.", "Draw a scatter plot for 'daily_vaccinations' and 'total_vaccinations' displaying the correlation between them for 'Albania'.", "Calculate the difference between 'people_vaccinated' and 'people_fully_vaccinated' for each entry.", "Add a new column which shows the month from the date column.", "Calculate the average total vaccinations per day for each country.", "Calculate the total 'daily_vaccinations' for each group of 'vaccines'.", "Group the data by 'country' and 'date' and calculate the sum of 'daily_vaccinations'.", "Determine the highest 'total_vaccinations' value for each 'vaccine' in every 'country'.", "Retrieve the records where the total number of vaccinations exceeds 300.", "Obtain the information for the nation 'Albania' and the 'total_vaccinations' count greater than 200.", "Replace all null values in the 'total_vaccinations_per_hundred' column with 0.", "Adjust the 'total_vaccinations_per_hundred' column to display the percentage by multiplying the current values by 100.", "Transform each date in the 'date' column to the 'yyyy-mm-dd' format.", "Rename the column \"people_fully_vaccinated_per_hundred\" to \"percentage_fully_vaccinated\".", "Rename the column 'source_name' to 'vaccine_source'.", "Update all 'iso_code' where 'country' is 'Albania' to 'AL'."]}
{"csv_path": "infiagent/csv/23.csv", "questions": ["Eliminate records with null 'number_of_dependents' to enhance data analysis precision.", "Eliminate any redundant records by virtue of 'revolving_utilization_of_unsecured_lines' and 'number_of_open_credit_lines_and_loans' being equal.", "Delete records where both 'monthly_income' and 'number_of_open_credit_lines_and_loans' are NULL to clean the data.", "Draw a pie chart showing the proportion of users with a 'low', 'medium', and 'high' 'income_level' to visualize income distribution.", "Plot a bar graph to show the distribution of 'number_of_open_credit_lines_and_loans' to examine credit lines or loans behavior.", "Construct a scatter plot to show the correlation between 'age' and 'debt_ratio', with 'age' on the x-axis and 'debt_ratio' on the y-axis, to visualize if age impacts debt ratio.", "Create a new column called 'total_due_days' that calculates the sum of 'number_of_time30-59_days_past_due_not_worse', 'number_of_times90_days_late', and 'number_of_times_late' to represent the overall lateness of the records.", "Insert a new column named 'income_level', classifying 'monthly_income' with criteria: 'low' if less than 3000, 'medium' from 3000 to 10000, and 'high' if more than 10000.", "Create a new column in the table named 'is_high_risk' that indicates whether 'serious_dlqin2yrs' is equal to 1 and 'number_of_time30-59_days_past_due_not_worse' is greater than or equal to 3.", "Calculate the total 'monthly_income' for all records.", "Compute the average 'monthly_income' for users having 'serious_dlqin2yrs' equal to 1 and analyze the influence of income on delinquency.", "Aggregate the records by 'number_of_dependents' and compute the count of 'serious_dlqin2yrs' for each group to comprehend the delinquency tendencies in accordance with the number of dependents.", "Retrieve all records where the 'number_of_open_credit_lines_and_loans' is greater than 7.", "Choose all records with an age higher than 30 and a 'revolving_utilization_of_unsecured_lines' below 0.5.", "Select all records where 'debt_ratio' is between 0.1 and 0.5.", "Substitute any null entries in the 'monthly_income' column with the average 'monthly_income' value of the entire table in order to avoid data loss.", "Update the 'monthly_income' values to the median of their respective age group.", "Transform all values in 'number_of_dependents' into integers for streamlined analysis.", "Rounding the values in the 'debt_ratio' column to two decimal places to ensure consistency and ease of analysis.", "Rename the column 'number_of_time60-89_days_past_due_not_worse' as 'number_of_times_late'."]}
{"csv_path": "infiagent/csv/24.csv", "questions": ["Remove duplicate rows with identical values in the 'Income' column and keep the original table structure.", "Delete all rows where 'Ethnicity' field contains white spaces at the start or end.", "Group the data by 'Ethnicity' and eliminate any Groups with less than 5 rows.", "Remove all rows where the value of 'Age' is within the range of 30 to 50.", "Construct a pie chart illustrating the relative percentage of 'Married' and 'Unmarried' customers in the given dataset.", "Create a histogram displaying the distribution of 'Income' data separated by 'Gender', including accurate labels.", "Draw a scatter plot between 'Income' and 'Limit' with distinct colors for 'Male' and 'Female'.", "Create a new column named 'Mean_Balance' and calculate the average balance for each 'Gender'.", "Change the column names to lowercase and add a new column 'income_to_limit_ratio' which shows the ratio of 'Income' to 'Limit' for each person.", "Convert the 'Student' column into a binary format, with 'Yes' represented as 1 and 'No' as 0, and store the resulting values in a new column called 'Student Binary'.", "Group the data by 'Married' and 'Student' status and add a new column depicting the count of each group.", "Compute the average 'Rating' score for each 'Ethnicity'. Return the results, sorted by the average 'Rating' in descending order.", "Find all rows where 'Education' is greater than 15 and 'Married' status is 'No'.", "Locate all rows with 'Female' in the 'Gender' column, 'Age' higher than 70, and 'Married' set to 'Yes'.", "Filter the table to display only the rows where Gender equals 'Male'.", "Display the first 5 characters of 'Ethnicity' column for all rows.", "Update 'Limit' column by subtracting 'Rating' from its current value for all rows.", "Increase the 'Balance' column value by 10% of the 'Income' for each row.", "Update the 'Income' column by replacing all values less than 50 with the mean 'Income' value.", "Update the 'Education' column to hold the value 'Higher Education' for rows where 'Education' is above the median."]}
{"csv_path": "infiagent/csv/25.csv", "questions": ["Remove duplicate rows from the entire dataset.", "Filter out all rows with 'DebtRatio' exceeding 1000.", "Remove all rows where the value in the 'age' column is below 18.", "Remove all records where 'NumberOfOpenCreditLinesAndLoans' is more than three standard deviations away from the mean.", "Create a pie chart showing the proportions of the number of dependents in the dataset.", "Create a histogram based on the 'age' column data.", "Calculate 'DebtPerDependent' by dividing 'DebtRatio' by 'NumberOfDependents' only if 'NumberOfDependents' is not zero. If 'NumberOfDependents' is zero, set 'DebtPerDependent' equal to 'DebtRatio'.", "Add a column called 'TotalLatePayments' which combines 'NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfTimes90DaysLate' and 'NumberOfTime60-89DaysPastDueNotWorse'.", "Insert a new row at the start with the values \"11, 0, 0.5, 30, 0, 0.1, 2000, 3, 0, 0, 0\".", "Create a new column 'IncomeBracket' by categorizing the 'MonthlyIncome' into four brackets: 'Low', 'Average', 'High' and 'Very High' based on quartiles.", "Categorize the 'age' column into 'Young' (18-30), 'Adult' (31-60), and 'Senior' (60+) by adding a new column 'AgeGroup'.", "Calculate the average monthly income for the entire dataset, grouping by the column 'NumberOfDependents'.", "Group all records by 'age' and calculate the sum of 'RevolvingUtilizationOfUnsecuredLines' for each age group.", "Determine the overall count of individuals aged 60 to 89 days late, classified by their age.", "Calculate the total sum of the 'MonthlyIncome' column.", "Filter the table to show only rows where the value in the \"NumberOfTime30-59DaysPastDueNotWorse\" column is greater than 1.", "Retrieve the rows corresponding to the oldest and youngest individuals with a '90DaysLate' status.", "Replace all 'NA' values in 'NumberRealEstateLoansOrLines' column with a default value of 0.", "Replace all the 'NaN' values in the 'MonthlyIncome' column with the median of the column.", "Rename the column 'SeriousDlqin2yrs' to 'Delinquencyin2Years'."]}
{"csv_path": "infiagent/csv/26.csv", "questions": ["Delete all rows where the value for 'WEIGHTING' is not available.", "Remove records where 'Wins' is less than the median 'Wins' value in the dataset.", "Generate a histogram to display the distribution of 'Wins' values across all teams.", "Generate a bar plot showing the total number of wins per school.", "Add a new column 'WIN_SCORE' calculated as 'Wins' multiplied by 'WEIGHTING'.", "Create a new column 'TEAM_ID' and assign distinct identifiers to every team.", "Generate a new column 'SCHOOL_SHORTNAME' by extracting the initial three characters from each school's name.", "Calculate the average number of 'Wins' for each school.", "Count the number of teams for each unique 'School'.", "Calculate the sum of 'NUM ROUNDS' for every 'School'.", "Count the number of records with 'Wins' above 4 for each school.", "Filter the table records where the 'Wins' column exceeds 3 and the 'School' column reads 'Archbishop Mitty'.", "Choose the 'Name' column for teams with 'NUM ROUNDS' falling within the top 25% for each school.", "Find all records where 'Name' contains 'Singh'.", "Identify the team with the maximum and minimum 'Wins'.", "Select records where 'Wins' is between 4 and 5, and 'NUM ROUNDS' is equal to 5.", "Replace missing 'WEIGHTING' values with the average 'WEIGHTING' from the dataset.", "Replace missing 'NUM ROUNDS' values with the median of 'NUM ROUNDS' for the respective school.", "Ensure all 'STANDARD TEAM NAME' values have a consistent number of characters, padding with spaces if necessary.", "Format the 'Name' column to display last names in uppercase."]}
{"csv_path": "infiagent/csv/27.csv", "questions": ["Eliminate all rows where the Calendar Date (TDB) exhibits duplicate values.", "Aggregate the data based on 'Calendar Date (TDB)' and remove groups with less than 5 rows.", "Filter rows with 'X' values below -2.9E+08.", "Retain rows where the 'Calendar Date (TDB)' is in the 'A.D.' era.", "Remove all rows that have a 'Calendar Date (TDB)' of 'A.D. 1999-Feb-10 00:58:29.0000'.", "Add a column named 'Sum' to the table that calculates the sum of the values in columns X, Y, and Z for each row.", "Add a new column named \"Total\" which is the sum of columns X, Y, and Z.", "Calculate the sum of 'X', 'Y', 'Z' for each data and present the results with the corresponding date.", "Extract the year from the 'Calendar Date (TDB)' and create a new column named 'Year'.", "Calculate the minimum, maximum and average of column 'Z'.", "Group the data by the date and calculate the average X, Y, Z coordinates for each date.", "Retrieve and present the day and month from the Calendar Date (TDB) where the value of column X exceeds -2.9E+08.", "Select the data from column X where the Calendar Date (TDB) is after A.D. 1999-Feb-10 00:58:29.0000.", "Retrieve all records where the value in column X is less than -2.9E+08 and the value in column Y is greater than 1.6E+07.", "Divide the values in column 'X' by 1E+06.", "Replace all zeros in the 'X' column with the average value of the entire column.", "Update the data in column 'Y' to its absolute value only for those rows where the value of 'Z' falls within the range (-2.735346128E+07, -2.575472613E+07).", "Substitute all occurrences of 'A.D.' in the Calendar Date (TDB) column with 'Anno Domini'.", "Locate the record containing the highest 'Z' value and substitute all occurrences of 'Z' within the table with this uppermost value.", "Convert 'Calendar Date (TDB)' to datetime format if it is not in datetime format."]}
{"csv_path": "infiagent/csv/28.csv", "questions": ["Remove the duplicate records based on \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\".", "Delete all records where the \"cut\" is \"Fair\".", "Remove all rows where the value of \"depth\" is within the range of 50 to 55.", "Remove records with \"clarity\" containing \"I\" (Imperfections).", "Remove any rows where the 'price' is less than 500.", "Create a bar graph to display the number of diamonds grouped by \"cut\" category.", "Draw a histogram showing the frequency distribution of \"price\".", "Create a scatter plot displaying \"carat\" values on the x-axis and \"price\" values on the y-axis to illustrate the connection between \"carat\" and \"price\".", "Insert a new column \"volume\" calculated as x*y*z for each diamond, representing the approximate volume of each diamond.", "Calculate the \"price_per_carat\" column for each diamond by dividing the \"price\" column by the \"carat\" column and insert the new column into the table.", "Calculate the average \"price\" of diamonds for each \"cut\" and rank them in descending order.", "Classify the records by \"color\" and \"clarity\" and calculate the number of diamonds in each category.", "Find the sum, average, minimum, and maximum of the \"price\" for each \"color\" group, and arrange the results in descending order based on the sum of the \"price\".", "Retrieve all records with \"Ideal\" as the \"cut\" and show the \"carat\", \"color\", \"clarity\", and \"price\" columns.", "Retrieve the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" details for the diamonds that have a \"volume\" greater than the average \"volume\".", "Determine the proportion of \"x\" to \"y\" for each diamond and present only the diamonds with a ratio greater than 1.", "Replace any missing values in the \"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\" columns with suitable placeholder or average value.", "Modify the \"depth\" column to display only two decimal places.", "Rename the column \"table\" to \"table_value\" to avoid confusion with database terminology.", "Modify the \"color\" to \"Colorless\" for rows where \"color\" is either \"D\" or \"E\" and \"clarity\" is either \"IF\" or \"VVS1\"."]}
{"csv_path": "infiagent/csv/29.csv", "questions": ["Remove rows with empty 'Volume' values.", "Remove any repeated values in the 'Date' column.", "Remove the records from the data where 'Low' is less than 75.5.", "Draw a line chart showing price (Y-axis) against dates (X-axis), where price lines include 'Open', 'High', 'Low', 'Close'.", "Create a scatter diagram displaying the correlation between 'Volume' (Y-axis) and 'Open' price (X-axis) for the entire dataset.", "Insert a new column 'Fluctuation' to the table which represent the daily price fluctuation using the formula (High-Low).", "Create a new column containing the product of 'Open' and 'Close' for each row.", "Insert a new column 'Extremely High Volume' that contains the value 'Yes' if 'Volume' is more than 100000000 and 'No' otherwise.", "Aggregate the data by 'Date' and compute the sum of 'Volume' traded on each distinct date.", "Find the average 'Close' price of the year 2014.", "Count the number of entries for each date and group the results by 'Date'.", "Select the 'Date' and 'High' price, where 'High' price is greater than 78.", "Generate a list of 'High' prices that are higher than the average 'High' price.", "Create a list of 'Low' prices that are below the typical 'Low' price.", "Choose the 'Open' and 'Close' data for each date in the table.", "Determine the highest 'Low' value in the table and associate it with its respective date.", "Update the 'Close' column by adding 10% to its existing value for the dates where 'Volume' exceeds 80000000.", "Substitute null values in 'Open' with the table's average 'Open' value.", "Subtract 1 from all 'Low' prices that were recorded on Fridays.", "Update the 'Open' price and increase it by 5% for all entries in the year 2014."]}
{"csv_path": "infiagent/csv/30.csv", "questions": ["Identify and remove any duplicate rows based on all columns.", "Remove entries with 'total_votes' value less than 50,000.", "Eliminate rows where 'votes_dem' is smaller than 'votes_gop' and 'state_abbr' is \"AK\".", "Draw a pie chart representing the distribution of 'votes_dem' and 'votes_gop' for 'state_abbr' \"AK\".", "Create a line chart displaying the fluctuations in 'votes_dem' and 'votes_gop' for each respective 'state_abbr'.", "Draw a scatter plot of 'votes_dem' vs 'votes_gop' color-coded based on 'state_abbr'.", "Add a new column named 'votes_ind', representing the votes for independent candidates, which can be calculated by subtracting 'votes_gop' and 'votes_dem' from 'total_votes'.", "Create a new column 'votes_total' by adding 'votes_dem' and 'votes_gop' columns together.", "Insert a new column 'total_per' that combines the 'per_dem' and 'per_gop' by multiplying both values by 100 and round them to the nearest whole number.", "Add a new column 'votes_total' that is the sum of 'votes_dem', 'votes_gop', and 'votes_ind'.", "Calculate the total number of votes ('total_votes') for each 'state_abbr'.", "Calculate the average 'votes_dem' and 'votes_gop' for each state_abbr in the table.", "Retrieve the rows where 'state_abbr' is \"AK\" and 'county_name' is \"Alaska\".", "Query all entries where 'per_dem' is greater than 0.4 and 'per_gop' is less than 0.6.", "Find the 'state_abbr' and 'county_name' for the entries with the top 10 highest values in 'total_votes'.", "Create a new column named 'votes_ratio' by dividing 'votes_dem' by 'votes_gop', and round the values to 2 decimal places.", "Ensure that there are no missing values in 'votes_gop'. If any missing values are found, replace them with the median value of the column.", "Modify the 'per_point_diff' column to remove the percentage symbol and convert the entries to decimal values.", "Update the 'diff' column to replace any comma with an empty space.", "Rename the column 'state_abbr' to 'state'."]}
{"csv_path": "infiagent/csv/31.csv", "questions": ["Eliminate the rows possessing identical values in the 'EQ3' column.", "Eliminate duplicate columns in the table.", "Delete all rows where 'EQ3' is less than -0.08.", "Retain the rows where the length of the 'EQ6' value is equal to or greater than 0.05.", "Plot a line chart to show the trend of the average value of 'EQ1' to 'EQ8'.", "Create a histogram of the 'EQ4' column to visually represent its distribution.", "Add a new column 'EQ_Sum' that is the sum of 'EQ1' to 'EQ8' for each row.", "Insert a new column 'Diff_EQ1_EX1' which is the difference between 'EQ1' and 'EX1'.", "Calculate the average value of each 'EX' column.", "Group the data by 'NZ' and calculate the sum of 'EQ2' for each group.", "Count the number of rows where 'EQ7' exceeds its average value.", "Group data by 'EX1', calculate the maximum, minimum and average of 'EQ2' for each group.", "Select the rows where 'EQ1' is greater than 0.01.", "Query 'EQ5' values which are larger than the average 'EQ5' value.", "Select the rows where 'EX4' is between -0.1 and 0.1.", "If 'NZ' is less than 0, multiply the corresponding cell in 'EQ_Sum' by -1.", "Separate the 'EX1' value into three equal parts based on the top 10 percentile of corresponding 'NZ' values.", "Replace all NULL values in 'EQ' columns with the average of the respective column.", "Transform each column label to lowercase.", "Set all negative values in columns 'EX1' to 'EX8' to zero."]}
{"csv_path": "infiagent/csv/32.csv", "questions": ["Filter out the rows where the value of cases_min is not available.", "Delete all records where the values for both 'No. of cases' and 'No. of deaths' are 0.", "Remove any duplicate rows present in the data.", "Delete rows containing 'No. of cases' equal to 0.", "Construct a graph illustrating the progression of reported cases in the 'Eastern Mediterranean' WHO region across different years.", "Create a line graph to display the changing pattern of 'No. of Cases' for the country Angola across different years.", "Add a new column that shows whether the value of cases_median falls within the range of 2000 to 50,000.", "Create a new column named 'Case Range' that calculates the difference between 'No. of cases_max' and 'No. of cases_min'.", "Compute the overall count of cases and deaths for each WHO region and display the WHO region, total number of cases, and total number of deaths.", "Tabulate the data based on 'Year', and compute the aggregated 'No. of cases' and 'No. of deaths' for each year.", "Compute the highest ('No. of deaths_max') and lowest ('No. of deaths_min') death counts for each WHO region.", "Calculate the average number of cases ('No. of cases') for each region and group the results by 'WHO Region'.", "Obtain the data for 'No. of Cases' and 'No. of Deaths' specifically related to the nation 'Afghanistan' in the year 2017 from the table.", "Retrieve records where the No. of cases_max is greater than 5000000.", "Retrieve the countries with the highest number of cases in the table.", "Multiply the 'No. of cases' column by 1000 to express the data as thousands.", "Fill missing values in 'No. of deaths' with 0 if they are NA or blank.", "Convert all country names to uppercase for data consistency.", "Format the 'Year' column as a datetime.", "Rename the column 'No. of cases' to 'Case Count' and 'No. of deaths' to 'Death Count'."]}
{"csv_path": "infiagent/csv/33.csv", "questions": ["Exclude records with 'abc-news' in the 'url' field and 'source' not equal to 'abc news'.", "Eliminate duplicate entries based on 'url' and 'publishedAt' columns.", "[rewritten instruction]: Delete all entries from the table where 'publishedAt' is earlier than '2018-08-28'.", "Plot a line chart reflecting the trend of publishing frequency over time.", "Add a 'sentiment_score' column calculated as 'pos' minus 'neg'.", "Insert a new column 'word_count' into the table to store the number of words in each 'description'.", "Create a new column 'Text_Length' that calculates the character count of the 'text' field.", "Add a new column 'description_length' to store the character counts for each description.", "Create a new column 'Text_Length' that calculates the character count of the 'text' field.", "Create a new column named 'text_summary' and populate each cell with the first 50 characters of the content in the 'text' column.", "Create a new column 'Title_Uppercase' to store the uppercase version of the 'title'.", "Display the total number of mentions for each source and arrange them in descending order.", "Group the records based on 'publishedAt' and 'source', then calculate the average 'pos', 'neu', and 'neg' scores for each group.", "Display the total number of articles published by each author, sorted by the month in which they were published.", "Choose every row in the table where the author field reads 'ABC News'.", "Extract all records where the author has exceeded 10 publication counts.", "Retrieve the records where the length of the 'text' is greater than 200 characters.", "Calculate the compound score by adding the values of 'pos' and 'neu', then subtracting the value of 'neg'.", "Locate any missing entries in the 'urlToImage' column and replace them with the corresponding 'url' values.", "Update the 'source' column to change 'abc-news' to 'ABC News' for rows where the 'source' column is currently 'abc-news'."]}
{"csv_path": "infiagent/csv/34.csv", "questions": ["Filter out rows with NULL values in 'positive_diffsel' and 'negative_diffsel' columns.", "Filter out all rows with a 'positive_diffsel' column value below 4.", "Remove rows where 'max_diffsel' value is below the median of 'max_diffsel' values.", "Delete records where 'site' field contains brackets ().", "Create a bar graph to display the count of each 'site'.", "Draw a pie chart depicting the proportion of each 'site' based on count in the data set.", "Plot a line graph representing 'positive_diffsel' values over 'negative_diffsel'.", "Create a scatter plot with 'site' on the horizontal axis and 'abs_diffsel' on the vertical axis.", "Calculate the average of 'positive_diffsel' column using a sub-query and insert the result into the 'positive_diffsel' column.", "Create a new column 'overall_diffsel' which is the result of multiplying 'positive_diffsel' and 'negative_diffsel'.", "Insert a new record to the table but if a duplicate 'site' value exists, ignore the insert.", "Determine the maximum length among all the 'site' strings and present the result.", "Group by 'site' and calculate the average 'abs_diffsel' value for each group.", "Determine the highest value among the 'positive_diffsel' entries.", "Extract all rows with an 'abs_diffsel' column greater than 8.", "Retrieve all records and sort them by 'abs_diffsel' in descending order, maintaining all other columns unchanged.", "Update the 'min_diffsel' field with results from a sub-query that computes the average of 'min_diffsel' for each 'site'.", "Group data by 'site' and update each group's 'positive_diffsel' with its average.", "Calculate and store the cumulative sum of 'max_diffsel' and 'abs_diffsel' in the 'max_diffsel' column.", "Calculate the difference between the maximum and minimum values of 'min_diffsel' and store the result in 'min_diffsel'."]}
{"csv_path": "infiagent/csv/35.csv", "questions": ["Eliminate duplicate rows in the dataset considering all columns.", "Eliminate all rows where the population is less than 1 million.", "Remove any records where the 'lifeexp' is less than 30.", "Delete all records for years before 1992.", "Draw a bar graph depicting the increase in life expectancy over the year for the country 'afghanistan'.", "Plot a line graph showcasing GDP per capita trend with year for each continent.", "Draw a scatter plot showcasing the correlation between life expectancy and GDP per capita for all continents.", "Add a new column called GDP where each data value is obtained by multiplying pop and gdppercap.", "Determine the aggregate population across all continents and present the individual continents along with their respective total populations.", "Find out the minimum, maximum and median population for each continent for all the records.", "Find the percentage difference in population for each country between 1952 and 1997.", "Aggregate all rows by continent and subsequently, group the data within each continent by country to display the average GDP per capita for each country.", "Select all the data where life expectancy is above 40 and GDP per capita is less than 800.", "Find all the records where the year is between 1980 and 2000, and population is more than 5 million.", "Retrieve all records with \"pop\" and \"country\" categories for the year 1962.", "Increase each value in the 'lifeexp' column by 5.", "Fill any missing values in the 'lifeexp' column with the average of the existing 'lifeexp' values.", "Update the 'country' column by converting all countries' names to uppercase.", "Modify the format of the \"year\" column's date values to only display the four-digit year (\"YYYY\").", "Modify the column title 'gdppercap' to 'GDP-per-capita'."]}
{"csv_path": "infiagent/csv/36.csv", "questions": ["Eliminate any duplicate entries that may exist, based on the country name.", "Filter out any outliers in the data, consider any GDP per capita above $50,000 or below $300 for the year 2007 as outliers and remove them from the dataset.", "Remove all entries where the GDP per capita in 1952 is less than $500.", "Delete all rows where the GDP per capita in 2007 is less than the average GDP per capita in 1952.", "Delete all rows where the 'country' contains the keyword 'China'.", "Plot the correlation between gdpPercap_1952 and gdpPercap_2007.", "Create a line chart to display the evolution of GDP per capita from 1952 to 2007 for India.", "Introduce a new column to the table that displays the percentage change in GDP per capita between the years 1952 and 2007 for every country.", "Create a new column indicating whether the 'gdpPercap_1987' value is a duplicate of any other 'gdpPercap_1987' value in the table.", "Add a new column that categorizes countries into developed and developing, based on the GDP per capita in 2007 (use $20,000 as the threshold).", "Compute the average GDP per capita for each nation in the year 2007, and arrange the results in ascending order.", "Calculate the total aggregate value of GDP per capita for all countries from 1952 to 2007.", "Compute the median GDP per capita across all countries for each year.", "Obtaining the GDP per capita figure for Bahrain in the year 2002.", "Select rows where 'gdpPercap_1977' is higher than the average 'gdpPercap_1977' of countries having 'gdpPercap_2007' greater than 20000.", "Fill any missing GDP per capita values on the year 2007 using the previous year GDP per capita values for the respective country.", "Modify the GDP per capita values to display up to two decimal points.", "Replace all NaN values in the table with the mean GDP per capita of the respective year.", "Rename the column \"gdpPercap_1967\" to \"GDP_per_capita_1967\".", "Increase the GDP per capita for 2007 by 10% for countries that had a GDP per capita less than $1000 in 2002."]}
{"csv_path": "infiagent/csv/37.csv", "questions": ["Delete the records that contain duplicate 'transcript_id'.", "Remove rows where the length of gloss is below 10.", "Eliminate records where the 'Age' is less than 10.", "Remove all records that have average 'Age' in their 'transcript_id' smaller than 5.", "Plot a pie chart of the distribution of 'Gender'.", "Create a bar chart with 'Age' on the x-axis and the count of different 'part_of_speech' on the y-axis.", "Plot the trend of the average 'Impaired' level for each 'Age'.", "Create a new column named 'gloss_word_count' that calculates the number of words in the 'gloss' column for each row.", "Add a new column 'average_age' that calculates the average 'Age' of records with the same 'transcript_id'.", "Create a new column 'speech_count' which is the count of different 'part_of_speech' in each 'transcript_id'.", "Add a column displaying the final word of each 'gloss'.", "Compute the total of all entries in the \"Age\" column.", "Find the total number of different 'transcript_id' for each 'Age'.", "Extract the rows with 'part_of_speech' containing 'n0prop' and 'Age' exceeding 5.", "Find the 'transcript_id' with the highest number of records and retrieve all the corresponding records.", "Increment the 'Age' column for each record by 1.", "Convert the data type of the \"id\" column to text.", "Set the 'Gender' column value to 0 if in the same 'transcript_id', the number of 0 is more than 1. Otherwise, set the 'Gender' column value to 1.", "Format the 'transcript_id' column as string and add a prefix 'TransID-' to each id.", "Set the 'gender' of all members with an 'Age' less than 10 to 0."]}
{"csv_path": "infiagent/csv/38.csv", "questions": ["Remove any rows that have missing data.", "Remove the rows where 'Average Price' is less than 550.", "Remove rows where '% Dly Qt to Traded Qty' is less than 20%.", "Plot a line graph of 'Close Price' over 'Date' for 'GODREJIND' in 'Series' 'EQ'.", "Add a column 'Price Difference' which is calculated as the difference between 'High Price' and 'Low Price'.", "Add a new column 'Price Range' calculated as 'High Price' minus 'Low Price'.", "Find the maximum 'Close Price' for the dates where 'No. of Trades' were higher than average.", "Show the average 'Close Price' grouped by 'Symbol'.", "Compute the overall 'Turnover' for all given dates.", "Compute the mean 'Total Traded Quantity' across the entire dataset.", "Display the relative change in 'Close Price' between the first and last dates for stock symbol 'GODREJIND'.", "Filter rows where 'Symbol' equals 'GODREJIND' and 'Series' is 'EQ'.", "Select from the table where 'High Price' is greater than 585 and 'Date' is after '18-May-2017'.", "Show the rows where \"Prev Close\" price is greater than \"Open Price\".", "Select records where 'Close Price' is between 570 and 590.", "Populate any empty cells in the 'Deliverable Qty' column with the median of the existing values in that column.", "Set the value of 'Last Price' for 'GODREJIND' to be the largest value of 'Close Price' in the dataset.", "If 'Turnover' is zero, set 'Turnover' equal to the product of 'Close Price' and 'Total Traded Quantity'.", "Eliminate any whitespace from the column names and transform them into lowercase.", "Rename the column 'No. of Trades' to 'Number of Trades'."]}
{"csv_path": "infiagent/csv/39.csv", "questions": ["Remove rows where 'Weight' or 'Height' fields have -99.0 as these might represent missing or incorrect data.", "Delete records of superheroes whose 'Skin color' is not provided (represented by '-').", "Plot a histogram of 'Height' data to view distribution.", "Draw a pie chart to represent the number of superheroes in each 'Race' category.", "Create a bar chart to compare the count of good and bad superheroes in each publishing company.", "Insert new column 'BMI' and calculate it as 'Weight' in kg divided by square of 'Height' in m.", "Generate a new column titled 'Eye-Hair Color Combination' by combining the values of 'Eye color' and 'Hair color'.", "Calculate the average weight and height of superheroes for each publisher.", "Aggregate the superheroes by 'Race' and retrieve the count for each group.", "Find out the maximum and minimum height and weight among all superheroes.", "Show a list of all male superheroes in the Marvel comics.", "Show me all superheroes with 'good' alignment and 'Marvel Comics' as publisher.", "Select the information where 'Eye color' is 'blue' and 'Hair color' is 'No Hair';", "Eliminate all superheroes with 'Human' in their 'Race' and whose 'Alignment' is 'bad'.", "Retrieve the details of superheroes with 'Blond' hair color and 'Human' race.", "Retrieve the names of all male superheroes who are taller than the average height of all superheroes.", "Increase the weight of all 'Human' superheroes by 10%.", "Add 5 to the height of all superheroes whose 'Hair color' is 'No Hair'.", "Replace all occurrences of -99.0 in 'Height' and 'Weight' columns to None as these are invalid entries.", "Renames column 'name' to 'Superhero Name' for enhanced clarity."]}
{"csv_path": "infiagent/csv/40.csv", "questions": ["Delete the duplicate rows in the table if any.", "Eliminate the rows from the table where the star rating is less than 3.0.", "Delete all rows where the 'hotel_id' matches the 'parent_brand_name' value.", "Remove entries where 'hotel_name' includes the term \"Court\".", "Create a bar graph to show the number of hotels under each parent brand name.", "Draw a pie chart denoting the proportion of hotels of different star_ratings in the New York City.", "Create a line graph to show how the number of reviews changes for each different bubble score.", "Add a column to capture the hotel's location category based on city name: 'Major' if the city is 'New York City'; otherwise, 'Other'.", "Determine the average star rating for each of the parent brand names.", "Calculate the average bubble score for each star rating.", "Fetch the hotel details where the hotel type is 'Hotel' and the star rating is equal to or more than 4.0.", "Retrieve hotel information with a star rating of 4.5 or higher and a review count exceeding 1000.", "Fetch all the hotels which have the number of reviews between 500 and 2000.", "Fetch the hotel details which belong to the parent brand name 'Choice Hotels International, Inc.'.", "Display a table containing the hotel names and their corresponding parent brand names from the database.", "Set the 'review_count' field to the median value of 'review_count' when the current 'review_count' is missing.", "Modify the 'bubble_score' column by dividing its values by 5 and then multiplying them by 100 to convert them to rating percentages.", "Set missing values in the 'brand_name' column to 'None' for all rows.", "Capitalize the first letter of each word in the 'hotel_name' column.", "Rename the column 'hotel_id' to 'Hotel_ID'."]}
{"csv_path": "infiagent/csv/42.csv", "questions": ["Retain only the rows where the 'standard_indentification_level_1' column is not empty.", "Delete all records that have an 'importance.score' less than 0.03.", "Delete all rows where 'row m/z' is less than the 10th percentile or greater than the 90th percentile - removing outliers.", "Create a bar chart displaying the distribution of 'importance.score'.", "Draw a line chart to illustrate the trend of 'importance.score' for different 'LibraryID'.", "Represent the correlation between 'importance.score' and 'row retention time' through a graphical depiction.", "Aggregate data by 'LibraryID', calculate the mean, and count the number of 'importance.score' for each group.", "Insert a new column for 'importance.score' percentage, calculated as (importance.score/sum of all 'importance.score')*100.", "Add a new column to indicate if the 'standard_indentification_level_1' value includes the word 'Match'.", "Add a column with group IDs, grouping records based on 'LibraryID'. If 'LibraryID' is not provided, assign to a default group.", "Insert a new row with data from the row with the maximum 'importance.score' from a subset of data where 'row retention time' is less than the average 'row retention time'.", "Find the average 'importance.score' for each distinct 'row m/z' value.", "Calculate the maximum, minimum, and average values of 'row retention time' for the records.", "Get the unique values in the 'standard_indentification_level_1' column.", "Filter the rows where 'standard_identification_level_1' is not empty.", "Obtain the data for the first 10 entries with the maximum 'importance.score'.", "Update the 'importance.score' column by adding a uniform random noise between -0.01 to 0.01 to each entry to improve privacy.", "Replace all null values in column 'LibraryID' with the string \"Unknown\".", "Set the value of the 'importance.score' column to 0.05 for all rows where the 'ID' column is less than 500.", "Format all values in the 'LibraryID' column to have consistent capitalization (upper case)."]}
{"csv_path": "infiagent/csv/43.csv", "questions": ["Find any duplicate rows based on all columns, keep only one copy of such rows and delete the rest.", "Delete the entries that have 'southeast' in the region column and 'female' in the sex column.", "Delete rows where the 'charges' are below the average 'charges'.", "Remove all records where 'age' is above 60 and 'smoker' is 'yes'.", "Draw a pie chart showing the percentage of records for each 'bmi_category'.", "Draw a line chart to show the trend of the average 'charges' with 'age' for 'smoker' and 'non-smoker'.", "Draw a graph showing the distribution of 'charges' in each 'region'.", "Draw a bar plot of 'region' vs average 'charges' for 'smoker'='yes' and 'smoker'='no'.", "Add a row at the end of the table showing the total number of 'children' in the dataset.", "Add a new field 'smoker_count' which is the count of 'smoker'='yes' for each 'region'.", "Convert the age column to age_range column such that if age is below 25 it's 'young', between 25-50 it's 'middle-aged', and above 50 it's 'senior'.", "Add a new column 'bmi_category' to the table such that if bmi is less than 18.5 it is 'Underweight', between 18.5 & 24.9 it is 'Normal', between 25 & 29.9 it is 'Overweight' and 30 or more it is 'Obese'.", "Count the number of records where the bmi is greater than 30.", "Calculate the average charge for each region and display them in descending order.", "Summarize the data by 'region' and 'sex', and calculate the average 'bmi'.", "Retrieve the female 'smoker' records with 'charges' greater than the overall average 'charges' table value.", "Increase the 'charges' by 10% for all records where 'smoker' is 'yes' and 'region' is 'northeast'.", "Subtract 5 from 'bmi' for all records where 'children' is 0.", "Modify the 'smoker' column by replacing 'yes' with '1' and 'no' with '0'.", "Update the 'sex' field to 'unknown' when the 'age' is less than 20 and the 'region' is 'southwest'."]}
{"csv_path": "infiagent/csv/44.csv", "questions": ["Delete any rows in the table that have duplicate values on the \"Date\" column.", "Eliminate any row where the \"Close\" value is less than the \"Open\" value.", "Delete rows where the \"Range\" value is less than 1.", "Plot a line graph displaying the trend of \"High\" values over the \"Date\" time.", "Create a bar chart indicating the number of records that fall into each \"Volume\" category ('Low', 'Medium', 'High').", "Create a scatter plot where \"High\" variables are plotted along the x-axis and \"Low\" variables along the y-axis.", "Create a new column named \"Range\" in the table, which is the result of subtracting the \"Low\" values from the \"High\" values.", "Create a new column that calculates the percentage change from the \"Open\" value to the \"Close\" value for each row. The formula for calculating the percentage change is ((Close-Open)/Open)*100.", "Generate a new column by multiplying the 'Open' value with the 'Volume' value for every row in the table.", "Create a new column named \"Weekday\" by converting the values in the \"Date\" column into weekdays.", "Compute the total sum of the \"Volume\" column across all rows in the table.", "Group the table by \"Date\" and compute the mean of \"Volume\", \"High\", and \"Low\" values for each group.", "Find the maximum value of \"High\", the minimum value of \"Low\", and the sum of \"Volume\" across the entire table.", "Select the \"Date\", \"High\", and \"Low\" columns from the table.", "Filter the table to show only rows where the value in the \"Volume\" column is greater than 30000000.", "Retrieve all records where the \"Close\" value was higher than the \"Open\" value and \"Volume\" was above 25000000.", "Find the date with the highest \"Range\" value in days for the high volume.", "Format the \"Date\" column in the table as yyyy-mm-dd.", "Set the \"Close\" column to the average \"Close\" value for rows where the \"Volume\" is below the overall average \"Volume\".", "Update values of \"High\" and \"Low\", replace them with their respective average values taken from their specific Volume groups('Low', 'Medium', 'High')."]}
{"csv_path": "infiagent/csv/45.csv", "questions": ["Remove duplicate entries (same values in all columns) from the table.", "Remove rows from the table where MedianHouseValue column falls below 1.", "Filter rows based on the \"Population\" being greater than or equal to the 10th percentile of all populations in the dataset.", "Draw a pie chart that shows the distribution of houses across different income categories (divide MedInc into categories: low, medium, high).", "Draw a line graph showing the trend of MedianHouseValue over different longitudes.", "Construct a histogram to display the distribution of house ages ('HouseAge').", "Create a scatter plot illustrating the correlation between House Age and the Median House Value.", "Add a column \"RoomsPerPopulation\" to the table which shows the average number of rooms per person for each population (calculate AveRooms divided by Population).", "Add a column that calculates the 'Population' divided by 'AveRooms'.", "Add a new column named \"PopulationDensity\" calculated as the division of Population by AveRooms.", "Count the number of houses that have an average number of rooms greater than 5.", "Group the houses based on \"HouseAge\" and calculate the average \"MedInc\" for each group.", "Filter the houses based on median age greater than 30 years.", "Filter the dataset to identify rows with \"Population\" greater than twice the minimum value in the column.", "Calculate the average value of the 'AveBedrms' column where the 'AveRooms' value is greater than 6 and update the 'AveBedrms' column with the calculated value.", "Increment each value in the \"HouseAge\" column by 1.", "Fill all missing values in the column \"HouseAge\" with the average of other valid values.", "Update the \"MedInc\" column by rounding off the values to the nearest whole number.", "Update the column label \"AveBedrms\" to \"AverageBedrooms\".", "Update the \"AveOccup\" values where Latitude is greater than 35 to the median of 'AveOccup'."]}
{"csv_path": "infiagent/csv/46.csv", "questions": ["Eliminate repetitive rows from the table by verifying each record.", "Exclude all records with exceptional 'Value' column values.", "Delete all rows with 'FREQUENCY' and 'A' where the 'Flag Codes' column is NULL.", "Delete all records belonging to frequency \"A\".", "Create a pie chart to display the proportion of 'Value' possessed by each 'LOCATION' for every record.", "Develop a line graph to demonstrate the trend of 'Value' over 'TIME' for 'MEASURE' as \"PC_GDP\".", "Add a new column indicating the annual difference in 'Value' for each 'SUBJECT' and 'LOCATION'.", "Include a new column 'DATE' which is obtained by parsing the 'TIME' column.", "Generate a listing to display each 'SUBJECT' along with the average of 'Value' respective to 'LOCATION'.", "Display the number of records for each distinct 'SUBJECT'.", "Summarize the total 'Value' for every 'SUBJECT' under 'FREQUENCY' labeled as \"A\" in the year 2012.", "Select entries with location equal to \"AUS\" and measure equal to \"PC_GDP\".", "Find all rows where the \"Value\" is greater than the average value of the \"Value\" column.", "Identify the 'SUBJECT' corresponding to each 'LOCATION' where the 'VALUE' falls below the average 'VALUE' across all 'LOCATIONS'.", "Filter the data to keep only the records where the indicator is 'EDUEXP' and the frequency is 'A' for the year 2013.", "Retrieve all records that belong to the year 2013.", "Increase the 'Value' column by 10% for all rows with 'LOCATION' field equal to \"AUS\".", "Modify the column 'Flag code' to 'IsFlagged' and replace its missing values with False.", "Set the update indicator for all 'AUT' location instances to \"UPDATED_INDICATOR\".", "Rename the column 'SUBJECT' to 'TOPIC'."]}
{"csv_path": "infiagent/csv/47.csv", "questions": ["Remove any duplicate entries from the table, if present.", "Remove rows where the 'Agriculture' value is less than 10.", "Remove rows with 'English' value less than 50 or 'Foreign Languages' value less than 70.", "Plot a bar chart with 'Year' on x-axis and 'Business' on y-axis to depict the trend over years.", "Create a line graph that displays the trend of 'Foreign Languages' over time.", "Plot a pie-chart to show the proportions of 'Agriculture', 'Biology', and 'Engineering' in the year 1976.", "Add a new column 'Sciences' which is an average of 'Biology', 'Physical Sciences' and 'Math and Statistics' for each row.", "Create a new column named 'Biology_to_Physical_Sciences' that represents the proportion of 'Biology' to 'Physical Sciences' for each row in the table.", "Add a new column 'Total' which is the sum of all the fields excluding 'Year' for each row.", "Determine the highest and lowest values for 'Health Professions' spanning from 1970 to 1979.", "Compute the mean of 'Biology', 'Physical Sciences', and 'Engineering' for each distinct value of 'Year' by aggregating the data.", "Find the average 'Art and Performance' value for the years where 'Social Sciences and History' is above its overall average.", "Calculate the total sum of 'Agriculture' and 'Architecture' for all years combined.", "Determine the percentage variation in 'Math and Statistics' between 1970 and 1979.", "Retrieve rows where the 'Math and Statistics' value is greater than 40 and 'Physical Sciences' is less than 20.", "Fetch data for 1979, including the values of 'Agriculture', 'Biology' and 'Foreign Languages'.", "Change column 'Computer Science' format to percentage by multiplying each value in the column by 100.", "Update 'Health Professions' column by reducing every value by a factor of 10.", "If any cells in 'Math and Statistics' contain null values, substitute them with the median of the corresponding column.", "Rename the column 'Public Administration' to 'Pub Admin'."]}
{"csv_path": "infiagent/csv/48.csv", "questions": ["Remove any potential duplicate rows from the table.", "Delete the row with the highest ID value.", "Delete all the rows in which 'class1' and 'class2' are both 0.", "Delete all rows where the 'class1' value is either 1 or the 'class3' value is less than 0.2.", "Clear all rows with all column values equal to 0.", "Create a bar chart displaying the relationship between 'ID' and 'class7', and assign colors to the bars according to the values in 'class8'.", "Generate a histogram displaying the distribution of 'class4' separated by 'class2'.", "Create a pie chart to display the percentage of each category with non-zero entries.", "Introduce a new column 'Total' into the table that calculates the sum of all class values for each corresponding row.", "Create a new class, 'class10', that encompasses 'class1' and 'class2', and assign a value of 1 when both are identical and 0 otherwise.", "Compute the cumulative sum for each class column and label it as 'Total_class'.", "Compute the frequency counts for each unique value in the 'class5' column.", "Select all rows where 'class9' is not NULL or 'class9' is not 0.", "Fetch all the rows where 'class9' is 1 and 'class5' is 0.", "Identify all distinct IDs in the table.", "Identify any missing values in the table and set them to 0.", "Set the value of 'class8' to 0.5 for all rows where 'class8' is equal to 1.", "Update the values in 'class4' as follows: if the value is '0.0', change it to 'No'; if the value is '1.0', change it to 'Yes'.", "Change the value of 'class6' to 1 where ID is less than 5.", "Rename the column 'class_three'."]}
{"csv_path": "infiagent/csv/49.csv", "questions": ["Delete rows where there are missing values in any column.", "Delete all records where the \"Volume\" is less than 50,000.", "Draw a line graph tracking the change in \"Close\" price over time.", "Draw a bar graph showing the total \"Volume\" of each hour.", "Calculate the price range for each time interval by subtracting the \"Low\" from the \"High\" and add a new column named \"Price Range\" containing the results.", "Create a new column called \"Price Change\" by subtracting the \"Close\" price of the previous row from the \"Close\" price of the current row.", "Calculate the total trade amount of each interval by multiplying \"Close\" and \"Volume\", and name this column as \"Trade Amount\".", "Add a new column to indicate if the 'Close' value is an outlier or not, where an outlier is defined as a value that is greater than 2 standard deviations away from the mean.", "Compute the maximum and minimum of \"High\" and \"Low\" prices across the entire dataset, as well as their average.", "Group the data by the hour of the \"Time\", and calculate the total volume traded each hour.", "Compute the total sum of \"Volume\" for each distinct \"Close\" price value.", "Compute the variance between the 'High' and 'Low' columns for each row.", "Select all data where the \"Open\" price is higher than the \"Close\" price.", "Select all rows where the 'Time' column contains the word '59'.", "Choose records with a \"Close\" price greater than the average \"Close\" price.", "Query the data where the \"Volume\" is above the 75th percentile of the column's data.", "Deduct 0.5% from every entry in the \"Open\" column to emulate a market decline at the start of trading.", "Convert the \"Time\" column to a datetime format.", "Rename the column \"Volume\" to \"Trading Volume\".", "Set \"High\" to \"Close\" if \"High\" is less than \"Open\"."]}
{"csv_path": "infiagent/csv/50.csv", "questions": ["Eliminate any duplicate values in the 'city' column.", "Eliminate duplicate entries in the 'temp' column.", "Remove instances where the wind speed is below 2.5 and the day is '2015-07-24 16:39:38'.", "Eliminate rows with pressure values falling outside the normal atmospheric pressure range of 980 to 1030 hPa.", "Filter out all rows where the description contains the word 'rain'.", "Construct a pie chart illustrating the proportion of each 'description' within the data set.", "Plot a line graph to show the trend of humidity over time on '2015-07-24'.", "Create a bar graph with the x-axis representing the distinct 'day' values and the y-axis displaying the mean 'temp' values.", "Add a new column to the table which calculates the pressure to humidity ratio.", "Create a new column indicating the product of the 'temp' and 'humidity' values.", "Create a new column indicating whether the 'wind_speed' value is above the average wind speed.", "Compute the minimum, maximum, and average humidity levels for each day.", "Compute the mean wind speed for every distinct 'city' entry.", "Calculate the sum of 'dist' for each 'city' and display the city name along with the corresponding total distance.", "Identify the dates and times at which the temperature surpassed the mean temperature.", "Find the rows with the maximum and minimum temperatures for each day.", "Utilizing a subquery, identify the dates and temperatures for the days with the maximum wind speed.", "Modify the 'description' column to 'light rain' whenever the wind speed is lower than 3.", "Capitalize all city names in the 'city' column.", "Update the 'description' column to group the similar weather conditions into broader categories such as 'Rain', 'Cloud', 'Sun', etc."]}
{"csv_path": "infiagent/csv/16.csv", "questions": ["Exclude records with significant outliers in the 'cnt' column from the dataset.", "Delete rows from the data table where the month is 5 and the temperature is less than 0.2.", "Delete records with humidity greater than 80% on working days.", "Create a histogram by dividing the total number of users ('cnt') into 20 equal intervals to visualize its distribution.", "Create a pie chart that displays the percentage of 'weathersit' occurrences throughout the entire dataset.", "Create a line plot that shows the trend of 'atemp' for the first 10 instances.", "Create a new column named 'feelslike_temp' calculated as (0.70 * atemp) + (0.30 * hum).", "Add a new column that shows the difference between the number of 'registered' and 'casual' bike rentals.", "Create a 'temperature_difference' column that calculates the difference between temp and atemp.", "Please calculate the average 'atemp' value for each season.", "Count the number of records for each 'weathersit' category and sort them in descending order.", "Calculate the maximum, minimum, and average number of 'casual' users for each 'yr', 'mnth', and 'hr'.", "Divide the data into groups based on the season and then determine the average number of casual, registered, and total users for each season group.", "Please select all columns for the records where the yr is equal to 0.", "Using the table data, please return the records that occurred in the fourth quarter (month 10 - 12).", "Filter the records of registered users in the second half of 2011 and keep only the ones which are greater than the average of registered users in the first half of 2011.", "Find records where the total number of users ('cnt') is greater than 100 and the 'weathersit' is 1, and it's not a holiday.", "Convert the 'windspeed' in each record to percentage form by multiplying it by 100.", "Alter the format of the 'dteday' column from 'yyyy-mm-dd' to 'dd-mm-yyyy' for improved readability.", "Update the column title \"cnt\" to \"Total Users\"."]}
{"csv_path": "infiagent/csv/1.csv", "questions": ["Filter the table by rows where 'Total Annual Memberships Sold' is greater than or equal to 125000.", "Filter rows where the 'Date' is in the list of dates where the 'Miles traveled today' was less than or equal to 50000.", "Create a pie chart that represents the percentage of 'Annual Member Sign-Ups', '24-Hour Passes Purchased', and '7-Day Passes Purchased' for the final date in the table.", "Create a bar chart with 'Date' on the x-axis and 'Miles traveled today (midnight to 11:59 pm)' on the y-axis.", "Create a line chart displaying the progression of 'Miles traveled to date:' as it changes over time.", "Create a scatter plot depicting the relationship between 'Annual Member Sign-Ups' and 'Date'.", "Add a new column to display the cumulative total of 'Miles traveled to date:'.", "Create a new column called 'Weekly Total Passes' that calculates the sum of '7-Day Passes Purchased' for each week.", "Compute a new column 'Average Daily Mileage' by dividing 'Miles traveled today' by 'Trips over the past 24-hours'.", "Add a column to convert 'Date' into day of the week.", "Add a new column that calculates the length of the 'Trips over the past 24-hours' data.", "Aggregate the records by 'Date' and compute the sum of '7-Day Passes Purchased' for each date.", "Calculate the maximum and minimum number of '24-Hour Passes Purchased (midnight to 11:59 pm)' within the given dates.", "Examine and display the count of 'Trips within the last 24 hours' on 10/4/2014.", "Group the data by 'Year-Month' and provide the count of 'Daily Trips' for each group.", "Summarize the data by 'Date' and update 'Cumulative trips' to show the total of 'Daily Trips'.", "Change 'Miles traveled today' to miles per trip for each day where 'Trips over the past 24-hours' exceeded 30000.", "Display the day of the week (Monday, Tuesday, etc.) in the 'Date' column instead of the date.", "Rename the column 'Trips over the past 24-hours (midnight to 11:59pm)' to 'Daily Trips'.", "Change the title of column 'Cumulative trips (since launch):' to 'Total trips since launch'."]}
{"csv_path": "infiagent/csv/19.csv", "questions": ["Delete all data belonging to the 'Community Services' department group.", "Remove rows where 'budget_year_end' is earlier than 'budget_year_start'.", "Delete the rows where the 'coa_dept_id' is even.", "Create a pie chart illustrating the proportion of departments within each 'dept_group'.", "Draw a histogram to compare the frequency of 'dept_group' across the department.", "Create a line graph that shows the trend of department numbers for each 'dept_group' between the years 2016 and 2017.", "Add a new column 'github_dept_id' by concatenating the prefix 'G-' with the 'coa_dept_id'.", "Introduce a new column 'department_size' that categorizes departments as 'Small', 'Medium', or 'Large' in accordance with their 'coa_dept_id' values.", "Calculate the total number of distinct 'coa_dept_id' values for each 'dept_group'.", "Display the department names, grouped by 'dept_group'.", "Find the 'Department Name' and 'budget_year_start' for the departments where 'coa_dept_id' is greater than the average 'coa_dept_id'.", "Choose department names where 'budget_year_start' is within the last 12 months.", "Filter out 'Department Name' and 'coa_dept_id' columns for all departments with 'dept_group' beginning with 'U'.", "Display the departments associated with the 'Utility and Other Enterprises' dept_group.", "Update values in the 'dept_group' column to 'Undocumented' if they are NULL or blank.", "Increase the 'coa_dept_id' of all departments in the 'Community Services' dept_group by 10%.", "Convert the 'budget_year_start' and 'budget_year_end' column formats to yyyy-mm-dd.", "Modify the value of 'github-dept-code' column to 'ENG' for rows where 'Department Name' is 'Austin Energy'.", "Update the 'dept_group' column to 'Services' for rows where the 'department name' is 'Animal Services' or 'Austin Code'.", "Update the column title 'Dept Group' to 'Department Group'."]}
{"csv_path": "infiagent/csv/41.csv", "questions": ["Remove the row when the basement square footage (sqft_basement) is equal to zero.", "Remove rows with 'waterfront' value equal to '0'.", "Filter the table to exclude rows where the 'zipcode' column matches any value in a subquery that retrieves the distinct 'zipcode' values from another table.", "Delete records where the 'long' value is less than -125 or greater than -120.", "Draw a chart to compare the frequency of properties based on their 'condition' rating.", "Draw a bar graph listing the average prices of properties, grouped by the number of bedrooms.", "Draw a scatter plot of 'price' against 'sqft_living' to examine the relationship between them.", "Create a new column 'price_per_sqft' by dividing the 'price' column by the 'sqft_living' column.", "Create a new column named 'age' in the table, which represents the age of a building by subtracting the value in the 'yr_built' column from the current year, 2022.", "Create a new column converting the 'lat' and 'long' values into a single formatted coordinate string.", "Insert a new column that classifies properties into categories according to their year built (yr_built): 'Vintage' for properties built before 1900, 'Old' for properties built between 1900 and 1970, and 'Modern' for properties built after 1970.", "Group the houses by the number of 'floors' and calculate the average price in each group.", "Calculate the mean 'sqft_living' for properties having over 2 bathrooms.", "Calculate the median 'price' for each 'zipcode' group.", "Calculate the median price for properties having 3 bedrooms.", "Select all properties where the number of bedrooms is greater than 3.", "Retrieve all records where the 'price' is greater than 500000.0 and the 'bathrooms' are more than 2.", "Calculate the sum of the 'sqft_living' and 'sqft_lot' for each property and then find the record with the maximum sum.", "Round the 'price' values to two decimal places in the table.", "Change the display format of 'lat' and 'long' columns by adding a '?' sign at the end to indicate that they are geographical coordinates."]}
